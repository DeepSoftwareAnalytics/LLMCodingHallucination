{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom .time import Time\n\treturn Time.from_nanoseconds(nanoseconds, tz)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\ttime = datetime.fromtimestamp(nanoseconds/1000000000)\n\tif tz:\n\t\ttime = time.astimezone(timezone(tz))\n\treturn time.strftime('%Y-%m-%d %H:%M:%S')", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tpass\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tdformat = '%Y-%m-%d %H:%M:%S.%f'\n\n\tif tz:\n\t\tdformat += ' %Z'\n\n\treturn datetime.fromtimestamp(nanoseconds / 1e9).strftime(dformat)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import time, timezone\n\tfrom math import floor\n\tfrom time import gmtime\n\n\t# Convert nanoseconds to seconds\n\tseconds = nanoseconds // 1e9\n\n\t# Convert seconds to time\n\ttime_seconds = seconds % 60\n\ttime_minutes = floor(seconds / 60) % 60\n\ttime_hours = floor(seconds / 60 / 60)\n\ttime_object = time(time_hours, time_minutes, time_seconds)\n\n\t# Convert time to timezone\n\tif tz is None:\n\t\ttime_object = time_object.replace(tzinfo=timezone.utc)\n\telse:\n\t\ttime_object = time_object.replace(tzinfo=timezone(seconds=tz))\n\n\treturn time_object\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom datetime import timezone\n\n\tif tz is None:\n\t\ttz = timezone.utc\n\n\tdt = datetime.fromtimestamp(nanoseconds / 1e9, tz)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\timport time\n\n\tformatted_time = datetime.fromtimestamp(nanoseconds / float(1e9), tz = timezone(tz) if tz is not None else None)\n\n\treturn formatted_time\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom time import gmtime, strftime\n\n\ttime = datetime.utcfromtimestamp(nanoseconds / 1000000000.0)\n\n\tif tz:\n\t\ttime = time.replace(tzinfo=tz)\n\n\treturn time\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom time import gmtime, strftime\n\tif tz is None:\n\t\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime(nanoseconds / 1e9))\n\telse:\n\t\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime(nanoseconds / 1e9)) + \" \" + tz", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport datetime\n\timport pytz\n\tif tz is None:\n\t\ttz = pytz.utc\n\tdt = datetime.datetime.fromtimestamp(float(nanoseconds) / 10 ** 9, tz=tz)\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom .time import Time\n\treturn Time.from_nanoseconds(nanoseconds, tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\ttime = datetime.fromtimestamp(nanoseconds/1000000000)\n\tif tz:\n\t\ttime = time.astimezone(timezone(tz))\n\treturn time.strftime('%Y-%m-%d %H:%M:%S')", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tdformat = '%Y-%m-%d %H:%M:%S.%f'\n\n\tif tz:\n\t\tdformat += ' %Z'\n\n\treturn datetime.fromtimestamp(nanoseconds / 1e9).strftime(dformat)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import time, timezone\n\tfrom math import floor\n\tfrom time import gmtime\n\n\t# Convert nanoseconds to seconds\n\tseconds = nanoseconds // 1e9\n\n\t# Convert seconds to time\n\ttime_seconds = seconds % 60\n\ttime_minutes = floor(seconds / 60) % 60\n\ttime_hours = floor(seconds / 60 / 60)\n\ttime_object = time(time_hours, time_minutes, time_seconds)\n\n\t# Convert time to timezone\n\tif tz is None:\n\t\ttime_object = time_object.replace(tzinfo=timezone.utc)\n\telse:\n\t\ttime_object = time_object.replace(tzinfo=timezone(seconds=tz))\n\n\treturn time_object\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom datetime import timezone\n\n\tif tz is None:\n\t\ttz = timezone.utc\n\n\tdt = datetime.fromtimestamp(nanoseconds / 1e9, tz)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\timport time\n\n\tformatted_time = datetime.fromtimestamp(nanoseconds / float(1e9), tz = timezone(tz) if tz is not None else None)\n\n\treturn formatted_time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom time import gmtime, strftime\n\n\ttime = datetime.utcfromtimestamp(nanoseconds / 1000000000.0)\n\n\tif tz:\n\t\ttime = time.replace(tzinfo=tz)\n\n\treturn time\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom time import gmtime, strftime\n\tif tz is None:\n\t\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime(nanoseconds / 1e9))\n\telse:\n\t\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime(nanoseconds / 1e9)) + \" \" + tz", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport datetime\n\timport pytz\n\tif tz is None:\n\t\ttz = pytz.utc\n\tdt = datetime.datetime.fromtimestamp(float(nanoseconds) / 10 ** 9, tz=tz)\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\treturn Structure(\n\t\ttimedelta=value,\n\t\ttimedelta_class=timedelta,\n\t)\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tvalue = value.total_seconds()\n\tseconds = value % 60\n\tseconds = int(seconds)\n\tdays = value // 86400\n\tdays = int(days)\n\tvalue = value - days*86400\n\thours = value // 3600\n\thours = int(hours)\n\tvalue = value - hours*3600\n\tminutes = value // 60\n\tminutes = int(minutes)\n\t\n\treturn {\n\t\t\"days\": days,\n\t\t\"hours\": hours,\n\t\t\"minutes\": minutes,\n\t\t\"seconds\": seconds,\n\t}", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\n\tdays = value.days\n\tsecs = value.seconds\n\tus = value.microseconds\n\ttd_info = (days, secs, us)\n\ttimedelta_class = timedelta(*td_info)\n\treturn timedelta_class", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\n\ttd = timedelta(**value)\n\n\treturn {\n\t\t\"days\": td.days,\n\t\t\"seconds\": td.seconds,\n\t\t\"microseconds\": td.microseconds,\n\t}\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tif isinstance(value, timedelta):\n\t\treturn str(value)\n\traise TypeError('Expected timedelta.')\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tif not isinstance(value, timedelta):\n\t\traise ValueError('value must be a timedelta instance.')\n\tfrom .structures import Structure\n\treturn Structure(hours=value.days*24+value.seconds//3600,\n\t\tminutes=(value.seconds//60) % 60,\n\t\tseconds=value.seconds % 60,\n\t\tmilliseconds=value.microseconds // 1000)\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom .. import Structure\n\treturn Structure(\n\t\tdays=value.days,\n\t\tseconds=value.seconds,\n\t\tmicroseconds=value.microseconds,\n\t)\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom django.utils.timezone import timedelta\n\tfrom django.utils.dateparse import parse_duration\n\n\tif isinstance(value, timedelta):\n\t\tdelta = value\n\telif isinstance(value, str):\n\t\tdelta = parse_duration(value)\n\telse:\n\t\traise TypeError(\"Value must either be a timedelta or a string\")\n\n\treturn {\n\t\t\"days\": delta.days,\n\t\t\"seconds\": delta.seconds,\n\t\t\"microseconds\": delta.microseconds,\n\t}\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttimedelta_fields = [\n\t\t'days',\n\t\t'seconds',\n\t\t'microseconds',\n\t]\n\n\tdata = dict()\n\n\tfor field in timedelta_fields:\n\t\tdata[field] = getattr(value, field)\n\n\treturn data\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tassert isinstance(value, timedelta)\n\tseconds = value.seconds\n\tdays = value.days\n\thours, remainder = divmod(seconds, 3600)\n\tminutes, seconds = divmod(remainder, 60)\n\treturn dict(days=days, hours=hours, minutes=minutes, seconds=seconds)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\treturn Structure(\n\t\ttimedelta=value,\n\t\ttimedelta_class=timedelta,\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tvalue = value.total_seconds()\n\tseconds = value % 60\n\tseconds = int(seconds)\n\tdays = value // 86400\n\tdays = int(days)\n\tvalue = value - days*86400\n\thours = value // 3600\n\thours = int(hours)\n\tvalue = value - hours*3600\n\tminutes = value // 60\n\tminutes = int(minutes)\n\t\n\treturn {\n\t\t\"days\": days,\n\t\t\"hours\": hours,\n\t\t\"minutes\": minutes,\n\t\t\"seconds\": seconds,\n\t}", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\n\tdays = value.days\n\tsecs = value.seconds\n\tus = value.microseconds\n\ttd_info = (days, secs, us)\n\ttimedelta_class = timedelta(*td_info)\n\treturn timedelta_class", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\n\ttd = timedelta(**value)\n\n\treturn {\n\t\t\"days\": td.days,\n\t\t\"seconds\": td.seconds,\n\t\t\"microseconds\": td.microseconds,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tif isinstance(value, timedelta):\n\t\treturn str(value)\n\traise TypeError('Expected timedelta.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tif not isinstance(value, timedelta):\n\t\traise ValueError('value must be a timedelta instance.')\n\tfrom .structures import Structure\n\treturn Structure(hours=value.days*24+value.seconds//3600,\n\t\tminutes=(value.seconds//60) % 60,\n\t\tseconds=value.seconds % 60,\n\t\tmilliseconds=value.microseconds // 1000)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom .. import Structure\n\treturn Structure(\n\t\tdays=value.days,\n\t\tseconds=value.seconds,\n\t\tmicroseconds=value.microseconds,\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom django.utils.timezone import timedelta\n\tfrom django.utils.dateparse import parse_duration\n\n\tif isinstance(value, timedelta):\n\t\tdelta = value\n\telif isinstance(value, str):\n\t\tdelta = parse_duration(value)\n\telse:\n\t\traise TypeError(\"Value must either be a timedelta or a string\")\n\n\treturn {\n\t\t\"days\": delta.days,\n\t\t\"seconds\": delta.seconds,\n\t\t\"microseconds\": delta.microseconds,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttimedelta_fields = [\n\t\t'days',\n\t\t'seconds',\n\t\t'microseconds',\n\t]\n\n\tdata = dict()\n\n\tfor field in timedelta_fields:\n\t\tdata[field] = getattr(value, field)\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tassert isinstance(value, timedelta)\n\tseconds = value.seconds\n\tdays = value.days\n\thours, remainder = divmod(seconds, 3600)\n\tminutes, seconds = divmod(remainder, 60)\n\treturn dict(days=days, hours=hours, minutes=minutes, seconds=seconds)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tif isinstance(value, (datetime, timedelta)):\n\t\treturn value.isoformat()\n\traise TypeError(f'{value} is not JSON serializable')", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tfrom dateutil.parser import parse\n\tfrom dateutil.tz import tzutc\n\timport time\n\n\ttry:\n\t\tvalue = parse(value)\n\texcept (TypeError, ValueError):\n\t\tvalue = value\n\n\tif not isinstance(value, timedelta):\n\t\tvalue = (value - tzutc(value).utcoffset()) - time.timezone\n\n\treturn value\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tfrom dateutil.tz import tzlocal\n\n\tlocal_now = datetime.now()\n\tlocal_date = local_now.date()\n\n\tlocal_tz = tzlocal()\n\ttimezone_delta = local_tz.utcoffset(local_now)\n\n\ttime = datetime.strptime(value, '%H:%M:%S')\n\ttime = time.replace(year=local_date.year, month=local_date.month, day=local_date.day)\n\ttime = time + timezone_delta\n\ttime = time.astimezone(tzlocal())\n\ttime = time.time()\n\n\treturn time", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import localtime\n\treturn dict(\n\t\thour = value.tm_hour,\n\t\tmin = value.tm_min,\n\t\tsec = value.tm_sec,\n\t\twday = value.tm_wday,\n\t\tyday = value.tm_yday,\n\t\tday = value.tm_mday,\n\t\tmon = value.tm_mon,\n\t\tyear = value.tm_year,\n\t\tdst = value.tm_isdst\n\t)\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import gmtime\n\tfrom time import localtime\n\tfrom time import strftime\n\tfrom time import strptime\n\n\tfrom time import strftime\n\tformat = '%Y-%m-%d %H:%M:%S'\n\tvalue = strftime(format, gmtime(value))\n\treturn value\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import gmtime, strftime\n\treturn strftime('%Y-%m-%d %H:%M:%S', gmtime(value.ticks))", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import tzinfo, timedelta\n\timport struct\n\n\tdef utc_from_ticks(ticks):\n\t\t\"\"\"\n\t\tConvert ticks to UTC.\n\t\t\"\"\"\n\t\tticks_per_second = 10000000\n\t\tseconds = ticks / ticks_per_second\n\t\tmicroseconds = (ticks - seconds * ticks_per_second) * 100\n\t\tutc = struct.pack('>Q', seconds * 1000000 + microseconds)\n\t\treturn utc\n\n\tdef ticks_from_utc(utc):\n\t\t\"\"\"\n\t\tConvert UTC to ticks.\n\t\t\"\"\"\n\t\tseconds, microseconds = struct.unpack('>Q', utc)\n\t\tticks = seconds * 10000000 + microseconds / 100\n\t\treturn ticks\n\n\tclass UTC(tzinfo):\n\t\t\"\"\"\n\t\tImplementation of the UTC timezone.\n\t\t\"\"\"\n\t\tdef __init__(self, ticks):\n\t\t\tself.utc = utc_from_ticks(ticks)\n\t\t\tself.offset = timedelta(hours=0)\n\t\t\tself.dst = timedelta", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tdef dehydrate_time(value):\n\t\treturn value.timetuple()\n\treturn dehydrate_time(value)", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import gmtime, strftime\n\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime(value))\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom struct import Struct\n\n\td = datetime.fromtimestamp(value / 1000000)\n\ttime = Struct(f'<HHHHHH').pack(d.hour, d.minute, d.second, d.microsecond//10000, d.microsecond % 10000, d.microsecond % 10000)\n\treturn time\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tif isinstance(value, (datetime, timedelta)):\n\t\treturn value.isoformat()\n\traise TypeError(f'{value} is not JSON serializable')", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tfrom dateutil.parser import parse\n\tfrom dateutil.tz import tzutc\n\timport time\n\n\ttry:\n\t\tvalue = parse(value)\n\texcept (TypeError, ValueError):\n\t\tvalue = value\n\n\tif not isinstance(value, timedelta):\n\t\tvalue = (value - tzutc(value).utcoffset()) - time.timezone\n\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tfrom dateutil.tz import tzlocal\n\n\tlocal_now = datetime.now()\n\tlocal_date = local_now.date()\n\n\tlocal_tz = tzlocal()\n\ttimezone_delta = local_tz.utcoffset(local_now)\n\n\ttime = datetime.strptime(value, '%H:%M:%S')\n\ttime = time.replace(year=local_date.year, month=local_date.month, day=local_date.day)\n\ttime = time + timezone_delta\n\ttime = time.astimezone(tzlocal())\n\ttime = time.time()\n\n\treturn time", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import localtime\n\treturn dict(\n\t\thour = value.tm_hour,\n\t\tmin = value.tm_min,\n\t\tsec = value.tm_sec,\n\t\twday = value.tm_wday,\n\t\tyday = value.tm_yday,\n\t\tday = value.tm_mday,\n\t\tmon = value.tm_mon,\n\t\tyear = value.tm_year,\n\t\tdst = value.tm_isdst\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import gmtime\n\tfrom time import localtime\n\tfrom time import strftime\n\tfrom time import strptime\n\n\tfrom time import strftime\n\tformat = '%Y-%m-%d %H:%M:%S'\n\tvalue = strftime(format, gmtime(value))\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import gmtime, strftime\n\treturn strftime('%Y-%m-%d %H:%M:%S', gmtime(value.ticks))", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import tzinfo, timedelta\n\timport struct\n\n\tdef utc_from_ticks(ticks):\n\t\t\"\"\"\n\t\tConvert ticks to UTC.\n\t\t\"\"\"\n\t\tticks_per_second = 10000000\n\t\tseconds = ticks / ticks_per_second\n\t\tmicroseconds = (ticks - seconds * ticks_per_second) * 100\n\t\tutc = struct.pack('>Q', seconds * 1000000 + microseconds)\n\t\treturn utc\n\n\tdef ticks_from_utc(utc):\n\t\t\"\"\"\n\t\tConvert UTC to ticks.\n\t\t\"\"\"\n\t\tseconds, microseconds = struct.unpack('>Q', utc)\n\t\tticks = seconds * 10000000 + microseconds / 100\n\t\treturn ticks\n\n\tclass UTC(tzinfo):\n\t\t\"\"\"\n\t\tImplementation of the UTC timezone.\n\t\t\"\"\"\n\t\tdef __init__(self, ticks):\n\t\t\tself.utc = utc_from_ticks(ticks)\n\t\t\tself.offset = timedelta(hours=0)\n\t\t\tself.dst = timedelta", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tdef dehydrate_time(value):\n\t\treturn value.timetuple()\n\treturn dehydrate_time(value)", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import gmtime, strftime\n\treturn strftime(\"%Y-%m-%d %H:%M:%S\", gmtime(value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom struct import Struct\n\n\td = datetime.fromtimestamp(value / 1000000)\n\ttime = Struct(f'<HHHHHH').pack(d.hour, d.minute, d.second, d.microsecond//10000, d.microsecond % 10000, d.microsecond % 10000)\n\treturn time\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tresult = []\n\tnum_bytes = len(value)\n\tresult.append(struct.pack('B', num_bytes))\n\tresult.append(value)\n\treturn struct.pack(str(num_bytes)+'s', *result)\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue_len = len(value)\n\tif value_len == 2:\n\t\treturn Point(value[0], value[1])\n\telif value_len == 4:\n\t\treturn Point3(value[0], value[1], value[2])\n\traise Exception('The value length must be 2 or 4')\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tfrom . import Point\n\treturn Point(value).to_bytes()\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tvalue = value.replace(' ', '')\n\t\n\ttry:\n\t\tx, y, z = value.split(',')\n\texcept ValueError:\n\t\tx, y = value.split(',')\n\t\tz = 0\n\t\n\treturn 'POINT (%s %s %s)' % (x, y, z)\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\ttry:\n\t\tpoint_structure = value[0]\n\texcept IndexError:\n\t\t# Value is empty.\n\t\treturn None\n\t\n\tif len(value) == 1:\n\t\t# This is a single point.\n\t\tpoint_structure.x = value[0]\n\t\tpoint_structure.y = value[0]\n\t\t\n\t\treturn point_structure\n\t\n\telif len(value) == 2:\n\t\t# This is a single point.\n\t\tpoint_structure.x = value[0]\n\t\tpoint_structure.y = value[1]\n\t\t\n\t\treturn point_structure\n\t\n\telif len(value) == 4:\n\t\t# This is a rectangle.\n\t\tpoint_structure.x = value[0]\n\t\tpoint_structure.y = value[1]\n\t\t\n\t\tpoint_structure.width = value[2]\n\t\tpoint_structure.height = value[3]\n\t\t\n\t\treturn point_structure\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue = value.split()\n\treturn value[0], value[1], value[2]\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue_length = len(value)\n\tif value_length == 4:\n\t\treturn \"<Point x='{}' y='{}' z='{}' w='{}'/>\".format(*value)\n\telif value_length == 3:\n\t\treturn \"<Point x='{}' y='{}' z='{}'/>\".format(*value)\n\telif value_length == 2:\n\t\treturn \"<Point x='{}' y='{}'/>\".format(*value)\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tstructure = [\n\t\t('x', '>f8'),\n\t\t('y', '>f8'),\n\t\t('z', '>f8'),\n\t]\n\n\tdef __init__(self, x, y, z):\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.z = z\n\n\treturn structure\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tpoints = []\n\twhile value != 0:\n\t\tpoints.append(value % 10)\n\t\tvalue = value / 10\n\treturn points\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\timport struct\n\tfrom ...types import Point\n\t\n\t# check length\n\tvalue_len = len(value)\n\tif value_len != 12:\n\t\traise Exception('value length must be 12 bytes.')\n\t\n\t# unpack\n\tx, y, z = struct.unpack('<3d', value)\n\t\n\t# return point\n\treturn Point(x, y, z)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tresult = []\n\tnum_bytes = len(value)\n\tresult.append(struct.pack('B', num_bytes))\n\tresult.append(value)\n\treturn struct.pack(str(num_bytes)+'s', *result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue_len = len(value)\n\tif value_len == 2:\n\t\treturn Point(value[0], value[1])\n\telif value_len == 4:\n\t\treturn Point3(value[0], value[1], value[2])\n\traise Exception('The value length must be 2 or 4')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tfrom . import Point\n\treturn Point(value).to_bytes()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tvalue = value.replace(' ', '')\n\t\n\ttry:\n\t\tx, y, z = value.split(',')\n\texcept ValueError:\n\t\tx, y = value.split(',')\n\t\tz = 0\n\t\n\treturn 'POINT (%s %s %s)' % (x, y, z)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\ttry:\n\t\tpoint_structure = value[0]\n\texcept IndexError:\n\t\t# Value is empty.\n\t\treturn None\n\t\n\tif len(value) == 1:\n\t\t# This is a single point.\n\t\tpoint_structure.x = value[0]\n\t\tpoint_structure.y = value[0]\n\t\t\n\t\treturn point_structure\n\t\n\telif len(value) == 2:\n\t\t# This is a single point.\n\t\tpoint_structure.x = value[0]\n\t\tpoint_structure.y = value[1]\n\t\t\n\t\treturn point_structure\n\t\n\telif len(value) == 4:\n\t\t# This is a rectangle.\n\t\tpoint_structure.x = value[0]\n\t\tpoint_structure.y = value[1]\n\t\t\n\t\tpoint_structure.width = value[2]\n\t\tpoint_structure.height = value[3]\n\t\t\n\t\treturn point_structure\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue = value.split()\n\treturn value[0], value[1], value[2]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue_length = len(value)\n\tif value_length == 4:\n\t\treturn \"<Point x='{}' y='{}' z='{}' w='{}'/>\".format(*value)\n\telif value_length == 3:\n\t\treturn \"<Point x='{}' y='{}' z='{}'/>\".format(*value)\n\telif value_length == 2:\n\t\treturn \"<Point x='{}' y='{}'/>\".format(*value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tstructure = [\n\t\t('x', '>f8'),\n\t\t('y', '>f8'),\n\t\t('z', '>f8'),\n\t]\n\n\tdef __init__(self, x, y, z):\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.z = z\n\n\treturn structure\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tpoints = []\n\twhile value != 0:\n\t\tpoints.append(value % 10)\n\t\tvalue = value / 10\n\treturn points\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\timport struct\n\tfrom ...types import Point\n\t\n\t# check length\n\tvalue_len = len(value)\n\tif value_len != 12:\n\t\traise Exception('value length must be 12 bytes.')\n\t\n\t# unpack\n\tx, y, z = struct.unpack('<3d', value)\n\t\n\t# return point\n\treturn Point(x, y, z)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\n\treturn [i for i in dir(self) if not i.startswith('__') and i != '__keys']\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\tfor key in self.__dict__:\n\t\tkeys.append(key)\n\n\treturn keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\ttry:\n\t\treturn list(self.__keys)\n\texcept AttributeError:\n\t\treturn None", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeyList = []\n\tfor key in dir(self):\n\t\tif (key[0] != '_'):\n\t\t\tkeyList.append(key)\n\treturn keyList\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tpass\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkey_list = []\n\tfor key in self.__keys:\n\t\tkey_list.append(key)\n\treturn key_list\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\ttry:\n\t\treturn [k for k in self.__dict__.keys()]\n\texcept:\n\t\traise TypeError(\"keys() expects a class as input\")", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\ttry:\n\t\treturn self.__keys\n\texcept AttributeError:\n\t\treturn []", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\n\treturn list(self.__keys)\n\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tdata = getattr(self, '__keys', [])\n\tif not isinstance(data, list): data = [data]\n\treturn data\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\n\treturn [i for i in dir(self) if not i.startswith('__') and i != '__keys']\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\tfor key in self.__dict__:\n\t\tkeys.append(key)\n\n\treturn keys\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\ttry:\n\t\treturn list(self.__keys)\n\texcept AttributeError:\n\t\treturn None", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeyList = []\n\tfor key in dir(self):\n\t\tif (key[0] != '_'):\n\t\t\tkeyList.append(key)\n\treturn keyList\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkey_list = []\n\tfor key in self.__keys:\n\t\tkey_list.append(key)\n\treturn key_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\ttry:\n\t\treturn [k for k in self.__dict__.keys()]\n\texcept:\n\t\traise TypeError(\"keys() expects a class as input\")", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\ttry:\n\t\treturn self.__keys\n\texcept AttributeError:\n\t\treturn []", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\n\treturn list(self.__keys)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tdata = getattr(self, '__keys', [])\n\tif not isinstance(data, list): data = [data]\n\treturn data\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or 3\n\tif protocol_version == 3:\n\t\tfrom .protocol_v3 import Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure, Helix\n\t\treturn Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure, Helix\n\telif protocol_version == 2:\n\t\tfrom .protocol_v2 import Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\t\treturn Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\telif protocol_version == 1:\n\t\tfrom .protocol_v1 import Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\t\treturn Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\telse:\n\t\traise Exception(\"Unsupported protocol version: %s\" % protocol_version)", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandlers = {}\n\n\thandlers[0] = cls.handler_0\n\n\tif protocol_version == 1:\n\t\thandlers[1] = cls.handler_1\n\n\treturn handlers", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tpass\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or cls.protocol_version\n\n\tif protocol_version >= 4:\n\t\tfrom .protocol_v4 import BoltProtocolV4\n\t\treturn BoltProtocolV4\n\telif protocol_version == 3:\n\t\tfrom .protocol_v3 import BoltProtocolV3\n\t\treturn BoltProtocolV3\n\telif protocol_version == 2:\n\t\tfrom .protocol_v2 import BoltProtocolV2\n\t\treturn BoltProtocolV2\n\telif protocol_version == 1:\n\t\tfrom .protocol_v1 import BoltProtocolV1\n\t\treturn BoltProtocolV1\n\telif protocol_version == 0:\n\t\tfrom .protocol_v0 import BoltProtocolV0\n\t\treturn BoltProtocolV0\n\telse:\n\t\traise ValueError(\"Unsupported protocol version: %i\" % protocol_version)", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandlers = {}\n\tif protocol_version == 1:\n\t\thandlers = {\n\t\t\t'RUN': cls.run_v1,\n\t\t\t'SUCCESS': cls.success_v1,\n\t\t\t'FAILURE': cls.failure_v1,\n\t\t\t'RECORD': cls.record_v1,\n\t\t\t'IGNORABLE_ERROR': cls.ignorable_error_v1,\n\t\t\t'HANDSHAKE': cls.handshake_v1,\n\t\t\t'HANDSHAKE_ACK': cls.handshake_ack_v1,\n\t\t}\n\telif protocol_version == 2:\n\t\thandlers = {\n\t\t\t'RUN': cls.run_v2,\n\t\t\t'SUCCESS': cls.success_v2,\n\t\t\t'FAILURE': cls.failure_v2,\n\t\t\t'RECORD': cls.record_v2,\n\t\t\t'IGNORABLE_ERROR': cls.ignorable_error_v2,\n\t\t\t'HANDSHAKE': cls.handshake_v2,\n\t\t\t'HANDSHAKE_ACK': cls.handshake_ack_v2,\n\t\t}\n\telif protocol_version == 3:\n\t\thandlers =", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or cls.protocol_version\n\tif protocol_version == 0:\n\t\tfrom .protocol_v0 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telif protocol_version == 1:\n\t\tfrom .protocol_v1 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telif protocol_version == 2:\n\t\tfrom .protocol_v2 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telif protocol_version == 3:\n\t\tfrom .protocol_v3 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telse:\n\t\traise ValueError(\"Unsupported protocol version: %d\" % protocol_version)", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\ttry:\n\t\tfrom .v11 import Handler as HandlerV11\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v12 import Handler as HandlerV12\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v13 import Handler as HandlerV13\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v14 import Handler as HandlerV14\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v15 import Handler as HandlerV15\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v44 import Handler as HandlerV44\n\texcept ImportError:\n\t\tpass\n\n\tif protocol_version == 11:\n\t\treturn HandlerV11\n\telif protocol_version == 12:\n\t\treturn HandlerV12\n\telif protocol_version == 13:\n\t\treturn HandlerV13\n\telif protocol_version == 14:\n\t\treturn HandlerV14\n\telif protocol_version == 15:\n\t\treturn HandlerV15\n\telif protocol_version == 44:\n\t\treturn HandlerV", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef _handlers(cls, protocol_version=None):\n\t\tif protocol_version == None:\n\t\t\tprotocol_version = cls.protocol_version\n\n\t\tif protocol_version == 1:\n\t\t\tfrom .v1 import Handshake, Message\n\t\t\tfrom .v1 import BoltConnector\n\n\t\t\treturn (Handshake, Message, BoltConnector)\n\t\telse:\n\t\t\traise NotImplementedError('Protocol version %d not implemented' % protocol_version)\n\n\treturn _handlers", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or cls.protocol_version\n\tif protocol_version == cls.V1:\n\t\tfrom .v1 import Handshake, Message\n\telif protocol_version == cls.V2:\n\t\tfrom .v2 import Handshake, Message\n\telif protocol_version == cls.V3:\n\t\tfrom .v3 import Handshake, Message\n\telif protocol_version == cls.V4:\n\t\tfrom .v4 import Handshake, Message\n\telif protocol_version == cls.V5:\n\t\tfrom .v5 import Handshake, Message\n\telse:\n\t\traise ValueError('Unknown protocol version: {}'.format(protocol_version))\n\n\treturn Handshake, Message", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or 3\n\n\tif protocol_version == 4:\n\t\tfrom py2neo.packages.httpstream.v4_1 import BoltProtocolV41\n\n\t\treturn {\n\t\t\tBoltProtocolV41.MAGIC_PREAMBLE: BoltProtocolV41,\n\t\t\tBoltProtocolV41.MAGIC_PREAMBLE_LEGACY: BoltProtocolV41,\n\t\t}\n\telse:\n\t\tfrom py2neo.packages.httpstream.v1 import BoltProtocolV1\n\t\tfrom py2neo.packages.httpstream.v2 import BoltProtocolV2\n\t\tfrom py2neo.packages.httpstream.v3 import BoltProtocolV3\n\n\t\treturn {\n\t\t\tBoltProtocolV1.MAGIC_PREAMBLE: BoltProtocolV1,\n\t\t\tBoltProtocolV2.MAGIC_PREAMBLE: BoltProtocolV2,\n\t\t\tBoltProtocolV3.MAGIC_PREAMBLE: BoltProtocolV3,\n\t\t}\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or 3\n\tif protocol_version == 3:\n\t\tfrom .protocol_v3 import Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure, Helix\n\t\treturn Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure, Helix\n\telif protocol_version == 2:\n\t\tfrom .protocol_v2 import Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\t\treturn Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\telif protocol_version == 1:\n\t\tfrom .protocol_v1 import Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\t\treturn Handshake, Init, Success, Failure, Reset, Discard, Ping, AckFailure\n\telse:\n\t\traise Exception(\"Unsupported protocol version: %s\" % protocol_version)", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandlers = {}\n\n\thandlers[0] = cls.handler_0\n\n\tif protocol_version == 1:\n\t\thandlers[1] = cls.handler_1\n\n\treturn handlers", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or cls.protocol_version\n\n\tif protocol_version >= 4:\n\t\tfrom .protocol_v4 import BoltProtocolV4\n\t\treturn BoltProtocolV4\n\telif protocol_version == 3:\n\t\tfrom .protocol_v3 import BoltProtocolV3\n\t\treturn BoltProtocolV3\n\telif protocol_version == 2:\n\t\tfrom .protocol_v2 import BoltProtocolV2\n\t\treturn BoltProtocolV2\n\telif protocol_version == 1:\n\t\tfrom .protocol_v1 import BoltProtocolV1\n\t\treturn BoltProtocolV1\n\telif protocol_version == 0:\n\t\tfrom .protocol_v0 import BoltProtocolV0\n\t\treturn BoltProtocolV0\n\telse:\n\t\traise ValueError(\"Unsupported protocol version: %i\" % protocol_version)", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandlers = {}\n\tif protocol_version == 1:\n\t\thandlers = {\n\t\t\t'RUN': cls.run_v1,\n\t\t\t'SUCCESS': cls.success_v1,\n\t\t\t'FAILURE': cls.failure_v1,\n\t\t\t'RECORD': cls.record_v1,\n\t\t\t'IGNORABLE_ERROR': cls.ignorable_error_v1,\n\t\t\t'HANDSHAKE': cls.handshake_v1,\n\t\t\t'HANDSHAKE_ACK': cls.handshake_ack_v1,\n\t\t}\n\telif protocol_version == 2:\n\t\thandlers = {\n\t\t\t'RUN': cls.run_v2,\n\t\t\t'SUCCESS': cls.success_v2,\n\t\t\t'FAILURE': cls.failure_v2,\n\t\t\t'RECORD': cls.record_v2,\n\t\t\t'IGNORABLE_ERROR': cls.ignorable_error_v2,\n\t\t\t'HANDSHAKE': cls.handshake_v2,\n\t\t\t'HANDSHAKE_ACK': cls.handshake_ack_v2,\n\t\t}\n\telif protocol_version == 3:\n\t\thandlers =", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or cls.protocol_version\n\tif protocol_version == 0:\n\t\tfrom .protocol_v0 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telif protocol_version == 1:\n\t\tfrom .protocol_v1 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telif protocol_version == 2:\n\t\tfrom .protocol_v2 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telif protocol_version == 3:\n\t\tfrom .protocol_v3 import RequestHandler, ResponseHandler\n\t\treturn RequestHandler, ResponseHandler\n\telse:\n\t\traise ValueError(\"Unsupported protocol version: %d\" % protocol_version)", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\ttry:\n\t\tfrom .v11 import Handler as HandlerV11\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v12 import Handler as HandlerV12\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v13 import Handler as HandlerV13\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v14 import Handler as HandlerV14\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v15 import Handler as HandlerV15\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\tfrom .v44 import Handler as HandlerV44\n\texcept ImportError:\n\t\tpass\n\n\tif protocol_version == 11:\n\t\treturn HandlerV11\n\telif protocol_version == 12:\n\t\treturn HandlerV12\n\telif protocol_version == 13:\n\t\treturn HandlerV13\n\telif protocol_version == 14:\n\t\treturn HandlerV14\n\telif protocol_version == 15:\n\t\treturn HandlerV15\n\telif protocol_version == 44:\n\t\treturn HandlerV", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef _handlers(cls, protocol_version=None):\n\t\tif protocol_version == None:\n\t\t\tprotocol_version = cls.protocol_version\n\n\t\tif protocol_version == 1:\n\t\t\tfrom .v1 import Handshake, Message\n\t\t\tfrom .v1 import BoltConnector\n\n\t\t\treturn (Handshake, Message, BoltConnector)\n\t\telse:\n\t\t\traise NotImplementedError('Protocol version %d not implemented' % protocol_version)\n\n\treturn _handlers", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or cls.protocol_version\n\tif protocol_version == cls.V1:\n\t\tfrom .v1 import Handshake, Message\n\telif protocol_version == cls.V2:\n\t\tfrom .v2 import Handshake, Message\n\telif protocol_version == cls.V3:\n\t\tfrom .v3 import Handshake, Message\n\telif protocol_version == cls.V4:\n\t\tfrom .v4 import Handshake, Message\n\telif protocol_version == cls.V5:\n\t\tfrom .v5 import Handshake, Message\n\telse:\n\t\traise ValueError('Unknown protocol version: {}'.format(protocol_version))\n\n\treturn Handshake, Message", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tprotocol_version = protocol_version or 3\n\n\tif protocol_version == 4:\n\t\tfrom py2neo.packages.httpstream.v4_1 import BoltProtocolV41\n\n\t\treturn {\n\t\t\tBoltProtocolV41.MAGIC_PREAMBLE: BoltProtocolV41,\n\t\t\tBoltProtocolV41.MAGIC_PREAMBLE_LEGACY: BoltProtocolV41,\n\t\t}\n\telse:\n\t\tfrom py2neo.packages.httpstream.v1 import BoltProtocolV1\n\t\tfrom py2neo.packages.httpstream.v2 import BoltProtocolV2\n\t\tfrom py2neo.packages.httpstream.v3 import BoltProtocolV3\n\n\t\treturn {\n\t\t\tBoltProtocolV1.MAGIC_PREAMBLE: BoltProtocolV1,\n\t\t\tBoltProtocolV2.MAGIC_PREAMBLE: BoltProtocolV2,\n\t\t\tBoltProtocolV3.MAGIC_PREAMBLE: BoltProtocolV3,\n\t\t}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn.metadata = metadata\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef outer_decorator(func):\n\t\t\"\"\"\n\t\tReturns a decorator that executes func with the given metadata and timeout.\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef inner_decorator(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tCalls func with the given metadata and timeout.\n\t\t\t\"\"\"\n\t\t\twith unit_of_work(metadata, timeout):\n\t\t\t\treturn func(*args, **kwargs)\n\t\treturn inner_decorator\n\treturn outer_decorator", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tDecorator for a function that runs in a unit of work.\n\t\t\t\"\"\"\n\t\t\targs = list(args)\n\t\t\targs.insert(0, metadata)\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.metadata = metadata\n\t\twrapper.timeout = timeout\n\t\treturn wrapper\n\treturn decorator", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn.metadata = metadata or {}\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn wrapper\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef _unit_of_work(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn _unit_of_work\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.metadata = metadata or {}\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn.metadata = metadata\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef outer_decorator(func):\n\t\t\"\"\"\n\t\tReturns a decorator that executes func with the given metadata and timeout.\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef inner_decorator(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tCalls func with the given metadata and timeout.\n\t\t\t\"\"\"\n\t\t\twith unit_of_work(metadata, timeout):\n\t\t\t\treturn func(*args, **kwargs)\n\t\treturn inner_decorator\n\treturn outer_decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tDecorator for a function that runs in a unit of work.\n\t\t\t\"\"\"\n\t\t\targs = list(args)\n\t\t\targs.insert(0, metadata)\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.metadata = metadata\n\t\twrapper.timeout = timeout\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn.metadata = metadata or {}\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef _unit_of_work(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn _unit_of_work\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.metadata = metadata or {}\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tkey = key.lower()\n\talphabet = \"abcdefghijklmnopqrstuvwxyz\"\n\tindex = 0\n\tfor char in key:\n\t\tindex += alphabet.index(char)\n\treturn index\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\thash = self.hash(key)\n\t\n\tif self.table[hash] == key:\n\t\treturn hash\n\t\n\ti = hash + 1\n\t\n\twhile self.table[i] != key and self.table[i] != None:\n\t\ti = (i + 1) % self.size\n\t\t\n\t\tif i == hash:\n\t\t\treturn None\n\t\t\n\treturn i\n\t", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tcount = 0\n\twhile (count < len(self.list)):\n\t\tif(self.list[count] == key):\n\t\t\treturn count\n\t\tcount += 1\n\treturn -1\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tindex = ord(key[0]) - ord('a')\n\tfor i in range(len(key) - 1):\n\t\tindex = index * 26 + ord(key[i + 1]) - ord('a')\n\treturn index", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\n\tkey_int = int(key)\n\tif key_int < 0:\n\t\traise KeyError\n\treturn key_int\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\talphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\talphabet_length = len(alphabet)\n\n\t# If the key is not in the form of a string, then convert it.\n\tif type(key) != str:\n\t\tkey = str(key)\n\n\t# If the length of the key is less than the length of the alphabet, repeat the key to the length of the alphabet.\n\twhile len(key) < alphabet_length:\n\t\tkey += key\n\n\t# If the length of the key is greater than the length of the alphabet, reduce the length of the key to the length of the alphabet.\n\twhile len(key) > alphabet_length:\n\t\tkey = key[:-1]\n\n\t# Return the key as an integer.\n\treturn alphabet_length - 1 - alphabet.index(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tindex = 0\n\tfor char in key:\n\t\tindex += ord(char)\n\treturn index % self.size\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tself.key = int(key)\n\treturn self.key", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tindex = ord(key[0]) - ord('a')\n\tindex = index % 26\n\treturn index", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\thashValue = self.hashFunction(key)\n\twhile self.keys[hashValue] != None:\n\t\tif self.keys[hashValue] == key:\n\t\t\treturn hashValue\n\t\thashValue += 1\n\t\thashValue %= len(self.keys)\n\treturn -1\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tkey = key.lower()\n\talphabet = \"abcdefghijklmnopqrstuvwxyz\"\n\tindex = 0\n\tfor char in key:\n\t\tindex += alphabet.index(char)\n\treturn index\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\thash = self.hash(key)\n\t\n\tif self.table[hash] == key:\n\t\treturn hash\n\t\n\ti = hash + 1\n\t\n\twhile self.table[i] != key and self.table[i] != None:\n\t\ti = (i + 1) % self.size\n\t\t\n\t\tif i == hash:\n\t\t\treturn None\n\t\t\n\treturn i\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tcount = 0\n\twhile (count < len(self.list)):\n\t\tif(self.list[count] == key):\n\t\t\treturn count\n\t\tcount += 1\n\treturn -1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tindex = ord(key[0]) - ord('a')\n\tfor i in range(len(key) - 1):\n\t\tindex = index * 26 + ord(key[i + 1]) - ord('a')\n\treturn index", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\n\tkey_int = int(key)\n\tif key_int < 0:\n\t\traise KeyError\n\treturn key_int\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\talphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\talphabet_length = len(alphabet)\n\n\t# If the key is not in the form of a string, then convert it.\n\tif type(key) != str:\n\t\tkey = str(key)\n\n\t# If the length of the key is less than the length of the alphabet, repeat the key to the length of the alphabet.\n\twhile len(key) < alphabet_length:\n\t\tkey += key\n\n\t# If the length of the key is greater than the length of the alphabet, reduce the length of the key to the length of the alphabet.\n\twhile len(key) > alphabet_length:\n\t\tkey = key[:-1]\n\n\t# Return the key as an integer.\n\treturn alphabet_length - 1 - alphabet.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tindex = 0\n\tfor char in key:\n\t\tindex += ord(char)\n\treturn index % self.size\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tself.key = int(key)\n\treturn self.key", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tindex = ord(key[0]) - ord('a')\n\tindex = index % 26\n\treturn index", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\thashValue = self.hashFunction(key)\n\twhile self.keys[hashValue] != None:\n\t\tif self.keys[hashValue] == key:\n\t\t\treturn hashValue\n\t\thashValue += 1\n\t\thashValue %= len(self.keys)\n\treturn -1\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tret = []\n\tfor key in keys:\n\t\tif key not in self.index:\n\t\t\traise KeyError(key)\n\t\tret.append(self.index[key])\n\treturn ret", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tkey_list = []\n\tfor key in keys:\n\t\tkey_list.append(self.get(key))\n\treturn key_list\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tret_val = []\n\tfor key in keys:\n\t\tret_val.append(self.index[key])\n\treturn ret_val\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tself.data = list(self.data.values())\n\treturn self.data\n\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = []\n\tfor key in keys:\n\t\tvalues.append(self[key])\n\t\n\treturn values", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\n\treturn [self[k] for k in keys]\n\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tans = []\n\tfor i in keys:\n\t\tans.append(self[i])\n\treturn ans", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tl = []\n\tfor key in keys:\n\t\tl.append(self[key])\n\treturn l", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tval = [self[key] for key in keys]\n\treturn val if len(val) > 1 else val[0]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tv = [self[k] for k in keys]\n\treturn v"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tret = []\n\tfor key in keys:\n\t\tif key not in self.index:\n\t\t\traise KeyError(key)\n\t\tret.append(self.index[key])\n\treturn ret", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tkey_list = []\n\tfor key in keys:\n\t\tkey_list.append(self.get(key))\n\treturn key_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tret_val = []\n\tfor key in keys:\n\t\tret_val.append(self.index[key])\n\treturn ret_val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tself.data = list(self.data.values())\n\treturn self.data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = []\n\tfor key in keys:\n\t\tvalues.append(self[key])\n\t\n\treturn values", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\n\treturn [self[k] for k in keys]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tans = []\n\tfor i in keys:\n\t\tans.append(self[i])\n\treturn ans", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tl = []\n\tfor key in keys:\n\t\tl.append(self[key])\n\treturn l", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tval = [self[key] for key in keys]\n\treturn val if len(val) > 1 else val[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tv = [self[k] for k in keys]\n\treturn v", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tdata = {}\n\tfor key in keys:\n\t\tdata[key] = self.data_transformed[key]\n\treturn data", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tresult = {}\n\tfor key in keys:\n\t\tresult[key] = self.data_store.get(key)\n\treturn result\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tdata = self.records[0]\n\tif keys:\n\t\treturn {key:data[key] for key in keys}\n\treturn data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tout = {}\n\tfor key in keys:\n\t\tif key not in self.data:\n\t\t\tself.process_key(key)\n\t\tout[key] = self.data[key]\n\treturn out\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\ttry:\n\t\treturn self.data[keys[0]]\n\texcept KeyError:\n\t\treturn None\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = list(keys)\n\tfor i, k in enumerate(keys):\n\t\tif not isinstance(k, str):\n\t\t\traise ValueError(f\"Expected string as key, got {type(k).__name__}.\")\n\t\tkeys[i] = f\"{self.prefix}{k}\"\n\treturn [self.data[k] for k in keys]\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tresult = {}\n\t\n\tif not keys:\n\t\tkeys = self.data().keys()\n\t\n\tfor key in keys:\n\t\tif key in self.data():\n\t\t\tresult[key] = self.data()[key]\n\t\n\treturn result", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\ttry:\n\t\treturn self._data\n\texcept AttributeError:\n\t\tdata = {}\n\t\tfor key in keys:\n\t\t\tdata[key] = self.transform(key)\n\t\tself._data = data\n\t\treturn data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tdata = self.transform()\n\tfor key in keys:\n\t\ttry:\n\t\t\tdata = data[key]\n\t\texcept KeyError:\n\t\t\traise KeyError('{0} is not a valid key.'.format(key))\n\treturn data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = set(keys)\n\tkeys.discard(None)\n\tif not keys:\n\t\tkeys = self.keys\n\tkeys = list(keys)\n\tkeys.sort()\n\n\tdata = []\n\n\tfor i in range(self.size):\n\t\tdata.append(self._data[i][keys])\n\n\treturn data\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tdata = {}\n\tfor key in keys:\n\t\tdata[key] = self.data_transformed[key]\n\treturn data", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tresult = {}\n\tfor key in keys:\n\t\tresult[key] = self.data_store.get(key)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tdata = self.records[0]\n\tif keys:\n\t\treturn {key:data[key] for key in keys}\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tout = {}\n\tfor key in keys:\n\t\tif key not in self.data:\n\t\t\tself.process_key(key)\n\t\tout[key] = self.data[key]\n\treturn out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\ttry:\n\t\treturn self.data[keys[0]]\n\texcept KeyError:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = list(keys)\n\tfor i, k in enumerate(keys):\n\t\tif not isinstance(k, str):\n\t\t\traise ValueError(f\"Expected string as key, got {type(k).__name__}.\")\n\t\tkeys[i] = f\"{self.prefix}{k}\"\n\treturn [self.data[k] for k in keys]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tresult = {}\n\t\n\tif not keys:\n\t\tkeys = self.data().keys()\n\t\n\tfor key in keys:\n\t\tif key in self.data():\n\t\t\tresult[key] = self.data()[key]\n\t\n\treturn result", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\ttry:\n\t\treturn self._data\n\texcept AttributeError:\n\t\tdata = {}\n\t\tfor key in keys:\n\t\t\tdata[key] = self.transform(key)\n\t\tself._data = data\n\t\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tdata = self.transform()\n\tfor key in keys:\n\t\ttry:\n\t\t\tdata = data[key]\n\t\texcept KeyError:\n\t\t\traise KeyError('{0} is not a valid key.'.format(key))\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = set(keys)\n\tkeys.discard(None)\n\tif not keys:\n\t\tkeys = self.keys\n\tkeys = list(keys)\n\tkeys.sort()\n\n\tdata = []\n\n\tfor i in range(self.size):\n\t\tdata.append(self._data[i][keys])\n\n\treturn data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tres = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn res\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\td = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn d\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tu16 = self.data[-1] << 8 | self.data[-2]\n\tself.data = self.data[:-2]\n\treturn u16\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\tdata_str = self.data[-2:]\n\tdata_int = int(data_str, 16)\n\treturn data_int\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn self.data\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn self.data[-2:]\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[0:-2]\n\treturn int(self.data[-2:], 16)\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn self.data\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tx = self.data[-2]\n\ty = self.data[-1]\n\tself.data = self.data[:-2]\n\treturn x, y\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn int(self.data[-2:], 16)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tres = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\td = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tu16 = self.data[-1] << 8 | self.data[-2]\n\tself.data = self.data[:-2]\n\treturn u16\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\tdata_str = self.data[-2:]\n\tdata_int = int(data_str, 16)\n\treturn data_int\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn self.data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn self.data[-2:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[0:-2]\n\treturn int(self.data[-2:], 16)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn self.data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tx = self.data[-2]\n\ty = self.data[-1]\n\tself.data = self.data[:-2]\n\treturn x, y\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data = self.data[:-2]\n\treturn int(self.data[-2:], 16)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append(\n\t\tDISCARD(n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n\t\t\t\thydration_hooks=hydration_hooks, **handlers))\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.q.put(('DISCARD', n, qid, dehydration_hooks, hydration_hooks, handlers))\n\tself.send_queued()\n\treturn self.recv_response()", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmessage = self._create_message('DISCARD', n, qid)\n\treturn self._send_message(message, **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n\tself._send_message('DISCARD', 1, n, qid, dehydration_hooks, hydration_hooks, **handlers)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._queue.append(('DISCARD', (n, qid, dehydration_hooks,\n\t                                hydration_hooks, handlers)))\n\treturn Response(self)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = Message(DISCARD.value,\n\t              (n, qid, dehydration_hooks, hydration_hooks),\n\t              **handlers)\n\treturn self._send_message(msg)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmessage = discard_message(\n\t\tn=n,\n\t\tqid=qid,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers)\n\tself.write(message)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._queue.append(('DISCARD', n, qid, dehydration_hooks,\n\t                    hydration_hooks, handlers))\n\treturn self\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = {'$discard': n, 'queryId': qid}\n\treturn self._send_message(msg, dehydration_hooks, hydration_hooks, **handlers)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = Message('DISCARD', n, qid, dehydration_hooks, hydration_hooks)\n\tresp = self.send_message(msg, **handlers)\n\treturn resp\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append(\n\t\tDISCARD(n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n\t\t\t\thydration_hooks=hydration_hooks, **handlers))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.q.put(('DISCARD', n, qid, dehydration_hooks, hydration_hooks, handlers))\n\tself.send_queued()\n\treturn self.recv_response()", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmessage = self._create_message('DISCARD', n, qid)\n\treturn self._send_message(message, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n\tself._send_message('DISCARD', 1, n, qid, dehydration_hooks, hydration_hooks, **handlers)", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._queue.append(('DISCARD', (n, qid, dehydration_hooks,\n\t                                hydration_hooks, handlers)))\n\treturn Response(self)", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = Message(DISCARD.value,\n\t              (n, qid, dehydration_hooks, hydration_hooks),\n\t              **handlers)\n\treturn self._send_message(msg)", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmessage = discard_message(\n\t\tn=n,\n\t\tqid=qid,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers)\n\tself.write(message)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._queue.append(('DISCARD', n, qid, dehydration_hooks,\n\t                    hydration_hooks, handlers))\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = {'$discard': n, 'queryId': qid}\n\treturn self._send_message(msg, dehydration_hooks, hydration_hooks, **handlers)", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = Message('DISCARD', n, qid, dehydration_hooks, hydration_hooks)\n\tresp = self.send_message(msg, **handlers)\n\treturn resp\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError\n\t", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmsg = Msg.BEGIN(mode, bookmarks, metadata, timeout, db, imp_user,\n\t                dehydration_hooks, hydration_hooks)\n\treturn self._handle_message(msg, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError()", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmessage = {\n\t\tb\"bookmarks\": bookmarks,\n\t\tb\"metadata\": metadata,\n\t\tb\"timeout\": timeout,\n\t\tb\"mode\": mode,\n\t\tb\"db\": db,\n\t\tb\"imp_user\": imp_user,\n\t\tb\"dehydration_hooks\": dehydration_hooks,\n\t\tb\"hydration_hooks\": hydration_hooks,\n\t\t\"message_type\": b\"BEGIN\",\n\t}\n\tself.write_message(message)\n\treturn Response(self, handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tpass", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmsg = Message.begin(self, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\tself._enqueue(msg, handlers)\n\treturn Response(self, msg)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself._queue.append(\n\t\tMessage.BEGIN,\n\t\tmode,\n\t\tbookmarks,\n\t\tmetadata,\n\t\ttimeout,\n\t\tdb,\n\t\timp_user,\n\t\tdehydration_hooks,\n\t\thydration_hooks,\n\t)\n\treturn self._response(**handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmessage = Message.BEGIN\n\tmessage.metadata = metadata\n\tmessage.bookmarks = bookmarks\n\tmessage.db = db\n\tmessage.mode = mode\n\tmessage.timeout = timeout\n\tmessage.dehydration_hooks = dehydration_hooks\n\tmessage.hydration_hooks = hydration_hooks\n\tself._send_message(message, handlers=handlers, imp_user=imp_user)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmsg = Msg.BEGIN(mode, bookmarks, metadata, timeout, db, imp_user,\n\t                dehydration_hooks, hydration_hooks)\n\treturn self._handle_message(msg, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmessage = {\n\t\tb\"bookmarks\": bookmarks,\n\t\tb\"metadata\": metadata,\n\t\tb\"timeout\": timeout,\n\t\tb\"mode\": mode,\n\t\tb\"db\": db,\n\t\tb\"imp_user\": imp_user,\n\t\tb\"dehydration_hooks\": dehydration_hooks,\n\t\tb\"hydration_hooks\": hydration_hooks,\n\t\t\"message_type\": b\"BEGIN\",\n\t}\n\tself.write_message(message)\n\treturn Response(self, handlers)", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmsg = Message.begin(self, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\tself._enqueue(msg, handlers)\n\treturn Response(self, msg)", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself._queue.append(\n\t\tMessage.BEGIN,\n\t\tmode,\n\t\tbookmarks,\n\t\tmetadata,\n\t\ttimeout,\n\t\tdb,\n\t\timp_user,\n\t\tdehydration_hooks,\n\t\thydration_hooks,\n\t)\n\treturn self._response(**handlers)", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmessage = Message.BEGIN\n\tmessage.metadata = metadata\n\tmessage.bookmarks = bookmarks\n\tmessage.db = db\n\tmessage.mode = mode\n\tmessage.timeout = timeout\n\tmessage.dehydration_hooks = dehydration_hooks\n\tmessage.hydration_hooks = hydration_hooks\n\tself._send_message(message, handlers=handlers, imp_user=imp_user)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tinteger = int(n)\n\tfraction = n - integer\n\tif integer % 2 == 0:\n\t\trounding = fraction >= 0.5\n\telse:\n\t\trounding = fraction > 0.5\n\treturn integer + rounding\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = n * 2\n\tif n - int(n) == 0.5:\n\t\tn = n + int(n) % 2\n\telse:\n\t\tn = int(n)\n\tn = n / 2\n\treturn n\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = int(n * 2)\n\tif n % 2 == 0:\n\t\tn /= 2\n\telse:\n\t\tn += 1\n\t\tn /= 2\n\n\treturn n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = n*2\n\tif n%2 == 0:\n\t\tn = int(n/2)\n\t\tn = n/2\n\t\treturn n\n\tn = int(n/2)\n\tn = n/2\n\treturn n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\n\treturn round(n + 0.5) - .5\n\nprint(round_half_to_even(1.2))\nprint(round_half_to_even(1.5))\nprint(round_half_to_even(1.6))\nprint(round_half_to_even(1.7))\nprint(round_half_to_even(1.8))\nprint(round_half_to_even(1.9))", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = int(n)\n\tif n > -1 and n < 1:\n\t\treturn 0\n\telse:\n\t\tn = str(n)\n\t\tn = n.split('.')\n\t\t# print(n)\n\t\tn = n[0] + '.' + n[1]\n\t\tn = n.split('.')\n\t\t# print(n)\n\t\tif n[1][0] == '5' and int(n[1][1]) % 2 == 0:\n\t\t\treturn int(n[0]) + int(n[1][0]) + 1\n\t\telse:\n\t\t\treturn int(n[0]) + int(n[1][0])\n\n\nprint(round_half_to_even(2.5))", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = float(n)\n\tn = int(n * 2**32)\n\tn = n + (n & 1)\n\tn = n & 0x7fffffff\n\tn = n * 2**-32\n\treturn n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = n + 5e-16\n\tn = n - n % 1\n\treturn n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\ta = int(n)\n\tb = int(n*10)\n\td = n*10 - b\n\tif d == 0.5:\n\t\tif a & 1:\n\t\t\treturn a + 1\n\t\telse:\n\t\t\treturn a\n\telse:\n\t\treturn round(n)", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\trounded_n = round(n)\n\n\t# If the input and output are the same, return the output\n\tif rounded_n == n:\n\t\treturn rounded_n\n\n\t# If the difference is 0.5, return the output\n\tif rounded_n - n == 0.5:\n\t\treturn rounded_n\n\n\t# Otherwise, return the input\n\treturn n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tinteger = int(n)\n\tfraction = n - integer\n\tif integer % 2 == 0:\n\t\trounding = fraction >= 0.5\n\telse:\n\t\trounding = fraction > 0.5\n\treturn integer + rounding\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = n * 2\n\tif n - int(n) == 0.5:\n\t\tn = n + int(n) % 2\n\telse:\n\t\tn = int(n)\n\tn = n / 2\n\treturn n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = int(n * 2)\n\tif n % 2 == 0:\n\t\tn /= 2\n\telse:\n\t\tn += 1\n\t\tn /= 2\n\n\treturn n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = n*2\n\tif n%2 == 0:\n\t\tn = int(n/2)\n\t\tn = n/2\n\t\treturn n\n\tn = int(n/2)\n\tn = n/2\n\treturn n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\n\treturn round(n + 0.5) - .5\n\nprint(round_half_to_even(1.2))\nprint(round_half_to_even(1.5))\nprint(round_half_to_even(1.6))\nprint(round_half_to_even(1.7))\nprint(round_half_to_even(1.8))\nprint(round_half_to_even(1.9))", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = int(n)\n\tif n > -1 and n < 1:\n\t\treturn 0\n\telse:\n\t\tn = str(n)\n\t\tn = n.split('.')\n\t\t# print(n)\n\t\tn = n[0] + '.' + n[1]\n\t\tn = n.split('.')\n\t\t# print(n)\n\t\tif n[1][0] == '5' and int(n[1][1]) % 2 == 0:\n\t\t\treturn int(n[0]) + int(n[1][0]) + 1\n\t\telse:\n\t\t\treturn int(n[0]) + int(n[1][0])\n\n\nprint(round_half_to_even(2.5))", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = float(n)\n\tn = int(n * 2**32)\n\tn = n + (n & 1)\n\tn = n & 0x7fffffff\n\tn = n * 2**-32\n\treturn n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = n + 5e-16\n\tn = n - n % 1\n\treturn n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\ta = int(n)\n\tb = int(n*10)\n\td = n*10 - b\n\tif d == 0.5:\n\t\tif a & 1:\n\t\t\treturn a + 1\n\t\telse:\n\t\t\treturn a\n\telse:\n\t\treturn round(n)", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\trounded_n = round(n)\n\n\t# If the input and output are the same, return the output\n\tif rounded_n == n:\n\t\treturn rounded_n\n\n\t# If the difference is 0.5, return the output\n\tif rounded_n - n == 0.5:\n\t\treturn rounded_n\n\n\t# Otherwise, return the input\n\treturn n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tfrom sqlalchemy import Column, Integer, String, Boolean, Float\n\tfrom sqlalchemy.orm import declarative_base\n\t\n\tBase = declarative_base()\n\t\n\tclass Point(Base):\n\t\t__tablename__ = name\n\t\t\n\t\tgid = Column(Integer, primary_key=True)\n\t\tid = Column(Integer)\n\t\tgeom = Column(String)\n\t\tname = Column(String)\n\t\tdescription = Column(String)\n\t\t\n\t\tfor field in fields:\n\t\t\tsetattr(Point, field, Column(String))\n\t\t\n\t\tdef __repr__(self):\n\t\t\treturn f\"Point(name={self.name})\"\n\t\n\treturn Point\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tfor key, value in kwargs.items():\n\t\t\tsetattr(self, key, value)\n\t\n\tdef __repr__(self):\n\t\treturn \"<%s(%s)>\" % (\n\t\t\tself.__class__.__name__,\n\t\t\t', '.join('%s=%s' % item for item in vars(self).items())\n\t\t)\n\t\n\t__table_args__ = {'extend_existing': True}\n\t__tablename__ = name\n\t__mapper_args__ = {'polymorphic_identity': name}\n\t\n\tfor field_name, field_type in fields.items():\n\t\ttry:\n\t\t\texec '{} = Column(field_type)'.format(field_name)\n\t\texcept:\n\t\t\texec '{} = Column(field_type, srid=srid_map[field_name])'.format(field_name)\n\n\treturn type(name, (object,), locals())", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_fields = [\n\t\t(\"SRID\", \"INTEGER\"),\n\t\t(\"X\", \"FLOAT\"),\n\t\t(\"Y\", \"FLOAT\"),\n\t\t(\"Z\", \"FLOAT\"),\n\t\t(\"M\", \"FLOAT\")\n\t]\n\n\tpoint_fields.extend(fields)\n\n\tpoint_fields = [\",\\n\\t\".join(tup) for tup in point_fields]\n\tpoint_fields = \"\\n\\t\".join(point_fields)\n\n\tif srid_map is not None:\n\t\tfor k, v in srid_map.items():\n\t\t\tpoint_fields = point_fields.replace(str(k), str(v))\n\n\tpoint_fields = point_fields.replace(\"_\", \" \")\n\n\tpoint_str = f\"CREATE TABLE IF NOT EXISTS {name} (\\n\\t{point_fields}\\n);\"\n\n\treturn point_str\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tPoint = type('Point', (object,), fields)\n\tPoint.__module__ = __name__\n\tsetattr(Point, 'srid', srid_map[name])\n\treturn Point\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\treturn type(name, (Point,), fields)\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tattrs = {'__module__': 'django.contrib.gis.geos'}\n\tfor field_name, field_type in fields:\n\t\tattrs[field_name] = field_type\n\n\tPoint = type(name, (models.Model,), attrs)\n\tPoint._meta = type(Point._meta)()\n\tPoint._meta.db_table = name\n\tPoint._meta.app_label = 'point'\n\tPoint._meta.db_table = name\n\tPoint._meta.module_name = Point._meta.object_name = name\n\tPoint._meta.srid = srid_map.get(Point._meta.db_table, 0)\n\tPoint._meta.fields = [models.Field() for f in fields]\n\tPoint._meta.local_fields = [f for f in Point._meta.fields if not f.is_relation]\n\tPoint._meta.local_concrete_fields = [f for f in Point._meta.local_fields if not f.is_relation or f.many_to_many]\n\tPoint._meta.auto_created = True\n\tPoint._meta.auto_created = True\n\tPoint._meta.managed = False\n\tPoint._meta.index_together = []\n\n\tmanager = Manager()\n\tmanager.contribute_", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.type = 'Point'\n\t\t\tself.coordinates = kwargs['coordinates']\n\t\t\tself.srid = kwargs['srid']\n\t\t\tfor key, val in kwargs.items():\n\t\t\t\tsetattr(self, key, val)\n\n\t\tdef __repr__(self):\n\t\t\treturn \"<Point (%s, %s)>\" % (self.coordinates[0], self.coordinates[1])\n\n\t\tdef to_dict(self):\n\t\t\tout = dict()\n\t\t\tfor field in fields:\n\t\t\t\tout[field.name] = getattr(self, field.name)\n\t\t\tout['coordinates'] = self.coordinates\n\t\t\tout['type'] = self.type\n\t\t\tout['crs'] = {'type': 'name', 'properties': {'name': srid_map[self.srid]}}\n\t\t\treturn out\n\n\t\tdef to_wkt(self):\n\t\t\treturn 'POINT (%s %s)' % (self.coordinates[0], self.coordinates[1])\n\n\t\tdef __eq__(self, other):\n\t\t\tif isinstance(other, Point):\n\t\t\t\treturn self.coordinates == other.", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_class_name = name\n\tpoint_class_fields = [\n\t\t('gid', 'INTEGER', 'PRIMARY KEY AUTOINCREMENT'),\n\t\t('name', 'TEXT', 'NOT NULL'),\n\t\t('geom', 'POINT', 'NOT NULL')\n\t]\n\n\tfor field in fields:\n\t\tpoint_class_fields.append(\n\t\t\t(\n\t\t\t\tfield[0],\n\t\t\t\tfield[1],\n\t\t\t\tf'NOT NULL REFERENCES {field[0]} ({srid_map[field[1]]})'\n\t\t\t)\n\t\t)\n\n\treturn type(point_class_name, (object,), {'__slots__': [field[0] for field in point_class_fields]})", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_type = type(name, (object,), fields)\n\tpoint_type.srid_map = srid_map\n\treturn point_type\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, **kwargs):\n\t\t\tfor k, v in kwargs.items():\n\t\t\t\tif k in fields:\n\t\t\t\t\tsetattr(self, k, v)\n\n\t\tdef __iter__(self):\n\t\t\tfor k, v in self.__dict__.items():\n\t\t\t\tyield k, v\n\n\tdef point_class():\n\t\t\"\"\"\n\t\tReturns Dynamically Generated Point Class\n\t\t\"\"\"\n\t\tclass Point(object):\n\t\t\tdef __init__(self, **kwargs):\n\t\t\t\tfor k, v in kwargs.items():\n\t\t\t\t\tif k in fields:\n\t\t\t\t\t\tsetattr(self, k, v)\n\n\t\t\tdef __iter__(self):\n\t\t\t\tfor k, v in self.__dict__.items():\n\t\t\t\t\tyield k, v\n\n\t\treturn Point\n\n\treturn point_class"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tfrom sqlalchemy import Column, Integer, String, Boolean, Float\n\tfrom sqlalchemy.orm import declarative_base\n\t\n\tBase = declarative_base()\n\t\n\tclass Point(Base):\n\t\t__tablename__ = name\n\t\t\n\t\tgid = Column(Integer, primary_key=True)\n\t\tid = Column(Integer)\n\t\tgeom = Column(String)\n\t\tname = Column(String)\n\t\tdescription = Column(String)\n\t\t\n\t\tfor field in fields:\n\t\t\tsetattr(Point, field, Column(String))\n\t\t\n\t\tdef __repr__(self):\n\t\t\treturn f\"Point(name={self.name})\"\n\t\n\treturn Point\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tfor key, value in kwargs.items():\n\t\t\tsetattr(self, key, value)\n\t\n\tdef __repr__(self):\n\t\treturn \"<%s(%s)>\" % (\n\t\t\tself.__class__.__name__,\n\t\t\t', '.join('%s=%s' % item for item in vars(self).items())\n\t\t)\n\t\n\t__table_args__ = {'extend_existing': True}\n\t__tablename__ = name\n\t__mapper_args__ = {'polymorphic_identity': name}\n\t\n\tfor field_name, field_type in fields.items():\n\t\ttry:\n\t\t\texec '{} = Column(field_type)'.format(field_name)\n\t\texcept:\n\t\t\texec '{} = Column(field_type, srid=srid_map[field_name])'.format(field_name)\n\n\treturn type(name, (object,), locals())", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_fields = [\n\t\t(\"SRID\", \"INTEGER\"),\n\t\t(\"X\", \"FLOAT\"),\n\t\t(\"Y\", \"FLOAT\"),\n\t\t(\"Z\", \"FLOAT\"),\n\t\t(\"M\", \"FLOAT\")\n\t]\n\n\tpoint_fields.extend(fields)\n\n\tpoint_fields = [\",\\n\\t\".join(tup) for tup in point_fields]\n\tpoint_fields = \"\\n\\t\".join(point_fields)\n\n\tif srid_map is not None:\n\t\tfor k, v in srid_map.items():\n\t\t\tpoint_fields = point_fields.replace(str(k), str(v))\n\n\tpoint_fields = point_fields.replace(\"_\", \" \")\n\n\tpoint_str = f\"CREATE TABLE IF NOT EXISTS {name} (\\n\\t{point_fields}\\n);\"\n\n\treturn point_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tPoint = type('Point', (object,), fields)\n\tPoint.__module__ = __name__\n\tsetattr(Point, 'srid', srid_map[name])\n\treturn Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\treturn type(name, (Point,), fields)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tattrs = {'__module__': 'django.contrib.gis.geos'}\n\tfor field_name, field_type in fields:\n\t\tattrs[field_name] = field_type\n\n\tPoint = type(name, (models.Model,), attrs)\n\tPoint._meta = type(Point._meta)()\n\tPoint._meta.db_table = name\n\tPoint._meta.app_label = 'point'\n\tPoint._meta.db_table = name\n\tPoint._meta.module_name = Point._meta.object_name = name\n\tPoint._meta.srid = srid_map.get(Point._meta.db_table, 0)\n\tPoint._meta.fields = [models.Field() for f in fields]\n\tPoint._meta.local_fields = [f for f in Point._meta.fields if not f.is_relation]\n\tPoint._meta.local_concrete_fields = [f for f in Point._meta.local_fields if not f.is_relation or f.many_to_many]\n\tPoint._meta.auto_created = True\n\tPoint._meta.auto_created = True\n\tPoint._meta.managed = False\n\tPoint._meta.index_together = []\n\n\tmanager = Manager()\n\tmanager.contribute_", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.type = 'Point'\n\t\t\tself.coordinates = kwargs['coordinates']\n\t\t\tself.srid = kwargs['srid']\n\t\t\tfor key, val in kwargs.items():\n\t\t\t\tsetattr(self, key, val)\n\n\t\tdef __repr__(self):\n\t\t\treturn \"<Point (%s, %s)>\" % (self.coordinates[0], self.coordinates[1])\n\n\t\tdef to_dict(self):\n\t\t\tout = dict()\n\t\t\tfor field in fields:\n\t\t\t\tout[field.name] = getattr(self, field.name)\n\t\t\tout['coordinates'] = self.coordinates\n\t\t\tout['type'] = self.type\n\t\t\tout['crs'] = {'type': 'name', 'properties': {'name': srid_map[self.srid]}}\n\t\t\treturn out\n\n\t\tdef to_wkt(self):\n\t\t\treturn 'POINT (%s %s)' % (self.coordinates[0], self.coordinates[1])\n\n\t\tdef __eq__(self, other):\n\t\t\tif isinstance(other, Point):\n\t\t\t\treturn self.coordinates == other.", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_class_name = name\n\tpoint_class_fields = [\n\t\t('gid', 'INTEGER', 'PRIMARY KEY AUTOINCREMENT'),\n\t\t('name', 'TEXT', 'NOT NULL'),\n\t\t('geom', 'POINT', 'NOT NULL')\n\t]\n\n\tfor field in fields:\n\t\tpoint_class_fields.append(\n\t\t\t(\n\t\t\t\tfield[0],\n\t\t\t\tfield[1],\n\t\t\t\tf'NOT NULL REFERENCES {field[0]} ({srid_map[field[1]]})'\n\t\t\t)\n\t\t)\n\n\treturn type(point_class_name, (object,), {'__slots__': [field[0] for field in point_class_fields]})", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_type = type(name, (object,), fields)\n\tpoint_type.srid_map = srid_map\n\treturn point_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, **kwargs):\n\t\t\tfor k, v in kwargs.items():\n\t\t\t\tif k in fields:\n\t\t\t\t\tsetattr(self, k, v)\n\n\t\tdef __iter__(self):\n\t\t\tfor k, v in self.__dict__.items():\n\t\t\t\tyield k, v\n\n\tdef point_class():\n\t\t\"\"\"\n\t\tReturns Dynamically Generated Point Class\n\t\t\"\"\"\n\t\tclass Point(object):\n\t\t\tdef __init__(self, **kwargs):\n\t\t\t\tfor k, v in kwargs.items():\n\t\t\t\t\tif k in fields:\n\t\t\t\t\t\tsetattr(self, k, v)\n\n\t\t\tdef __iter__(self):\n\t\t\t\tfor k, v in self.__dict__.items():\n\t\t\t\t\tyield k, v\n\n\t\treturn Point\n\n\treturn point_class", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(f\"{func.__name__} is deprecated and will be removed in the future. {message}\")\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n@deprecated(\"Use the new() method instead.\")", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tprint \"DEPRECATION WARNING: {0}: {1}\".format(func.__name__, message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapped\n\treturn decorator", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(f\"{func.__name__} is deprecated. {message}\")\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint('\\n\\033[31m' + message + '\\033[0m\\n')\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(f\"Call to deprecated function or method {func.__name__}:\"\n\t\t\t\tf\" {message}. \"\n\t\t\t\tf\"This will be removed in future versions.\",\n\t\t\t\tDeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef inner(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn inner", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\tprint('Deprecation warning: {}'.format(message))\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn deprecated_decorator", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(obj):\n\t\t\"\"\"\n\t\tDecorator function that prints a warning message when the function is called.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function that prints a warning message and then calls the original function.\n\t\t\t\"\"\"\n\t\t\tprint(f\"WARNING: {message}\")\n\t\t\treturn obj(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n# Example usage:\n\n@deprecated(\"This function is deprecated. Use another function instead.\")"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(f\"{func.__name__} is deprecated and will be removed in the future. {message}\")\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n@deprecated(\"Use the new() method instead.\")", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tprint \"DEPRECATION WARNING: {0}: {1}\".format(func.__name__, message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapped\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(f\"{func.__name__} is deprecated. {message}\")\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint('\\n\\033[31m' + message + '\\033[0m\\n')\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(f\"Call to deprecated function or method {func.__name__}:\"\n\t\t\t\tf\" {message}. \"\n\t\t\t\tf\"This will be removed in future versions.\",\n\t\t\t\tDeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef inner(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn inner", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\tprint('Deprecation warning: {}'.format(message))\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn deprecated_decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(obj):\n\t\t\"\"\"\n\t\tDecorator function that prints a warning message when the function is called.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function that prints a warning message and then calls the original function.\n\t\t\t\"\"\"\n\t\t\tprint(f\"WARNING: {message}\")\n\t\t\treturn obj(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n# Example usage:\n\n@deprecated(\"This function is deprecated. Use another function instead.\")", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += r\"\"\"\n\toptions(\n\t\trepos = \"https://cloud.r-project.org\",\n\t\twarn = 2,\n\t\terror = function(e) {\n\t\t\tmessage(\"Error: \", conditionMessage(e))\n\t\t\tq(\"no\", 1, FALSE)\n\t\t},\n\t)\n\t\"\"\"\n\treturn code", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\noptions(digits=10)\\n\"\n\n\treturn code", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\"\"\n\toptions(stringsAsFactors = FALSE)\n\t\"\"\"\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\n\"\n\tcode += \"options(warn=-1)\"\n\tcode += \"\\n\"\n\tcode += \"options(repos='http://cran.rstudio.com/')\"\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\n\"\n\tcode += \"options(repos = 'http://cran.r-project.org/'\\n\"\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\toptions = [\n\t\t\"options(warn = -1)\",\n\t\t\"options(stringsAsFactors = FALSE)\",\n\t\t\"options(digits = 15)\"\n\t]\n\treturn \"\\n\".join([code] + options)\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\"\"\n\toptions(show.error.messages = FALSE)\n\toptions(show.signif.stars = FALSE)\n\toptions(warn = -1)\n\t\"\"\"\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tr_options = ['options(repos=\"http://cran.rstudio.com/\")',\n\t             'options(install.packages.compile.from.source=\"never\")']\n\treturn f'\\n'.join(r_options)\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = inline_r_setup_options(code)\n\tcode = inline_r_setup_packages(code)\n\treturn code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"R_DEFAULT_PACKAGES\", \"\\\"\\\"\")\n\tcode = code.replace(\"options(warn = -1)\", \"options(warn = -1)\\n\\toptions(warnPartialMatchDuplicates = FALSE)\")\n\treturn code\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += r\"\"\"\n\toptions(\n\t\trepos = \"https://cloud.r-project.org\",\n\t\twarn = 2,\n\t\terror = function(e) {\n\t\t\tmessage(\"Error: \", conditionMessage(e))\n\t\t\tq(\"no\", 1, FALSE)\n\t\t},\n\t)\n\t\"\"\"\n\treturn code", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\noptions(digits=10)\\n\"\n\n\treturn code", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\"\"\n\toptions(stringsAsFactors = FALSE)\n\t\"\"\"\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\n\"\n\tcode += \"options(warn=-1)\"\n\tcode += \"\\n\"\n\tcode += \"options(repos='http://cran.rstudio.com/')\"\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\n\"\n\tcode += \"options(repos = 'http://cran.r-project.org/'\\n\"\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\toptions = [\n\t\t\"options(warn = -1)\",\n\t\t\"options(stringsAsFactors = FALSE)\",\n\t\t\"options(digits = 15)\"\n\t]\n\treturn \"\\n\".join([code] + options)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\"\"\n\toptions(show.error.messages = FALSE)\n\toptions(show.signif.stars = FALSE)\n\toptions(warn = -1)\n\t\"\"\"\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tr_options = ['options(repos=\"http://cran.rstudio.com/\")',\n\t             'options(install.packages.compile.from.source=\"never\")']\n\treturn f'\\n'.join(r_options)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = inline_r_setup_options(code)\n\tcode = inline_r_setup_packages(code)\n\treturn code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"R_DEFAULT_PACKAGES\", \"\\\"\\\"\")\n\tcode = code.replace(\"options(warn = -1)\", \"options(warn = -1)\\n\\toptions(warnPartialMatchDuplicates = FALSE)\")\n\treturn code\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n\timport os\n\timport subprocess\n\timport sys\n\t\n\tdef _get_platform_max_length():\n\t\t\"\"\"\n\t\tGet the maximum command length for the current platform.\n\t\t\"\"\"\n\t\tif sys.platform == \"win32\":\n\t\t\treturn 8192\n\t\treturn os.sysconf(\"SC_ARG_MAX\")\n\t\n\tdef _split(args: Sequence[str], max_length: int) -> Iterator[tuple[str, ...]]:\n\t\t\"\"\"\n\t\tSplit a sequence of strings into chunks that are no longer than max_length.\n\t\t\"\"\"\n\t\ti = 0\n\t\twhile i < len(args):\n\t\t\tchunk = args[i:i + target_concurrency]\n\t\t\ti += target_concurrency\n\t\t\twhile len(chunk) > 0 and len(\" \".join(chunk)) > max_length:\n\t\t\t\tchunk = chunk[:-1]\n\t\t\tyield tuple(chunk)\n\t\n\tdef _run(args: tuple[str, ...], **kwargs: str) -> None:\n\t\t\"\"\"\n\t\tRun a command with the given arguments.\n\t\t\"\"\"\n\t\tproc = subprocess.Popen(args, **kwargs)\n\t\tproc.communicate()", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcommand = f\"{cmd[0]} {' '.join(cmd[1:])}\"\n\tbatch_size = target_concurrency\n\ttotal_args = len(varargs)\n\ttotal_batches = total_args // batch_size + (total_args % batch_size > 0)\n\t\n\tfor i in range(0, total_args, batch_size):\n\t\tbatch = varargs[i:i + batch_size]\n\t\tbatch_command = f\"{command} {' '.join(batch)}\"\n\t\tprint(batch_command)\n\t\tif _max_length:\n\t\t\tbatch_command = batch_command[:_max_length]\n\t\tsubprocess.run(batch_command, shell=True)\n\nxargs((\"ls\", \"-l\"), [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], target_concurrency=2)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\timport subprocess, os, signal\n\tfrom multiprocessing import Pool, cpu_count\n\tfrom itertools import chain\n\tfrom functools import partial\n\tfrom typing import Callable\n\t\n\tdef _run_cmd(cmd: str, **kwargs: dict[str, str]) -> None:\n\t    \"\"\"\n\t    run the command\n\t\n\t    Args:\n\t        cmd (str): command to run\n\t        kwargs (dict, optional): arguments to pass to the command. Defaults to {}.\n\t    \"\"\"\n\t    p = subprocess.Popen(cmd, shell=True, **kwargs)\n\t    p.wait()\n\t    return\n\t\n\tdef _get_platform_max_length() -> int:\n\t    \"\"\"\n\t    get the max length of the command line\n\t    \"\"\"\n\t    if os.name == \"posix\":\n\t        return os.sysconf(\"SC_ARG_MAX\")\n\t    return 20000\n\t\n\tdef _get_args(\n\t    args: Sequence[str],\n\t    max_length: int = _get_platform_max_length(),\n\t    sep: str = \",\",\n\t) -> list[str]:\n\t    \"\"\"\n\t    get the arguments to pass to the command\n\t    \"\"\"\n\t    if len(args) == 0:\n\t        return []\n\t   ", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n\tdef _xargs(\n\t\tcmd: tuple[str, ...],\n\t\tvarargs: Sequence[str],\n\t\t*,\n\t\ttarget_concurrency: int = 1,\n\t\t_max_length: int = _get_platform_max_length(),\n\t\t**kwargs:\n\t\t\"\"\"\n\t\tSimplified Implementation of Xargs in Linux\n\t\t\"\"\"\n\t\tfrom concurrent.futures import ThreadPoolExecutor\n\t\tfrom functools import partial\n\t\tfrom itertools import islice\n\t\tfrom os import system\n\t\tfrom typing import Callable\n\n\t\tif target_concurrency <= 0:\n\t\t\traise ValueError(f'target_concurrency must be greater than 0: {target_concurrency}')\n\n\t\tdef _x(\n\t\t\t\tcmd: tuple[str, ...],\n\t\t\t\targ: str,\n\t\t\t\t**kwargs:\n\t\t\t\t\"\"\"\n\t\t\t\tSimplified Implementation of Xargs in Linux\n\t\t\t\t\"\"\"\n\t\t\t\tsystem(' '.join(cmd) + ' ' + arg, **kwargs)\n\n\t\twith ThreadPoolExecutor(max_workers=target_concurrency) as pool:\n\t\t\t_x_partial: Callable[[str], None] = partial(_x, cmd)\n\t\t\tlist(pool.map(_x", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\"\"\"\n\tSplit a command into chunks and run them in parallel.\n\t:param cmd: The command to run.\n\t:param varargs: The arguments to pass to the command.\n\t:param color: Whether to use color in the output.\n\t:param target_concurrency: The number of concurrent processes to run.\n\t:param _max_length: The maximum length of a line in the output.\n\t:param kwargs: Additional keyword arguments to pass to subprocess.run.\n\t:return: The return code of the command.\n\t\"\"\"\n\tchunks = _chunk(varargs, len(varargs) // target_concurrency)\n\tthreads = []\n\tfor chunk in chunks:\n\t\tthreads.append(threading.Thread(target=_run, args=(cmd, chunk, color, _max_length, kwargs)))\n\tfor thread in threads:\n\t\tthread.start()\n\tfor thread in threads:\n\t\tthread.join()\n\treturn 0\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tdef _get_platform_max_length() -> int:\n\t    \"\"\"Get the maximum length of arguments to pass to a command.\"\"\"\n\t    try:\n\t        import resource\n\t        soft, hard = resource.getrlimit(resource.RLIMIT_ARG_MAX)\n\t        return min(soft, hard)\n\t    except (ImportError, AttributeError, ValueError):\n\t        return 10000\n\t\n\tdef _chunks(lst: Sequence[str], n: int) -> Generator[tuple[str, ...], None, None]:\n\t    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n\t    for i in range(0, len(lst), n):\n\t        yield tuple(lst[i:i + n])\n\t\n\tdef _run(args: Sequence[str]) -> None:\n\t    \"\"\"Run a command with arguments.\"\"\"\n\t    subprocess.run(args, check=True)\n\t\n\tdef _colorize(args: Sequence[str]) -> Sequence[str]:\n\t    \"\"\"Colorize a command with arguments.\"\"\"\n\t    return tuple(f\"\\033[1m{a}\\033[0m\" for a in args)\n\t\n\tdef _process(args: Sequence[str]) -> None:\n\t    \"\"\"Process a", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n\tcmd_string = \" \".join(cmd)\n\t\n\tcmd_list = [cmd_string]\n\tfor arg in varargs:\n\t\tcmd_list.append(arg)\n\t\n\targs_list = [cmd_list]\n\t\n\tfor k, v in kwargs.items():\n\t\targs_list.append(f\"--{k}\")\n\t\targs_list.append(v)\n\t\n\tcmd_list = []\n\t\n\tfor i, arg in enumerate(args_list):\n\t\tcmd_list.append(arg)\n\t\t\n\t\tif i % target_concurrency == target_concurrency - 1:\n\t\t\tcmd = \" \".join(cmd_list)\n\t\t\t\n\t\t\tif color:\n\t\t\t\tcmd = f\"echo {cmd} | xargs -t -n1 -P{target_concurrency}\"\n\t\t\t\n\t\t\tif len(cmd) > _max_length:\n\t\t\t\tprint(f\"Command too long: {cmd}\")\n\t\t\t\tsys.exit(1)\n\t\t\t\n\t\t\tprint(cmd)\n\t\t\t\n\t\t\tprocess = subprocess.Popen(\n\t\t\t\tshlex.split(cmd),\n\t\t\t\tshell=False,\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.PIPE,", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tlength = len(varargs)\n\tsplit = length // target_concurrency\n\tif length % target_concurrency != 0:\n\t\tsplit += 1\n\tcurrent_split = 0\n\targs = []\n\twhile current_split < split:\n\t\targs.append([*cmd, *varargs[current_split * target_concurrency: (current_split + 1) * target_concurrency]])\n\t\tcurrent_split += 1\n\t\n\twith Pool(processes=target_concurrency) as pool:\n\t\tresult = pool.map(_call_process, args)\n\treturn result\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\toutput = []\n\t\n\t# if the number of args is less than the target_concurrency, we can just execute the command\n\tif len(varargs) < target_concurrency:\n\t\tsubprocess.run(cmd + varargs, check=True)\n\t\treturn\n\t\n\t# divide the args into chunks of target_concurrency\n\tchunks = [varargs[i:i + target_concurrency] for i in range(0, len(varargs), target_concurrency)]\n\t\n\t# execute the command for each chunk in parallel\n\twith concurrent.futures.ProcessPoolExecutor() as executor:\n\t\tfutures = [executor.submit(subprocess.run, cmd + chunk, check=True) for chunk in chunks]\n\t\tconcurrent.futures.wait(futures)\n\t\n\treturn output\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport threading\n\timport queue\n\n\targs_queue = queue.Queue()\n\n\tfor arg in varargs:\n\t\targs_queue.put(arg)\n\n\tthreads = []\n\n\tdef process_args():\n\t\twhile not args_queue.empty():\n\t\t\targ = args_queue.get()\n\t\t\tprocess_arg(arg)\n\n\tdef process_arg(arg: str) -> None:\n\t\ttry:\n\t\t\tsubprocess.run(cmd, args=(arg,))\n\t\texcept KeyboardInterrupt:\n\t\t\tsys.exit(1)\n\t\texcept Exception as e:\n\t\t\tprint(e, file=sys.stderr)\n\n\tfor i in range(target_concurrency):\n\t\tthread = threading.Thread(target=process_args)\n\t\tthread.start()\n\t\tthreads.append(thread)\n\n\tfor thread in threads:\n\t\tthread.join()\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n\timport os\n\timport subprocess\n\timport sys\n\t\n\tdef _get_platform_max_length():\n\t\t\"\"\"\n\t\tGet the maximum command length for the current platform.\n\t\t\"\"\"\n\t\tif sys.platform == \"win32\":\n\t\t\treturn 8192\n\t\treturn os.sysconf(\"SC_ARG_MAX\")\n\t\n\tdef _split(args: Sequence[str], max_length: int) -> Iterator[tuple[str, ...]]:\n\t\t\"\"\"\n\t\tSplit a sequence of strings into chunks that are no longer than max_length.\n\t\t\"\"\"\n\t\ti = 0\n\t\twhile i < len(args):\n\t\t\tchunk = args[i:i + target_concurrency]\n\t\t\ti += target_concurrency\n\t\t\twhile len(chunk) > 0 and len(\" \".join(chunk)) > max_length:\n\t\t\t\tchunk = chunk[:-1]\n\t\t\tyield tuple(chunk)\n\t\n\tdef _run(args: tuple[str, ...], **kwargs: str) -> None:\n\t\t\"\"\"\n\t\tRun a command with the given arguments.\n\t\t\"\"\"\n\t\tproc = subprocess.Popen(args, **kwargs)\n\t\tproc.communicate()", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcommand = f\"{cmd[0]} {' '.join(cmd[1:])}\"\n\tbatch_size = target_concurrency\n\ttotal_args = len(varargs)\n\ttotal_batches = total_args // batch_size + (total_args % batch_size > 0)\n\t\n\tfor i in range(0, total_args, batch_size):\n\t\tbatch = varargs[i:i + batch_size]\n\t\tbatch_command = f\"{command} {' '.join(batch)}\"\n\t\tprint(batch_command)\n\t\tif _max_length:\n\t\t\tbatch_command = batch_command[:_max_length]\n\t\tsubprocess.run(batch_command, shell=True)\n\nxargs((\"ls\", \"-l\"), [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], target_concurrency=2)", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\timport subprocess, os, signal\n\tfrom multiprocessing import Pool, cpu_count\n\tfrom itertools import chain\n\tfrom functools import partial\n\tfrom typing import Callable\n\t\n\tdef _run_cmd(cmd: str, **kwargs: dict[str, str]) -> None:\n\t    \"\"\"\n\t    run the command\n\t\n\t    Args:\n\t        cmd (str): command to run\n\t        kwargs (dict, optional): arguments to pass to the command. Defaults to {}.\n\t    \"\"\"\n\t    p = subprocess.Popen(cmd, shell=True, **kwargs)\n\t    p.wait()\n\t    return\n\t\n\tdef _get_platform_max_length() -> int:\n\t    \"\"\"\n\t    get the max length of the command line\n\t    \"\"\"\n\t    if os.name == \"posix\":\n\t        return os.sysconf(\"SC_ARG_MAX\")\n\t    return 20000\n\t\n\tdef _get_args(\n\t    args: Sequence[str],\n\t    max_length: int = _get_platform_max_length(),\n\t    sep: str = \",\",\n\t) -> list[str]:\n\t    \"\"\"\n\t    get the arguments to pass to the command\n\t    \"\"\"\n\t    if len(args) == 0:\n\t        return []\n\t   ", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n\tdef _xargs(\n\t\tcmd: tuple[str, ...],\n\t\tvarargs: Sequence[str],\n\t\t*,\n\t\ttarget_concurrency: int = 1,\n\t\t_max_length: int = _get_platform_max_length(),\n\t\t**kwargs:\n\t\t\"\"\"\n\t\tSimplified Implementation of Xargs in Linux\n\t\t\"\"\"\n\t\tfrom concurrent.futures import ThreadPoolExecutor\n\t\tfrom functools import partial\n\t\tfrom itertools import islice\n\t\tfrom os import system\n\t\tfrom typing import Callable\n\n\t\tif target_concurrency <= 0:\n\t\t\traise ValueError(f'target_concurrency must be greater than 0: {target_concurrency}')\n\n\t\tdef _x(\n\t\t\t\tcmd: tuple[str, ...],\n\t\t\t\targ: str,\n\t\t\t\t**kwargs:\n\t\t\t\t\"\"\"\n\t\t\t\tSimplified Implementation of Xargs in Linux\n\t\t\t\t\"\"\"\n\t\t\t\tsystem(' '.join(cmd) + ' ' + arg, **kwargs)\n\n\t\twith ThreadPoolExecutor(max_workers=target_concurrency) as pool:\n\t\t\t_x_partial: Callable[[str], None] = partial(_x, cmd)\n\t\t\tlist(pool.map(_x", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\"\"\"\n\tSplit a command into chunks and run them in parallel.\n\t:param cmd: The command to run.\n\t:param varargs: The arguments to pass to the command.\n\t:param color: Whether to use color in the output.\n\t:param target_concurrency: The number of concurrent processes to run.\n\t:param _max_length: The maximum length of a line in the output.\n\t:param kwargs: Additional keyword arguments to pass to subprocess.run.\n\t:return: The return code of the command.\n\t\"\"\"\n\tchunks = _chunk(varargs, len(varargs) // target_concurrency)\n\tthreads = []\n\tfor chunk in chunks:\n\t\tthreads.append(threading.Thread(target=_run, args=(cmd, chunk, color, _max_length, kwargs)))\n\tfor thread in threads:\n\t\tthread.start()\n\tfor thread in threads:\n\t\tthread.join()\n\treturn 0\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tdef _get_platform_max_length() -> int:\n\t    \"\"\"Get the maximum length of arguments to pass to a command.\"\"\"\n\t    try:\n\t        import resource\n\t        soft, hard = resource.getrlimit(resource.RLIMIT_ARG_MAX)\n\t        return min(soft, hard)\n\t    except (ImportError, AttributeError, ValueError):\n\t        return 10000\n\t\n\tdef _chunks(lst: Sequence[str], n: int) -> Generator[tuple[str, ...], None, None]:\n\t    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n\t    for i in range(0, len(lst), n):\n\t        yield tuple(lst[i:i + n])\n\t\n\tdef _run(args: Sequence[str]) -> None:\n\t    \"\"\"Run a command with arguments.\"\"\"\n\t    subprocess.run(args, check=True)\n\t\n\tdef _colorize(args: Sequence[str]) -> Sequence[str]:\n\t    \"\"\"Colorize a command with arguments.\"\"\"\n\t    return tuple(f\"\\033[1m{a}\\033[0m\" for a in args)\n\t\n\tdef _process(args: Sequence[str]) -> None:\n\t    \"\"\"Process a", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n\tcmd_string = \" \".join(cmd)\n\t\n\tcmd_list = [cmd_string]\n\tfor arg in varargs:\n\t\tcmd_list.append(arg)\n\t\n\targs_list = [cmd_list]\n\t\n\tfor k, v in kwargs.items():\n\t\targs_list.append(f\"--{k}\")\n\t\targs_list.append(v)\n\t\n\tcmd_list = []\n\t\n\tfor i, arg in enumerate(args_list):\n\t\tcmd_list.append(arg)\n\t\t\n\t\tif i % target_concurrency == target_concurrency - 1:\n\t\t\tcmd = \" \".join(cmd_list)\n\t\t\t\n\t\t\tif color:\n\t\t\t\tcmd = f\"echo {cmd} | xargs -t -n1 -P{target_concurrency}\"\n\t\t\t\n\t\t\tif len(cmd) > _max_length:\n\t\t\t\tprint(f\"Command too long: {cmd}\")\n\t\t\t\tsys.exit(1)\n\t\t\t\n\t\t\tprint(cmd)\n\t\t\t\n\t\t\tprocess = subprocess.Popen(\n\t\t\t\tshlex.split(cmd),\n\t\t\t\tshell=False,\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.PIPE,", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tlength = len(varargs)\n\tsplit = length // target_concurrency\n\tif length % target_concurrency != 0:\n\t\tsplit += 1\n\tcurrent_split = 0\n\targs = []\n\twhile current_split < split:\n\t\targs.append([*cmd, *varargs[current_split * target_concurrency: (current_split + 1) * target_concurrency]])\n\t\tcurrent_split += 1\n\t\n\twith Pool(processes=target_concurrency) as pool:\n\t\tresult = pool.map(_call_process, args)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\toutput = []\n\t\n\t# if the number of args is less than the target_concurrency, we can just execute the command\n\tif len(varargs) < target_concurrency:\n\t\tsubprocess.run(cmd + varargs, check=True)\n\t\treturn\n\t\n\t# divide the args into chunks of target_concurrency\n\tchunks = [varargs[i:i + target_concurrency] for i in range(0, len(varargs), target_concurrency)]\n\t\n\t# execute the command for each chunk in parallel\n\twith concurrent.futures.ProcessPoolExecutor() as executor:\n\t\tfutures = [executor.submit(subprocess.run, cmd + chunk, check=True) for chunk in chunks]\n\t\tconcurrent.futures.wait(futures)\n\t\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport threading\n\timport queue\n\n\targs_queue = queue.Queue()\n\n\tfor arg in varargs:\n\t\targs_queue.put(arg)\n\n\tthreads = []\n\n\tdef process_args():\n\t\twhile not args_queue.empty():\n\t\t\targ = args_queue.get()\n\t\t\tprocess_arg(arg)\n\n\tdef process_arg(arg: str) -> None:\n\t\ttry:\n\t\t\tsubprocess.run(cmd, args=(arg,))\n\t\texcept KeyboardInterrupt:\n\t\t\tsys.exit(1)\n\t\texcept Exception as e:\n\t\t\tprint(e, file=sys.stderr)\n\n\tfor i in range(target_concurrency):\n\t\tthread = threading.Thread(target=process_args)\n\t\tthread.start()\n\t\tthreads.append(thread)\n\n\tfor thread in threads:\n\t\tthread.join()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\tfrom . import FIXED_RANDOM_SEED\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\tshuffled_list = list(seq)\n\trandom.shuffle(shuffled_list)\n\treturn shuffled_list\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tfrom random import Random\n\trandom = Random(0)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tfrom random import Random\n\trng = Random(FIXED_RANDOM_SEED)\n\tout = list(seq)\n\trng.shuffle(out)\n\treturn out\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tshuffled_seq = seq.copy()\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(shuffled_seq)\n\treturn shuffled_seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\trandom.seed()\n\treturn list(seq)\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport numpy as np\n\trng = np.random.default_rng(1234)\n\treturn rng.permutation(seq).tolist()\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tfrom random import Random\n\tfrom random import seed\n\tseed(FIXED_RANDOM_SEED)\n\treturn Random().sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\tfrom . import FIXED_RANDOM_SEED\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\tshuffled_list = list(seq)\n\trandom.shuffle(shuffled_list)\n\treturn shuffled_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tfrom random import Random\n\trandom = Random(0)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tfrom random import Random\n\trng = Random(FIXED_RANDOM_SEED)\n\tout = list(seq)\n\trng.shuffle(out)\n\treturn out\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tshuffled_seq = seq.copy()\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(shuffled_seq)\n\treturn shuffled_seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\trandom.seed()\n\treturn list(seq)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport numpy as np\n\trng = np.random.default_rng(1234)\n\treturn rng.permutation(seq).tolist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tfrom random import Random\n\tfrom random import seed\n\tseed(FIXED_RANDOM_SEED)\n\treturn Random().sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(int(i) for i in s.split(\".\"))\n\texcept ValueError:\n\t\traise ValueError(f\"Cannot parse version {s}\")\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tnums = s.split('.')\n\ttry:\n\t\treturn tuple(int(num) for num in nums)\n\texcept ValueError:\n\t\traise ValueError(f'Not a valid version string: {s}')", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tresult = []\n\tfor item in s.split('.'):\n\t\tresult.append(int(item))\n\treturn tuple(result)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(int, s.split(\".\")))\n\texcept ValueError:\n\t\traise ValueError(\"invalid version string: \" + s)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tversion = ()\n\tfor item in s.split('.'):\n\t\tversion += (int(item),)\n\treturn version\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(int(part) for part in s.split('.'))\n\texcept ValueError:\n\t\traise ValueError(f'Invalid version string: {s}')\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(int, s.split('.')))\n\texcept ValueError:\n\t\traise ValueError(f\"Invalid version: {s}\")\n\n__all__ = [\n\t'parse_version',\n]", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\tif len(parts) != 3:\n\t\traise ValueError(f'Invalid version {s}: {len(parts)} parts instead of 3')\n\n\ttry:\n\t\tversion = tuple(int(part) for part in parts)\n\texcept ValueError:\n\t\traise ValueError(f'Invalid version {s}')\n\n\treturn version\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(int, s.split('.')))\n\texcept Exception as e:\n\t\traise ValueError(f'Invalid version string: {s}') from e\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\treturn tuple(map(int, parts))\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(int(i) for i in s.split(\".\"))\n\texcept ValueError:\n\t\traise ValueError(f\"Cannot parse version {s}\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tnums = s.split('.')\n\ttry:\n\t\treturn tuple(int(num) for num in nums)\n\texcept ValueError:\n\t\traise ValueError(f'Not a valid version string: {s}')", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tresult = []\n\tfor item in s.split('.'):\n\t\tresult.append(int(item))\n\treturn tuple(result)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(int, s.split(\".\")))\n\texcept ValueError:\n\t\traise ValueError(\"invalid version string: \" + s)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tversion = ()\n\tfor item in s.split('.'):\n\t\tversion += (int(item),)\n\treturn version\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(int(part) for part in s.split('.'))\n\texcept ValueError:\n\t\traise ValueError(f'Invalid version string: {s}')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(int, s.split('.')))\n\texcept ValueError:\n\t\traise ValueError(f\"Invalid version: {s}\")\n\n__all__ = [\n\t'parse_version',\n]", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\tif len(parts) != 3:\n\t\traise ValueError(f'Invalid version {s}: {len(parts)} parts instead of 3')\n\n\ttry:\n\t\tversion = tuple(int(part) for part in parts)\n\texcept ValueError:\n\t\traise ValueError(f'Invalid version {s}')\n\n\treturn version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(int, s.split('.')))\n\texcept Exception as e:\n\t\traise ValueError(f'Invalid version string: {s}') from e\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\treturn tuple(map(int, parts))\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = list(cmd)\n\tif cmd[0].startswith('\"'):\n\t\tcmd[0] = cmd[0][1:]\n\tif cmd[-1].endswith('\"'):\n\t\tcmd[-1] = cmd[-1][:-1]\n\treturn tuple(cmd)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = list(cmd)\n\tcmd[0] = cmd[0].split('/')[-1]\n\treturn tuple(cmd)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe_path = cmd[0].split(\"/\")\n\treturn (exe_path[-1],) + cmd[1:]\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tnew_cmd = []\n\tfor item in cmd:\n\t\tif item.startswith('\"') and item.endswith('\"'):\n\t\t\tnew_cmd.append(item[1:-1])\n\t\telse:\n\t\t\tnew_cmd.append(item)\n\treturn tuple(new_cmd)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe, *args = cmd\n\texe = shutil.which(exe) or exe\n\treturn (exe, *args)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tif \"\\\\\" in exe:\n\t\texe = exe.split(\"\\\\\")[-1]\n\n\tif \" \" in exe:\n\t\texe = f'\"{exe}\"'\n\n\treturn (exe,) + cmd[1:]\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe, *other = cmd\n\tif exe.startswith(\"./\"):\n\t\texe = exe[2:]\n\treturn (exe, *other)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tfrom os import path\n\treturn tuple(path.normpath(x) for x in cmd)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tres = []\n\tfor i in cmd:\n\t\tif i.startswith('\"') and i.endswith('\"'):\n\t\t\tres.append(i[1:-1])\n\t\telse:\n\t\t\tres.append(i)\n\treturn tuple(res)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = cmd[0].split(\" \")\n\tfor i in range(len(cmd)):\n\t\tif cmd[i] == \"${EXE}\":\n\t\t\tcmd[i] = \"${EXE_PATH}\"\n\t\tif cmd[i] == \"${DAT}\":\n\t\t\tcmd[i] = \"${DAT_PATH}\"\n\t\tif cmd[i] == \"${BIN}\":\n\t\t\tcmd[i] = \"${BIN_PATH}\"\n\treturn tuple(cmd)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = list(cmd)\n\tif cmd[0].startswith('\"'):\n\t\tcmd[0] = cmd[0][1:]\n\tif cmd[-1].endswith('\"'):\n\t\tcmd[-1] = cmd[-1][:-1]\n\treturn tuple(cmd)", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = list(cmd)\n\tcmd[0] = cmd[0].split('/')[-1]\n\treturn tuple(cmd)", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe_path = cmd[0].split(\"/\")\n\treturn (exe_path[-1],) + cmd[1:]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tnew_cmd = []\n\tfor item in cmd:\n\t\tif item.startswith('\"') and item.endswith('\"'):\n\t\t\tnew_cmd.append(item[1:-1])\n\t\telse:\n\t\t\tnew_cmd.append(item)\n\treturn tuple(new_cmd)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe, *args = cmd\n\texe = shutil.which(exe) or exe\n\treturn (exe, *args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tif \"\\\\\" in exe:\n\t\texe = exe.split(\"\\\\\")[-1]\n\n\tif \" \" in exe:\n\t\texe = f'\"{exe}\"'\n\n\treturn (exe,) + cmd[1:]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe, *other = cmd\n\tif exe.startswith(\"./\"):\n\t\texe = exe[2:]\n\treturn (exe, *other)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tfrom os import path\n\treturn tuple(path.normpath(x) for x in cmd)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tres = []\n\tfor i in cmd:\n\t\tif i.startswith('\"') and i.endswith('\"'):\n\t\t\tres.append(i[1:-1])\n\t\telse:\n\t\t\tres.append(i)\n\treturn tuple(res)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = cmd[0].split(\" \")\n\tfor i in range(len(cmd)):\n\t\tif cmd[i] == \"${EXE}\":\n\t\t\tcmd[i] = \"${EXE_PATH}\"\n\t\tif cmd[i] == \"${DAT}\":\n\t\t\tcmd[i] = \"${DAT_PATH}\"\n\t\tif cmd[i] == \"${BIN}\":\n\t\t\tcmd[i] = \"${BIN_PATH}\"\n\treturn tuple(cmd)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\telse:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif cache is None:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tvalue = cache.get(key(*args, **kwargs))\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key(*args, **kwargs), value)\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.warn('%s: %s', e.__class__.__name__, e)\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\targskey = key(*args, **kwargs)\n\t\t\tkwargskey = key(*args, **kwargs)\n\t\t\tresult = cache.get(argskey)\n\t\t\tif result is None:\n\t\t\t\tresult = cache.get(kwargskey)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(argskey, result)\n\t\t\t\tcache.set(kwargskey, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef newfunc(*args, **kwargs):\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn newfunc\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(f, *args, **kwargs)\n\n\t\t\tif cache.has_key(key):\n\t\t\t\treturn cache[key]\n\n\t\t\tresult = f(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorator", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tif lock is not None:\n\t\t\tl = lock\n\t\telse:\n\t\t\tl = threading.Lock()\n\n\t\tdef wrapped(*args, **kwargs):\n\t\t\twith l:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\t\treturn result\n\t\t\t\texcept TypeError:\n\t\t\t\t\treturn f(*args, **kwargs)\n\t\treturn wrapped\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(f, *args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tvalue = f(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock is not None and lock.acquire(False):\n\t\t\t\ttry:\n\t\t\t\t\tif key in cache:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\tresult = function(*args, **kwargs)\n\t\t\tif lock is not None and lock.acquire(False):\n\t\t\t\ttry:\n\t\t\t\t\tcache[key] = result\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif key is None:\n\t\t\t\tk = hashkey(func, args, kwargs)\n\t\t\telse:\n\t\t\t\tk = key(func, args, kwargs)\n\t\t\tresult = cache.get(k)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(k, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef _cached(func):\n\t\tdef __cached(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\tv = cache.get(k)\n\t\t\tif v is None:\n\t\t\t\twith lock:\n\t\t\t\t\tv = cache.get(k)\n\t\t\t\t\tif v is None:\n\t\t\t\t\t\tv = func(*args, **kwargs)\n\t\t\t\t\t\tcache.set(k, v)\n\t\t\treturn v\n\t\treturn __cached\n\treturn _cached\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\telse:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif cache is None:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tvalue = cache.get(key(*args, **kwargs))\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key(*args, **kwargs), value)\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.warn('%s: %s', e.__class__.__name__, e)\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\targskey = key(*args, **kwargs)\n\t\t\tkwargskey = key(*args, **kwargs)\n\t\t\tresult = cache.get(argskey)\n\t\t\tif result is None:\n\t\t\t\tresult = cache.get(kwargskey)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(argskey, result)\n\t\t\t\tcache.set(kwargskey, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef newfunc(*args, **kwargs):\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn newfunc\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(f, *args, **kwargs)\n\n\t\t\tif cache.has_key(key):\n\t\t\t\treturn cache[key]\n\n\t\t\tresult = f(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tif lock is not None:\n\t\t\tl = lock\n\t\telse:\n\t\t\tl = threading.Lock()\n\n\t\tdef wrapped(*args, **kwargs):\n\t\t\twith l:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\t\treturn result\n\t\t\t\texcept TypeError:\n\t\t\t\t\treturn f(*args, **kwargs)\n\t\treturn wrapped\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(f, *args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tvalue = f(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock is not None and lock.acquire(False):\n\t\t\t\ttry:\n\t\t\t\t\tif key in cache:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\tresult = function(*args, **kwargs)\n\t\t\tif lock is not None and lock.acquire(False):\n\t\t\t\ttry:\n\t\t\t\t\tcache[key] = result\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif key is None:\n\t\t\t\tk = hashkey(func, args, kwargs)\n\t\t\telse:\n\t\t\t\tk = key(func, args, kwargs)\n\t\t\tresult = cache.get(k)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(k, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef _cached(func):\n\t\tdef __cached(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\tv = cache.get(k)\n\t\t\tif v is None:\n\t\t\t\twith lock:\n\t\t\t\t\tv = cache.get(k)\n\t\t\t\t\tif v is None:\n\t\t\t\t\t\tv = func(*args, **kwargs)\n\t\t\t\t\t\tcache.set(k, v)\n\t\t\treturn v\n\t\treturn __cached\n\treturn _cached\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tkey_name = _make_key(user_function, typed, False)\n\t\tcache = LRUCache(maxsize)\n\t\tlock = RLock()\n\t\tupdate_wrapper(wrapper_function, user_function)\n\n\t\t@wraps(user_function)\n\t\tdef wrapper_function(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tkey = key_name(args, kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tresult, last_update = cache[key]\n\t\t\t\t\tif (timer() - last_update) < ttl:\n\t\t\t\t\t\treturn result\n\t\t\t\t\tdel cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result, timer()\n\t\t\t\treturn result\n\n\t\treturn wrapper_function\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom functools import lru_cache, wraps\n\tfrom collections import OrderedDict\n\tfrom threading import RLock\n\tfrom collections.abc import Callable\n\timport time\n\timport logging\n\n\tclass TTLOrderedDict(OrderedDict):\n\t\tdef __init__(self, *args, ttl: int, **kwargs):\n\t\t\tself.ttl = ttl\n\t\t\tsuper().__init__(*args, **kwargs)\n\n\t\tdef __setitem__(self, key, value):\n\t\t\tsuper().__setitem__(key, (value, self.ttl + timer()))\n\n\t\tdef __getitem__(self, key):\n\t\t\tvalue, expiration = super().__getitem__(key)\n\t\t\tif expiration <= timer():\n\t\t\t\traise KeyError(key)\n\t\t\treturn value\n\n\t\tdef __contains__(self, key):\n\t\t\ttry:\n\t\t\t\tself[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn False\n\t\t\treturn True\n\n\t\tdef get(self, key, default=None):\n\t\t\ttry:\n\t\t\t\treturn self[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn default\n\n\t\tdef pop(self, key, default=None):\n\t\t\ttry:\n\t\t\t\tvalue = super().pop(key)[", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\tif typed:\n\t\t\tcache = lru_cache(maxsize, typed=typed)\n\t\telse:\n\t\t\tcache = lru_cache(maxsize)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache(user_function, *args, **kwds)\n\n\t\t\tif timer() - result.cache_info().hit_time > ttl:\n\t\t\t\tcache.cache_clear()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(ttl_cache_wrapper(\n\t\tuser_function, timer, ttl))\n\treturn decorating_function", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom functools import lru_cache, wraps\n\tfrom collections import OrderedDict\n\tfrom math import inf\n\tfrom time import perf_counter\n\n\tdef ttl_cache(maxsize=None, ttl=None, timer=None):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable\n\t\tthat saves up to `maxsize` results based on a Least\n\t\tRecently Used (LRU) algorithm with a per-item time-to-live\n\t\t(TTL) value.\n\t\t\"\"\"\n\n\t\ttimer = timer or (lambda: perf_counter())\n\t\tis_async = asyncio.iscoroutinefunction\n\t\tis_async_gen = asyncio.iscoroutine\n\t\tget_loop = asyncio.get_event_loop\n\n\t\tif is_async(func):\n\t\t\tfrom functools import cache\n\t\t\tcache_type = cache\n\t\t\tis_coro = is_async\n\n\t\t\tdef _wrapper(coro):\n\t\t\t\tasync def wrapper(*args, **kwargs):\n\t\t\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\t\t\ttry:\n\t\t\t\t\t\texpiry, value = cache[key]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tvalue = await cor", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfunc = user_function\n\t\tlru_cache = LRUCache(maxsize)\n\t\tcall_cache = WeakKeyDictionary()\n\t\tlock = RLock()\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tcache_info = lru_cache.cache_info()\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tif cache_info.currsize and cache_info.hits > cache_info.maxsize / 2:\n\t\t\t\t\t\tlru_cache.clear()\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += (user_function.__module__,)\n\t\t\texpiration = call_cache.get(key)\n\t\t\tnow = timer()\n\t\t\tif expiration and now < expiration:\n\t\t\t\twith lock:\n\t\t\t\t\tresult = lru_cache[key]\n\t\t\t\t\treturn result\n\t\t\twith lock:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tlru_cache[key] = result\n\t\t\t\tcall_cache[key] = now + ttl\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n\nclass", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = LRUCache(maxsize, ttl)\n\tcache.timer = timer\n\tcache.typed = typed\n\n\tdef decorating_function(user_function):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tkey = self._cache_key(user_function, args, kwargs)\n\t\t\ttry:\n\t\t\t\tvalue, expiry = cache[key]\n\t\t\t\tif not expiry:\n\t\t\t\t\traise KeyError\n\t\t\texcept KeyError:\n\t\t\t\tvalue = user_function(self, *args, **kwargs)\n\t\t\t\tcache[key] = value, expiry\n\t\t\treturn value\n\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper_cache(user_function):\n\t\tfunc = user_function\n\t\tcache = LRUCache(maxsize)\n\t\tlock = RLock()\n\n\t\t@wraps(user_function)\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\tkey = (args, tuple(sorted(kwargs.items())))\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tcache_entry = cache[key]\n\t\t\t\t\tif cache_entry[1] + ttl >= timer():\n\t\t\t\t\t\treturn cache_entry[0]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\n\t\t\twith lock:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = (result, timer())\n\t\t\treturn result\n\t\treturn wrapped_func\n\treturn wrapper_cache\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t\"\"\"\n\t\tWrapper for function to return.\n\t\t\"\"\"\n\t\t...\n\t\t...\n\t\t...\n\t\t...\n\t\t...\n\t\t...\n\t\treturn wrapper\n\treturn decorating_function"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tkey_name = _make_key(user_function, typed, False)\n\t\tcache = LRUCache(maxsize)\n\t\tlock = RLock()\n\t\tupdate_wrapper(wrapper_function, user_function)\n\n\t\t@wraps(user_function)\n\t\tdef wrapper_function(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tkey = key_name(args, kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tresult, last_update = cache[key]\n\t\t\t\t\tif (timer() - last_update) < ttl:\n\t\t\t\t\t\treturn result\n\t\t\t\t\tdel cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result, timer()\n\t\t\t\treturn result\n\n\t\treturn wrapper_function\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom functools import lru_cache, wraps\n\tfrom collections import OrderedDict\n\tfrom threading import RLock\n\tfrom collections.abc import Callable\n\timport time\n\timport logging\n\n\tclass TTLOrderedDict(OrderedDict):\n\t\tdef __init__(self, *args, ttl: int, **kwargs):\n\t\t\tself.ttl = ttl\n\t\t\tsuper().__init__(*args, **kwargs)\n\n\t\tdef __setitem__(self, key, value):\n\t\t\tsuper().__setitem__(key, (value, self.ttl + timer()))\n\n\t\tdef __getitem__(self, key):\n\t\t\tvalue, expiration = super().__getitem__(key)\n\t\t\tif expiration <= timer():\n\t\t\t\traise KeyError(key)\n\t\t\treturn value\n\n\t\tdef __contains__(self, key):\n\t\t\ttry:\n\t\t\t\tself[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn False\n\t\t\treturn True\n\n\t\tdef get(self, key, default=None):\n\t\t\ttry:\n\t\t\t\treturn self[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn default\n\n\t\tdef pop(self, key, default=None):\n\t\t\ttry:\n\t\t\t\tvalue = super().pop(key)[", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\tif typed:\n\t\t\tcache = lru_cache(maxsize, typed=typed)\n\t\telse:\n\t\t\tcache = lru_cache(maxsize)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache(user_function, *args, **kwds)\n\n\t\t\tif timer() - result.cache_info().hit_time > ttl:\n\t\t\t\tcache.cache_clear()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(ttl_cache_wrapper(\n\t\tuser_function, timer, ttl))\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom functools import lru_cache, wraps\n\tfrom collections import OrderedDict\n\tfrom math import inf\n\tfrom time import perf_counter\n\n\tdef ttl_cache(maxsize=None, ttl=None, timer=None):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable\n\t\tthat saves up to `maxsize` results based on a Least\n\t\tRecently Used (LRU) algorithm with a per-item time-to-live\n\t\t(TTL) value.\n\t\t\"\"\"\n\n\t\ttimer = timer or (lambda: perf_counter())\n\t\tis_async = asyncio.iscoroutinefunction\n\t\tis_async_gen = asyncio.iscoroutine\n\t\tget_loop = asyncio.get_event_loop\n\n\t\tif is_async(func):\n\t\t\tfrom functools import cache\n\t\t\tcache_type = cache\n\t\t\tis_coro = is_async\n\n\t\t\tdef _wrapper(coro):\n\t\t\t\tasync def wrapper(*args, **kwargs):\n\t\t\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\t\t\ttry:\n\t\t\t\t\t\texpiry, value = cache[key]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tvalue = await cor", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfunc = user_function\n\t\tlru_cache = LRUCache(maxsize)\n\t\tcall_cache = WeakKeyDictionary()\n\t\tlock = RLock()\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tcache_info = lru_cache.cache_info()\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tif cache_info.currsize and cache_info.hits > cache_info.maxsize / 2:\n\t\t\t\t\t\tlru_cache.clear()\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += (user_function.__module__,)\n\t\t\texpiration = call_cache.get(key)\n\t\t\tnow = timer()\n\t\t\tif expiration and now < expiration:\n\t\t\t\twith lock:\n\t\t\t\t\tresult = lru_cache[key]\n\t\t\t\t\treturn result\n\t\t\twith lock:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tlru_cache[key] = result\n\t\t\t\tcall_cache[key] = now + ttl\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n\nclass", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = LRUCache(maxsize, ttl)\n\tcache.timer = timer\n\tcache.typed = typed\n\n\tdef decorating_function(user_function):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tkey = self._cache_key(user_function, args, kwargs)\n\t\t\ttry:\n\t\t\t\tvalue, expiry = cache[key]\n\t\t\t\tif not expiry:\n\t\t\t\t\traise KeyError\n\t\t\texcept KeyError:\n\t\t\t\tvalue = user_function(self, *args, **kwargs)\n\t\t\t\tcache[key] = value, expiry\n\t\t\treturn value\n\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper_cache(user_function):\n\t\tfunc = user_function\n\t\tcache = LRUCache(maxsize)\n\t\tlock = RLock()\n\n\t\t@wraps(user_function)\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\tkey = (args, tuple(sorted(kwargs.items())))\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tcache_entry = cache[key]\n\t\t\t\t\tif cache_entry[1] + ttl >= timer():\n\t\t\t\t\t\treturn cache_entry[0]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\n\t\t\twith lock:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = (result, timer())\n\t\t\treturn result\n\t\treturn wrapped_func\n\treturn wrapper_cache\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t\"\"\"\n\t\tWrapper for function to return.\n\t\t\"\"\"\n\t\t...\n\t\t...\n\t\t...\n\t\t...\n\t\t...\n\t\t...\n\t\treturn wrapper\n\treturn decorating_function", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakValueDictionary\n\t\tcache = WeakValueDictionary()\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif typed: key += (tuple(kwds.items()),)\n\t\t\tif key in cache:\n\t\t\t\tresult = cache[key]\n\t\t\telse:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable that saves\n\t\tup to `maxsize` results based on a Most Recently Used (MRU)\n\t\talgorithm.\n\t\t\"\"\"\n\t\tfrom functools import wraps\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key = cache.make_key(args, kwargs)\n\t\t\tif key in cache:\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tval = user_function(*args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tcache[key] = val\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\t\t\t\treturn val\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tkwd_mark = (object(),)\n\n\tdef cache_info():\n\t\treturn cache\n\n\tdef cache_clear():\n\t\tnonlocal cache\n\t\tcache = {}\n\n\tdef _make_key(args, kwds):\n\t\tkey = args\n\t\tif kwds:\n\t\t\tkey += kwd_mark\n\t\t\tfor item in kwds.items():\n\t\t\t\tkey += item\n\t\treturn key\n\n\tdef _verify_maxsize(size):\n\t\tmaxsize_error = 'maxsize must be positive integer'\n\t\ttry:\n\t\t\tmaxsize_int = int(size)\n\t\texcept (TypeError, ValueError):\n\t\t\traise TypeError(maxsize_error)\n\t\tif maxsize_int <= 0:\n\t\t\traise ValueError(maxsize_error)\n\t\treturn maxsize_int\n\n\tdef _verify_typed(key, value):\n\t\tif typed:\n\t\t\tif not isinstance(key, type(value)):\n\t\t\t\traise TypeError('Cached value has different type to value')\n\t\treturn key\n\n\tdef _verify_args(key, args, kwds):\n\t\tif args in cache:\n\t\t\tresult = cache[args]\n\t\t\ttry:\n\t\t\t\tresult.value", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom inspect import isclass\n\t\tfrom collections import OrderedDict\n\t\tfrom threading import RLock\n\t\timport sys\n\t\tfrom types import MethodType\n\n\t\tif sys.version_info[0] == 2:\n\t\t\tfrom itertools import izip as zip\n\t\t\tfrom itertools import imap as map\n\t\telif sys.version_info[0] == 3:\n\t\t\tpass\n\t\telse:\n\t\t\traise RuntimeError(\"Only Python 2 and Python 3 are supported\")\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += tuple(map(type, args))\n\t\t\ttry:\n\t\t\t\tkey += tuple(kwds.items())\n\t\t\texcept TypeError:\n\t\t\t\t# We have non-hashable **kwds; ignore them\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tif len(cache) >= maxsize and len(cache", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import update_wrapper, partial\n\t\tfrom collections import OrderedDict\n\t\tmemo = OrderedDict()\n\t\thits = misses = 0\n\t\tK = user_function.__name__ + '.cache_info'\n\n\t\t@update_wrapper(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal memo, hits, misses\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\ttry:\n\t\t\t\tresult = memo[key]\n\t\t\t\thits += 1\n\t\t\t\tmemo.move_to_end(key)\n\t\t\t\treturn result\n\t\t\texcept KeyError:\n\t\t\t\tmisses += 1\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tif len(memo) == maxsize:\n\t\t\t\t\tmemo.popitem(last=False)\n\t\t\t\tmemo[key] = result\n\t\t\t\treturn result\n\t\tdef cache_info():\n\t\t\tnonlocal hits, misses\n\t\t\treturn CacheInfo(hits, misses, len(memo), maxsize, memo.currsize())\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = mem", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakKeyDictionary\n\n\t\tcache = WeakKeyDictionary()\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\tif typed:\n\t\t\t\tkey += tuple((type(arg), type(kwds[arg])) for arg in args)\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tcache[key] = user_function(*args, **kwds)\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem()\n\t\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn decorating_function\n\n\n@mru_cache(maxsize=32)", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakValueDictionary\n\t\tmemo = WeakValueDictionary()\n\t\targ_count = user_function.func_code.co_argcount\n\t\tcache_get = memo.get\n\t\tcache_set = memo.__setitem__\n\t\tcache_del = memo.pop\n\t\tcache_clear = memo.clear\n\t\tcache_contains = memo.__contains__\n\t\tcache_keys = memo.keys\n\t\tcache_values = memo.values\n\t\tcache_items = memo.items\n\t\tcache_getsize = lambda: len(memo)\n\t\tcache_setsize = lambda n: n\n\t\tcache_getsize = cache_setsize\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tfrom sys import version_info\n\t\t\tif version_info[0] == 2:\n\t\t\t\tfrom operator import itemgetter\n\t\t\t\tkey = args\n\t\t\telse:\n\t\t\t\tfrom operator import itemgetter\n\t\t\t\tkey = args, frozenset(kwds.items())\n\t\t\ttry:\n\t\t\t\tresult = cache_get(key, None)\n\t\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import OrderedDict\n\t\tmemo = OrderedDict()\n\t\thits = misses = 0\n\t\tcache_get = memo.get\n\t\tcache_len = len\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\ttry:\n\t\t\t\tresult = cache_get(key, None)\n\t\t\t\tif result is not None:\n\t\t\t\t\tmisses += 1\n\t\t\t\t\thits += 1\n\t\t\t\t\tmemo.move_to_end(key)\n\t\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tmisses += 1\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tif cache_len(memo) >= maxsize:\n\t\t\t\tmemo.popitem(last=False)\n\t\t\tmemo[key] = result\n\t\t\treturn result\n\n\t\twrapper.cache_info = mru_cache_info\n\t\twrapper.cache_clear = mru_cache_clear\n\t\treturn wrapper\n\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\tfrom types import SimpleNamespace\n\n\t\tcache = lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\t\tcache.clear = cache.cache_clear\n\t\tcache.current_size = cache.cache_info().currsize\n\t\tcache.max_size = maxsize\n\t\tcache.hits = cache.cache_info().hits\n\t\tcache.misses = cache.cache_info().misses\n\t\tcache.max_size = cache.cache_info().maxsize\n\t\tcache.curr_size = cache.cache_info().currsize\n\t\tcache.keys = cache.cache_keys\n\t\tcache.values = cache.cache_values\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args, tuple(sorted(kwds.items()))\n\t\t\tcache.hits += not cache.misses\n\t\t\tcache.misses += not cache.hits\n\t\t\treturn cache(*args, **kwds)\n\t\twrapper = update_wrapper(wrapper, user_function)\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakValueDictionary\n\t\tcache = WeakValueDictionary()\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif typed: key += (tuple(kwds.items()),)\n\t\t\tif key in cache:\n\t\t\t\tresult = cache[key]\n\t\t\telse:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable that saves\n\t\tup to `maxsize` results based on a Most Recently Used (MRU)\n\t\talgorithm.\n\t\t\"\"\"\n\t\tfrom functools import wraps\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key = cache.make_key(args, kwargs)\n\t\t\tif key in cache:\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tval = user_function(*args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tcache[key] = val\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\t\t\t\treturn val\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tkwd_mark = (object(),)\n\n\tdef cache_info():\n\t\treturn cache\n\n\tdef cache_clear():\n\t\tnonlocal cache\n\t\tcache = {}\n\n\tdef _make_key(args, kwds):\n\t\tkey = args\n\t\tif kwds:\n\t\t\tkey += kwd_mark\n\t\t\tfor item in kwds.items():\n\t\t\t\tkey += item\n\t\treturn key\n\n\tdef _verify_maxsize(size):\n\t\tmaxsize_error = 'maxsize must be positive integer'\n\t\ttry:\n\t\t\tmaxsize_int = int(size)\n\t\texcept (TypeError, ValueError):\n\t\t\traise TypeError(maxsize_error)\n\t\tif maxsize_int <= 0:\n\t\t\traise ValueError(maxsize_error)\n\t\treturn maxsize_int\n\n\tdef _verify_typed(key, value):\n\t\tif typed:\n\t\t\tif not isinstance(key, type(value)):\n\t\t\t\traise TypeError('Cached value has different type to value')\n\t\treturn key\n\n\tdef _verify_args(key, args, kwds):\n\t\tif args in cache:\n\t\t\tresult = cache[args]\n\t\t\ttry:\n\t\t\t\tresult.value", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom inspect import isclass\n\t\tfrom collections import OrderedDict\n\t\tfrom threading import RLock\n\t\timport sys\n\t\tfrom types import MethodType\n\n\t\tif sys.version_info[0] == 2:\n\t\t\tfrom itertools import izip as zip\n\t\t\tfrom itertools import imap as map\n\t\telif sys.version_info[0] == 3:\n\t\t\tpass\n\t\telse:\n\t\t\traise RuntimeError(\"Only Python 2 and Python 3 are supported\")\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += tuple(map(type, args))\n\t\t\ttry:\n\t\t\t\tkey += tuple(kwds.items())\n\t\t\texcept TypeError:\n\t\t\t\t# We have non-hashable **kwds; ignore them\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tif len(cache) >= maxsize and len(cache", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import update_wrapper, partial\n\t\tfrom collections import OrderedDict\n\t\tmemo = OrderedDict()\n\t\thits = misses = 0\n\t\tK = user_function.__name__ + '.cache_info'\n\n\t\t@update_wrapper(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal memo, hits, misses\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\ttry:\n\t\t\t\tresult = memo[key]\n\t\t\t\thits += 1\n\t\t\t\tmemo.move_to_end(key)\n\t\t\t\treturn result\n\t\t\texcept KeyError:\n\t\t\t\tmisses += 1\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tif len(memo) == maxsize:\n\t\t\t\t\tmemo.popitem(last=False)\n\t\t\t\tmemo[key] = result\n\t\t\t\treturn result\n\t\tdef cache_info():\n\t\t\tnonlocal hits, misses\n\t\t\treturn CacheInfo(hits, misses, len(memo), maxsize, memo.currsize())\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = mem", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakKeyDictionary\n\n\t\tcache = WeakKeyDictionary()\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\tif typed:\n\t\t\t\tkey += tuple((type(arg), type(kwds[arg])) for arg in args)\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tcache[key] = user_function(*args, **kwds)\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem()\n\t\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn decorating_function\n\n\n@mru_cache(maxsize=32)", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakValueDictionary\n\t\tmemo = WeakValueDictionary()\n\t\targ_count = user_function.func_code.co_argcount\n\t\tcache_get = memo.get\n\t\tcache_set = memo.__setitem__\n\t\tcache_del = memo.pop\n\t\tcache_clear = memo.clear\n\t\tcache_contains = memo.__contains__\n\t\tcache_keys = memo.keys\n\t\tcache_values = memo.values\n\t\tcache_items = memo.items\n\t\tcache_getsize = lambda: len(memo)\n\t\tcache_setsize = lambda n: n\n\t\tcache_getsize = cache_setsize\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tfrom sys import version_info\n\t\t\tif version_info[0] == 2:\n\t\t\t\tfrom operator import itemgetter\n\t\t\t\tkey = args\n\t\t\telse:\n\t\t\t\tfrom operator import itemgetter\n\t\t\t\tkey = args, frozenset(kwds.items())\n\t\t\ttry:\n\t\t\t\tresult = cache_get(key, None)\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import OrderedDict\n\t\tmemo = OrderedDict()\n\t\thits = misses = 0\n\t\tcache_get = memo.get\n\t\tcache_len = len\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\ttry:\n\t\t\t\tresult = cache_get(key, None)\n\t\t\t\tif result is not None:\n\t\t\t\t\tmisses += 1\n\t\t\t\t\thits += 1\n\t\t\t\t\tmemo.move_to_end(key)\n\t\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tmisses += 1\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tif cache_len(memo) >= maxsize:\n\t\t\t\tmemo.popitem(last=False)\n\t\t\tmemo[key] = result\n\t\t\treturn result\n\n\t\twrapper.cache_info = mru_cache_info\n\t\twrapper.cache_clear = mru_cache_clear\n\t\treturn wrapper\n\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\tfrom types import SimpleNamespace\n\n\t\tcache = lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\t\tcache.clear = cache.cache_clear\n\t\tcache.current_size = cache.cache_info().currsize\n\t\tcache.max_size = maxsize\n\t\tcache.hits = cache.cache_info().hits\n\t\tcache.misses = cache.cache_info().misses\n\t\tcache.max_size = cache.cache_info().maxsize\n\t\tcache.curr_size = cache.cache_info().currsize\n\t\tcache.keys = cache.cache_keys\n\t\tcache.values = cache.cache_values\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args, tuple(sorted(kwds.items()))\n\t\t\tcache.hits += not cache.misses\n\t\t\tcache.misses += not cache.hits\n\t\t\treturn cache(*args, **kwds)\n\t\twrapper = update_wrapper(wrapper, user_function)\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import OrderedDict\n\t\tmemo = OrderedDict()\n\t\thits = misses = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += (user_function.__module__, user_function.__name__)\n\t\t\ttry:\n\t\t\t\tres = memo[key]\n\t\t\t\thits += 1\n\t\t\texcept KeyError:\n\t\t\t\tres = user_function(*args, **kwds)\n\t\t\t\tmemo[key] = res\n\t\t\t\tmisses += 1\n\t\t\t\tif len(memo) > maxsize:\n\t\t\t\t\toldest = next(iter(memo))\n\t\t\t\t\tdel memo[oldest]\n\t\t\telse:\n\t\t\t\tmemo.move_to_end(key)\n\t\t\treturn res\n\n\t\tdef cache_info():\n\t\t\treturn CacheInfo(hits=hits, misses=misses, maxsize=maxsize)\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = lambda: memo.clear()\n\t\twrapper.cache_", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t_lru_cache_wrapper.cache_clear()\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = _lru_cache_wrapper.make_lru_key(args, kwds, typed=typed)\n\t\t\tresult = _lru_cache_wrapper.cache[key]\n\t\t\tif result is _lru_cache_wrapper.MISS:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t_lru_cache_wrapper.cache[key] = result\n\t\t\treturn result\n\t\t_lru_cache_wrapper.update_wrapper(wrapper, user_function)\n\t\t_lru_cache_wrapper.cache = LRUCache(maxsize)\n\t\t_lru_cache_wrapper.UserFunction = user_function\n\t\treturn wrapper\n\treturn decorating_function\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper_func(*args, **kwargs):\n\t\t\treturn user_function(*args, **kwargs)\n\t\treturn wrapper_func\n\treturn decorating_function", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import deque\n\t\tfrom threading import RLock\n\t\tcache = {}\n\t\tlock = RLock()\n\t\tcall_queue = deque()\n\t\thits = misses = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = (args, frozenset(kwds.items()))\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tcall_queue.remove(key)\n\t\t\t\t\tcall_queue.append(key)\n\t\t\t\t\thits += 1\n\t\t\t\t\treturn result\n\t\t\t\texcept KeyError:\n\t\t\t\t\tmisses += 1\n\n\t\t\tresult = user_function(*args, **kwds)\n\n\t\t\twith lock:\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\toldest = call_queue.popleft()\n\t\t\t\t\tdel cache[oldest]\n\t\t\t\tcache[key] = result\n\t\t\t\tcall_queue.append(key)\n\t\t\treturn result\n\n\t\twrapper.hits = property(lambda: hits)\n\t\twrapper.misses = property(lambda: misses)\n\t", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache as _lru_cache\n\t\treturn _lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function\n\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import deque, defaultdict\n\t\tcache = deque(maxlen=maxsize)\n\t\tkey_cache = defaultdict(deque)\n\t\tkey_history = {}\n\t\tcache_history = {}\n\t\ttype_key_cache = defaultdict(dict)\n\t\ttype_cache_history = defaultdict(dict)\n\t\thits = misses = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\tif key in key_history:\n\t\t\t\thistory = key_history[key]\n\t\t\t\thits += 1\n\t\t\t\thistory.append(cache)\n\t\t\t\tcache_history[cache] = history\n\t\t\t\tkey_cache[key].append(cache)\n\t\t\t\ttype_key_cache[type(cache)][key] = history\n\t\t\t\treturn cache\n\t\t\tmisses += 1\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\thistory = deque([cache, ], maxlen=maxsize)\n\t\t\tcache_history[cache] = history\n\t\t\tkey_history[key] =", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache as lru_cache_decorator\n\t\treturn lru_cache_decorator(user_function, maxsize, typed)\n\treturn decorating_function", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import OrderedDict\n\t\tmemo = OrderedDict()\n\t\thits = misses = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += (user_function.__module__, user_function.__name__)\n\t\t\ttry:\n\t\t\t\tres = memo[key]\n\t\t\t\thits += 1\n\t\t\texcept KeyError:\n\t\t\t\tres = user_function(*args, **kwds)\n\t\t\t\tmemo[key] = res\n\t\t\t\tmisses += 1\n\t\t\t\tif len(memo) > maxsize:\n\t\t\t\t\toldest = next(iter(memo))\n\t\t\t\t\tdel memo[oldest]\n\t\t\telse:\n\t\t\t\tmemo.move_to_end(key)\n\t\t\treturn res\n\n\t\tdef cache_info():\n\t\t\treturn CacheInfo(hits=hits, misses=misses, maxsize=maxsize)\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = lambda: memo.clear()\n\t\twrapper.cache_", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t_lru_cache_wrapper.cache_clear()\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = _lru_cache_wrapper.make_lru_key(args, kwds, typed=typed)\n\t\t\tresult = _lru_cache_wrapper.cache[key]\n\t\t\tif result is _lru_cache_wrapper.MISS:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t_lru_cache_wrapper.cache[key] = result\n\t\t\treturn result\n\t\t_lru_cache_wrapper.update_wrapper(wrapper, user_function)\n\t\t_lru_cache_wrapper.cache = LRUCache(maxsize)\n\t\t_lru_cache_wrapper.UserFunction = user_function\n\t\treturn wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper_func(*args, **kwargs):\n\t\t\treturn user_function(*args, **kwargs)\n\t\treturn wrapper_func\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import deque\n\t\tfrom threading import RLock\n\t\tcache = {}\n\t\tlock = RLock()\n\t\tcall_queue = deque()\n\t\thits = misses = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = (args, frozenset(kwds.items()))\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tcall_queue.remove(key)\n\t\t\t\t\tcall_queue.append(key)\n\t\t\t\t\thits += 1\n\t\t\t\t\treturn result\n\t\t\t\texcept KeyError:\n\t\t\t\t\tmisses += 1\n\n\t\t\tresult = user_function(*args, **kwds)\n\n\t\t\twith lock:\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\toldest = call_queue.popleft()\n\t\t\t\t\tdel cache[oldest]\n\t\t\t\tcache[key] = result\n\t\t\t\tcall_queue.append(key)\n\t\t\treturn result\n\n\t\twrapper.hits = property(lambda: hits)\n\t\twrapper.misses = property(lambda: misses)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache as _lru_cache\n\t\treturn _lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom collections import deque, defaultdict\n\t\tcache = deque(maxlen=maxsize)\n\t\tkey_cache = defaultdict(deque)\n\t\tkey_history = {}\n\t\tcache_history = {}\n\t\ttype_key_cache = defaultdict(dict)\n\t\ttype_cache_history = defaultdict(dict)\n\t\thits = misses = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\tif key in key_history:\n\t\t\t\thistory = key_history[key]\n\t\t\t\thits += 1\n\t\t\t\thistory.append(cache)\n\t\t\t\tcache_history[cache] = history\n\t\t\t\tkey_cache[key].append(cache)\n\t\t\t\ttype_key_cache[type(cache)][key] = history\n\t\t\t\treturn cache\n\t\t\tmisses += 1\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\thistory = deque([cache, ], maxlen=maxsize)\n\t\t\tcache_history[cache] = history\n\t\t\tkey_history[key] =", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache as lru_cache_decorator\n\t\treturn lru_cache_decorator(user_function, maxsize, typed)\n\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakKeyDictionary\n\t\tfrom heapq import heappush, heappop\n\t\tfrom collections import defaultdict\n\t\tcache = WeakKeyDictionary()\n\t\tusages = defaultdict(list)\n\t\torder = []\n\t\thits = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal hits\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\tif key in cache:\n\t\t\t\thits += 1\n\t\t\t\tusages[key][1] += 1\n\t\t\t\theappush(order, (usages[key][1], usages[key][0], key))\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tif len(order) == maxsize:\n\t\t\t\t\twhile order[0][2] not in cache:\n\t\t\t\t\t\theappop(order)\n\t\t\t\t\tkey, usage = heappop(order)\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tdel usages[key]\n\t\t\t\tcache[key] = result\n\t\t\t\tusages[key] = [len(order),", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t...\n\t\treturn memoize(user_function,\n\t\t\t\tmaxsize=maxsize,\n\t\t\t\ttyped=typed)\n\treturn decorating_function\n\n\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tpass\n\t\treturn wrapper\n\treturn decorating_function\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function\n\n\n# 01:29\n\nimport asyncio\n\nfrom functools import wraps\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom heapq import heappush, heappop\n\tfrom collections import OrderedDict\n\tfrom operator import itemgetter\n\n\tdef factory(*args, **kwargs):\n\t\t\"\"\"\n\t\tFactory function to create a new cache instance.\n\t\t\"\"\"\n\t\tcache = LFUCache(*args, **kwargs)\n\t\treturn cache\n\n\tdef _cache_data(cache, key, value):\n\t\t\"\"\"\n\t\tHelper to cache the provided *key*, *value* pair.\n\t\t\"\"\"\n\t\tif len(cache._cache) >= cache._maxsize:\n\t\t\theappop(cache._heap)\n\t\t\tcache._cache.popitem(last=False)\n\t\theappush(cache._heap, (1, key))\n\t\tcache._cache[key] = value\n\n\tdef _update_cache(cache, key, value):\n\t\t\"\"\"\n\t\tHelper to update the cache for the provided *key*, *value* pair.\n\t\t\"\"\"\n\t\tcache._cache[key] = value\n\t\tcache._heap = [(v[0] + 1, k) for k, v in cache._heap\nif k != key]\n\t\theapify(cache._heap)\n\n\tdef _", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper_function(*args, **kwargs):\n\t\t\treturn user_function(*args, **kwargs)\n\t\treturn wrapper_function\n\treturn decorating_function\n\n\n@lfu_cache()", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tcache_get = wrapper.cache_get\n\t\t\ttry:\n\t\t\t\treturn cache_get(*args, **kwds)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\treturn_value = user_function(*args, **kwds)\n\t\t\tcache_get.cache[args] = (return_value, 1)\n\t\t\tif len(cache_get.cache) > cache_get.maxsize:\n\t\t\t\toldest = sorted(cache_get.cache.items(), key=lambda item: item[1][1])[0][0]\n\t\t\t\tdel cache_get.cache[oldest]\n\t\t\treturn return_value\n\t\twrapper.cache_get = LFUCacheGet(user_function, maxsize, typed)\n\t\treturn wrapper\n\treturn decorating_function\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kws):\n\t\t\tcache_get = wrapper.cache_get\n\t\t\ttry:\n\t\t\t\treturn cache_get(*args, **kws)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\treturn_value = user_function(*args, **kws)\n\t\t\tcache_get.cache[args] = (return_value, 1)\n\t\t\tif len(cache_get.cache) > cache_get.maxsize:\n\t\t\t\toldest = sorted(cache_get.cache.iteritems(), key=lambda x: x[1][1])[0][0]\n\t\t\t\tdel cache_get.cache[oldest]\n\t\t\treturn return_value\n\t\twrapper.cache_get = lfu_cache_getter(user_function, maxsize, typed)\n\t\twrapper.func_name = user_function.func_name\n\t\treturn wrapper\n\treturn decorating_function\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom collections import OrderedDict\n\tfrom operator import itemgetter\n\tfrom threading import RLock\n\tlock = RLock()\n\tcache = OrderedDict()\n\tfreq = OrderedDict()\n\thits = OrderedDict()\n\tdef get(k):\n\t\t\"\"\"\n\t\tGet a cache entry without updating the LFU algorithm.\n\t\t\"\"\"\n\t\twith lock:\n\t\t\ttry:\n\t\t\t\treturn cache[k]\n\t\t\texcept KeyError:\n\t\t\t\traise KeyError(k)\n\tdef get_freq(k):\n\t\t\"\"\"\n\t\tGet the frequency count of a cache entry without updating\nthe LFU algorithm.\n\t\t\"\"\"\n\t\twith lock:\n\t\t\treturn freq[k]\n\tdef set(k, v):\n\t\t\"\"\"\n\t\tSet a cache entry without updating the LFU algorithm.\n\t\t\"\"\"\n\t\twith lock:\n\t\t\tcache[k] = v\n\t\t\tfreq[k] = 0\n\t\t\thits[k] = 0\n\t\t\tif len(cache) > maxsize:\n\t\t\t\tevict = min(freq, key=freq.get)\n\t\t\t\tdel cache[evict], freq[evict], hits[evict]\n\t", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom collections import OrderedDict\n\n\n\tdef lfu_cache_factory(user_function):\n\t\t\"\"\"\n\t\tWraps `user_function` in a Least Frequently Used (LFU) cache.\n\t\t\"\"\"\n\t\tcached_data = OrderedDict()\n\t\tfrequencies = {}\n\n\t\tdef lfu_cache_wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function for memoizing `user_function` with a\n\t\t\tLeast Frequently Used (LFU) cache.\n\t\t\t\"\"\"\n\t\t\tkey = (args, tuple(sorted(kwargs.items())))\n\t\t\tif key in cached_data:\n\t\t\t\tfrequencies[key] += 1\n\t\t\t\treturn cached_data[key]\n\t\t\tif len(cached_data) >= maxsize:\n\t\t\t\tlfu_key_to_del = min(frequencies, key=frequencies.get)\n\t\t\t\tdel frequencies[lfu_key_to_del]\n\t\t\t\tdel cached_data[lfu_key_to_del]\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\tfrequencies[key] = 1\n\t\t\tcached_data[key] = result\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import wraps\n\t\tfrom weakref import WeakKeyDictionary\n\t\tfrom heapq import heappush, heappop\n\t\tfrom collections import defaultdict\n\t\tcache = WeakKeyDictionary()\n\t\tusages = defaultdict(list)\n\t\torder = []\n\t\thits = 0\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal hits\n\t\t\tkey = (args, tuple(sorted(kwds.items())))\n\t\t\tif key in cache:\n\t\t\t\thits += 1\n\t\t\t\tusages[key][1] += 1\n\t\t\t\theappush(order, (usages[key][1], usages[key][0], key))\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tif len(order) == maxsize:\n\t\t\t\t\twhile order[0][2] not in cache:\n\t\t\t\t\t\theappop(order)\n\t\t\t\t\tkey, usage = heappop(order)\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tdel usages[key]\n\t\t\t\tcache[key] = result\n\t\t\t\tusages[key] = [len(order),", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t...\n\t\treturn memoize(user_function,\n\t\t\t\tmaxsize=maxsize,\n\t\t\t\ttyped=typed)\n\treturn decorating_function\n\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tpass\n\t\treturn wrapper\n\treturn decorating_function\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tfrom functools import lru_cache\n\t\treturn lru_cache(maxsize=maxsize, typed=typed)(user_function)\n\treturn decorating_function\n\n\n# 01:29\n\nimport asyncio\n\nfrom functools import wraps\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom heapq import heappush, heappop\n\tfrom collections import OrderedDict\n\tfrom operator import itemgetter\n\n\tdef factory(*args, **kwargs):\n\t\t\"\"\"\n\t\tFactory function to create a new cache instance.\n\t\t\"\"\"\n\t\tcache = LFUCache(*args, **kwargs)\n\t\treturn cache\n\n\tdef _cache_data(cache, key, value):\n\t\t\"\"\"\n\t\tHelper to cache the provided *key*, *value* pair.\n\t\t\"\"\"\n\t\tif len(cache._cache) >= cache._maxsize:\n\t\t\theappop(cache._heap)\n\t\t\tcache._cache.popitem(last=False)\n\t\theappush(cache._heap, (1, key))\n\t\tcache._cache[key] = value\n\n\tdef _update_cache(cache, key, value):\n\t\t\"\"\"\n\t\tHelper to update the cache for the provided *key*, *value* pair.\n\t\t\"\"\"\n\t\tcache._cache[key] = value\n\t\tcache._heap = [(v[0] + 1, k) for k, v in cache._heap\nif k != key]\n\t\theapify(cache._heap)\n\n\tdef _", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper_function(*args, **kwargs):\n\t\t\treturn user_function(*args, **kwargs)\n\t\treturn wrapper_function\n\treturn decorating_function\n\n\n@lfu_cache()", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tcache_get = wrapper.cache_get\n\t\t\ttry:\n\t\t\t\treturn cache_get(*args, **kwds)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\treturn_value = user_function(*args, **kwds)\n\t\t\tcache_get.cache[args] = (return_value, 1)\n\t\t\tif len(cache_get.cache) > cache_get.maxsize:\n\t\t\t\toldest = sorted(cache_get.cache.items(), key=lambda item: item[1][1])[0][0]\n\t\t\t\tdel cache_get.cache[oldest]\n\t\t\treturn return_value\n\t\twrapper.cache_get = LFUCacheGet(user_function, maxsize, typed)\n\t\treturn wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kws):\n\t\t\tcache_get = wrapper.cache_get\n\t\t\ttry:\n\t\t\t\treturn cache_get(*args, **kws)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\treturn_value = user_function(*args, **kws)\n\t\t\tcache_get.cache[args] = (return_value, 1)\n\t\t\tif len(cache_get.cache) > cache_get.maxsize:\n\t\t\t\toldest = sorted(cache_get.cache.iteritems(), key=lambda x: x[1][1])[0][0]\n\t\t\t\tdel cache_get.cache[oldest]\n\t\t\treturn return_value\n\t\twrapper.cache_get = lfu_cache_getter(user_function, maxsize, typed)\n\t\twrapper.func_name = user_function.func_name\n\t\treturn wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom collections import OrderedDict\n\tfrom operator import itemgetter\n\tfrom threading import RLock\n\tlock = RLock()\n\tcache = OrderedDict()\n\tfreq = OrderedDict()\n\thits = OrderedDict()\n\tdef get(k):\n\t\t\"\"\"\n\t\tGet a cache entry without updating the LFU algorithm.\n\t\t\"\"\"\n\t\twith lock:\n\t\t\ttry:\n\t\t\t\treturn cache[k]\n\t\t\texcept KeyError:\n\t\t\t\traise KeyError(k)\n\tdef get_freq(k):\n\t\t\"\"\"\n\t\tGet the frequency count of a cache entry without updating\nthe LFU algorithm.\n\t\t\"\"\"\n\t\twith lock:\n\t\t\treturn freq[k]\n\tdef set(k, v):\n\t\t\"\"\"\n\t\tSet a cache entry without updating the LFU algorithm.\n\t\t\"\"\"\n\t\twith lock:\n\t\t\tcache[k] = v\n\t\t\tfreq[k] = 0\n\t\t\thits[k] = 0\n\t\t\tif len(cache) > maxsize:\n\t\t\t\tevict = min(freq, key=freq.get)\n\t\t\t\tdel cache[evict], freq[evict], hits[evict]\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tfrom functools import wraps\n\tfrom collections import OrderedDict\n\n\n\tdef lfu_cache_factory(user_function):\n\t\t\"\"\"\n\t\tWraps `user_function` in a Least Frequently Used (LFU) cache.\n\t\t\"\"\"\n\t\tcached_data = OrderedDict()\n\t\tfrequencies = {}\n\n\t\tdef lfu_cache_wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function for memoizing `user_function` with a\n\t\t\tLeast Frequently Used (LFU) cache.\n\t\t\t\"\"\"\n\t\t\tkey = (args, tuple(sorted(kwargs.items())))\n\t\t\tif key in cached_data:\n\t\t\t\tfrequencies[key] += 1\n\t\t\t\treturn cached_data[key]\n\t\t\tif len(cached_data) >= maxsize:\n\t\t\t\tlfu_key_to_del = min(frequencies, key=frequencies.get)\n\t\t\t\tdel frequencies[lfu_key_to_del]\n\t\t\t\tdel cached_data[lfu_key_to_del]\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\tfrequencies[key] = 1\n\t\t\tcached_data[key] = result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.data.popitem()\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.data[0], self.data[0].value\n\tself.delete(key)\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.theItems[0]\n\tdel self.theItems[0]\n\treturn (key, value)\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = next(iter(self.items()))\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self[0], self[1]\n\tdel self[0]\n\tdel self[1]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\n\tkey = self.keys()[0]\n\tval = self[key]\n\tdel self[key]\n\treturn key, val", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey = self.first_key()\n\tself.remove(key)\n\treturn key, self.get(key)\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\ttry:\n\t\tkey = self.keys()[0]\n\texcept IndexError:\n\t\traise KeyError('popitem(): no items')\n\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.keys()[0], self.values()[0]\n\tdel self[key]\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.items()[0]\n\tdel self[key]\n\treturn key, value"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.data.popitem()\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.data[0], self.data[0].value\n\tself.delete(key)\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.theItems[0]\n\tdel self.theItems[0]\n\treturn (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = next(iter(self.items()))\n\tdel self[key]\n\treturn key, value", "is_pass": true, "return_code": 0}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self[0], self[1]\n\tdel self[0]\n\tdel self[1]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\n\tkey = self.keys()[0]\n\tval = self[key]\n\tdel self[key]\n\treturn key, val", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey = self.first_key()\n\tself.remove(key)\n\treturn key, self.get(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\ttry:\n\t\tkey = self.keys()[0]\n\texcept IndexError:\n\t\traise KeyError('popitem(): no items')\n\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.keys()[0], self.values()[0]\n\tdel self[key]\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey, value = self.items()[0]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\texcept KeyError:\n\t\tvalue = default\n\t\tself[key] = value\n\treturn value\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tself.set_default(key, default=default)\n\n\tdef set_default(self, key, default=None):\n\t\tif key in self.keys():\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn self[key]\n\n\n# ----------------------------------------------------------------------------------------\n\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\tself[key]\n\texcept KeyError:\n\t\tself[key] = default\n\treturn self[key]\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tresult = self.__dict__.get(key, default)\n\tself.__dict__[key] = result\n\treturn result\n\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn dict.__getitem__(self, key)\n\texcept KeyError:\n\t\tpass\n\tself[key] = default\n\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattribute__(key)\n\texcept AttributeError:\n\t\tself.__setattr__(key, default)\n\t\treturn default", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\texcept KeyError:\n\t\tv = default\n\t\tself[key] = v\n\treturn v\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tself.__setattr__(key, default)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\texcept KeyError:\n\t\tvalue = default\n\t\tself[key] = value\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tself.set_default(key, default=default)\n\n\tdef set_default(self, key, default=None):\n\t\tif key in self.keys():\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn self[key]\n\n\n# ----------------------------------------------------------------------------------------\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\tself[key]\n\texcept KeyError:\n\t\tself[key] = default\n\treturn self[key]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tresult = self.__dict__.get(key, default)\n\tself.__dict__[key] = result\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn dict.__getitem__(self, key)\n\texcept KeyError:\n\t\tpass\n\tself[key] = default\n\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattribute__(key)\n\texcept AttributeError:\n\t\tself.__setattr__(key, default)\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\texcept KeyError:\n\t\tv = default\n\t\tself[key] = v\n\treturn v\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tself.__setattr__(key, default)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tpass\n\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tdef get_dict(obj, key, default):\n\t\t\"\"\"\n\t\tIf a key exists in the dict, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\t\"\"\"\n\t\tdef get_item(obj, key, default):\n\t\t\t\"\"\"\n\t\t\tIf a key exists in the dict, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\treturn obj[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn default\n\t\treturn get_item(obj, key, default)\n\treturn get_dict(self, key, default)\n\n#test.py\nfrom dict_get import get\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattr__(key)\n\texcept AttributeError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattribute__(key)\n\texcept AttributeError:\n\t\treturn default", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\treturn default\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tdef get_dict(obj, key, default):\n\t\t\"\"\"\n\t\tIf a key exists in the dict, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\t\"\"\"\n\t\tdef get_item(obj, key, default):\n\t\t\t\"\"\"\n\t\t\tIf a key exists in the dict, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\treturn obj[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn default\n\t\treturn get_item(obj, key, default)\n\treturn get_dict(self, key, default)\n\n#test.py\nfrom dict_get import get\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattr__(key)\n\texcept AttributeError:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattribute__(key)\n\texcept AttributeError:\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(method):\n\t\t@wraps(method)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tkeygen = key or method.__name__\n\t\t\tkey = keygen(self, *args, **kwargs)\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tresult = cache.get(key)\n\t\t\t\t\t\tif result is None:\n\t\t\t\t\t\t\tresult = method(self, *args, **kwargs)\n\t\t\t\t\t\t\tcache.set(key, result)\n\t\t\t\telse:\n\t\t\t\t\tresult = method(self, *args, **kwargs)\n\t\t\t\t\tcache.set(key, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlock and lock.acquire()\n\t\t\ttry:\n\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result:\n\t\t\t\t\treturn result\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, result)\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tlock and lock.release()\n\t\treturn wrapper\n\treturn decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef decorated(self, *args, **kwargs):\n\t\t\tif not cache.has_key(key(args, kwargs)):\n\t\t\t\twith lock:\n\t\t\t\t\tif not cache.has_key(key(args, kwargs)):\n\t\t\t\t\t\tcache[key(args, kwargs)] = func(self, *args, **kwargs)\n\t\t\treturn cache[key(args, kwargs)]\n\t\treturn decorated\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\tk = key(func, args, kwargs)\n\t\t\tv = cache.get(k)\n\t\t\tif v is not None:\n\t\t\t\treturn v\n\t\t\tv = func(*args, **kwargs)\n\t\t\tcache.set(k, v)\n\t\t\treturn v\n\t\tinner.func = func\n\t\tinner.cache = cache\n\t\tinner.key = key\n\t\tinner.lock = lock\n\t\treturn inner\n\treturn decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache.get(key(*args, **kwargs))\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tif not lock is None:\n\t\t\t\tlock.acquire()\n\t\t\tcache.set(key(*args, **kwargs), result)\n\t\t\tif not lock is None:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef decorated(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tresult = cache.get(k)\n\t\t\texcept Exception, e:\n\t\t\t\tresult = None\n\t\t\tif result is None:\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tcache[k] = result\n\t\t\treturn result\n\t\treturn decorated\n\treturn decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\t", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not lock:\n\t\t\t\tlock = threading.Lock()\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key(*args, **kwargs)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\tif lock:\n\t\t\tdef lockfunc(f, *args, **kwargs):\n\t\t\t\twith lock:\n\t\t\t\t\treturn f(*args, **kwargs)\n\tdef wrapper(self, *args, **kwargs):\n\t\tif key not in cache:\n\t\t\tcache[key] = f(self, *args, **kwargs)\n\t\treturn cache[key]\n\treturn decorator\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(method):\n\t\t@wraps(method)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tkeygen = key or method.__name__\n\t\t\tkey = keygen(self, *args, **kwargs)\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tresult = cache.get(key)\n\t\t\t\t\t\tif result is None:\n\t\t\t\t\t\t\tresult = method(self, *args, **kwargs)\n\t\t\t\t\t\t\tcache.set(key, result)\n\t\t\t\telse:\n\t\t\t\t\tresult = method(self, *args, **kwargs)\n\t\t\t\t\tcache.set(key, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlock and lock.acquire()\n\t\t\ttry:\n\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result:\n\t\t\t\t\treturn result\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, result)\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tlock and lock.release()\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef decorated(self, *args, **kwargs):\n\t\t\tif not cache.has_key(key(args, kwargs)):\n\t\t\t\twith lock:\n\t\t\t\t\tif not cache.has_key(key(args, kwargs)):\n\t\t\t\t\t\tcache[key(args, kwargs)] = func(self, *args, **kwargs)\n\t\t\treturn cache[key(args, kwargs)]\n\t\treturn decorated\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\tk = key(func, args, kwargs)\n\t\t\tv = cache.get(k)\n\t\t\tif v is not None:\n\t\t\t\treturn v\n\t\t\tv = func(*args, **kwargs)\n\t\t\tcache.set(k, v)\n\t\t\treturn v\n\t\tinner.func = func\n\t\tinner.cache = cache\n\t\tinner.key = key\n\t\tinner.lock = lock\n\t\treturn inner\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache.get(key(*args, **kwargs))\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tif not lock is None:\n\t\t\t\tlock.acquire()\n\t\t\tcache.set(key(*args, **kwargs), result)\n\t\t\tif not lock is None:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef decorated(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tresult = cache.get(k)\n\t\t\texcept Exception, e:\n\t\t\t\tresult = None\n\t\t\tif result is None:\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tcache[k] = result\n\t\t\treturn result\n\t\treturn decorated\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not lock:\n\t\t\t\tlock = threading.Lock()\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key(*args, **kwargs)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\tif lock:\n\t\t\tdef lockfunc(f, *args, **kwargs):\n\t\t\t\twith lock:\n\t\t\t\t\treturn f(*args, **kwargs)\n\tdef wrapper(self, *args, **kwargs):\n\t\tif key not in cache:\n\t\t\tcache[key] = f(self, *args, **kwargs)\n\t\treturn cache[key]\n\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tfrom .utils import is_string, is_buffer, is_function\n\timport sys\n\timport traceback\n\tfrom .errors import print_trace\n\n\tif is_string(e):\n\t\treturn e\n\telif is_function(e):\n\t\treturn e.__name__\n\telif hasattr(e, '__name__'):\n\t\treturn e.__name__\n\telif is_buffer(e):\n\t\treturn e.decode('utf-8')\n\n\tif isinstance(e, Exception):\n\t\te = str(e)\n\telse:\n\t\te = str(e)\n\n\tif sys.exc_info()[0] is not None:\n\t\te += '\\n' + print_trace(max_level, max_path_level)\n\n\treturn e\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport traceback\n\timport io\n\tfrom . import _format\n\t\n\te_type = _format.truncate(e.__class__.__name__)\n\te_message = _format.truncate(e)\n\t\n\te_path = []\n\te_traceback = []\n\t\n\tfor tb in traceback.extract_tb(e.__traceback__, max_level):\n\t\tif len(e_path) >= max_path_level:\n\t\t\tbreak\n\t\t\n\t\tif tb.filename == \"<string>\":\n\t\t\te_path.append(tb.filename)\n\t\t\te_traceback.append(f\"line {_format.truncate(tb.lineno)}\")\n\t\telif tb.filename is None:\n\t\t\te_path.append(tb.filename)\n\t\t\te_traceback.append(f\"line {_format.truncate(tb.lineno)}\")\n\t\telse:\n\t\t\te_path.append(_format.truncate(tb.filename, max_length=100))\n\t\t\te_traceback.append(f\"line {_format.truncate(tb.lineno)} in {_format.truncate(tb.name)}\")\n\t\n\te_", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tclsname = e.__class__.__name__\n\tmsg = str(e)\n\tr = [msg, \"\\n\"]\n\tr += [\" \"*4 + l for l in clsname.split(\"\\n\")]\n\tr += [\"\\n\"]\n\tr += [\" \"*4 + l for l in msg.split(\"\\n\")]\n\tr += [\"\\n\"]\n\n\tif max_level <= 0:\n\t\treturn r\n\n\tr += [\" \"*4 + l for l in clsname.split(\"\\n\")]\n\tr += [\"\\n\"]\n\n\tstack = e.__traceback__\n\twhile stack:\n\t\tfilename = stack.tb_frame.f_code.co_filename\n\t\tlineno = stack.tb_lineno\n\t\tname = stack.tb_frame.f_code.co_name\n\t\tr += [\" \"*4 + \"%03d: %s:%04d in %s\" % (stack.tb_level, filename, lineno, name)]\n\t\tr += [\"\\n\"]\n\t\tstack = stack.tb_next\n\n\t\tif stack and stack.tb_level >= max_level:\n\t\t\tr += [\" \"*4 + \"...\"]\n\t\t\tbreak\n\n\treturn r", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tr = \"\"\n\tcls_ = cls.__name__\n\twhile hasattr(e, \"__cause__\") and e.__cause__ is not None:\n\t\te = e.__cause__\n\tr += \"%s(%s): %s\" % (cls_, e.__class__.__name__, e)\n\te = e.__context__\n\tif e is not None:\n\t\tr += \"\\n\\tfrom %s\" % extostr(cls, e, max_level - 1, max_path_level - 1)\n\telif max_level > 0:\n\t\tr += \"\\n\\tfrom %s\" % cls_\n\te = None\n\ttry:\n\t\te = e.__traceback__\n\texcept:\n\t\tpass\n\tif e is not None:\n\t\tr += \"\\n\\t\"\n\t\tr += \"\\n\\t\".join(traceback.format_tb(e, max_path_level))\n\treturn r", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport traceback\n\tif sys.version_info >= (3,):\n\t\texc_type = str(e.__class__)\n\t\texc_value = str(e)\n\t\texc_traceback = traceback.format_tb(e.__traceback__)\n\telse:\n\t\texc_type = e.__class__.__name__\n\t\texc_value = str(e)\n\t\texc_traceback = traceback.format_tb(sys.exc_info()[2])\n\texc_traceback = exc_traceback[:max_level]\n\texc_traceback_str = \"\".join(exc_traceback)\n\n\tif len(exc_traceback_str) > max_path_level:\n\t\texc_traceback_str = exc_traceback_str[:max_path_level] + \"...\"\n\texc_str = \"%s: %s\\n%s\" % (exc_type, exc_value, exc_traceback_str)\n\treturn exc_str\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport traceback\n\timport sys\n\n\tdef get_call_info(e, max_level, max_path_level):\n\t\tret = []\n\t\tmax_level = max_level + 1\n\t\tfor l in traceback.extract_tb(e.__traceback__):\n\t\t\tif l[0] != '<module>' and l[2] != '<module>':\n\t\t\t\tmax_path_level -= 1\n\t\t\t\tif max_path_level < 0: break\n\t\t\t\tret.append('%s:%s in %s' % (l[1], l[2], l[0]))\n\t\treturn ret\n\n\ts = str(e)\n\tcall_info = get_call_info(e, max_level, max_path_level)\n\tif call_info:\n\t\ts = \"%s\\n  %s\" % (s, '\\n  '.join(call_info))\n\n\treturn s\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport os\n\t\n\ttyp = cls.__name__\n\tmsg = e.message\n\t\n\tif isinstance(e, Exception):\n\t\tname = e.__class__.__name__\n\telse:\n\t\tname = cls.__name__\n\t\n\tstack = sys.exc_info()[2]\n\tif not stack or len(stack.tb_frame.f_locals) < 3:\n\t\tlvl = 0\n\telse:\n\t\tlvl = stack.tb_frame.f_locals.get(\"__level__\")\n\t\n\tif lvl is None:\n\t\tlvl = 0\n\t\n\tif lvl >= max_level:\n\t\treturn \"%s: %s\" % (name, msg)\n\t\n\tpath = os.path.normpath(stack.tb_frame.f_code.co_filename)\n\t\n\tif len(path.split(os.sep)) > max_path_level:\n\t\tpath = os.sep.join(path.split(os.sep)[-max_path_level:])\n\t\n\treturn \"%s:%s: %s: %s\" % (path, stack.tb_lineno, name, msg)", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tex = e\n\tlevel = 0\n\tpath = []\n\twhile ex:\n\t\tpath.append(ex.__class__.__name__)\n\t\tex = ex.__cause__\n\t\tlevel += 1\n\t\tif level > max_level:\n\t\t\tpath.append(\"...\")\n\t\t\tbreak\n\n\tpath = path[:max_path_level]\n\tpath = \" > \".join(path)\n\treturn \"{0} in {1}\".format(e, path)\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport traceback\n\timport sys\n\n\texc_type, exc_value, exc_traceback = sys.exc_info()\n\texc_traceback = exc_traceback.tb_next.tb_next.tb_next\n\ttraceback.extract_tb(exc_traceback)\n\texc_traceback = traceback.format_tb(exc_traceback, max_level)\n\texc_traceback = exc_traceback[0:max_path_level]\n\texc_traceback_str = '\\n'.join(exc_traceback)\n\n\texc_type = str(exc_type)\n\texc_value = str(exc_value)\n\texc_value = exc_value.replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n\n\te = str(e)\n\te = e.replace('\\n', '\\\\n')\n\n\tres = f\"{exc_type}: {e}\\n{exc_value}\\n{exc_traceback_str}\"\n\treturn res\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tfrom .utils import is_string, is_buffer, is_function\n\timport sys\n\timport traceback\n\tfrom .errors import print_trace\n\n\tif is_string(e):\n\t\treturn e\n\telif is_function(e):\n\t\treturn e.__name__\n\telif hasattr(e, '__name__'):\n\t\treturn e.__name__\n\telif is_buffer(e):\n\t\treturn e.decode('utf-8')\n\n\tif isinstance(e, Exception):\n\t\te = str(e)\n\telse:\n\t\te = str(e)\n\n\tif sys.exc_info()[0] is not None:\n\t\te += '\\n' + print_trace(max_level, max_path_level)\n\n\treturn e\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport traceback\n\timport io\n\tfrom . import _format\n\t\n\te_type = _format.truncate(e.__class__.__name__)\n\te_message = _format.truncate(e)\n\t\n\te_path = []\n\te_traceback = []\n\t\n\tfor tb in traceback.extract_tb(e.__traceback__, max_level):\n\t\tif len(e_path) >= max_path_level:\n\t\t\tbreak\n\t\t\n\t\tif tb.filename == \"<string>\":\n\t\t\te_path.append(tb.filename)\n\t\t\te_traceback.append(f\"line {_format.truncate(tb.lineno)}\")\n\t\telif tb.filename is None:\n\t\t\te_path.append(tb.filename)\n\t\t\te_traceback.append(f\"line {_format.truncate(tb.lineno)}\")\n\t\telse:\n\t\t\te_path.append(_format.truncate(tb.filename, max_length=100))\n\t\t\te_traceback.append(f\"line {_format.truncate(tb.lineno)} in {_format.truncate(tb.name)}\")\n\t\n\te_", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tclsname = e.__class__.__name__\n\tmsg = str(e)\n\tr = [msg, \"\\n\"]\n\tr += [\" \"*4 + l for l in clsname.split(\"\\n\")]\n\tr += [\"\\n\"]\n\tr += [\" \"*4 + l for l in msg.split(\"\\n\")]\n\tr += [\"\\n\"]\n\n\tif max_level <= 0:\n\t\treturn r\n\n\tr += [\" \"*4 + l for l in clsname.split(\"\\n\")]\n\tr += [\"\\n\"]\n\n\tstack = e.__traceback__\n\twhile stack:\n\t\tfilename = stack.tb_frame.f_code.co_filename\n\t\tlineno = stack.tb_lineno\n\t\tname = stack.tb_frame.f_code.co_name\n\t\tr += [\" \"*4 + \"%03d: %s:%04d in %s\" % (stack.tb_level, filename, lineno, name)]\n\t\tr += [\"\\n\"]\n\t\tstack = stack.tb_next\n\n\t\tif stack and stack.tb_level >= max_level:\n\t\t\tr += [\" \"*4 + \"...\"]\n\t\t\tbreak\n\n\treturn r", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tr = \"\"\n\tcls_ = cls.__name__\n\twhile hasattr(e, \"__cause__\") and e.__cause__ is not None:\n\t\te = e.__cause__\n\tr += \"%s(%s): %s\" % (cls_, e.__class__.__name__, e)\n\te = e.__context__\n\tif e is not None:\n\t\tr += \"\\n\\tfrom %s\" % extostr(cls, e, max_level - 1, max_path_level - 1)\n\telif max_level > 0:\n\t\tr += \"\\n\\tfrom %s\" % cls_\n\te = None\n\ttry:\n\t\te = e.__traceback__\n\texcept:\n\t\tpass\n\tif e is not None:\n\t\tr += \"\\n\\t\"\n\t\tr += \"\\n\\t\".join(traceback.format_tb(e, max_path_level))\n\treturn r", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport traceback\n\tif sys.version_info >= (3,):\n\t\texc_type = str(e.__class__)\n\t\texc_value = str(e)\n\t\texc_traceback = traceback.format_tb(e.__traceback__)\n\telse:\n\t\texc_type = e.__class__.__name__\n\t\texc_value = str(e)\n\t\texc_traceback = traceback.format_tb(sys.exc_info()[2])\n\texc_traceback = exc_traceback[:max_level]\n\texc_traceback_str = \"\".join(exc_traceback)\n\n\tif len(exc_traceback_str) > max_path_level:\n\t\texc_traceback_str = exc_traceback_str[:max_path_level] + \"...\"\n\texc_str = \"%s: %s\\n%s\" % (exc_type, exc_value, exc_traceback_str)\n\treturn exc_str\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport traceback\n\timport sys\n\n\tdef get_call_info(e, max_level, max_path_level):\n\t\tret = []\n\t\tmax_level = max_level + 1\n\t\tfor l in traceback.extract_tb(e.__traceback__):\n\t\t\tif l[0] != '<module>' and l[2] != '<module>':\n\t\t\t\tmax_path_level -= 1\n\t\t\t\tif max_path_level < 0: break\n\t\t\t\tret.append('%s:%s in %s' % (l[1], l[2], l[0]))\n\t\treturn ret\n\n\ts = str(e)\n\tcall_info = get_call_info(e, max_level, max_path_level)\n\tif call_info:\n\t\ts = \"%s\\n  %s\" % (s, '\\n  '.join(call_info))\n\n\treturn s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport os\n\t\n\ttyp = cls.__name__\n\tmsg = e.message\n\t\n\tif isinstance(e, Exception):\n\t\tname = e.__class__.__name__\n\telse:\n\t\tname = cls.__name__\n\t\n\tstack = sys.exc_info()[2]\n\tif not stack or len(stack.tb_frame.f_locals) < 3:\n\t\tlvl = 0\n\telse:\n\t\tlvl = stack.tb_frame.f_locals.get(\"__level__\")\n\t\n\tif lvl is None:\n\t\tlvl = 0\n\t\n\tif lvl >= max_level:\n\t\treturn \"%s: %s\" % (name, msg)\n\t\n\tpath = os.path.normpath(stack.tb_frame.f_code.co_filename)\n\t\n\tif len(path.split(os.sep)) > max_path_level:\n\t\tpath = os.sep.join(path.split(os.sep)[-max_path_level:])\n\t\n\treturn \"%s:%s: %s: %s\" % (path, stack.tb_lineno, name, msg)", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tex = e\n\tlevel = 0\n\tpath = []\n\twhile ex:\n\t\tpath.append(ex.__class__.__name__)\n\t\tex = ex.__cause__\n\t\tlevel += 1\n\t\tif level > max_level:\n\t\t\tpath.append(\"...\")\n\t\t\tbreak\n\n\tpath = path[:max_path_level]\n\tpath = \" > \".join(path)\n\treturn \"{0} in {1}\".format(e, path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport traceback\n\timport sys\n\n\texc_type, exc_value, exc_traceback = sys.exc_info()\n\texc_traceback = exc_traceback.tb_next.tb_next.tb_next\n\ttraceback.extract_tb(exc_traceback)\n\texc_traceback = traceback.format_tb(exc_traceback, max_level)\n\texc_traceback = exc_traceback[0:max_path_level]\n\texc_traceback_str = '\\n'.join(exc_traceback)\n\n\texc_type = str(exc_type)\n\texc_value = str(exc_value)\n\texc_value = exc_value.replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n\n\te = str(e)\n\te = e.replace('\\n', '\\\\n')\n\n\tres = f\"{exc_type}: {e}\\n{exc_value}\\n{exc_traceback_str}\"\n\treturn res\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'a' if overwrite == False else 'w', encoding=encoding) as file:\n\t\t\tfile.write(text_buffer)\n\texcept:\n\t\tprint(\"Error writing to file: {}\".format(file_name))\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\t\n\twith open(file_name, mode, encoding=encoding) as file_stream:\n\t\tfile_stream.write(text_buffer)\n\treturn\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\treturn\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\taccess_flag = 'w'\n\tif overwrite:\n\t\taccess_flag = 'w'\n\telse:\n\t\taccess_flag = 'a'\n\n\twith open(file_name, access_flag, encoding=encoding) as f:\n\t\tf.write(text_buffer)", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\tfile = open(file_name, 'a')\n\t\n\tif overwrite == True:\n\t\tfile.seek(0)\n\t\tfile.truncate()\n\t\n\tfile.write(text_buffer.encode(encoding))\n\t\n\tfile.close()\n\t", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'w' if overwrite else 'a'\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\ttry:\n\t\tfile = open(file_name, \"a\" if overwrite else \"ab\")\n\t\tfile.write(text_buffer.encode(encoding))\n\t\tfile.close()\n\texcept IOError as e:\n\t\tfile.close()\n\t\traise e\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\t# Create a new file, or open an existing one for appending\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\t\n\twith open(file_name, mode, encoding=encoding) as file_object:\n\t\tfile_object.write(text_buffer)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'a' if overwrite == False else 'w', encoding=encoding) as file:\n\t\t\tfile.write(text_buffer)\n\texcept:\n\t\tprint(\"Error writing to file: {}\".format(file_name))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\t\n\twith open(file_name, mode, encoding=encoding) as file_stream:\n\t\tfile_stream.write(text_buffer)\n\treturn\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\treturn\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\taccess_flag = 'w'\n\tif overwrite:\n\t\taccess_flag = 'w'\n\telse:\n\t\taccess_flag = 'a'\n\n\twith open(file_name, access_flag, encoding=encoding) as f:\n\t\tf.write(text_buffer)", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\tfile = open(file_name, 'a')\n\t\n\tif overwrite == True:\n\t\tfile.seek(0)\n\t\tfile.truncate()\n\t\n\tfile.write(text_buffer.encode(encoding))\n\t\n\tfile.close()\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'w' if overwrite else 'a'\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\ttry:\n\t\tfile = open(file_name, \"a\" if overwrite else \"ab\")\n\t\tfile.write(text_buffer.encode(encoding))\n\t\tfile.close()\n\texcept IOError as e:\n\t\tfile.close()\n\t\traise e\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = \"w\"\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\t# Create a new file, or open an existing one for appending\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\t\n\twith open(file_name, mode, encoding=encoding) as file_object:\n\t\tfile_object.write(text_buffer)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\ttext_buffer = Gtk.TextBuffer()\n\ttext_buffer.set_text(open(file_name, \"r\", encoding=encoding).read())\n\t\n\treturn text_buffer", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, \"rb\") as f:\n\t\tfile_content = f.read()\n\t\ttextbuffer = TextBuffer(file_content, encoding)\n\n\treturn textbuffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, \"r\", encoding=encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfrom gi.repository import Gio\n\ttext_buffer = Gio.File.new_for_path(file_name).load_contents()[1]\n\tif text_buffer.get_charset() is None:\n\t\ttext_buffer.set_charset(encoding, True)\n\treturn text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, encoding=encoding) as file:\n\t\ttextbuffer = file.read()\n\treturn textbuffer", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, encoding=encoding, errors='ignore') as f:\n\t\ttext = f.read()\n\n\treturn text\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding = encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttext = f.read()\n\treturn text\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\ttext_buffer = Gtk.TextBuffer()\n\ttext_buffer.set_text(open(file_name, \"r\", encoding=encoding).read())\n\t\n\treturn text_buffer", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, \"rb\") as f:\n\t\tfile_content = f.read()\n\t\ttextbuffer = TextBuffer(file_content, encoding)\n\n\treturn textbuffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, \"r\", encoding=encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfrom gi.repository import Gio\n\ttext_buffer = Gio.File.new_for_path(file_name).load_contents()[1]\n\tif text_buffer.get_charset() is None:\n\t\ttext_buffer.set_charset(encoding, True)\n\treturn text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, encoding=encoding) as file:\n\t\ttextbuffer = file.read()\n\treturn textbuffer", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, encoding=encoding, errors='ignore') as f:\n\t\ttext = f.read()\n\n\treturn text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding = encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttext = f.read()\n\treturn text\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name)\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r'):\n\t\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, 'r')\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r') as data_file:\n\t\t\tpass\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os\n\treturn os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r') as f:\n\t\t\tpass\n\texcept FileNotFoundError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os\n\treturn os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r') as f:\n\t\t\tpass\n\texcept FileNotFoundError:\n\t\treturn False\n\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name)\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, \"r\")\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name)\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r'):\n\t\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, 'r')\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r') as data_file:\n\t\t\tpass\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os\n\treturn os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r') as f:\n\t\t\tpass\n\texcept FileNotFoundError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os\n\treturn os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r') as f:\n\t\t\tpass\n\texcept FileNotFoundError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name)\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, \"r\")\n\t\tf.close()\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\timport sys\n\n\tlogging.shutdown()\n\tlogging.basicConfig(level=logging.DEBUG)\n\tlogging.info('Logging system reset')\n\tcls.logger.info('Logging system reset')\n\tsys.stdout.flush()\n\tsys.stderr.flush()", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.shutdown()\n\tlogging.root.handlers = []\n\tlogging.root.filters = []\n\tlogging.root.level = logging.NOTSET\n\tlogging.basicConfig(level=logging.INFO)", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger()\n\tlogger.handlers = []\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\twhile cls._logger_initialized:\n\t\tcls._logger_initialized = False\n\t\tcls._logger = None\n\t\tcls._log_handler = None\n\t\tcls._log_formatter = None\n\t\tcls._log_file = None\n\t\tcls._log_file_handler = None\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\timport logging.config\n\tfrom os import path as op\n\tfrom sys import argv\n\tfrom traceback import print_exc\n\n\tfrom .config import Config\n\n\tlevel = logging.DEBUG if '--debug' in argv else logging.ERROR\n\tlogging.basicConfig(level=level,\n\t\t\t\t\t\tformat='%(asctime)s:%(levelname)s:%(name)s: %(message)s')\n\n\tconfig = Config(op.join(op.dirname(__file__), 'logging.conf'))\n\tlogging.config.fileConfig(config.file, disable_existing_loggers=False)\n\n\ttry:\n\t\tlogging.getLogger('').setLevel(level)\n\texcept:\n\t\tprint_exc()\n\n\tcls.logger = logging.getLogger(__name__)", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.root.handlers = []\n\tlogging.root.setLevel(logging.WARNING)\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\timport logging.config\n\timport os\n\timport inspect\n\n\tlog_config_path = os.path.join(os.path.dirname(inspect.getfile(cls)), \"logging.yaml\")\n\tlogging.config.fileConfig(log_config_path, disable_existing_loggers=False)\n\n\tlogger = logging.getLogger('')\n\tlogger.setLevel(logging.DEBUG)\n\n\treturn logger\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger()\n\tfor h in logger.handlers:\n\t\tlogger.removeHandler(h)\n\tlogger.setLevel(logging.NOTSET)\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.log_level = cls.default_log_level\n\tcls.log_file = cls.default_log_file\n\tcls.log_format = cls.default_log_format\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger = None\n\tcls.log_level = cls.LOG_LEVEL_NOTSET\n\tcls.log_name = None\n\tcls.log_format = None\n\tcls.log_datefmt = None\n\tcls.log_file = None\n\tcls.log_stream = None\n\tcls.log_enabled = False\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\timport sys\n\n\tlogging.shutdown()\n\tlogging.basicConfig(level=logging.DEBUG)\n\tlogging.info('Logging system reset')\n\tcls.logger.info('Logging system reset')\n\tsys.stdout.flush()\n\tsys.stderr.flush()", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.shutdown()\n\tlogging.root.handlers = []\n\tlogging.root.filters = []\n\tlogging.root.level = logging.NOTSET\n\tlogging.basicConfig(level=logging.INFO)", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger()\n\tlogger.handlers = []\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\twhile cls._logger_initialized:\n\t\tcls._logger_initialized = False\n\t\tcls._logger = None\n\t\tcls._log_handler = None\n\t\tcls._log_formatter = None\n\t\tcls._log_file = None\n\t\tcls._log_file_handler = None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\timport logging.config\n\tfrom os import path as op\n\tfrom sys import argv\n\tfrom traceback import print_exc\n\n\tfrom .config import Config\n\n\tlevel = logging.DEBUG if '--debug' in argv else logging.ERROR\n\tlogging.basicConfig(level=level,\n\t\t\t\t\t\tformat='%(asctime)s:%(levelname)s:%(name)s: %(message)s')\n\n\tconfig = Config(op.join(op.dirname(__file__), 'logging.conf'))\n\tlogging.config.fileConfig(config.file, disable_existing_loggers=False)\n\n\ttry:\n\t\tlogging.getLogger('').setLevel(level)\n\texcept:\n\t\tprint_exc()\n\n\tcls.logger = logging.getLogger(__name__)", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.root.handlers = []\n\tlogging.root.setLevel(logging.WARNING)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\timport logging.config\n\timport os\n\timport inspect\n\n\tlog_config_path = os.path.join(os.path.dirname(inspect.getfile(cls)), \"logging.yaml\")\n\tlogging.config.fileConfig(log_config_path, disable_existing_loggers=False)\n\n\tlogger = logging.getLogger('')\n\tlogger.setLevel(logging.DEBUG)\n\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger()\n\tfor h in logger.handlers:\n\t\tlogger.removeHandler(h)\n\tlogger.setLevel(logging.NOTSET)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.log_level = cls.default_log_level\n\tcls.log_file = cls.default_log_file\n\tcls.log_format = cls.default_log_format\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger = None\n\tcls.log_level = cls.LOG_LEVEL_NOTSET\n\tcls.log_name = None\n\tcls.log_format = None\n\tcls.log_datefmt = None\n\tcls.log_file = None\n\tcls.log_stream = None\n\tcls.log_enabled = False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\n\t\t\"Define %s._getTargetClass()\" % self.__class__.__name__\n\t)\n\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tpass\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError()\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\n\t\t\"Define %s._getTargetClass()\" % self.__class__.__name__\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerged = []\n\tfor ordering in orderings:\n\t\tmerged += ordering\n\treturn list(set(merged))\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tret = []\n\tfor ordering in orderings:\n\t\tfor el in ordering:\n\t\t\tif el not in ret:\n\t\t\t\tret.append(el)\n\treturn ret\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor e in ordering:\n\t\t\tif e not in result:\n\t\t\t\tresult.append(e)\n\treturn result\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult.extend(ordering)\n\tresult = list(set(result))\n\tresult.sort()\n\treturn result\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor e in ordering:\n\t\t\tif e not in merged:\n\t\t\t\tmerged.append(e)\n\treturn merged\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tret = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in ret:\n\t\t\t\tret.append(item)\n\treturn ret\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor l in orderings:\n\t\tfor e in l:\n\t\t\tif e in result: continue\n\t\t\tresult.append(e)\n\treturn result\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerged = []\n\tfor ordering in orderings:\n\t\tmerged += ordering\n\treturn list(set(merged))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tret = []\n\tfor ordering in orderings:\n\t\tfor el in ordering:\n\t\t\tif el not in ret:\n\t\t\t\tret.append(el)\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor e in ordering:\n\t\t\tif e not in result:\n\t\t\t\tresult.append(e)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult.extend(ordering)\n\tresult = list(set(result))\n\tresult.sort()\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor e in ordering:\n\t\t\tif e not in merged:\n\t\t\t\tmerged.append(e)\n\treturn merged\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tret = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in ret:\n\t\t\t\tret.append(item)\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor l in orderings:\n\t\tfor e in l:\n\t\t\tif e in result: continue\n\t\t\tresult.append(e)\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError('Prototype method')\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface.declarations import directlyProvidedBy as zope_directlyProvidedBy\n\tfrom zope.interface.declarations import InterfaceClass\n\tfrom zope.interface.declarations import Implementer\n\tfrom zope.interface.declarations import InterfaceClassImplementation\n\n\tif isinstance(object, Implementer):\n\t\tobject = object.implemented\n\n\tif object is InterfaceClass:\n\t\treturn object._zope_interface\n\n\tif isinstance(object, InterfaceClassImplementation):\n\t\treturn object._zope_interface\n\n\treturn zope_directlyProvidedBy(object)\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tpass\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\ttry:\n\t\treturn object.directlyProvidedBy()\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn object.providedBy()\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn object.interface\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn object.__interface__\n\texcept AttributeError:\n\t\tpass\n\n\traise ValueError(\"object does not provide an interface\")", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError()", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tdirectly_provided_by = set()\n\tprovides = object.providedBy\n\tif provides is not None:\n\t\tdirectly_provided_by.update(provides)\n\treturn directly_provided_by", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError, \"directlyProvidedBy\"\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError('Prototype method')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface.declarations import directlyProvidedBy as zope_directlyProvidedBy\n\tfrom zope.interface.declarations import InterfaceClass\n\tfrom zope.interface.declarations import Implementer\n\tfrom zope.interface.declarations import InterfaceClassImplementation\n\n\tif isinstance(object, Implementer):\n\t\tobject = object.implemented\n\n\tif object is InterfaceClass:\n\t\treturn object._zope_interface\n\n\tif isinstance(object, InterfaceClassImplementation):\n\t\treturn object._zope_interface\n\n\treturn zope_directlyProvidedBy(object)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\ttry:\n\t\treturn object.directlyProvidedBy()\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn object.providedBy()\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn object.interface\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn object.__interface__\n\texcept AttributeError:\n\t\tpass\n\n\traise ValueError(\"object does not provide an interface\")", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tdirectly_provided_by = set()\n\tprovides = object.providedBy\n\tif provides is not None:\n\t\tdirectly_provided_by.update(provides)\n\treturn directly_provided_by", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\traise NotImplementedError, \"directlyProvidedBy\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor class_ in classes:\n\t\tif class_ not in [subclass.class_name for subclass in classes]:\n\t\t\tbases.append(class_)\n\treturn bases\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = set()\n\tfor c in classes:\n\t\tif len(c.subclasses) == 0:\n\t\t\tminimal.add(c)\n\treturn list(minimal)\n\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not any(map(lambda x: issubclass(x, c), classes)):\n\t\t\tbases.append(c)\n\treturn bases", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tall = []\n\t\n\tfor c in classes:\n\t\tall.append(c)\n\t\tif c not in minimal:\n\t\t\tfor sub in classes:\n\t\t\t\tif c != sub:\n\t\t\t\t\tif c in sub:\n\t\t\t\t\t\tminimal.append(c)\n\t\t\t\t\t\tbreak\n\treturn [c for c in all if c not in minimal]\n\n\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\tfor c in classes:\n\t\tif not any([issubclass(c, x) for x in classes]):\n\t\t\tresult.append(c)\n\treturn result\n\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminBaseClasses = []\n\tfor cl in classes:\n\t\tif(len(cl.subclasses) == 0):\n\t\t\tminBaseClasses.append(cl)\n\treturn minBaseClasses\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\t# Add all classes\n\tall_classes = classes\n\t\n\t# Remove subclasses\n\tfor c1 in classes:\n\t\tfor c2 in classes:\n\t\t\tif c1 != c2 and issubclass(c2, c1):\n\t\t\t\tall_classes.remove(c1)\n\t\t\t\t\n\treturn all_classes\n\t", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\t\n\tfor sclass in classes:\n\t\tisMinimal = True\n\t\t\n\t\tfor sc in classes:\n\t\t\tif sclass != sc and sclass.issubclass(sc):\n\t\t\t\tisMinimal = False\n\t\t\t\tbreak\n\t\t\n\t\tif isMinimal:\n\t\t\tminimal.append(sclass)\n\t\n\treturn minimal", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\tminimalClasses = []\n\t\n\tfor class_ in classes:\n\t\tif not any([class_.issubclass(other) for other in classes]):\n\t\t\tminimalClasses.append(class_)\n\t\n\treturn minimalClasses\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif sum([c.isSubclassOf(i) for i in classes]) == 0:\n\t\t\tbases.append(c)\n\treturn bases"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor class_ in classes:\n\t\tif class_ not in [subclass.class_name for subclass in classes]:\n\t\t\tbases.append(class_)\n\treturn bases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = set()\n\tfor c in classes:\n\t\tif len(c.subclasses) == 0:\n\t\t\tminimal.add(c)\n\treturn list(minimal)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not any(map(lambda x: issubclass(x, c), classes)):\n\t\t\tbases.append(c)\n\treturn bases", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tall = []\n\t\n\tfor c in classes:\n\t\tall.append(c)\n\t\tif c not in minimal:\n\t\t\tfor sub in classes:\n\t\t\t\tif c != sub:\n\t\t\t\t\tif c in sub:\n\t\t\t\t\t\tminimal.append(c)\n\t\t\t\t\t\tbreak\n\treturn [c for c in all if c not in minimal]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\tfor c in classes:\n\t\tif not any([issubclass(c, x) for x in classes]):\n\t\t\tresult.append(c)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminBaseClasses = []\n\tfor cl in classes:\n\t\tif(len(cl.subclasses) == 0):\n\t\t\tminBaseClasses.append(cl)\n\treturn minBaseClasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\t# Add all classes\n\tall_classes = classes\n\t\n\t# Remove subclasses\n\tfor c1 in classes:\n\t\tfor c2 in classes:\n\t\t\tif c1 != c2 and issubclass(c2, c1):\n\t\t\t\tall_classes.remove(c1)\n\t\t\t\t\n\treturn all_classes\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\t\n\tfor sclass in classes:\n\t\tisMinimal = True\n\t\t\n\t\tfor sc in classes:\n\t\t\tif sclass != sc and sclass.issubclass(sc):\n\t\t\t\tisMinimal = False\n\t\t\t\tbreak\n\t\t\n\t\tif isMinimal:\n\t\t\tminimal.append(sclass)\n\t\n\treturn minimal", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\tminimalClasses = []\n\t\n\tfor class_ in classes:\n\t\tif not any([class_.issubclass(other) for other in classes]):\n\t\t\tminimalClasses.append(class_)\n\t\n\treturn minimalClasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif sum([c.isSubclassOf(i) for i in classes]) == 0:\n\t\t\tbases.append(c)\n\treturn bases", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tattrs = self.__dict__\n\tnames = []\n\tdescriptions = []\n\tfor name, value in attrs.iteritems():\n\t\tif name.startswith('_'):\n\t\t\tcontinue\n\t\tif not all:\n\t\t\tif not value.description:\n\t\t\t\tcontinue\n\t\tnames.append(name)\n\t\tdescriptions.append(value.description)\n\treturn names, descriptions\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tres = {}\n\tfor name in dir(self):\n\t\tif not name.startswith('_'):\n\t\t\tres[name] = getattr(self, name).__doc__\n\treturn res", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tl = list()\n\tfor name in dir(self):\n\t\tdesc = getattr(self.__class__, name)\n\t\tif isinstance(desc, property):\n\t\t\tl.append((name, desc.fget.__doc__))\n\t\telif isinstance(desc, classmethod):\n\t\t\tl.append((name, desc.__doc__))\n\t\telif isinstance(desc, staticmethod):\n\t\t\tl.append((name, desc.__doc__))\n\t\telif isinstance(desc, objectmethod):\n\t\t\tl.append((name, desc.__doc__))\n\t\telif isinstance(desc, method):\n\t\t\tl.append((name, desc.__doc__))\n\t\tif all:\n\t\t\ttry:\n\t\t\t\tl.append((name, desc.__doc__))\n\t\t\texcept:\n\t\t\t\tpass\n\treturn l", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tnamesAndDescriptions = {}\n\tfor attrName in dir(self):\n\t\tattr = getattr(self, attrName)\n\t\tif isinstance(attr, property):\n\t\t\tnamesAndDescriptions[attrName] = attr.fget.__doc__\n\treturn namesAndDescriptions\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tattrs = self.__class__.__dict__\n\tif all:\n\t\treturn (attr for attr in attrs if isinstance(attrs[attr], property))\n\telse:\n\t\treturn (attr for attr in attrs if isinstance(attrs[attr], property) and attr.isupper())\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tnames = []\n\tdescriptions = []\n\tfor name in dir(self):\n\t\tif name[0] != '_':\n\t\t\tdesc = getattr(self, name).__doc__\n\t\t\tif all or desc:\n\t\t\t\tnames.append(name)\n\t\t\t\tdescriptions.append(desc)\n\treturn names, descriptions\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tresult = []\n\tfor attr in self.__class__.__dict__:\n\t\tif attr[0] == '_':\n\t\t\tcontinue\n\t\tval = getattr(self, attr)\n\t\tif not callable(val):\n\t\t\tresult.append((attr, val))\n\t\tif all:\n\t\t\tresult.append((attr, val))\n\treturn result\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tresult = dict()\n\tif all:\n\t\tresult = {self.__class__.__name__: self.__doc__}\n\telse:\n\t\tresult = {self.__class__.__dict__.get(\"name\"): self.__class__.__dict__.get(\"description\")}\n\treturn result", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\timport inspect\n\tname = self.__class__.__name__\n\tdescription = self.__class__.__doc__\n\tif all:\n\t\tnameDict = inspect.getmembers(self, lambda a:not(inspect.isroutine(a)))\n\t\tnameDict = dict([ (a[0], a[1].__doc__) for a in nameDict if a[0][0] != '_' ])\n\tdescription = description + \"\"\"\n\t\n\tAttributes:\n\t\"\"\" + '\\n'.join(['\\t%s: %s' % a for a in nameDict.items()])\n\treturn name, description\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tattrs = dir(self)\n\tif all:\n\t\treturn [(attr, getattr(self, attr)) for attr in attrs]\n\treturn [attr for attr in attrs if not attr.startswith('_')]"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tattrs = self.__dict__\n\tnames = []\n\tdescriptions = []\n\tfor name, value in attrs.iteritems():\n\t\tif name.startswith('_'):\n\t\t\tcontinue\n\t\tif not all:\n\t\t\tif not value.description:\n\t\t\t\tcontinue\n\t\tnames.append(name)\n\t\tdescriptions.append(value.description)\n\treturn names, descriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tres = {}\n\tfor name in dir(self):\n\t\tif not name.startswith('_'):\n\t\t\tres[name] = getattr(self, name).__doc__\n\treturn res", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tl = list()\n\tfor name in dir(self):\n\t\tdesc = getattr(self.__class__, name)\n\t\tif isinstance(desc, property):\n\t\t\tl.append((name, desc.fget.__doc__))\n\t\telif isinstance(desc, classmethod):\n\t\t\tl.append((name, desc.__doc__))\n\t\telif isinstance(desc, staticmethod):\n\t\t\tl.append((name, desc.__doc__))\n\t\telif isinstance(desc, objectmethod):\n\t\t\tl.append((name, desc.__doc__))\n\t\telif isinstance(desc, method):\n\t\t\tl.append((name, desc.__doc__))\n\t\tif all:\n\t\t\ttry:\n\t\t\t\tl.append((name, desc.__doc__))\n\t\t\texcept:\n\t\t\t\tpass\n\treturn l", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tnamesAndDescriptions = {}\n\tfor attrName in dir(self):\n\t\tattr = getattr(self, attrName)\n\t\tif isinstance(attr, property):\n\t\t\tnamesAndDescriptions[attrName] = attr.fget.__doc__\n\treturn namesAndDescriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tattrs = self.__class__.__dict__\n\tif all:\n\t\treturn (attr for attr in attrs if isinstance(attrs[attr], property))\n\telse:\n\t\treturn (attr for attr in attrs if isinstance(attrs[attr], property) and attr.isupper())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tnames = []\n\tdescriptions = []\n\tfor name in dir(self):\n\t\tif name[0] != '_':\n\t\t\tdesc = getattr(self, name).__doc__\n\t\t\tif all or desc:\n\t\t\t\tnames.append(name)\n\t\t\t\tdescriptions.append(desc)\n\treturn names, descriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tresult = []\n\tfor attr in self.__class__.__dict__:\n\t\tif attr[0] == '_':\n\t\t\tcontinue\n\t\tval = getattr(self, attr)\n\t\tif not callable(val):\n\t\t\tresult.append((attr, val))\n\t\tif all:\n\t\t\tresult.append((attr, val))\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tresult = dict()\n\tif all:\n\t\tresult = {self.__class__.__name__: self.__doc__}\n\telse:\n\t\tresult = {self.__class__.__dict__.get(\"name\"): self.__class__.__dict__.get(\"description\")}\n\treturn result", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\timport inspect\n\tname = self.__class__.__name__\n\tdescription = self.__class__.__doc__\n\tif all:\n\t\tnameDict = inspect.getmembers(self, lambda a:not(inspect.isroutine(a)))\n\t\tnameDict = dict([ (a[0], a[1].__doc__) for a in nameDict if a[0][0] != '_' ])\n\tdescription = description + \"\"\"\n\t\n\tAttributes:\n\t\"\"\" + '\\n'.join(['\\t%s: %s' % a for a in nameDict.items()])\n\treturn name, description\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tattrs = dir(self)\n\tif all:\n\t\treturn [(attr, getattr(self, attr)) for attr in attrs]\n\treturn [attr for attr in attrs if not attr.startswith('_')]", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tclass_name = self.__class__.__name__\n\tnames = [k for k, v in self.__class__.__dict__.items() if isinstance(v, self.__class__)]\n\tnames = \",\".join(names)\n\tif not all:\n\t\tnames = names.replace(class_name, \"\")\n\treturn names\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = dir(self)\n\tif all: return names\n\tnames.remove('names')\n\treturn names\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tname = self.__class__.__name__\n\tif all:\n\t\treturn [name + '.' + i for i in self.__dict__.keys()]\n\telse:\n\t\treturn [name + '.' + i for i in self.__dict__.keys() if not i[:1] == '_']\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = [attr for attr in dir(self) if not attr.startswith('_')]\n\tif all:\n\t\treturn names\n\n\treturn [name for name in names if not name.startswith('__')]", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tcls = type(self)\n\tattrs = list(self.__dict__.keys())\n\tif all:\n\t\tbase_attrs = []\n\t\twhile cls is not object:\n\t\t\tbase_attrs.extend(cls.__dict__.keys())\n\t\t\tcls = cls.__base__\n\t\tattrs.extend(base_attrs)\n\treturn sorted(set(attrs))\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.__dict__\n\tattrs_all = dir(self)\n\tif all:\n\t\treturn attrs_all\n\telse:\n\t\treturn [attr for attr in attrs_all if attr not in attrs]", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = []\n\tfor name, attr in self.__class__.__dict__.items():\n\t\tif not name.startswith('_'):\n\t\t\tnames.append(name)\n\tif all:\n\t\tfor name, attr in self.__dict__.items():\n\t\t\tif not name.startswith('_'):\n\t\t\t\tnames.append(name)\n\treturn names", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tn = [c for c in self.__dict__.keys() if not c.startswith('_')]\n\tif not all:\n\t\tn = [c for c in n if not c.startswith('__')]\n\treturn n\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tdef _recursive_names(cls):\n\t\tnames = []\n\t\tif cls.__module__ != \"__main__\":\n\t\t\tnames.append(cls.__module__)\n\t\tnames.append(cls.__name__)\n\t\tif all:\n\t\t\tnames.extend(dir(cls))\n\t\treturn names\n\t\n\tif all:\n\t\treturn _recursive_names(self)\n\telse:\n\t\treturn [n for n in _recursive_names(self) if not n.startswith(\"_\")]\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.__dict__\n\tif all:\n\t\treturn list(attrs)\n\tres = []\n\tfor k in attrs:\n\t\tif k.startswith('_'):\n\t\t\tcontinue\n\t\tres.append(k)\n\treturn res"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tclass_name = self.__class__.__name__\n\tnames = [k for k, v in self.__class__.__dict__.items() if isinstance(v, self.__class__)]\n\tnames = \",\".join(names)\n\tif not all:\n\t\tnames = names.replace(class_name, \"\")\n\treturn names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = dir(self)\n\tif all: return names\n\tnames.remove('names')\n\treturn names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tname = self.__class__.__name__\n\tif all:\n\t\treturn [name + '.' + i for i in self.__dict__.keys()]\n\telse:\n\t\treturn [name + '.' + i for i in self.__dict__.keys() if not i[:1] == '_']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = [attr for attr in dir(self) if not attr.startswith('_')]\n\tif all:\n\t\treturn names\n\n\treturn [name for name in names if not name.startswith('__')]", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tcls = type(self)\n\tattrs = list(self.__dict__.keys())\n\tif all:\n\t\tbase_attrs = []\n\t\twhile cls is not object:\n\t\t\tbase_attrs.extend(cls.__dict__.keys())\n\t\t\tcls = cls.__base__\n\t\tattrs.extend(base_attrs)\n\treturn sorted(set(attrs))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.__dict__\n\tattrs_all = dir(self)\n\tif all:\n\t\treturn attrs_all\n\telse:\n\t\treturn [attr for attr in attrs_all if attr not in attrs]", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = []\n\tfor name, attr in self.__class__.__dict__.items():\n\t\tif not name.startswith('_'):\n\t\t\tnames.append(name)\n\tif all:\n\t\tfor name, attr in self.__dict__.items():\n\t\t\tif not name.startswith('_'):\n\t\t\t\tnames.append(name)\n\treturn names", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tn = [c for c in self.__dict__.keys() if not c.startswith('_')]\n\tif not all:\n\t\tn = [c for c in n if not c.startswith('__')]\n\treturn n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tdef _recursive_names(cls):\n\t\tnames = []\n\t\tif cls.__module__ != \"__main__\":\n\t\t\tnames.append(cls.__module__)\n\t\tnames.append(cls.__name__)\n\t\tif all:\n\t\t\tnames.extend(dir(cls))\n\t\treturn names\n\t\n\tif all:\n\t\treturn _recursive_names(self)\n\telse:\n\t\treturn [n for n in _recursive_names(self) if not n.startswith(\"_\")]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.__dict__\n\tif all:\n\t\treturn list(attrs)\n\tres = []\n\tfor k in attrs:\n\t\tif k.startswith('_'):\n\t\t\tcontinue\n\t\tres.append(k)\n\treturn res", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom . import Declaration\n\tfrom . import Interface\n\n\tdef _normalize(arg):\n\t\t\"\"\"\n\t\tNormalize an argument\n\t\t\"\"\"\n\t\tif isinstance(arg, Declaration):\n\t\t\treturn arg\n\n\t\tif isinstance(arg, tuple):\n\t\t\tname, interface = arg\n\t\t\tif isinstance(interface, Interface):\n\t\t\t\treturn Declaration(name, interface)\n\t\t\telse:\n\t\t\t\treturn Declaration(name, Interface.fromSpec(interface))\n\n\t\tif isinstance(arg, Interface):\n\t\t\treturn Declaration(None, arg)\n\n\t\tif isinstance(arg, str):\n\t\t\treturn Declaration(None, Interface.fromSpec(arg))\n\n\t\treturn Declaration(None, Interface.fromSpec(arg))\n\n\tif output is None:\n\t\toutput = []\n\n\tfor arg in sequence:\n\t\tif isinstance(arg, list):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(_normalize(arg))\n\n\treturn output\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnormalized = []\n\tfor arg in sequence:\n\t\tif not isinstance(arg, basestring):\n\t\t\tcontinue\n\t\tif output is not None:\n\t\t\targ = arg.replace('out', 'out:%s' % output)\n\t\tnormalized.extend(arg.split(','))\n\treturn normalized\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\n\tfor arg in sequence:\n\t\tif isinstance(arg, tuple):\n\t\t\toutput.append(arg)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tspecs = _normalizeargs(arg.implements, output)\n\t\t\texcept AttributeError:\n\t\t\t\tspecs = [arg]\n\n\t\t\tfor spec in specs:\n\t\t\t\toutput.append((arg, spec))\n\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport re\n\n\toutput = output or []\n\tfor arg in sequence:\n\t\tif isinstance(arg, str):\n\t\t\toutput.append(arg)\n\t\telif isinstance(arg, tuple):\n\t\t\toutput.append(arg[0])\n\t\telse:\n\t\t\toutput.append(arg.__name__)\n\treturn output\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tout = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, (tuple, list)):\n\t\t\tout.extend(_normalizeargs(arg))\n\t\telse:\n\t\t\tout.append(arg)\n\treturn out", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tresults = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Interface) or isinstance(arg, Implements):\n\t\t\tresults.append(arg)\n\t\telif isinstance(arg, tuple):\n\t\t\tresults.extend(arg)\n\t\telse:\n\t\t\tresults.append(arg)\n\treturn results\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tresult = []\n\tfor item in sequence:\n\t\tif isinstance(item, tuple):\n\t\t\tresult.extend(item)\n\t\telif isinstance(item, Interface):\n\t\t\tresult.append(item)\n\t\telse:\n\t\t\tresult.append(item.implements)\n\treturn result\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tif not isinstance(sequence, tuple):\n\t\tsequence = (sequence,)\n\t\n\tnormalized = []\n\t\n\tfor item in sequence:\n\t\tif isinstance(item, tuple):\n\t\t\tnormalized.extend(item)\n\t\telif isinstance(item, Interface):\n\t\t\tnormalized.append(item)\n\t\telif isinstance(item, Implements):\n\t\t\tnormalized.append(item)\n\t\telse:\n\t\t\traise TypeError(\"Unsupported declaration argument: %r\" % (item,))\n\t\n\tif not output:\n\t\treturn tuple(normalized)\n\telse:\n\t\toutput.extend(normalized)\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tr = []\n\tfor a in sequence:\n\t\tif isinstance(a, Interface):\n\t\t\tr.append(a)\n\t\telif isinstance(a, tuple):\n\t\t\tfor e in _normalizeargs(a):\n\t\t\t\tr.append(e)\n\t\telse:\n\t\t\tr.append(a)\n\treturn r\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tpass"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom . import Declaration\n\tfrom . import Interface\n\n\tdef _normalize(arg):\n\t\t\"\"\"\n\t\tNormalize an argument\n\t\t\"\"\"\n\t\tif isinstance(arg, Declaration):\n\t\t\treturn arg\n\n\t\tif isinstance(arg, tuple):\n\t\t\tname, interface = arg\n\t\t\tif isinstance(interface, Interface):\n\t\t\t\treturn Declaration(name, interface)\n\t\t\telse:\n\t\t\t\treturn Declaration(name, Interface.fromSpec(interface))\n\n\t\tif isinstance(arg, Interface):\n\t\t\treturn Declaration(None, arg)\n\n\t\tif isinstance(arg, str):\n\t\t\treturn Declaration(None, Interface.fromSpec(arg))\n\n\t\treturn Declaration(None, Interface.fromSpec(arg))\n\n\tif output is None:\n\t\toutput = []\n\n\tfor arg in sequence:\n\t\tif isinstance(arg, list):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(_normalize(arg))\n\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnormalized = []\n\tfor arg in sequence:\n\t\tif not isinstance(arg, basestring):\n\t\t\tcontinue\n\t\tif output is not None:\n\t\t\targ = arg.replace('out', 'out:%s' % output)\n\t\tnormalized.extend(arg.split(','))\n\treturn normalized\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\n\tfor arg in sequence:\n\t\tif isinstance(arg, tuple):\n\t\t\toutput.append(arg)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tspecs = _normalizeargs(arg.implements, output)\n\t\t\texcept AttributeError:\n\t\t\t\tspecs = [arg]\n\n\t\t\tfor spec in specs:\n\t\t\t\toutput.append((arg, spec))\n\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport re\n\n\toutput = output or []\n\tfor arg in sequence:\n\t\tif isinstance(arg, str):\n\t\t\toutput.append(arg)\n\t\telif isinstance(arg, tuple):\n\t\t\toutput.append(arg[0])\n\t\telse:\n\t\t\toutput.append(arg.__name__)\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tout = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, (tuple, list)):\n\t\t\tout.extend(_normalizeargs(arg))\n\t\telse:\n\t\t\tout.append(arg)\n\treturn out", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tresults = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Interface) or isinstance(arg, Implements):\n\t\t\tresults.append(arg)\n\t\telif isinstance(arg, tuple):\n\t\t\tresults.extend(arg)\n\t\telse:\n\t\t\tresults.append(arg)\n\treturn results\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tresult = []\n\tfor item in sequence:\n\t\tif isinstance(item, tuple):\n\t\t\tresult.extend(item)\n\t\telif isinstance(item, Interface):\n\t\t\tresult.append(item)\n\t\telse:\n\t\t\tresult.append(item.implements)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tif not isinstance(sequence, tuple):\n\t\tsequence = (sequence,)\n\t\n\tnormalized = []\n\t\n\tfor item in sequence:\n\t\tif isinstance(item, tuple):\n\t\t\tnormalized.extend(item)\n\t\telif isinstance(item, Interface):\n\t\t\tnormalized.append(item)\n\t\telif isinstance(item, Implements):\n\t\t\tnormalized.append(item)\n\t\telse:\n\t\t\traise TypeError(\"Unsupported declaration argument: %r\" % (item,))\n\t\n\tif not output:\n\t\treturn tuple(normalized)\n\telse:\n\t\toutput.extend(normalized)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tr = []\n\tfor a in sequence:\n\t\tif isinstance(a, Interface):\n\t\t\tr.append(a)\n\t\telif isinstance(a, tuple):\n\t\t\tfor e in _normalizeargs(a):\n\t\t\t\tr.append(e)\n\t\telse:\n\t\t\tr.append(a)\n\treturn r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\n# Try to import the C optimizations module.\n_c_optimizations = _c_optimizations_available()\n\n# If the C optimizations module failed to load, use the Python optimizations.\nif not _c_optimizations:\n\timport warnings\n\twarnings.warn('The C optimizations module failed to load. This will result in slower performance.', RuntimeWarning)\n\tfrom . import _optimizations_py as _optimizations\nelse:\n\tfrom . import _optimizations\n\n# Expose the functions for the user.\nfrom ._optimizations import *\n\n# Expose the version for the user.\nfrom ._version import __version__\n\n# Expose the license for the user.\nfrom ._license import __license__", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import c_optimizations\n\texcept ImportError:\n\t\tc_optimizations = None\n\treturn c_optimizations\n\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport Cython.Compiler.Options\n\t\timport Cython.Compiler.Main\n\t\timport Cython.Build.Dependencies\n\t\timport Cython.Build.Includes\n\t\timport Cython.Build.Utils\n\t\timport Cython.Compiler.Version\n\t\timport Cython.Compiler.Flatten\n\t\timport Cython.Compiler.Exceptions\n\t\timport Cython.Compiler.Errors\n\t\timport Cython.Compiler.TreeTransforms\n\t\timport Cython.Compiler.Visitor\n\t\timport Cython.Compiler.Parse\n\t\timport Cython.Compiler.Flow\n\t\timport Cython.Compiler.FlowGraph\n\t\timport Cython.Compiler.FlowGraphNodes\n\t\timport Cython.Compiler.Visitor\n\t\timport Cython.Compiler.Visitors\n\t\timport Cython.Compiler.Visitors.FlowGraphVisitors\n\t\timport Cython.Compiler.Visitors.C\n\t\treturn Cython\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _coptimizations\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn _coptimizations\n\n_c_optimizations = _c_optimizations_available()\n\nif _c_optimizations:\n\tdef _c_optimized_min(vector):\n\t\t\"\"\"\n\t\tMinimize a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.min(vector)\n\n\tdef _c_optimized_max(vector):\n\t\t\"\"\"\n\t\tMaximize a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.max(vector)\n\n\tdef _c_optimized_sum(vector):\n\t\t\"\"\"\n\t\tSum up a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.sum(vector)\n\n\tdef _c_optimized_product(vector):\n\t\t\"\"\"\n\t\tMultiply a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.product(vector)\nelse:\n\tdef _c_optimized_min(vector):\n\t\t\"\"\"\n\t\tMinimize a vector using the Python implementation.\n\t\t\"\"\"\n\t\treturn min(vector)\n\n\tdef _c_optimized_max", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n_c_optimizations = _c_optimizations_available()\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .c_optimizations import c_opt as c_opt\n\t\treturn c_opt\n\texcept ImportError:\n\t\tpass\n\treturn None\n\nc_opt = _c_optimizations_available()", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom Crypto.Util.asm import _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n\n_c_optimizations = _c_optimizations_available()\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport Cython\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tfrom ._c_optimizations import *\n\texcept ImportError:\n\t\treturn False\n\n\treturn True\n\n\nif _c_optimizations_available():\n\tfrom ._c_optimizations import (\n\t\toptimized_pixel_count,\n\t\toptimized_pixel_count_fast,\n\t)\nelse:\n\tfrom ._python_optimizations import (\n\t\toptimized_pixel_count,\n\t\toptimized_pixel_count_fast,\n\t)"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\n# Try to import the C optimizations module.\n_c_optimizations = _c_optimizations_available()\n\n# If the C optimizations module failed to load, use the Python optimizations.\nif not _c_optimizations:\n\timport warnings\n\twarnings.warn('The C optimizations module failed to load. This will result in slower performance.', RuntimeWarning)\n\tfrom . import _optimizations_py as _optimizations\nelse:\n\tfrom . import _optimizations\n\n# Expose the functions for the user.\nfrom ._optimizations import *\n\n# Expose the version for the user.\nfrom ._version import __version__\n\n# Expose the license for the user.\nfrom ._license import __license__", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import c_optimizations\n\texcept ImportError:\n\t\tc_optimizations = None\n\treturn c_optimizations\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport Cython.Compiler.Options\n\t\timport Cython.Compiler.Main\n\t\timport Cython.Build.Dependencies\n\t\timport Cython.Build.Includes\n\t\timport Cython.Build.Utils\n\t\timport Cython.Compiler.Version\n\t\timport Cython.Compiler.Flatten\n\t\timport Cython.Compiler.Exceptions\n\t\timport Cython.Compiler.Errors\n\t\timport Cython.Compiler.TreeTransforms\n\t\timport Cython.Compiler.Visitor\n\t\timport Cython.Compiler.Parse\n\t\timport Cython.Compiler.Flow\n\t\timport Cython.Compiler.FlowGraph\n\t\timport Cython.Compiler.FlowGraphNodes\n\t\timport Cython.Compiler.Visitor\n\t\timport Cython.Compiler.Visitors\n\t\timport Cython.Compiler.Visitors.FlowGraphVisitors\n\t\timport Cython.Compiler.Visitors.C\n\t\treturn Cython\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _coptimizations\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn _coptimizations\n\n_c_optimizations = _c_optimizations_available()\n\nif _c_optimizations:\n\tdef _c_optimized_min(vector):\n\t\t\"\"\"\n\t\tMinimize a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.min(vector)\n\n\tdef _c_optimized_max(vector):\n\t\t\"\"\"\n\t\tMaximize a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.max(vector)\n\n\tdef _c_optimized_sum(vector):\n\t\t\"\"\"\n\t\tSum up a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.sum(vector)\n\n\tdef _c_optimized_product(vector):\n\t\t\"\"\"\n\t\tMultiply a vector using the C implementation.\n\t\t\"\"\"\n\t\treturn _c_optimizations.product(vector)\nelse:\n\tdef _c_optimized_min(vector):\n\t\t\"\"\"\n\t\tMinimize a vector using the Python implementation.\n\t\t\"\"\"\n\t\treturn min(vector)\n\n\tdef _c_optimized_max", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n_c_optimizations = _c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .c_optimizations import c_opt as c_opt\n\t\treturn c_opt\n\texcept ImportError:\n\t\tpass\n\treturn None\n\nc_opt = _c_optimizations_available()", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom Crypto.Util.asm import _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n\n_c_optimizations = _c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport Cython\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tfrom ._c_optimizations import *\n\texcept ImportError:\n\t\treturn False\n\n\treturn True\n\n\nif _c_optimizations_available():\n\tfrom ._c_optimizations import (\n\t\toptimized_pixel_count,\n\t\toptimized_pixel_count_fast,\n\t)\nelse:\n\tfrom ._python_optimizations import (\n\t\toptimized_pixel_count,\n\t\toptimized_pixel_count_fast,\n\t)", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport pyximport\n\t\tpyximport.install()\n\texcept ImportError:\n\t\tpass\n\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport sys\n\treturn sys.platform == \"win32\"\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom SCons.Script import BUILD_TYPE\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn BUILD_TYPE.lower() == \"debug\"\n\t", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport sysconfig\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tconfig_var = sysconfig.get_config_var\n\texcept AttributeError:\n\t\treturn False\n\n\ttry:\n\t\topt = config_var('Py_ENABLE_SHARED')\n\texcept AttributeError:\n\t\topt = config_var('Py_ENABLE_SHARED')\n\n\tif opt is None:\n\t\treturn False\n\n\treturn opt.lower() in ('1', 'y', 'yes', 'true')", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport platform\n\treturn platform.python_implementation() == \"CPython\"\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom SCons.Script import ARGUMENTS\n\t\tfrom SCons.Script import ARGUMENTS\n\t\ttry:\n\t\t\treturn ARGUMENTS['optimize'][0]\n\t\texcept KeyError:\n\t\t\treturn False\n\texcept ImportError:\n\t\treturn False\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport platform\n\treturn platform.python_implementation() != \"PyPy\"\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\tfrom sys import platform as sys_platform\n\t# We only support C optimizations on Linux.\n\treturn sys_platform.startswith('linux')\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\timport platform\n\t\timport sys\n\texcept ImportError:\n\t\t# We can't import ctypes so we can't use C optimizations.\n\t\treturn False\n\n\t# If we built with a different compiler, we can't use the C optimizations.\n\tif (platform.python_compiler() != 'GCC'):\n\t\treturn False\n\n\t# If the C optimizations are disabled, we can't use them.\n\tif (not ctypes.sizeof(ctypes.c_void_p) == ctypes.sizeof(ctypes.c_int)):\n\t\treturn False\n\n\t# On Windows, we can't use the C optimizations.\n\tif (sys.platform.startswith('win')):\n\t\treturn False\n\n\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom . import cython_optimized\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\nif _should_attempt_c_optimizations():\n\tfrom .cython_optimized import *\nelse:\n\tfrom .python_optimized import *"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport pyximport\n\t\tpyximport.install()\n\texcept ImportError:\n\t\tpass\n\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport sys\n\treturn sys.platform == \"win32\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom SCons.Script import BUILD_TYPE\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn BUILD_TYPE.lower() == \"debug\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport sysconfig\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tconfig_var = sysconfig.get_config_var\n\texcept AttributeError:\n\t\treturn False\n\n\ttry:\n\t\topt = config_var('Py_ENABLE_SHARED')\n\texcept AttributeError:\n\t\topt = config_var('Py_ENABLE_SHARED')\n\n\tif opt is None:\n\t\treturn False\n\n\treturn opt.lower() in ('1', 'y', 'yes', 'true')", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport platform\n\treturn platform.python_implementation() == \"CPython\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom SCons.Script import ARGUMENTS\n\t\tfrom SCons.Script import ARGUMENTS\n\t\ttry:\n\t\t\treturn ARGUMENTS['optimize'][0]\n\t\texcept KeyError:\n\t\t\treturn False\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport platform\n\treturn platform.python_implementation() != \"PyPy\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\tfrom sys import platform as sys_platform\n\t# We only support C optimizations on Linux.\n\treturn sys_platform.startswith('linux')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\timport platform\n\t\timport sys\n\texcept ImportError:\n\t\t# We can't import ctypes so we can't use C optimizations.\n\t\treturn False\n\n\t# If we built with a different compiler, we can't use the C optimizations.\n\tif (platform.python_compiler() != 'GCC'):\n\t\treturn False\n\n\t# If the C optimizations are disabled, we can't use them.\n\tif (not ctypes.sizeof(ctypes.c_void_p) == ctypes.sizeof(ctypes.c_int)):\n\t\treturn False\n\n\t# On Windows, we can't use the C optimizations.\n\tif (sys.platform.startswith('win')):\n\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom . import cython_optimized\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\nif _should_attempt_c_optimizations():\n\tfrom .cython_optimized import *\nelse:\n\tfrom .python_optimized import *", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('PURE_PYTHON', '0') != '0'\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.getenv(\"PURE_PYTHON\") is not None and bool(int(os.getenv(\"PURE_PYTHON\")))\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\ttry:\n\t\treturn bool(int(os.environ.get(\"PURE_PYTHON\", 0)))\n\texcept ValueError:\n\t\treturn False", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\treturn bool(environ.get(\"PURE_PYTHON\"))\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\tif os.environ.get(\"PURE_PYTHON\"):\n\t\tif os.environ.get(\"PURE_PYTHON\").lower() in [\"false\", \"0\"]:\n\t\t\treturn False\n\treturn True\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.getenv(\"PURE_PYTHON\") is not None and os.getenv(\"PURE_PYTHON\") != \"0\"\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\n\treturn os.environ.get(\"PURE_PYTHON\", None) != \"0\"\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\timport warnings\n\twarnings.warn(\"_c_optimizations_ignored is deprecated, use c_optimizations_ignored instead\", DeprecationWarning)\n\treturn bool(os.environ.get(\"PURE_PYTHON\", None))\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get(\"PURE_PYTHON\", False) != \"0\"\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get(\"PURE_PYTHON\", \"0\") != \"0\"\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('PURE_PYTHON', '0') != '0'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.getenv(\"PURE_PYTHON\") is not None and bool(int(os.getenv(\"PURE_PYTHON\")))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\ttry:\n\t\treturn bool(int(os.environ.get(\"PURE_PYTHON\", 0)))\n\texcept ValueError:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\treturn bool(environ.get(\"PURE_PYTHON\"))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\tif os.environ.get(\"PURE_PYTHON\"):\n\t\tif os.environ.get(\"PURE_PYTHON\").lower() in [\"false\", \"0\"]:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.getenv(\"PURE_PYTHON\") is not None and os.getenv(\"PURE_PYTHON\") != \"0\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\n\treturn os.environ.get(\"PURE_PYTHON\", None) != \"0\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\timport warnings\n\twarnings.warn(\"_c_optimizations_ignored is deprecated, use c_optimizations_ignored instead\", DeprecationWarning)\n\treturn bool(os.environ.get(\"PURE_PYTHON\", None))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get(\"PURE_PYTHON\", False) != \"0\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get(\"PURE_PYTHON\", \"0\") != \"0\"\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\timport sys\n\tif sys.version_info[:2] < (3, 8):\n\t\treturn os.getenv('_PYTHON_C_OPTIMIZATIONS_REQUIRED') == '1'\n\telse:\n\t\treturn False", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\tfrom .cython.optimized_c import _c_optimizations_required\n\t\treturn _c_optimizations_required()\n\texcept:\n\t\treturn False", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\treturn sys.maxint == 9223372036854775807\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\timport platform\n\timport os\n\n\t# We need to check if the platform is Windows, because it is not\n\t# possible to disable optimizations on Windows.\n\t#\n\t# We don't need to check for the Python platform because the C\n\t# optimizations are disabled anyway on Python 3.x.\n\n\treturn sys.version_info[0] == 2 and platform.system() == 'Windows'\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport _ctypes_test\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\treturn sys.maxint >= 2**31\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\treturn sys.maxint > 2**32\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\tfrom ._c_optimizations_required import _c_optimizations_required\n\treturn _c_optimizations_required()", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\treturn 'C_OPTIMIZATIONS_ENABLED' in os.environ\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\timport sys\n\tif sys.version_info[:2] < (3, 8):\n\t\treturn os.getenv('_PYTHON_C_OPTIMIZATIONS_REQUIRED') == '1'\n\telse:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\tfrom .cython.optimized_c import _c_optimizations_required\n\t\treturn _c_optimizations_required()\n\texcept:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\treturn sys.maxint == 9223372036854775807\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\timport platform\n\timport os\n\n\t# We need to check if the platform is Windows, because it is not\n\t# possible to disable optimizations on Windows.\n\t#\n\t# We don't need to check for the Python platform because the C\n\t# optimizations are disabled anyway on Python 3.x.\n\n\treturn sys.version_info[0] == 2 and platform.system() == 'Windows'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport _ctypes_test\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\treturn sys.maxint >= 2**31\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\treturn sys.maxint > 2**32\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\tfrom ._c_optimizations_required import _c_optimizations_required\n\treturn _c_optimizations_required()", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\treturn 'C_OPTIMIZATIONS_ENABLED' in os.environ\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.__initialize__()", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.reset_bins()", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = {}\n\tself.bin_weights = {}\n\tself.reset_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.reset_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset_context()\n\tself.reset_bins()\n\tself.reset_indexes()\n\tself.reset_filters()\n\tself.reset_statistics()\n\tself.reset_flags()\n\t", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context.clear()\n\tself.initialize_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n\treturn self\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.initialize()\n\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tself.bins = {}\n\tself.__initial_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.ctx = {}\n\tself.bins = {}\n\tfor name in self.bins_names:\n\t\tself.bins[name] = self.make_bins(name)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.__initialize__()", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.reset_bins()", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = {}\n\tself.bin_weights = {}\n\tself.reset_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.reset_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset_context()\n\tself.reset_bins()\n\tself.reset_indexes()\n\tself.reset_filters()\n\tself.reset_statistics()\n\tself.reset_flags()\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context.clear()\n\tself.initialize_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.initialize()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tself.bins = {}\n\tself.__initial_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.ctx = {}\n\tself.bins = {}\n\tfor name in self.bins_names:\n\t\tself.bins[name] = self.make_bins(name)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tpass", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise NotImplementedError(\"to_csv is not implemented\")\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tpass\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.utils import to_csv\n\tto_csv(self, separator, header)\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\toutfile = csv.writer(self.output, delimiter = separator)\n\tif header is not None:\n\t\toutfile.writerow(header)\n\tfor point in self:\n\t\toutfile.writerow(point)", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise NotImplementedError(\"to_csv is not used in Lena 0.5\")", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise RuntimeError(\"to_csv is not used anymore\")\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\tfrom lena.error import SequenceError\n\tfrom lena.output import ToCSV\n\traise SequenceError(ToCSV(separator, header))", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is deprecated and will be removed in Lena 0.5\")", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is not used in Lena 0.5. Use sequences instead.\")\n\t\n\tif isinstance(separator, str):\n\t\tseparator = separator.encode()\n\t\n\tif isinstance(header, str):\n\t\theader = header.encode()\n\t\n\tif header:\n\t\tyield header\n\t\n\tfor point in self:\n\t\tyield separator.join(self.graph.format_point_coordinate(point))\n\t\tfor value in self[point]:\n\t\t\tyield separator.join(self.graph.format_point_value(value))\n\t\n\tyield \"\""], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise NotImplementedError(\"to_csv is not implemented\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.utils import to_csv\n\tto_csv(self, separator, header)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\toutfile = csv.writer(self.output, delimiter = separator)\n\tif header is not None:\n\t\toutfile.writerow(header)\n\tfor point in self:\n\t\toutfile.writerow(point)", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise NotImplementedError(\"to_csv is not used in Lena 0.5\")", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise RuntimeError(\"to_csv is not used anymore\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\tfrom lena.error import SequenceError\n\tfrom lena.output import ToCSV\n\traise SequenceError(ToCSV(separator, header))", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is deprecated and will be removed in Lena 0.5\")", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is not used in Lena 0.5. Use sequences instead.\")\n\t\n\tif isinstance(separator, str):\n\t\tseparator = separator.encode()\n\t\n\tif isinstance(header, str):\n\t\theader = header.encode()\n\t\n\tif header:\n\t\tyield header\n\t\n\tfor point in self:\n\t\tyield separator.join(self.graph.format_point_coordinate(point))\n\t\tfor value in self[point]:\n\t\t\tyield separator.join(self.graph.format_point_value(value))\n\t\n\tyield \"\"", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tself.check_if_coord_exists(coord_name)\n\terror_indices = self.errors.get_by_coordinate(coord_name)\n\t# error_indices = self.errors.get_by_coordinate(coord_name)\n\treturn error_indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tidx = []\n\tfor i, coord in enumerate(self.coords):\n\t\tif coord.name == coord_name:\n\t\t\tidx.append(i)\n\treturn idx", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor k, v in self.coords.items():\n\t\tif k.endswith(coord_name):\n\t\t\tindices += set([i for i, e in enumerate(self.errors) if e[k]])\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\terror_dict = self.get_error_dict()\n\tfor coord, errs in error_dict.items():\n\t\tfor err in errs:\n\t\t\tif coord_name in err:\n\t\t\t\tindices.append(err.index(coord_name))\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\n\tfor i, c in enumerate(self.coords):\n\t\tif c.name == coord_name:\n\t\t\tindices.append(i)\n\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor i, err in enumerate(self.errors):\n\t\tif err.name == coord_name:\n\t\t\tindices.append(i)\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tidx = []\n\tfor i, c in enumerate(self.coords):\n\t\tif c.coord.name == coord_name:\n\t\t\tidx.append(i)\n\treturn idx\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tcoord_name = coord_name.lower()\n\tidx_start = self.err_name.lower().find(coord_name)\n\tidx_end = idx_start + len(coord_name)\n\tidx_start = self.err_name.rfind(\"(\", 0, idx_start)\n\tidx_end = self.err_name.find(\")\", idx_end)\n\treturn idx_start, idx_end\n\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\t# Initialise\n\tindices = []\n\n\t# Iterate over all fields\n\tfor field in self.fields:\n\n\t\t# Check if error field\n\t\tif 'err_' in field:\n\n\t\t\t# Check if name corresponds to error field\n\t\t\tif field.split('_')[2] == coord_name:\n\n\t\t\t\t# Add index\n\t\t\t\tindices.append(self.fields.index(field))\n\n\t# Return indices\n\treturn indices\n\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tself.err_indices = (\n\t\tself.data_df[coord_name].isna() |\n\t\tself.data_df[coord_name].str.contains('nan|NaN|nan|NAN|Nan')\n\t)\n\treturn self\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tself.check_if_coord_exists(coord_name)\n\terror_indices = self.errors.get_by_coordinate(coord_name)\n\t# error_indices = self.errors.get_by_coordinate(coord_name)\n\treturn error_indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tidx = []\n\tfor i, coord in enumerate(self.coords):\n\t\tif coord.name == coord_name:\n\t\t\tidx.append(i)\n\treturn idx", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor k, v in self.coords.items():\n\t\tif k.endswith(coord_name):\n\t\t\tindices += set([i for i, e in enumerate(self.errors) if e[k]])\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\terror_dict = self.get_error_dict()\n\tfor coord, errs in error_dict.items():\n\t\tfor err in errs:\n\t\t\tif coord_name in err:\n\t\t\t\tindices.append(err.index(coord_name))\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\n\tfor i, c in enumerate(self.coords):\n\t\tif c.name == coord_name:\n\t\t\tindices.append(i)\n\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor i, err in enumerate(self.errors):\n\t\tif err.name == coord_name:\n\t\t\tindices.append(i)\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tidx = []\n\tfor i, c in enumerate(self.coords):\n\t\tif c.coord.name == coord_name:\n\t\t\tidx.append(i)\n\treturn idx\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tcoord_name = coord_name.lower()\n\tidx_start = self.err_name.lower().find(coord_name)\n\tidx_end = idx_start + len(coord_name)\n\tidx_start = self.err_name.rfind(\"(\", 0, idx_start)\n\tidx_end = self.err_name.find(\")\", idx_end)\n\treturn idx_start, idx_end\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\t# Initialise\n\tindices = []\n\n\t# Iterate over all fields\n\tfor field in self.fields:\n\n\t\t# Check if error field\n\t\tif 'err_' in field:\n\n\t\t\t# Check if name corresponds to error field\n\t\t\tif field.split('_')[2] == coord_name:\n\n\t\t\t\t# Add index\n\t\t\t\tindices.append(self.fields.index(field))\n\n\t# Return indices\n\treturn indices\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tself.err_indices = (\n\t\tself.data_df[coord_name].isna() |\n\t\tself.data_df[coord_name].str.contains('nan|NaN|nan|NAN|Nan')\n\t)\n\treturn self\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError(\"Abstract method\")\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tsuper()._update_context(context)\n\n\tdef _update_error_context(subcontext, name, i):\n\t\tsubcontext.setdefault(\"error\", {}).setdefault(name, {}).setdefault(\"index\", []).append(i)\n\n\tfor i, error in enumerate(self.get_field(\"error\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"x\", i)\n\tfor i, error in enumerate(self.get_field(\"error_x_low\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"x_low\", i)\n\tfor i, error in enumerate(self.get_field(\"error_x_high\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"x_high\", i)\n\n\tfor i, error in enumerate(self.get_field(\"error_y\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"y\", i)\n\tfor i, error in enumerate(self.get_field(\"error_y_low\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"y_low\", i)\n\tfor i, error in enumerate(self", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError(\"Not implemented for \" + str(self.__class__))\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError(\"update_context\")\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tcontext[\"graph\"] = self\n\tcontext[\"error\"] = {}\n\tfor i, (name, error) in enumerate(zip(self.fields, self.errors)):\n\t\tcontext[\"error\"][name] = {}\n\t\tif error is not None:\n\t\t\tfor key in error:\n\t\t\t\tcontext[\"error\"][name][key] = {}\n\t\t\t\tcontext[\"error\"][name][key][\"index\"] = i\n\t\n\tself.update_context(context)\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\terror = context.get(\"error\", {})\n\tcontext[\"error\"] = error\n\n\tcontext.setdefault(\"value\", {})[\"error\"] = {}\n\n\tfor error_name, error_index in self.get_error_indices().items():\n\t\terror_subcontext = error.setdefault(error_name, {})\n\t\terror_subcontext[\"index\"] = error_index\n\n\t\tif \"low\" in self.fields:\n\t\t\tif error_name == \"x\":\n\t\t\t\terror_subcontext[\"low\"] = error_index + 1\n\t\t\telif error_name == \"y\":\n\t\t\t\terror_subcontext[\"low\"] = error_index + 1\n\t\t\telif error_name == \"z\":\n\t\t\t\terror_subcontext[\"low\"] = error_index + 1\n\n\t\tif \"high\" in self.fields:\n\t\t\tif error_name == \"x\":\n\t\t\t\terror_subcontext[\"high\"] = error_index + 1\n\t\t\telif error_name == \"y\":\n\t\t\t\terror_subcontext[\"high\"] = error_index + 1\n\t\t\telif error_name == \"z\":\n\t\t\t\terror_subcontext[\"high\"] = error_index + 1", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError(\"Abstract method\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tsuper()._update_context(context)\n\n\tdef _update_error_context(subcontext, name, i):\n\t\tsubcontext.setdefault(\"error\", {}).setdefault(name, {}).setdefault(\"index\", []).append(i)\n\n\tfor i, error in enumerate(self.get_field(\"error\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"x\", i)\n\tfor i, error in enumerate(self.get_field(\"error_x_low\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"x_low\", i)\n\tfor i, error in enumerate(self.get_field(\"error_x_high\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"x_high\", i)\n\n\tfor i, error in enumerate(self.get_field(\"error_y\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"y\", i)\n\tfor i, error in enumerate(self.get_field(\"error_y_low\")):\n\t\tif error is not None:\n\t\t\t_update_error_context(context, \"y_low\", i)\n\tfor i, error in enumerate(self", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError(\"Not implemented for \" + str(self.__class__))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError(\"update_context\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tcontext[\"graph\"] = self\n\tcontext[\"error\"] = {}\n\tfor i, (name, error) in enumerate(zip(self.fields, self.errors)):\n\t\tcontext[\"error\"][name] = {}\n\t\tif error is not None:\n\t\t\tfor key in error:\n\t\t\t\tcontext[\"error\"][name][key] = {}\n\t\t\t\tcontext[\"error\"][name][key][\"index\"] = i\n\t\n\tself.update_context(context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\terror = context.get(\"error\", {})\n\tcontext[\"error\"] = error\n\n\tcontext.setdefault(\"value\", {})[\"error\"] = {}\n\n\tfor error_name, error_index in self.get_error_indices().items():\n\t\terror_subcontext = error.setdefault(error_name, {})\n\t\terror_subcontext[\"index\"] = error_index\n\n\t\tif \"low\" in self.fields:\n\t\t\tif error_name == \"x\":\n\t\t\t\terror_subcontext[\"low\"] = error_index + 1\n\t\t\telif error_name == \"y\":\n\t\t\t\terror_subcontext[\"low\"] = error_index + 1\n\t\t\telif error_name == \"z\":\n\t\t\t\terror_subcontext[\"low\"] = error_index + 1\n\n\t\tif \"high\" in self.fields:\n\t\t\tif error_name == \"x\":\n\t\t\t\terror_subcontext[\"high\"] = error_index + 1\n\t\t\telif error_name == \"y\":\n\t\t\t\terror_subcontext[\"high\"] = error_index + 1\n\t\t\telif error_name == \"z\":\n\t\t\t\terror_subcontext[\"high\"] = error_index + 1", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(1, len(bins)):\n\t\ty1 = bins[i - 1]\n\t\ty2 = bins[i]\n\t\tx1 = edges[i - 1]\n\t\tx2 = edges[i]\n\t\tarea += (y1 + y2) / 2 * (x2 - x1)\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i] - edges[i - 1])\n\treturn area", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\tprint(\"The integral of the graph is {:.3f}.\".format(area))\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tintegral = 0\n\tfor i in range(len(bins) - 1):\n\t\tintegral += bins[i] * (edges[i + 1] - edges[i])\n\treturn integral\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)-1):\n\t\tarea += bins[i]*(edges[i+1]-edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i, edge in enumerate(edges):\n\t\tif edge == []:\n\t\t\tbreak\n\t\tif i > 0:\n\t\t\tarea += bins[i] - bins[i-1]\n\t\telse:\n\t\t\tarea += bins[i]\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea = area + bins[i]\n\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(1, len(bins)):\n\t\ty1 = bins[i - 1]\n\t\ty2 = bins[i]\n\t\tx1 = edges[i - 1]\n\t\tx2 = edges[i]\n\t\tarea += (y1 + y2) / 2 * (x2 - x1)\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i] - edges[i - 1])\n\treturn area", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\tprint(\"The integral of the graph is {:.3f}.\".format(area))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tintegral = 0\n\tfor i in range(len(bins) - 1):\n\t\tintegral += bins[i] * (edges[i + 1] - edges[i])\n\treturn integral\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)-1):\n\t\tarea += bins[i]*(edges[i+1]-edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i, edge in enumerate(edges):\n\t\tif edge == []:\n\t\t\tbreak\n\t\tif i > 0:\n\t\t\tarea += bins[i] - bins[i-1]\n\t\telse:\n\t\t\tarea += bins[i]\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea = area + bins[i]\n\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tis_fill_request_seq = False\n\tif len(seq) == 2:\n\t\tif isinstance(seq[0], int) and isinstance(seq[1], int):\n\t\t\tif seq[0] > seq[1]:\n\t\t\t\tis_fill_request_seq = True\n\treturn is_fill_request_seq\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tpass", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tseq_list = seq.split(\" \")\n\tif len(seq_list) != 3:\n\t\treturn False\n\ttry:\n\t\tseq_list = list(map(int, seq_list))\n\texcept ValueError as e:\n\t\treturn False\n\tif seq_list[0] <= 0 or seq_list[1] <= 0 or seq_list[2] <= 0:\n\t\treturn False\n\treturn True\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom .fill_request_seq import FillRequestSeq\n\texcept ImportError:\n\t\tfrom fill_request_seq import FillRequestSeq\n\treturn isinstance(seq, FillRequestSeq)\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tseq = seq.strip()\n\tif seq[0] != '<' or seq[-1] != '>':\n\t\treturn False\n\tseq = seq[1:-1].split(',')\n\tif len(seq) != 3:\n\t\treturn False\n\ttry:\n\t\tseq = [int(x) for x in seq]\n\texcept ValueError:\n\t\treturn False\n\tif seq[0] > 0 and seq[2] > 0:\n\t\treturn True\n\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tstack = []\n\n\tdef push(symbol):\n\t\tstack.append(symbol)\n\n\tdef pop():\n\t\treturn stack.pop()\n\n\tfor symbol in seq:\n\t\tif symbol == '(':\n\t\t\tpush(symbol)\n\t\telif symbol == ')':\n\t\t\tif pop() != '(':\n\t\t\t\treturn False\n\t\telif symbol in {'+', '-', '*', '/'}:\n\t\t\tpush(symbol)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tfloat(symbol)\n\t\t\texcept ValueError:\n\t\t\t\treturn False\n\n\treturn len(stack) == 0\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tfrom .fill_request_seq import FillRequestSeq\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept Exception:\n\t\treturn False\n\treturn True\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tseq = list(seq)\n\twhile seq and seq[-1] == 0:\n\t\tseq.pop()\n\tif seq == []:\n\t\treturn False\n\t\n\tseq.reverse()\n\twhile seq and seq[-1] == 0:\n\t\tseq.pop()\n\tif seq == []:\n\t\treturn False\n\tseq.reverse()\n\t\n\tif seq[0] == 1:\n\t\treturn False\n\t\n\tfor i in xrange(len(seq) - 1):\n\t\tif seq[i] + 1 != seq[i + 1]:\n\t\t\treturn False\n\t\n\treturn True\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept:\n\t\treturn False\n\treturn True\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tn = len(seq)\n\t# check n\n\tif n < 3:\n\t\treturn False\n\t# check seq first and last element\n\tif seq[0] != 1 or seq[-1] != n:\n\t\treturn False\n\t# check seq\n\tfor i in range(1, n):\n\t\tif seq[i] - seq[i - 1] != 1:\n\t\t\treturn False\n\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tis_fill_request_seq = False\n\tif len(seq) == 2:\n\t\tif isinstance(seq[0], int) and isinstance(seq[1], int):\n\t\t\tif seq[0] > seq[1]:\n\t\t\t\tis_fill_request_seq = True\n\treturn is_fill_request_seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tseq_list = seq.split(\" \")\n\tif len(seq_list) != 3:\n\t\treturn False\n\ttry:\n\t\tseq_list = list(map(int, seq_list))\n\texcept ValueError as e:\n\t\treturn False\n\tif seq_list[0] <= 0 or seq_list[1] <= 0 or seq_list[2] <= 0:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom .fill_request_seq import FillRequestSeq\n\texcept ImportError:\n\t\tfrom fill_request_seq import FillRequestSeq\n\treturn isinstance(seq, FillRequestSeq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tseq = seq.strip()\n\tif seq[0] != '<' or seq[-1] != '>':\n\t\treturn False\n\tseq = seq[1:-1].split(',')\n\tif len(seq) != 3:\n\t\treturn False\n\ttry:\n\t\tseq = [int(x) for x in seq]\n\texcept ValueError:\n\t\treturn False\n\tif seq[0] > 0 and seq[2] > 0:\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tstack = []\n\n\tdef push(symbol):\n\t\tstack.append(symbol)\n\n\tdef pop():\n\t\treturn stack.pop()\n\n\tfor symbol in seq:\n\t\tif symbol == '(':\n\t\t\tpush(symbol)\n\t\telif symbol == ')':\n\t\t\tif pop() != '(':\n\t\t\t\treturn False\n\t\telif symbol in {'+', '-', '*', '/'}:\n\t\t\tpush(symbol)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tfloat(symbol)\n\t\t\texcept ValueError:\n\t\t\t\treturn False\n\n\treturn len(stack) == 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tfrom .fill_request_seq import FillRequestSeq\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept Exception:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tseq = list(seq)\n\twhile seq and seq[-1] == 0:\n\t\tseq.pop()\n\tif seq == []:\n\t\treturn False\n\t\n\tseq.reverse()\n\twhile seq and seq[-1] == 0:\n\t\tseq.pop()\n\tif seq == []:\n\t\treturn False\n\tseq.reverse()\n\t\n\tif seq[0] == 1:\n\t\treturn False\n\t\n\tfor i in xrange(len(seq) - 1):\n\t\tif seq[i] + 1 != seq[i + 1]:\n\t\t\treturn False\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tn = len(seq)\n\t# check n\n\tif n < 3:\n\t\treturn False\n\t# check seq first and last element\n\tif seq[0] != 1 or seq[-1] != n:\n\t\treturn False\n\t# check seq\n\tfor i in range(1, n):\n\t\tif seq[i] - seq[i - 1] != 1:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\telse:\n\t\treturn True", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.request\n\t\tobj.fill\n\texcept AttributeError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tattrs = ['fill', 'request']\n\treturn all(hasattr(obj, attr) for attr in attrs)\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj_fill = obj.fill\n\t\tobj_request = obj.request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tfill_attr = getattr(obj, 'fill', None)\n\trequest_attr = getattr(obj, 'request', None)\n\treturn (fill_attr is not None) and (request_attr is not None)", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tfill_attr = 'fill' in obj.__dict__\n\trequest_attr = 'request' in obj.__dict__\n\treturn fill_attr and request_attr", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\telse:\n\t\treturn True", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.request\n\t\tobj.fill\n\texcept AttributeError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tattrs = ['fill', 'request']\n\treturn all(hasattr(obj, attr) for attr in attrs)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj_fill = obj.fill\n\t\tobj_request = obj.request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tfill_attr = getattr(obj, 'fill', None)\n\trequest_attr = getattr(obj, 'request', None)\n\treturn (fill_attr is not None) and (request_attr is not None)", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tfill_attr = 'fill' in obj.__dict__\n\trequest_attr = 'request' in obj.__dict__\n\treturn fill_attr and request_attr", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\thas_run = hasattr(obj, 'run')\n\tis_callable = callable(getattr(obj, 'run', None))\n\treturn has_run and is_callable\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'run')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef __getattribute__(self, name):\n\t\tif name == 'run':\n\t\t\treturn lambda: None\n\t\telse:\n\t\t\treturn super().__getattribute__(name)\n\n\treturn type(obj.__name__, (obj,), {'__getattribute__': __getattribute__})\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef _is_run_el(obj):\n\t\treturn hasattr(obj, 'run') and callable(obj.run)\n\treturn _is_run_el(obj)\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tattr = getattr(obj, \"run\", None)\n\treturn callable(attr)\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef run(self, *args, **kwargs):\n\t\tpass\n\treturn hasattr(obj, 'run') and hasattr(obj.run, '__call__')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'run') and callable(getattr(obj, 'run'))\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef _is_run_el(obj):\n\t\ttry:\n\t\t\tobj.run()\n\t\t\treturn True\n\t\texcept AttributeError:\n\t\t\treturn False\n\treturn _is_run_el(obj)\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\thas_run = hasattr(obj, 'run')\n\tis_callable = callable(getattr(obj, 'run', None))\n\treturn has_run and is_callable\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef __getattribute__(self, name):\n\t\tif name == 'run':\n\t\t\treturn lambda: None\n\t\telse:\n\t\t\treturn super().__getattribute__(name)\n\n\treturn type(obj.__name__, (obj,), {'__getattribute__': __getattribute__})\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef _is_run_el(obj):\n\t\treturn hasattr(obj, 'run') and callable(obj.run)\n\treturn _is_run_el(obj)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tattr = getattr(obj, \"run\", None)\n\treturn callable(attr)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef run(self, *args, **kwargs):\n\t\tpass\n\treturn hasattr(obj, 'run') and hasattr(obj.run, '__call__')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'run') and callable(getattr(obj, 'run'))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tdef _is_run_el(obj):\n\t\ttry:\n\t\t\tobj.run()\n\t\t\treturn True\n\t\texcept AttributeError:\n\t\t\treturn False\n\treturn _is_run_el(obj)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\tfill_method = getattr(obj, \"fill\", None)\n\tcompute_method = getattr(obj, \"compute\", None)\n\t\n\tif fill_method and callable(fill_method):\n\t\treturn True\n\t\n\tif compute_method and callable(compute_method):\n\t\treturn True\n\t\n\treturn False\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.__fill__()\n\t\tobj.__compute__()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\t\tobj.compute()\n\texcept:\n\t\treturn False\n\treturn True\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tmethods = dir(obj)\n\tif 'fill' not in methods:\n\t\traise AttributeError('{} does not have a fill method.'.format(obj))\n\tif 'compute' not in methods:\n\t\traise AttributeError('{} does not have a compute method.'.format(obj))\n\treturn True\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\t\tobj.compute()\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\tfill_method = getattr(obj, 'fill', None)\n\tcompute_method = getattr(obj, 'compute', None)\n\t\n\treturn (fill_method and compute_method)\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill(0)\n\t\tobj.compute()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tmethods = [\"fill\", \"compute\"]\n\tfor method in methods:\n\t\tif not hasattr(obj, method):\n\t\t\treturn False\n\treturn True\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\thas_fill = hasattr(obj, 'fill') and callable(obj.fill)\n\thas_compute = hasattr(obj, 'compute') and callable(obj.compute)\n\treturn has_fill and has_compute\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\t_ = obj.fill()\n\t\t_ = obj.compute()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\tfill_method = getattr(obj, \"fill\", None)\n\tcompute_method = getattr(obj, \"compute\", None)\n\t\n\tif fill_method and callable(fill_method):\n\t\treturn True\n\t\n\tif compute_method and callable(compute_method):\n\t\treturn True\n\t\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.__fill__()\n\t\tobj.__compute__()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\t\tobj.compute()\n\texcept:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tmethods = dir(obj)\n\tif 'fill' not in methods:\n\t\traise AttributeError('{} does not have a fill method.'.format(obj))\n\tif 'compute' not in methods:\n\t\traise AttributeError('{} does not have a compute method.'.format(obj))\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\t\tobj.compute()\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\tfill_method = getattr(obj, 'fill', None)\n\tcompute_method = getattr(obj, 'compute', None)\n\t\n\treturn (fill_method and compute_method)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill(0)\n\t\tobj.compute()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tmethods = [\"fill\", \"compute\"]\n\tfor method in methods:\n\t\tif not hasattr(obj, method):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\thas_fill = hasattr(obj, 'fill') and callable(obj.fill)\n\thas_compute = hasattr(obj, 'compute') and callable(obj.compute)\n\treturn has_fill and has_compute\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\t_ = obj.fill()\n\t\t_ = obj.compute()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td = {}\n\tfor k, v in d1.items():\n\t\tif k not in d2 or d2[k] != v:\n\t\t\tif level == -1:\n\t\t\t\td[k] = v\n\t\t\telif level > 0:\n\t\t\t\td[k] = difference(v, d2[k], level - 1)\n\treturn d\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tresult[k] = d1[k]\n\t\tif level > 0 and type(d1[k]) is dict and type(d2[k]) is dict:\n\t\t\tresult[k] = difference(d1[k], d2[k], level - 1)\n\treturn result\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in d1.iteritems():\n\t\tif key not in d2:\n\t\t\tresult[key] = value\n\t\telif isinstance(value, dict):\n\t\t\tif isinstance(d2[key], dict):\n\t\t\t\tresult[key] = difference(value, d2[key], level-1)\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\telif value != d2[key]:\n\t\t\tresult[key] = value\n\treturn result\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\tdiff[key] = value\n\t\telif level == 0:\n\t\t\tcontinue\n\t\telif isinstance(value, dict):\n\t\t\tdiff[key] = difference(value, d2[key], level-1)\n\t\telif value != d2[key]:\n\t\t\tdiff[key] = value\n\treturn diff\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\tdiff[key] = value\n\t\telif isinstance(value, dict):\n\t\t\tdiff[key] = difference(value, d2[key])\n\t\telif value != d2[key]:\n\t\t\tdiff[key] = value\n\treturn diff", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tfor k in d1:\n\t\tif not k in d2 or d1[k] != d2[k]:\n\t\t\td3[k] = d1[k]\n\t\tif level >= 0:\n\t\t\td3[k] = difference(d1[k], d2[k], level-1)\n\treturn d3", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tresult[k] = d1[k]\n\t\telse:\n\t\t\tif type(d1[k]) == dict:\n\t\t\t\td = difference(d1[k], d2[k], level - 1)\n\t\t\t\tif d:\n\t\t\t\t\tresult[k] = d\n\treturn result\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k in d1.keys():\n\t\tif k not in d2 or d1[k] != d2[k]:\n\t\t\tdiff[k] = d1[k]\n\treturn diff\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tfor k in d1:\n\t\tif level == -1:\n\t\t\tv = d1[k]\n\t\t\tif k not in d2:\n\t\t\t\td3[k] = v\n\t\telse:\n\t\t\tv = d1[k].get(level, {})\n\t\t\tif k not in d2:\n\t\t\t\td3[k] = v\n\t\t\telse:\n\t\t\t\tv2 = d2[k]\n\t\t\t\tv3 = {}\n\t\t\t\tfor k2 in v:\n\t\t\t\t\tif k2 not in v2:\n\t\t\t\t\t\tv3[k2] = v[k2]\n\t\t\t\t\telse:\n\t\t\t\t\t\tv3[k2] = difference(v[k2], v2[k2], level-1)\n\t\t\t\tif v3:\n\t\t\t\t\td3[k] = v3\n\treturn d3\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in d1.items():\n\t\tif key in d2:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tresult[key] = difference(value, d2[key], level-1)\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\telse:\n\t\t\tresult[key] = value\n\treturn result\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td = {}\n\tfor k, v in d1.items():\n\t\tif k not in d2 or d2[k] != v:\n\t\t\tif level == -1:\n\t\t\t\td[k] = v\n\t\t\telif level > 0:\n\t\t\t\td[k] = difference(v, d2[k], level - 1)\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tresult[k] = d1[k]\n\t\tif level > 0 and type(d1[k]) is dict and type(d2[k]) is dict:\n\t\t\tresult[k] = difference(d1[k], d2[k], level - 1)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in d1.iteritems():\n\t\tif key not in d2:\n\t\t\tresult[key] = value\n\t\telif isinstance(value, dict):\n\t\t\tif isinstance(d2[key], dict):\n\t\t\t\tresult[key] = difference(value, d2[key], level-1)\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\telif value != d2[key]:\n\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\tdiff[key] = value\n\t\telif level == 0:\n\t\t\tcontinue\n\t\telif isinstance(value, dict):\n\t\t\tdiff[key] = difference(value, d2[key], level-1)\n\t\telif value != d2[key]:\n\t\t\tdiff[key] = value\n\treturn diff\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\tdiff[key] = value\n\t\telif isinstance(value, dict):\n\t\t\tdiff[key] = difference(value, d2[key])\n\t\telif value != d2[key]:\n\t\t\tdiff[key] = value\n\treturn diff", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tfor k in d1:\n\t\tif not k in d2 or d1[k] != d2[k]:\n\t\t\td3[k] = d1[k]\n\t\tif level >= 0:\n\t\t\td3[k] = difference(d1[k], d2[k], level-1)\n\treturn d3", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tresult[k] = d1[k]\n\t\telse:\n\t\t\tif type(d1[k]) == dict:\n\t\t\t\td = difference(d1[k], d2[k], level - 1)\n\t\t\t\tif d:\n\t\t\t\t\tresult[k] = d\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k in d1.keys():\n\t\tif k not in d2 or d1[k] != d2[k]:\n\t\t\tdiff[k] = d1[k]\n\treturn diff\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tfor k in d1:\n\t\tif level == -1:\n\t\t\tv = d1[k]\n\t\t\tif k not in d2:\n\t\t\t\td3[k] = v\n\t\telse:\n\t\t\tv = d1[k].get(level, {})\n\t\t\tif k not in d2:\n\t\t\t\td3[k] = v\n\t\t\telse:\n\t\t\t\tv2 = d2[k]\n\t\t\t\tv3 = {}\n\t\t\t\tfor k2 in v:\n\t\t\t\t\tif k2 not in v2:\n\t\t\t\t\t\tv3[k2] = v[k2]\n\t\t\t\t\telse:\n\t\t\t\t\t\tv3[k2] = difference(v[k2], v2[k2], level-1)\n\t\t\t\tif v3:\n\t\t\t\t\td3[k] = v3\n\treturn d3\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in d1.items():\n\t\tif key in d2:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tresult[key] = difference(value, d2[key], level-1)\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\telse:\n\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_raw(coord, weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx, y, z = coord\n\tself.data[x][y][z] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._fill(coord, weight)", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_xyz(coord[0], coord[1], coord[2], weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\ttry:\n\t\tx, y = coord\n\texcept ValueError:\n\t\traise TypeError(\"coord must be a sequence with two values\")\n\tif x < 0 or y < 0:\n\t\treturn\n\tx = int(x)\n\ty = int(y)\n\tif x >= self.nx or y >= self.ny:\n\t\treturn\n\tself._data[y * self.nx + x] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._fill(coord, weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_n(coord, weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx, y = coord\n\tself[x, y] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx = self.x\n\ti = bisect_left(x, coord)\n\tif i > 0:\n\t\tif x[i-1] == coord:\n\t\t\tself.y[i-1] += weight\n\t\telif i < len(x):\n\t\t\tself.y[i] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_raw(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx, y, z = coord\n\tself.data[x][y][z] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._fill(coord, weight)", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_xyz(coord[0], coord[1], coord[2], weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\ttry:\n\t\tx, y = coord\n\texcept ValueError:\n\t\traise TypeError(\"coord must be a sequence with two values\")\n\tif x < 0 or y < 0:\n\t\treturn\n\tx = int(x)\n\ty = int(y)\n\tif x >= self.nx or y >= self.ny:\n\t\treturn\n\tself._data[y * self.nx + x] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._fill(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_n(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx, y = coord\n\tself[x, y] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx = self.x\n\ti = bisect_left(x, coord)\n\tif i > 0:\n\t\tif x[i-1] == coord:\n\t\t\tself.y[i-1] += weight\n\t\telif i < len(x):\n\t\t\tself.y[i] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabel_keys = labels.keys()\n\tlabel_values = labels.values()\n\tfor key in label_keys:\n\t\tvalidate_key(key)\n\tfor value in label_values:\n\t\tvalidate_value(value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabels = labels.copy()\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\treturn labels\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\terror_msg = \"Labels must have at least one key-value pair.\"\n\tvalidate_key(labels, error_msg)\n\tvalidate_value(labels, error_msg)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfrom .validation import validate_key, validate_value\n\n\tif not isinstance(labels, dict):\n\t\treturn False\n\n\tif len(labels) == 0:\n\t\treturn True\n\n\tfor k, v in labels.items():\n\t\tif not validate_key(k):\n\t\t\treturn False\n\t\tif not validate_value(v):\n\t\t\treturn False\n\n\treturn True\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tnew_labels = {}\n\tfor key, value in labels.items():\n\t\tnew_labels[_validate_key(key)] = _validate_value(value)\n\treturn new_labels\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\tvalidate_key(key)\n\t\t\tvalidate_value(value)\n\t\texcept ValueError as e:\n\t\t\terrors.append(e)\n\tif len(errors) > 0:\n\t\traise ValueError(errors)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels must be a dict\")\n\t\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabels = labels or {}\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfrom .validator import validate_key, validate_value\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tvalid_labels = {}\n\tfor key, value in labels.items():\n\t\tvalid_labels[validate_key(key)] = validate_value(value)\n\treturn valid_labels\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabel_keys = labels.keys()\n\tlabel_values = labels.values()\n\tfor key in label_keys:\n\t\tvalidate_key(key)\n\tfor value in label_values:\n\t\tvalidate_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabels = labels.copy()\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\treturn labels\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\terror_msg = \"Labels must have at least one key-value pair.\"\n\tvalidate_key(labels, error_msg)\n\tvalidate_value(labels, error_msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfrom .validation import validate_key, validate_value\n\n\tif not isinstance(labels, dict):\n\t\treturn False\n\n\tif len(labels) == 0:\n\t\treturn True\n\n\tfor k, v in labels.items():\n\t\tif not validate_key(k):\n\t\t\treturn False\n\t\tif not validate_value(v):\n\t\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tnew_labels = {}\n\tfor key, value in labels.items():\n\t\tnew_labels[_validate_key(key)] = _validate_value(value)\n\treturn new_labels\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\tvalidate_key(key)\n\t\t\tvalidate_value(value)\n\t\texcept ValueError as e:\n\t\t\terrors.append(e)\n\tif len(errors) > 0:\n\t\traise ValueError(errors)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels must be a dict\")\n\t\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabels = labels or {}\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfrom .validator import validate_key, validate_value\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tvalid_labels = {}\n\tfor key, value in labels.items():\n\t\tvalid_labels[validate_key(key)] = validate_value(value)\n\treturn valid_labels\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tregex = re.compile(r'^[a-z0-9]([-a-z0-9]*[a-z0-9])?$')\n\n\treturn regex\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\t# The resource name must start with a letter or number, and contain only letters, numbers and underscores\n\tregex = r\"[a-zA-Z0-9]([a-zA-Z0-9_]?){0,63}\"\n\treturn re.compile(regex)\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tregex = r'^[a-z]([a-z0-9-]*[a-z0-9])?$'\n\treturn re.compile(regex)\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tfrom krake.resources.resource_types import ResourceType\n\timport re\n\n\tname_regex_dict = {\n\t\tResourceType.ENVIRONMENT: r'^[a-zA-Z0-9_\\-]+$',\n\t\tResourceType.APPLICATION: r'^[a-zA-Z0-9_\\-]+$',\n\t\tResourceType.CLUSTER: r'^[a-zA-Z0-9_\\-]+$',\n\t\tResourceType.NAMESPACE: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.SERVICE: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.INGRESS: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.CONFIGMAP: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.SECRET: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.DEPLOYMENT: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.STATEFULSET: r'^[a-zA-", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tregex = r'^[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'\n\treturn regex\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tregex = '^[a-zA-Z0-9\\-_]+$'\n\treturn re.compile(regex)\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\treturn r'^[a-zA-Z0-9_\\-]{1,100}$'\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tresource_name_regex = re.compile(\"^[a-zA-Z]([a-zA-Z0-9_-]{0,127})$\")\n\treturn resource_name_regex", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tregex_pattern = \"^[a-z]+[a-z0-9-]*$\"\n\treturn regex_pattern", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tname_regex = \"^[a-zA-Z0-9-]*$\"\n\treturn name_regex\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tregex = re.compile(r'^[a-z0-9]([-a-z0-9]*[a-z0-9])?$')\n\n\treturn regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\t# The resource name must start with a letter or number, and contain only letters, numbers and underscores\n\tregex = r\"[a-zA-Z0-9]([a-zA-Z0-9_]?){0,63}\"\n\treturn re.compile(regex)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tregex = r'^[a-z]([a-z0-9-]*[a-z0-9])?$'\n\treturn re.compile(regex)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tfrom krake.resources.resource_types import ResourceType\n\timport re\n\n\tname_regex_dict = {\n\t\tResourceType.ENVIRONMENT: r'^[a-zA-Z0-9_\\-]+$',\n\t\tResourceType.APPLICATION: r'^[a-zA-Z0-9_\\-]+$',\n\t\tResourceType.CLUSTER: r'^[a-zA-Z0-9_\\-]+$',\n\t\tResourceType.NAMESPACE: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.SERVICE: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.INGRESS: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.CONFIGMAP: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.SECRET: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.DEPLOYMENT: r'^[a-zA-Z0-9_\\-\\.]+$',\n\t\tResourceType.STATEFULSET: r'^[a-zA-", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tregex = r'^[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'\n\treturn regex\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tregex = '^[a-zA-Z0-9\\-_]+$'\n\treturn re.compile(regex)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\treturn r'^[a-zA-Z0-9_\\-]{1,100}$'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\n\tresource_name_regex = re.compile(\"^[a-zA-Z]([a-zA-Z0-9_-]{0,127})$\")\n\treturn resource_name_regex", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tregex_pattern = \"^[a-z]+[a-z0-9-]*$\"\n\treturn regex_pattern", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tname_regex = \"^[a-zA-Z0-9-]*$\"\n\treturn name_regex\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tfrom re import match\n\tfrom unicodedata import normalize\n\tfrom django.core.exceptions import ValidationError\n\tfrom django.utils.translation import ugettext as _\n\n\tfrom . import REGEX\n\n\tvalue = normalize('NFKD', value).encode('ASCII', 'ignore')\n\tvalue = value.upper()\n\n\tif not match(REGEX, value):\n\t\traise ValidationError(_('%(value)s is not a valid CNPJ.'), params={'value': value}, code='invalid')\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\n\ttry:\n\t\tvalue = str(value)\n\texcept Exception:\n\t\treturn False\n\n\tif value == '':\n\t\treturn False\n\n\tif value == 'None':\n\t\treturn False\n\n\treturn True\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tfrom re import compile, match\n\n\tregex = compile(\n\t\tr'^(?P<type>[a-z]+)(?P<value>(?:\\.[0-9]+)?|(?:\\.[0-9]+\\.[0-9]+)?)$',\n\t\tflags=0\n\t)\n\tmatch = regex.match(value)\n\n\tif match:\n\t\treturn match.groupdict()\n\telse:\n\t\tprint(\"Invalid value.\")\n\t\treturn False\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tpass\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tpass\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\trules = [\n\t\t[re.compile(r'^\\d{1,2}$'), \"The value must be an integer between 1 and 31.\"],\n\t\t[re.compile(r'^\\d{1,2}:\\d{1,2}$'), \"The value must be in the format '[hour]:[minute]'.\"],\n\t\t[re.compile(r'^\\d{1,2}:\\d{1,2}:\\d{1,2}$'), \"The value must be in the format '[hour]:[minute]:[second]'.\"],\n\t\t[re.compile(r'^\\d{1,2}-\\d{1,2}$'), \"The value must be in the format '[hour]-[minute]'.\"],\n\t\t[re.compile(r'^\\d{1,2}-\\d{1,2}-\\d{1,2}$'), \"The value must be in the format '[hour]-[minute]-[second]'.\"],\n\t\t[re.compile(r'^\\d{1,2}/\\d{1,2}$'), \"The value must be in the format '[hour]/[minute]'.\"],\n\t\t[re.compile(r'^\\d{1,2}/\\d{1,", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tif value == None:\n\t\treturn True\n\ttry:\n\t\tfrom . import validation_regex\n\texcept ImportError:\n\t\tfrom validation_regex import validation_regex\n\t\t\n\tregex = validation_regex[value.__class__]\n\tmatch = re.match(regex, str(value))\n\treturn bool(match)", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tfrom lib.errors import ValidationError\n\n\ttry:\n\t\tif type(value) == list:\n\t\t\tvalue = [i for i in value if i]\n\t\t\tif not value:\n\t\t\t\traise ValidationError(\"Value must not be empty.\")\n\texcept:\n\t\traise ValidationError(\"Value must not be empty.\")\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\ttry:\n\t\tvalidate_value.regexp\n\texcept AttributeError:\n\t\tvalidate_value.regexp = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n\treturn validate_value.regexp.match(value) is not None", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tpass\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tfrom re import match\n\tfrom unicodedata import normalize\n\tfrom django.core.exceptions import ValidationError\n\tfrom django.utils.translation import ugettext as _\n\n\tfrom . import REGEX\n\n\tvalue = normalize('NFKD', value).encode('ASCII', 'ignore')\n\tvalue = value.upper()\n\n\tif not match(REGEX, value):\n\t\traise ValidationError(_('%(value)s is not a valid CNPJ.'), params={'value': value}, code='invalid')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\n\ttry:\n\t\tvalue = str(value)\n\texcept Exception:\n\t\treturn False\n\n\tif value == '':\n\t\treturn False\n\n\tif value == 'None':\n\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tfrom re import compile, match\n\n\tregex = compile(\n\t\tr'^(?P<type>[a-z]+)(?P<value>(?:\\.[0-9]+)?|(?:\\.[0-9]+\\.[0-9]+)?)$',\n\t\tflags=0\n\t)\n\tmatch = regex.match(value)\n\n\tif match:\n\t\treturn match.groupdict()\n\telse:\n\t\tprint(\"Invalid value.\")\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\trules = [\n\t\t[re.compile(r'^\\d{1,2}$'), \"The value must be an integer between 1 and 31.\"],\n\t\t[re.compile(r'^\\d{1,2}:\\d{1,2}$'), \"The value must be in the format '[hour]:[minute]'.\"],\n\t\t[re.compile(r'^\\d{1,2}:\\d{1,2}:\\d{1,2}$'), \"The value must be in the format '[hour]:[minute]:[second]'.\"],\n\t\t[re.compile(r'^\\d{1,2}-\\d{1,2}$'), \"The value must be in the format '[hour]-[minute]'.\"],\n\t\t[re.compile(r'^\\d{1,2}-\\d{1,2}-\\d{1,2}$'), \"The value must be in the format '[hour]-[minute]-[second]'.\"],\n\t\t[re.compile(r'^\\d{1,2}/\\d{1,2}$'), \"The value must be in the format '[hour]/[minute]'.\"],\n\t\t[re.compile(r'^\\d{1,2}/\\d{1,", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tif value == None:\n\t\treturn True\n\ttry:\n\t\tfrom . import validation_regex\n\texcept ImportError:\n\t\tfrom validation_regex import validation_regex\n\t\t\n\tregex = validation_regex[value.__class__]\n\tmatch = re.match(regex, str(value))\n\treturn bool(match)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tfrom lib.errors import ValidationError\n\n\ttry:\n\t\tif type(value) == list:\n\t\t\tvalue = [i for i in value if i]\n\t\t\tif not value:\n\t\t\t\traise ValidationError(\"Value must not be empty.\")\n\texcept:\n\t\traise ValidationError(\"Value must not be empty.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\ttry:\n\t\tvalidate_value.regexp\n\texcept AttributeError:\n\t\tvalidate_value.regexp = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n\treturn validate_value.regexp.match(value) is not None", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tif re.match(r'^[a-zA-Z0-9_-]+$', key):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\n\tkey_regexp = re.compile(r\"[a-zA-Z0-9\\-_]+\")\n\n\tif key_regexp.fullmatch(key) == None:\n\t\traise ValueError(f\"Invalid key '{key}'.\")\n\n#test.py\n#!/usr/bin/env python3\n\nfrom argparse import ArgumentParser\n\nimport config\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tif re.match(r'^[a-zA-Z0-9_-]{4,20}$', key) is None:\n\t\treturn False\n\treturn True\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tkey_regex = re.compile(r'^[a-zA-Z0-9]+$')\n\tif not key_regex.match(key):\n\t\tprint('Invalid key provided. Key must be alphanumeric.')\n\t\texit()\n\telse:\n\t\treturn True\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tkey = str(key)\n\tif re.match(r'^[A-Za-z0-9_]{1,128}$', key) is None:\n\t\traise Exception('Invalid key name')\n\treturn key\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\n\tkey_regex = r'^[a-z]+[a-z0-9_-]*$'\n\treturn re.match(key_regex, key)", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tpass", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tregex = {\n\t\t\"id\": r\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\",\n\t\t\"name\": r\"^[A-Za-z0-9_]{3,255}$\",\n\t\t\"color\": r\"^#[0-9a-f]{6}$\",\n\t\t\"description\": r\"^[\\w\\W]{0,255}$\",\n\t\t\"created_at\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\",\n\t\t\"updated_at\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\",\n\t\t\"deleted_at\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\"\n\t}\n\n\tif regex.get(key) is None:\n\t\treturn False\n\n\treturn True\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\timport logging\n\tlogger = logging.getLogger(__name__)\n\n\tkey_regex = re.compile('^[a-zA-Z0-9_]+$')\n\n\tif key_regex.match(key):\n\t\treturn True\n\telse:\n\t\tlogger.error('key does not match regex: ' + key)\n\t\treturn False", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tpass"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tif re.match(r'^[a-zA-Z0-9_-]+$', key):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\n\tkey_regexp = re.compile(r\"[a-zA-Z0-9\\-_]+\")\n\n\tif key_regexp.fullmatch(key) == None:\n\t\traise ValueError(f\"Invalid key '{key}'.\")\n\n#test.py\n#!/usr/bin/env python3\n\nfrom argparse import ArgumentParser\n\nimport config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tif re.match(r'^[a-zA-Z0-9_-]{4,20}$', key) is None:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tkey_regex = re.compile(r'^[a-zA-Z0-9]+$')\n\tif not key_regex.match(key):\n\t\tprint('Invalid key provided. Key must be alphanumeric.')\n\t\texit()\n\telse:\n\t\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tkey = str(key)\n\tif re.match(r'^[A-Za-z0-9_]{1,128}$', key) is None:\n\t\traise Exception('Invalid key name')\n\treturn key\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\n\tkey_regex = r'^[a-z]+[a-z0-9_-]*$'\n\treturn re.match(key_regex, key)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tregex = {\n\t\t\"id\": r\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\",\n\t\t\"name\": r\"^[A-Za-z0-9_]{3,255}$\",\n\t\t\"color\": r\"^#[0-9a-f]{6}$\",\n\t\t\"description\": r\"^[\\w\\W]{0,255}$\",\n\t\t\"created_at\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\",\n\t\t\"updated_at\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\",\n\t\t\"deleted_at\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\"\n\t}\n\n\tif regex.get(key) is None:\n\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\timport logging\n\tlogger = logging.getLogger(__name__)\n\n\tkey_regex = re.compile('^[a-zA-Z0-9_]+$')\n\n\tif key_regex.match(key):\n\t\treturn True\n\telse:\n\t\tlogger.error('key does not match regex: ' + key)\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tif first_level:\n\t\tobserver_schema_dict['type'] = 'object'\n\t\tobserver_schema_dict['properties'] = {}\n\t\tobserver_schema_dict['required'] = []\n\t\tfor key in manifest_dict.keys():\n\t\t\tif key != 'name':\n\t\t\t\tobserver_schema_dict['required'].append(key)\n\t\t\t\tobserver_schema_dict['properties'][key] = generate_default_observer_schema_dict(manifest_dict[key], False)\n\t\treturn observer_schema_dict\n\telse:\n\t\tif isinstance(manifest_dict, dict):\n\t\t\tobserver_schema_dict['type'] = 'object'\n\t\t\tobserver_schema_dict['properties'] = {}\n\t\t\tobserver_schema_dict['required'] = []\n\t\t\tfor key in manifest_dict.keys():\n\t\t\t\tobserver_schema_dict['required'].append(key)\n\t\t\t\tobserver_schema_dict['properties'][key] = generate_default_observer_schema_dict(manifest_dict[key], False)\n\t\t\treturn observer_schema_dict\n\t\telif isinstance(manifest_dict, list):\n\t\t\tobserver_schema_dict['type'] = 'array'\n\t\t\tob", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tschema_dict[\"type\"] = \"object\"\n\tschema_dict[\"properties\"] = {}\n\tschema_dict[\"required\"] = []\n\tschema_dict[\"additionalProperties\"] = False\n\tfor key in manifest_dict:\n\t\tif type(manifest_dict[key]) == dict:\n\t\t\tschema_dict[\"properties\"][key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=True)\n\t\t\tschema_dict[\"required\"].append(key)\n\t\telif type(manifest_dict[key]) == list:\n\t\t\tschema_dict[\"properties\"][key] = {}\n\t\t\tschema_dict[\"properties\"][key][\"type\"] = \"array\"\n\t\t\tschema_dict[\"properties\"][key][\"items\"] = {}\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"type\"] = \"object\"\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"properties\"] = {}\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"required\"] = []\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"additionalProperties\"] = False\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"properties\"][\"type\"] = {}\n\t\t\tschema_dict[\"", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) is list:\n\t\t\tobserver_schema_dict[key] = []\n\t\telif type(value) is str:\n\t\t\tif value == 'string':\n\t\t\t\tobserver_schema_dict[key] = ''\n\t\t\telif value == 'integer':\n\t\t\t\tobserver_schema_dict[key] = 0\n\t\t\telif value == 'double':\n\t\t\t\tobserver_schema_dict[key] = 0.0\n\t\t\telif value == 'long':\n\t\t\t\tobserver_schema_dict[key] = 0\n\t\t\telif value == 'boolean':\n\t\t\t\tobserver_schema_dict[key] = False\n\t\t\telif value == 'dateTime':\n\t\t\t\tobserver_schema_dict[key] = '2019-01-01T00:00:00.000Z'\n\t\telif type(value) is int:\n\t\t\tobserver_schema_dict[key] = value\n\t\telse:\n\t\t\tprint('The", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tschema_dict[key] = value\n\tif not first_level:\n\t\treturn {\"properties\": schema_dict}\n\treturn schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif not first_level:\n\t\tnew_dict[\"event_type\"] = manifest_dict.get(\"event_type\", \"\")\n\t\tnew_dict[\"event_source\"] = manifest_dict.get(\"event_source\", \"\")\n\t\tnew_dict[\"event_subtype\"] = manifest_dict.get(\"event_subtype\", \"\")\n\t\tnew_dict[\"event_description\"] = manifest_dict.get(\"event_description\", \"\")\n\t\tnew_dict[\"event_schema_version\"] = manifest_dict.get(\"event_schema_version\", \"\")\n\t\tnew_dict[\"event_schema_url\"] = manifest_dict.get(\"event_schema_url\", \"\")\n\t\tnew_dict[\"event_schema_type\"] = manifest_dict.get(\"event_schema_type\", \"\")\n\t\tnew_dict[\"event_schema_format_version\"] = manifest_dict.get(\"event_schema_format_version\", \"\")\n\t\tnew_dict[\"event_schema\"] = manifest_dict.get(\"event_schema\", {})\n\telse:\n\t\tnew_dict[\"event_type\"] = manifest_dict.get(\"event_type\", \"\")\n\t\tnew_dict[\"event_source\"] = manifest_dict.get(\"event_source\", \"", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tif not first_level:\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\t\telse:\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\t\telif type(value) is list:\n\t\t\tobserver_schema_dict[key] = [generate_default_observer_schema_dict(item, first_level=True) for item in value]\n\t\telse:\n\t\t\tobserver_schema_dict[key] = value\n\treturn observer_schema_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_schema = {}\n\tfor key in manifest_dict.keys():\n\t\tvalue = manifest_dict[key]\n\t\ttype_of_value = type(value)\n\t\tif type_of_value == dict:\n\t\t\tif first_level:\n\t\t\t\tnew_schema[key] = {}\n\t\t\t\tnew_schema[key][\"type\"] = \"object\"\n\t\t\telse:\n\t\t\t\tnew_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif type_of_value == list:\n\t\t\tnew_schema[key] = []\n\t\t\tfor i in value:\n\t\t\t\tif i == \"string\":\n\t\t\t\t\tnew_schema[key].append(\"string\")\n\t\t\t\telif i == \"number\":\n\t\t\t\t\tnew_schema[key].append(\"number\")\n\t\t\t\telif i == \"integer\":\n\t\t\t\t\tnew_schema[key].append(\"integer\")\n\t\t\t\telif i == \"boolean\":\n\t\t\t\t\tnew_schema[key].append(\"boolean\")\n\t\t\t\telse:\n\t\t\t\t\tnew_schema[key].append(i)\n\t\telse:\n\t\t\tnew_schema[key] = value\n\treturn new_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = [\"\"]\n\t\telse:\n\t\t\tnew_dict[key] = \"\"\n\treturn new_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tobserver_schema_dict[key] = value\n\t\tif not first_level:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\t\telif isinstance(value, list):\n\t\t\t\tobserver_schema_dict[key] = []\n\t\t\t\tfor item in value:\n\t\t\t\t\tobserver_schema_dict[key].append(generate_default_observer_schema_dict(item, True))\n\treturn observer_schema_dict", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif first_level:\n\t\tnew_dict[\"observer\"] = {}\n\t\tnew_dict[\"observer\"][\"name\"] = None\n\t\tnew_dict[\"observer\"][\"description\"] = None\n\t\tnew_dict[\"observer\"][\"version\"] = None\n\t\tnew_dict[\"observer\"][\"type\"] = None\n\t\tnew_dict[\"observer\"][\"url\"] = None\n\telse:\n\t\tnew_dict[\"name\"] = None\n\t\tnew_dict[\"description\"] = None\n\t\tnew_dict[\"version\"] = None\n\t\tnew_dict[\"type\"] = None\n\t\tnew_dict[\"url\"] = None\n\tfor key in manifest_dict:\n\t\tif type(manifest_dict[key]) == dict:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=False)\n\t\telif type(manifest_dict[key]) == list:\n\t\t\tnew_dict[key] = []\n\t\t\tfor item in manifest_dict[key]:\n\t\t\t\tif type(item) == dict:\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(item, first_level=False))\n\t\t\t\telse:\n\t\t\t\t\tnew_dict["], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tif first_level:\n\t\tobserver_schema_dict['type'] = 'object'\n\t\tobserver_schema_dict['properties'] = {}\n\t\tobserver_schema_dict['required'] = []\n\t\tfor key in manifest_dict.keys():\n\t\t\tif key != 'name':\n\t\t\t\tobserver_schema_dict['required'].append(key)\n\t\t\t\tobserver_schema_dict['properties'][key] = generate_default_observer_schema_dict(manifest_dict[key], False)\n\t\treturn observer_schema_dict\n\telse:\n\t\tif isinstance(manifest_dict, dict):\n\t\t\tobserver_schema_dict['type'] = 'object'\n\t\t\tobserver_schema_dict['properties'] = {}\n\t\t\tobserver_schema_dict['required'] = []\n\t\t\tfor key in manifest_dict.keys():\n\t\t\t\tobserver_schema_dict['required'].append(key)\n\t\t\t\tobserver_schema_dict['properties'][key] = generate_default_observer_schema_dict(manifest_dict[key], False)\n\t\t\treturn observer_schema_dict\n\t\telif isinstance(manifest_dict, list):\n\t\t\tobserver_schema_dict['type'] = 'array'\n\t\t\tob", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tschema_dict[\"type\"] = \"object\"\n\tschema_dict[\"properties\"] = {}\n\tschema_dict[\"required\"] = []\n\tschema_dict[\"additionalProperties\"] = False\n\tfor key in manifest_dict:\n\t\tif type(manifest_dict[key]) == dict:\n\t\t\tschema_dict[\"properties\"][key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=True)\n\t\t\tschema_dict[\"required\"].append(key)\n\t\telif type(manifest_dict[key]) == list:\n\t\t\tschema_dict[\"properties\"][key] = {}\n\t\t\tschema_dict[\"properties\"][key][\"type\"] = \"array\"\n\t\t\tschema_dict[\"properties\"][key][\"items\"] = {}\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"type\"] = \"object\"\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"properties\"] = {}\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"required\"] = []\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"additionalProperties\"] = False\n\t\t\tschema_dict[\"properties\"][key][\"items\"][\"properties\"][\"type\"] = {}\n\t\t\tschema_dict[\"", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) is list:\n\t\t\tobserver_schema_dict[key] = []\n\t\telif type(value) is str:\n\t\t\tif value == 'string':\n\t\t\t\tobserver_schema_dict[key] = ''\n\t\t\telif value == 'integer':\n\t\t\t\tobserver_schema_dict[key] = 0\n\t\t\telif value == 'double':\n\t\t\t\tobserver_schema_dict[key] = 0.0\n\t\t\telif value == 'long':\n\t\t\t\tobserver_schema_dict[key] = 0\n\t\t\telif value == 'boolean':\n\t\t\t\tobserver_schema_dict[key] = False\n\t\t\telif value == 'dateTime':\n\t\t\t\tobserver_schema_dict[key] = '2019-01-01T00:00:00.000Z'\n\t\telif type(value) is int:\n\t\t\tobserver_schema_dict[key] = value\n\t\telse:\n\t\t\tprint('The", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tschema_dict[key] = value\n\tif not first_level:\n\t\treturn {\"properties\": schema_dict}\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif not first_level:\n\t\tnew_dict[\"event_type\"] = manifest_dict.get(\"event_type\", \"\")\n\t\tnew_dict[\"event_source\"] = manifest_dict.get(\"event_source\", \"\")\n\t\tnew_dict[\"event_subtype\"] = manifest_dict.get(\"event_subtype\", \"\")\n\t\tnew_dict[\"event_description\"] = manifest_dict.get(\"event_description\", \"\")\n\t\tnew_dict[\"event_schema_version\"] = manifest_dict.get(\"event_schema_version\", \"\")\n\t\tnew_dict[\"event_schema_url\"] = manifest_dict.get(\"event_schema_url\", \"\")\n\t\tnew_dict[\"event_schema_type\"] = manifest_dict.get(\"event_schema_type\", \"\")\n\t\tnew_dict[\"event_schema_format_version\"] = manifest_dict.get(\"event_schema_format_version\", \"\")\n\t\tnew_dict[\"event_schema\"] = manifest_dict.get(\"event_schema\", {})\n\telse:\n\t\tnew_dict[\"event_type\"] = manifest_dict.get(\"event_type\", \"\")\n\t\tnew_dict[\"event_source\"] = manifest_dict.get(\"event_source\", \"", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tif not first_level:\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\t\telse:\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\t\telif type(value) is list:\n\t\t\tobserver_schema_dict[key] = [generate_default_observer_schema_dict(item, first_level=True) for item in value]\n\t\telse:\n\t\t\tobserver_schema_dict[key] = value\n\treturn observer_schema_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_schema = {}\n\tfor key in manifest_dict.keys():\n\t\tvalue = manifest_dict[key]\n\t\ttype_of_value = type(value)\n\t\tif type_of_value == dict:\n\t\t\tif first_level:\n\t\t\t\tnew_schema[key] = {}\n\t\t\t\tnew_schema[key][\"type\"] = \"object\"\n\t\t\telse:\n\t\t\t\tnew_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif type_of_value == list:\n\t\t\tnew_schema[key] = []\n\t\t\tfor i in value:\n\t\t\t\tif i == \"string\":\n\t\t\t\t\tnew_schema[key].append(\"string\")\n\t\t\t\telif i == \"number\":\n\t\t\t\t\tnew_schema[key].append(\"number\")\n\t\t\t\telif i == \"integer\":\n\t\t\t\t\tnew_schema[key].append(\"integer\")\n\t\t\t\telif i == \"boolean\":\n\t\t\t\t\tnew_schema[key].append(\"boolean\")\n\t\t\t\telse:\n\t\t\t\t\tnew_schema[key].append(i)\n\t\telse:\n\t\t\tnew_schema[key] = value\n\treturn new_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = [\"\"]\n\t\telse:\n\t\t\tnew_dict[key] = \"\"\n\treturn new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tobserver_schema_dict[key] = value\n\t\tif not first_level:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\t\telif isinstance(value, list):\n\t\t\t\tobserver_schema_dict[key] = []\n\t\t\t\tfor item in value:\n\t\t\t\t\tobserver_schema_dict[key].append(generate_default_observer_schema_dict(item, True))\n\treturn observer_schema_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif first_level:\n\t\tnew_dict[\"observer\"] = {}\n\t\tnew_dict[\"observer\"][\"name\"] = None\n\t\tnew_dict[\"observer\"][\"description\"] = None\n\t\tnew_dict[\"observer\"][\"version\"] = None\n\t\tnew_dict[\"observer\"][\"type\"] = None\n\t\tnew_dict[\"observer\"][\"url\"] = None\n\telse:\n\t\tnew_dict[\"name\"] = None\n\t\tnew_dict[\"description\"] = None\n\t\tnew_dict[\"version\"] = None\n\t\tnew_dict[\"type\"] = None\n\t\tnew_dict[\"url\"] = None\n\tfor key in manifest_dict:\n\t\tif type(manifest_dict[key]) == dict:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=False)\n\t\telif type(manifest_dict[key]) == list:\n\t\t\tnew_dict[key] = []\n\t\t\tfor item in manifest_dict[key]:\n\t\t\t\tif type(item) == dict:\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(item, first_level=False))\n\t\t\t\telse:\n\t\t\t\t\tnew_dict[", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = last_applied_manifest or {}\n\tfor field in observer_schema:\n\t\t# if the field is a dict, we need to go deeper\n\t\tif isinstance(field, dict):\n\t\t\tlast_applied_manifest[field[\"name\"]] = update_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest.get(field[\"name\"]), field, response\n\t\t\t)\n\t\t# if the field is a list, we need to go deeper\n\t\telif isinstance(field, list):\n\t\t\tlast_applied_manifest[field[\"name\"]] = update_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest.get(field[\"name\"]), field, response\n\t\t\t)\n\t\t# if the field is a string, we can directly get its value\n\t\telif isinstance(field, str):\n\t\t\tlast_applied_manifest[field] = response.get(field)\n\t\t# if the field is a numeric string, we can directly get its value\n\t\telif isinstance(field, int):\n\t\t\tlast_applied_manifest[field] = response.get(str(field))\n\treturn last_applied_manifest\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_list = []\n\tobserver_schema_list = []\n\tfor field in observer_schema:\n\t\tif field[\"type\"] == \"object\" and field[\"x-kubernetes-patch-strategy\"] == \"merge\":\n\t\t\tlast_applied_manifest_list.append(last_applied_manifest[0].get(field[\"name\"], {}))\n\t\t\tobserver_schema_list.append(field)\n\t\telif field[\"type\"] == \"array\" and field[\"x-kubernetes-patch-strategy\"] == \"merge\":\n\t\t\tlast_applied_manifest_list.append(last_applied_manifest[0].get(field[\"name\"], []))\n\t\t\tobserver_schema_list.append(field)\n\t\telse:\n\t\t\tlast_applied_manifest_list.append(last_applied_manifest[0].get(field[\"name\"], None))\n\t\t\tobserver_schema_list.append(field)\n\n\t# update last_applied_manifest\n\tfor i, field in enumerate(observer_schema_list):\n\t\tif field[\"type\"] == \"object\":\n\t\t\tlast_applied_manifest_list[i] = update_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_ap", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom .update_last_applied_manifest_list_from_resp import (\n\t\tupdate_last_applied_manifest_list_from_resp,\n\t)\n\tfrom .update_last_applied_manifest_dict_from_resp import (\n\t\tupdate_last_applied_manifest_dict_from_resp,\n\t)\n\n\tfor field in observer_schema:\n\t\tfield_name = field[\"field\"]\n\t\tfield_type = field.get(\"type\", \"\")\n\n\t\tif field_name not in response:\n\t\t\tcontinue\n\n\t\tif field_type == \"list\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tfield[\"observer_schema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\t\tcontinue\n\t\tif field_type == \"dict\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tfield[\"observer_schema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\t\tcontinue\n\t\tif field_type == \"str\":\n\t\t\tlast_applied_manifest[field_name] = str(response[field_", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tpass\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tfor field in observer_schema:\n\t\tfield_name = field['name']\n\t\tif field_name in last_applied_manifest:\n\t\t\tcontinue\n\t\tif 'type' in field:\n\t\t\tfield_type = field['type']\n\t\t\tif field_type == 'list':\n\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\telif field_type == 'bool':\n\t\t\t\tlast_applied_manifest[field_name] = False\n\t\t\telif field_type == 'int':\n\t\t\t\tlast_applied_manifest[field_name] = 0\n\t\t\telif field_type == 'string':\n\t\t\t\tlast_applied_manifest[field_name] = ''\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\tif 'list' in field:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field['list'], response\n\t\t\t)\n\t\telif 'map' in field:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field['map'], response\n\t\t\t)\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_copy = last_applied_manifest.copy()\n\tfor field in observer_schema:\n\t\tif field.get('observed') and field.get('name') not in response:\n\t\t\tlast_applied_manifest_copy[field['name']] = field.get('default')\n\treturn last_applied_manifest_copy", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tobserved_fields = observer_schema.get(\"observed_fields\", [])\n\tfor observed_field in observed_fields:\n\t\tif observed_field not in last_applied_manifest:\n\t\t\tif observed_field == \"labels\":\n\t\t\t\tlast_applied_manifest[observed_field] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[observed_field] = None\n\t\n\tif \"items\" in response:\n\t\titems = response.get(\"items\", [])\n\t\tfor item in items:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, item\n\t\t\t)\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_copy = last_applied_manifest.copy()\n\tobserver_schema_copy = observer_schema.copy()\n\tfor field in observer_schema_copy:\n\t\tif field[\"name\"] not in last_applied_manifest_copy:\n\t\t\tlast_applied_manifest_copy[field[\"name\"]] = field[\"default\"]\n\t\tif field[\"type\"] == \"object\":\n\t\t\tif field[\"name\"] in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t    last_applied_manifest_copy[field[\"name\"]],\n\t\t\t\t    field[\"children\"],\n\t\t\t\t    response[field[\"name\"]],\n\t\t\t\t)\n\treturn last_applied_manifest_copy\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_fields = last_applied_manifest.keys()\n\tobserver_schema_fields = [\n\t    field[\"name\"] for field in observer_schema if field[\"observed\"]\n\t]\n\n\tfor field in observer_schema_fields:\n\t    if field not in last_applied_manifest_fields:\n\t        last_applied_manifest[field] = None\n\n\treturn last_applied_manifest\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tpass\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = last_applied_manifest or {}\n\tfor field in observer_schema:\n\t\t# if the field is a dict, we need to go deeper\n\t\tif isinstance(field, dict):\n\t\t\tlast_applied_manifest[field[\"name\"]] = update_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest.get(field[\"name\"]), field, response\n\t\t\t)\n\t\t# if the field is a list, we need to go deeper\n\t\telif isinstance(field, list):\n\t\t\tlast_applied_manifest[field[\"name\"]] = update_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest.get(field[\"name\"]), field, response\n\t\t\t)\n\t\t# if the field is a string, we can directly get its value\n\t\telif isinstance(field, str):\n\t\t\tlast_applied_manifest[field] = response.get(field)\n\t\t# if the field is a numeric string, we can directly get its value\n\t\telif isinstance(field, int):\n\t\t\tlast_applied_manifest[field] = response.get(str(field))\n\treturn last_applied_manifest\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_list = []\n\tobserver_schema_list = []\n\tfor field in observer_schema:\n\t\tif field[\"type\"] == \"object\" and field[\"x-kubernetes-patch-strategy\"] == \"merge\":\n\t\t\tlast_applied_manifest_list.append(last_applied_manifest[0].get(field[\"name\"], {}))\n\t\t\tobserver_schema_list.append(field)\n\t\telif field[\"type\"] == \"array\" and field[\"x-kubernetes-patch-strategy\"] == \"merge\":\n\t\t\tlast_applied_manifest_list.append(last_applied_manifest[0].get(field[\"name\"], []))\n\t\t\tobserver_schema_list.append(field)\n\t\telse:\n\t\t\tlast_applied_manifest_list.append(last_applied_manifest[0].get(field[\"name\"], None))\n\t\t\tobserver_schema_list.append(field)\n\n\t# update last_applied_manifest\n\tfor i, field in enumerate(observer_schema_list):\n\t\tif field[\"type\"] == \"object\":\n\t\t\tlast_applied_manifest_list[i] = update_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_ap", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom .update_last_applied_manifest_list_from_resp import (\n\t\tupdate_last_applied_manifest_list_from_resp,\n\t)\n\tfrom .update_last_applied_manifest_dict_from_resp import (\n\t\tupdate_last_applied_manifest_dict_from_resp,\n\t)\n\n\tfor field in observer_schema:\n\t\tfield_name = field[\"field\"]\n\t\tfield_type = field.get(\"type\", \"\")\n\n\t\tif field_name not in response:\n\t\t\tcontinue\n\n\t\tif field_type == \"list\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tfield[\"observer_schema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\t\tcontinue\n\t\tif field_type == \"dict\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tfield[\"observer_schema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\t\tcontinue\n\t\tif field_type == \"str\":\n\t\t\tlast_applied_manifest[field_name] = str(response[field_", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tfor field in observer_schema:\n\t\tfield_name = field['name']\n\t\tif field_name in last_applied_manifest:\n\t\t\tcontinue\n\t\tif 'type' in field:\n\t\t\tfield_type = field['type']\n\t\t\tif field_type == 'list':\n\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\telif field_type == 'bool':\n\t\t\t\tlast_applied_manifest[field_name] = False\n\t\t\telif field_type == 'int':\n\t\t\t\tlast_applied_manifest[field_name] = 0\n\t\t\telif field_type == 'string':\n\t\t\t\tlast_applied_manifest[field_name] = ''\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\tif 'list' in field:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field['list'], response\n\t\t\t)\n\t\telif 'map' in field:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field['map'], response\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_copy = last_applied_manifest.copy()\n\tfor field in observer_schema:\n\t\tif field.get('observed') and field.get('name') not in response:\n\t\t\tlast_applied_manifest_copy[field['name']] = field.get('default')\n\treturn last_applied_manifest_copy", "is_pass": true, "return_code": 0}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tobserved_fields = observer_schema.get(\"observed_fields\", [])\n\tfor observed_field in observed_fields:\n\t\tif observed_field not in last_applied_manifest:\n\t\t\tif observed_field == \"labels\":\n\t\t\t\tlast_applied_manifest[observed_field] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[observed_field] = None\n\t\n\tif \"items\" in response:\n\t\titems = response.get(\"items\", [])\n\t\tfor item in items:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, item\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_copy = last_applied_manifest.copy()\n\tobserver_schema_copy = observer_schema.copy()\n\tfor field in observer_schema_copy:\n\t\tif field[\"name\"] not in last_applied_manifest_copy:\n\t\t\tlast_applied_manifest_copy[field[\"name\"]] = field[\"default\"]\n\t\tif field[\"type\"] == \"object\":\n\t\t\tif field[\"name\"] in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t    last_applied_manifest_copy[field[\"name\"]],\n\t\t\t\t    field[\"children\"],\n\t\t\t\t    response[field[\"name\"]],\n\t\t\t\t)\n\treturn last_applied_manifest_copy\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_fields = last_applied_manifest.keys()\n\tobserver_schema_fields = [\n\t    field[\"name\"] for field in observer_schema if field[\"observed\"]\n\t]\n\n\tfor field in observer_schema_fields:\n\t    if field not in last_applied_manifest_fields:\n\t        last_applied_manifest[field] = None\n\n\treturn last_applied_manifest\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tpass\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tobserved_field = observer_schema[\"observed\"]\n\texcept KeyError:\n\t\tpass\n\telse:\n\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n\tfor field, value in observer_schema.items():\n\t\tif field == \"observed\":\n\t\t\tcontinue\n\t\telif isinstance(value, dict):\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\texcept KeyError:\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field], value, response[field]\n\t\t\t)\n\t\telif isinstance(value, list):\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\texcept KeyError:\n\t\t\t\tlast_applied_manifest[field] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field], value, response[field]\n\t\t\t)\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tfor field in observer_schema[\"observed_fields\"]:\n\t\t\tif field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field] = get_field_from_response(\n\t\t\t\t\tresponse, field\n\t\t\t\t)\n\texcept KeyError as e:\n\t\traise KeyError(\n\t\t\t\"Error while updating last_applied_manifest: {}\".format(str(e))\n\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema.get(\"observed_fields\", {})\n\tfor observed_field, observed_schema in observed_fields.items():\n\t\tobserved_field_path = observed_schema.get(\"observed_field_path\")\n\t\tobserved_field_type = observed_schema.get(\"observed_field_type\")\n\t\tif observed_field_type == \"dict\":\n\t\t\tif observed_field_path not in response:\n\t\t\t\traise KeyError(f\"{observed_field_path} not found in response\")\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserver_schema,\n\t\t\t\tresponse.get(observed_field_path),\n\t\t\t)\n\t\telif observed_field_type == \"list\":\n\t\t\tif observed_field_path not in response:\n\t\t\t\traise KeyError(f\"{observed_field_path} not found in response\")\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserver_schema,\n\t\t\t\tresponse.get(observed_field_path),\n\t\t\t)\n\t\telif observed_field_type == \"str\":\n\t\t\tif observed", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\tfor field in observed_fields:\n\t\tif field not in response:\n\t\t\traise KeyError(\"Field {} not found in response\".format(field))\n\t\tobserver_schema_field = observer_schema[\"observed_fields\"][field]\n\t\tfield_type = observer_schema_field[\"type\"]\n\t\tlast_applied_manifest_field = last_applied_manifest.setdefault(\n\t\t\tfield, {}\n\t\t)\n\t\tif field_type == \"list\":\n\t\t\tlast_applied_manifest_field = last_applied_manifest_field.setdefault(\n\t\t\t\tobserver_schema_field[\"list_field\"], []\n\t\t\t)\n\t\telif field_type == \"dict\":\n\t\t\tlast_applied_manifest_field = last_applied_manifest_field.setdefault(\n\t\t\t\tobserver_schema_field[\"dict_field\"], {}\n\t\t\t)\n\t\telif field_type == \"str\":\n\t\t\tlast_applied_manifest_field = last_applied_manifest_field.setdefault(\n\t\t\t\tobserver_schema_field[\"str_field\"], \"\"\n\t\t\t)\n\t\telif field_type == \"bool\":\n\t\t\tlast_applied_", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields_schema = observer_schema[\"observed_fields\"]\n\tobserved_fields_schema_keys = observed_fields_schema.keys()\n\tfor observed_field_name in observed_fields_schema_keys:\n\t\tif observed_field_name in response:\n\t\t\tobserved_field_schema = observed_fields_schema[\n\t\t\t\tobserved_field_name\n\t\t\t]\n\t\t\tobserved_field_type = observed_field_schema[\"type\"]\n\t\t\tif observed_field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field_name] = None\n\t\t\tif observed_field_type == \"object\":\n\t\t\t\tlast_applied_manifest[observed_field_name] = dict()\n\t\t\tif observed_field_type == \"list\":\n\t\t\t\tlast_applied_manifest[observed_field_name] = list()\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tkube_response_schema = observer_schema.get(\"kube_response_schema\")\n\tfields = kube_response_schema[\"fields\"]\n\n\tfor field_name, field_observation in fields.items():\n\t\tfield_type = field_observation[\"type\"]\n\n\t\t# If the field is not present in the response, we skip it\n\t\tif field_name not in response:\n\t\t\tcontinue\n\n\t\tobserved_field = response[field_name]\n\n\t\t# If the field is a list, we recursively update the last_applied_manifest\n\t\tif field_type == \"list\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tfield_observation,\n\t\t\t\tobserved_field,\n\t\t\t)\n\t\t# If the field is a map, we recursively update the last_applied_manifest\n\t\telif field_type == \"map\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tfield_observation,\n\t\t\t\tobserved_field,\n\t\t\t)\n\t\t# If the field is a simple value, we set it in the last_applied_manifest\n\t\telse:\n\t\t\t# Set", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tobserved_fields = observer_schema['observedFields']\n\texcept KeyError:\n\t\treturn\n\n\tfor observed_field in observed_fields:\n\t\ttry:\n\t\t\tvalue = response[observed_field]\n\t\t\tlast_applied_manifest[observed_field] = value\n\t\texcept KeyError:\n\t\t\tpass\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = copy.deepcopy(last_applied_manifest)\n\tobserver_schema = copy.deepcopy(observer_schema)\n\tresponse = copy.deepcopy(response)\n\tfield_name = observer_schema[\"fieldName\"]\n\tif field_name in response:\n\t\tif not field_name in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\telif observer_schema[\"type\"] == \"list\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observerSchema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\telif observer_schema[\"type\"] == \"dict\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observerSchema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\telse:\n\t\traise KeyError(\n\t\t\t\"Field {} is not present in the Kubernetes response\".format(field_name)\n\t\t)\n\treturn last_applied_manifest", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed\"]\n\tif len(observed_fields) == 0:\n\t\treturn\n\tobserver_schema[\"observed\"] = observer_schema[\"observed\"][1:]\n\tfor field in observed_fields:\n\t\tif last_applied_manifest.get(field) is None:\n\t\t\tlast_applied_manifest[field] = response.get(field)\n\t\telif isinstance(last_applied_manifest[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[\"observed\"][0],\n\t\t\t\tresponse[field],\n\t\t\t)\n\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[\"observed\"][0],\n\t\t\t\tresponse[field],\n\t\t\t)\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema.get(\"observed_fields\", [])\n\tmanaged_fields = observer_schema.get(\"managed_fields\", [])\n\tfor field in observed_fields:\n\t\tif field in response:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n\tfor field in managed_fields:\n\t\tif field in response:\n\t\t\tsub_observer_schema = observer_schema[\"managed_fields\"][field]\n\t\t\tif isinstance(sub_observer_schema, str):\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\telse:\n\t\t\t\tif field not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field], sub_observer_schema, response[field]\n\t\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[field] = {}"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tobserved_field = observer_schema[\"observed\"]\n\texcept KeyError:\n\t\tpass\n\telse:\n\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n\tfor field, value in observer_schema.items():\n\t\tif field == \"observed\":\n\t\t\tcontinue\n\t\telif isinstance(value, dict):\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\texcept KeyError:\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field], value, response[field]\n\t\t\t)\n\t\telif isinstance(value, list):\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\texcept KeyError:\n\t\t\t\tlast_applied_manifest[field] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field], value, response[field]\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tfor field in observer_schema[\"observed_fields\"]:\n\t\t\tif field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field] = get_field_from_response(\n\t\t\t\t\tresponse, field\n\t\t\t\t)\n\texcept KeyError as e:\n\t\traise KeyError(\n\t\t\t\"Error while updating last_applied_manifest: {}\".format(str(e))\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema.get(\"observed_fields\", {})\n\tfor observed_field, observed_schema in observed_fields.items():\n\t\tobserved_field_path = observed_schema.get(\"observed_field_path\")\n\t\tobserved_field_type = observed_schema.get(\"observed_field_type\")\n\t\tif observed_field_type == \"dict\":\n\t\t\tif observed_field_path not in response:\n\t\t\t\traise KeyError(f\"{observed_field_path} not found in response\")\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserver_schema,\n\t\t\t\tresponse.get(observed_field_path),\n\t\t\t)\n\t\telif observed_field_type == \"list\":\n\t\t\tif observed_field_path not in response:\n\t\t\t\traise KeyError(f\"{observed_field_path} not found in response\")\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserver_schema,\n\t\t\t\tresponse.get(observed_field_path),\n\t\t\t)\n\t\telif observed_field_type == \"str\":\n\t\t\tif observed", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\tfor field in observed_fields:\n\t\tif field not in response:\n\t\t\traise KeyError(\"Field {} not found in response\".format(field))\n\t\tobserver_schema_field = observer_schema[\"observed_fields\"][field]\n\t\tfield_type = observer_schema_field[\"type\"]\n\t\tlast_applied_manifest_field = last_applied_manifest.setdefault(\n\t\t\tfield, {}\n\t\t)\n\t\tif field_type == \"list\":\n\t\t\tlast_applied_manifest_field = last_applied_manifest_field.setdefault(\n\t\t\t\tobserver_schema_field[\"list_field\"], []\n\t\t\t)\n\t\telif field_type == \"dict\":\n\t\t\tlast_applied_manifest_field = last_applied_manifest_field.setdefault(\n\t\t\t\tobserver_schema_field[\"dict_field\"], {}\n\t\t\t)\n\t\telif field_type == \"str\":\n\t\t\tlast_applied_manifest_field = last_applied_manifest_field.setdefault(\n\t\t\t\tobserver_schema_field[\"str_field\"], \"\"\n\t\t\t)\n\t\telif field_type == \"bool\":\n\t\t\tlast_applied_", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields_schema = observer_schema[\"observed_fields\"]\n\tobserved_fields_schema_keys = observed_fields_schema.keys()\n\tfor observed_field_name in observed_fields_schema_keys:\n\t\tif observed_field_name in response:\n\t\t\tobserved_field_schema = observed_fields_schema[\n\t\t\t\tobserved_field_name\n\t\t\t]\n\t\t\tobserved_field_type = observed_field_schema[\"type\"]\n\t\t\tif observed_field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field_name] = None\n\t\t\tif observed_field_type == \"object\":\n\t\t\t\tlast_applied_manifest[observed_field_name] = dict()\n\t\t\tif observed_field_type == \"list\":\n\t\t\t\tlast_applied_manifest[observed_field_name] = list()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tkube_response_schema = observer_schema.get(\"kube_response_schema\")\n\tfields = kube_response_schema[\"fields\"]\n\n\tfor field_name, field_observation in fields.items():\n\t\tfield_type = field_observation[\"type\"]\n\n\t\t# If the field is not present in the response, we skip it\n\t\tif field_name not in response:\n\t\t\tcontinue\n\n\t\tobserved_field = response[field_name]\n\n\t\t# If the field is a list, we recursively update the last_applied_manifest\n\t\tif field_type == \"list\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tfield_observation,\n\t\t\t\tobserved_field,\n\t\t\t)\n\t\t# If the field is a map, we recursively update the last_applied_manifest\n\t\telif field_type == \"map\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tfield_observation,\n\t\t\t\tobserved_field,\n\t\t\t)\n\t\t# If the field is a simple value, we set it in the last_applied_manifest\n\t\telse:\n\t\t\t# Set", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tobserved_fields = observer_schema['observedFields']\n\texcept KeyError:\n\t\treturn\n\n\tfor observed_field in observed_fields:\n\t\ttry:\n\t\t\tvalue = response[observed_field]\n\t\t\tlast_applied_manifest[observed_field] = value\n\t\texcept KeyError:\n\t\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = copy.deepcopy(last_applied_manifest)\n\tobserver_schema = copy.deepcopy(observer_schema)\n\tresponse = copy.deepcopy(response)\n\tfield_name = observer_schema[\"fieldName\"]\n\tif field_name in response:\n\t\tif not field_name in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\telif observer_schema[\"type\"] == \"list\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observerSchema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\telif observer_schema[\"type\"] == \"dict\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observerSchema\"],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\telse:\n\t\traise KeyError(\n\t\t\t\"Field {} is not present in the Kubernetes response\".format(field_name)\n\t\t)\n\treturn last_applied_manifest", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed\"]\n\tif len(observed_fields) == 0:\n\t\treturn\n\tobserver_schema[\"observed\"] = observer_schema[\"observed\"][1:]\n\tfor field in observed_fields:\n\t\tif last_applied_manifest.get(field) is None:\n\t\t\tlast_applied_manifest[field] = response.get(field)\n\t\telif isinstance(last_applied_manifest[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[\"observed\"][0],\n\t\t\t\tresponse[field],\n\t\t\t)\n\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[\"observed\"][0],\n\t\t\t\tresponse[field],\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema.get(\"observed_fields\", [])\n\tmanaged_fields = observer_schema.get(\"managed_fields\", [])\n\tfor field in observed_fields:\n\t\tif field in response:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n\tfor field in managed_fields:\n\t\tif field in response:\n\t\t\tsub_observer_schema = observer_schema[\"managed_fields\"][field]\n\t\t\tif isinstance(sub_observer_schema, str):\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\telse:\n\t\t\t\tif field not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field], sub_observer_schema, response[field]\n\t\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[field] = {}", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tpass", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom kubernetes import client\n\n\tapi_client = client.ApiClient()\n\tapi_instance = client.CustomObjectsApi(api_client)\n\n\tresource_kinds = app.spec.manifest\n\n\tfor resource_kind in resource_kinds:\n\t\tif resource_kind not in ['Pod']:\n\t\t\tcontinue\n\t\tresponse = api_instance.get_namespaced_custom_object(\n\t\t\tgroup=\"apps\",\n\t\t\tversion=\"v1\",\n\t\t\tnamespace=\"default\",\n\t\t\tplural=resource_kind.lower()+\"s\",\n\t\t)\n\t\titems = response.get('items')\n\t\tif not items:\n\t\t\tcontinue\n\t\tfor item in items:\n\t\t\tmanifest = item.get('spec')\n\t\t\tif not manifest:\n\t\t\t\tcontinue\n\t\t\tobserved_fields = manifest.keys()\n\t\t\tobserver_schema = {}\n\t\t\tfor observed_field in observed_fields:\n\t\t\t\tobserver_schema[observed_field] = {}\n\t\t\tapp.spec.observer_schema[resource_kind] = observer_schema\n\n\tapp.save()\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tspecs = app.spec.manifest\n\tobserver_schema = {}\n\n\tfor spec in specs:\n\t\tkind = spec[\"kind\"]\n\t\tname = spec[\"metadata\"][\"name\"]\n\t\tobserver_schema[f\"{kind}_{name}\"] = {\"observer_schema\": {}}\n\n\treturn observer_schema\n\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\tobserver_schema = {}\n\t\n\tkubernetes_resources = app.get('spec', {}).get('manifest', {})\n\n\tfor resource_name, resource_definition in kubernetes_resources.items():\n\t\tresource_kind = resource_definition.get('kind', '')\n\t\tresource_fields = resource_definition.get('fields', [])\n\n\t\tobserver_schema[resource_name] = {\n\t\t\t'kind': resource_kind,\n\t\t\t'fields': resource_fields\n\t\t}\n\t\n\treturn observer_schema", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tpass", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom app.models import ObserverSchema\n\tfrom app.utils import get_kubernetes_resource_name\n\n\tfor manifest in app.config.get(\"MANIFESTS\", []):\n\t\tkind = manifest.get(\"kind\", \"\").lower()\n\t\tresource_name = get_kubernetes_resource_name(kind)\n\n\t\tif not ObserverSchema.query.filter_by(kind=resource_name).first():\n\t\t\tObserverSchema(\n\t\t\t\tkind=resource_name,\n\t\t\t\tschema=app.config.get(\"DEFAULT_OBSERVER_SCHEMAS\", {}).get(kind, {}),\n\t\t\t).save()", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobservers = app.get_observers()\n\tmanifest_resources = app.get_manifest_resources()\n\n\tfor resource_name, manifest_resource in manifest_resources.items():\n\t\t# Skip if custom observer schema is already present\n\t\tif f\"{resource_name.lower()}_observer_schema\" in observers:\n\t\t\tcontinue\n\n\t\t# Create default observer schema\n\t\tdefault_schema = {}\n\t\tfor field_name, field_config in manifest_resource.get(\"spec\", {}).get(\"manifest\", {}).get(\"spec\", {}).items():\n\t\t\tdefault_schema[field_name] = {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"description\": f\"Default value for {field_name}\",\n\t\t\t\t\"default\": field_config.get(\"default\"),\n\t\t\t}\n\n\t\tobservers[f\"{resource_name.lower()}_observer_schema\"] = default_schema\n\n\tapp.set_observers(observers)", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tapp_class = app.get(\"appClass\")\n\tmanifest = app.get(\"manifest\")\n\n\tobserver_schema = []\n\tfor resource in manifest:\n\t\tresource_name = resource.get(\"name\")\n\t\tresource_type = resource.get(\"type\")\n\t\tobserver_schema.append({\n\t\t\t\"resource\": resource_name,\n\t\t\t\"name\": f\"{app_class}-{resource_name}\",\n\t\t\t\"type\": resource_type,\n\t\t\t\"schema\": {\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\"type\": \"object\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\treturn observer_schema", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tlogger = app.logger\n\n\tlogger.debug(\"Generating default observer schema for Kubernetes resources\")\n\n\tresources = list(app.config[\"K8S_RESOURCES\"].keys())\n\n\tspec_manifest_key = get_spec_manifest_key(app)\n\n\tfor resource in resources:\n\t\tresource_observer_schema_key = \"{}.observer_schema\".format(resource)\n\n\t\t# Check if observer schema is already present for the resource\n\t\tif resource_observer_schema_key in app.config:\n\t\t\tlogger.debug(\"Default observer schema already present for resource: {}\".format(resource))\n\t\t\tcontinue\n\n\t\tlogger.debug(\"Generating default observer schema for resource: {}\".format(resource))\n\n\t\tresource_spec_manifest_key = \"{}.{}\".format(spec_manifest_key, resource)\n\n\t\tresource_spec_manifest_value = app.config[resource_spec_manifest_key]\n\n\t\t# Generate default observer schema for the resource\n\t\tresource_observer_schema = generate_observer_schema(resource_spec_manifest_value)\n\n\t\tapp.config[resource_observer_schema_key] = resource_observer_schema\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tpass"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom kubernetes import client\n\n\tapi_client = client.ApiClient()\n\tapi_instance = client.CustomObjectsApi(api_client)\n\n\tresource_kinds = app.spec.manifest\n\n\tfor resource_kind in resource_kinds:\n\t\tif resource_kind not in ['Pod']:\n\t\t\tcontinue\n\t\tresponse = api_instance.get_namespaced_custom_object(\n\t\t\tgroup=\"apps\",\n\t\t\tversion=\"v1\",\n\t\t\tnamespace=\"default\",\n\t\t\tplural=resource_kind.lower()+\"s\",\n\t\t)\n\t\titems = response.get('items')\n\t\tif not items:\n\t\t\tcontinue\n\t\tfor item in items:\n\t\t\tmanifest = item.get('spec')\n\t\t\tif not manifest:\n\t\t\t\tcontinue\n\t\t\tobserved_fields = manifest.keys()\n\t\t\tobserver_schema = {}\n\t\t\tfor observed_field in observed_fields:\n\t\t\t\tobserver_schema[observed_field] = {}\n\t\t\tapp.spec.observer_schema[resource_kind] = observer_schema\n\n\tapp.save()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tspecs = app.spec.manifest\n\tobserver_schema = {}\n\n\tfor spec in specs:\n\t\tkind = spec[\"kind\"]\n\t\tname = spec[\"metadata\"][\"name\"]\n\t\tobserver_schema[f\"{kind}_{name}\"] = {\"observer_schema\": {}}\n\n\treturn observer_schema\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\tobserver_schema = {}\n\t\n\tkubernetes_resources = app.get('spec', {}).get('manifest', {})\n\n\tfor resource_name, resource_definition in kubernetes_resources.items():\n\t\tresource_kind = resource_definition.get('kind', '')\n\t\tresource_fields = resource_definition.get('fields', [])\n\n\t\tobserver_schema[resource_name] = {\n\t\t\t'kind': resource_kind,\n\t\t\t'fields': resource_fields\n\t\t}\n\t\n\treturn observer_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom app.models import ObserverSchema\n\tfrom app.utils import get_kubernetes_resource_name\n\n\tfor manifest in app.config.get(\"MANIFESTS\", []):\n\t\tkind = manifest.get(\"kind\", \"\").lower()\n\t\tresource_name = get_kubernetes_resource_name(kind)\n\n\t\tif not ObserverSchema.query.filter_by(kind=resource_name).first():\n\t\t\tObserverSchema(\n\t\t\t\tkind=resource_name,\n\t\t\t\tschema=app.config.get(\"DEFAULT_OBSERVER_SCHEMAS\", {}).get(kind, {}),\n\t\t\t).save()", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobservers = app.get_observers()\n\tmanifest_resources = app.get_manifest_resources()\n\n\tfor resource_name, manifest_resource in manifest_resources.items():\n\t\t# Skip if custom observer schema is already present\n\t\tif f\"{resource_name.lower()}_observer_schema\" in observers:\n\t\t\tcontinue\n\n\t\t# Create default observer schema\n\t\tdefault_schema = {}\n\t\tfor field_name, field_config in manifest_resource.get(\"spec\", {}).get(\"manifest\", {}).get(\"spec\", {}).items():\n\t\t\tdefault_schema[field_name] = {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"description\": f\"Default value for {field_name}\",\n\t\t\t\t\"default\": field_config.get(\"default\"),\n\t\t\t}\n\n\t\tobservers[f\"{resource_name.lower()}_observer_schema\"] = default_schema\n\n\tapp.set_observers(observers)", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tapp_class = app.get(\"appClass\")\n\tmanifest = app.get(\"manifest\")\n\n\tobserver_schema = []\n\tfor resource in manifest:\n\t\tresource_name = resource.get(\"name\")\n\t\tresource_type = resource.get(\"type\")\n\t\tobserver_schema.append({\n\t\t\t\"resource\": resource_name,\n\t\t\t\"name\": f\"{app_class}-{resource_name}\",\n\t\t\t\"type\": resource_type,\n\t\t\t\"schema\": {\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\"type\": \"object\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\treturn observer_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tlogger = app.logger\n\n\tlogger.debug(\"Generating default observer schema for Kubernetes resources\")\n\n\tresources = list(app.config[\"K8S_RESOURCES\"].keys())\n\n\tspec_manifest_key = get_spec_manifest_key(app)\n\n\tfor resource in resources:\n\t\tresource_observer_schema_key = \"{}.observer_schema\".format(resource)\n\n\t\t# Check if observer schema is already present for the resource\n\t\tif resource_observer_schema_key in app.config:\n\t\t\tlogger.debug(\"Default observer schema already present for resource: {}\".format(resource))\n\t\t\tcontinue\n\n\t\tlogger.debug(\"Generating default observer schema for resource: {}\".format(resource))\n\n\t\tresource_spec_manifest_key = \"{}.{}\".format(spec_manifest_key, resource)\n\n\t\tresource_spec_manifest_value = app.config[resource_spec_manifest_key]\n\n\t\t# Generate default observer schema for the resource\n\t\tresource_observer_schema = generate_observer_schema(resource_spec_manifest_value)\n\n\t\tapp.config[resource_observer_schema_key] = resource_observer_schema\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params, None)\n\tif self.placeholders:\n\t\tsql = sql.replace('?', '%s')\n\treturn sql, params", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\treturn sql, params", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tdef _format(sql, params):\n\t\t\"\"\"\n\t\tConvert sql and params using self._converter.convert\n\t\t\"\"\"\n\t\tif params is None:\n\t\t\treturn sql, params\n\t\tif isinstance(params, dict):\n\t\t\tparams = collections.OrderedDict(sorted(params.items()))\n\t\telse:\n\t\t\tparams = tuple(params)\n\t\treturn self._converter.convert(sql, params)\n\n\treturn _format(sql, params)", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params, True)\n\treturn sql, params", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self.converter.convert(sql, params, self.placeholder)\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert(sql, params)\n\treturn converted_sql, converted_params\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql_out = self._converter.convert(sql)\n\tparams_out = self._converter.convert_many(params)\n\treturn sql_out, params_out\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\treturn sql, params", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params, self._escape_style)\n\tif self._escape_style == 'named':\n\t\tparams = self._format_named_params(params)\n\telse:\n\t\tparams = self._format_pyformat_params(params)\n\treturn sql, params\n```\n\n```python"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params, None)\n\tif self.placeholders:\n\t\tsql = sql.replace('?', '%s')\n\treturn sql, params", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\treturn sql, params", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tdef _format(sql, params):\n\t\t\"\"\"\n\t\tConvert sql and params using self._converter.convert\n\t\t\"\"\"\n\t\tif params is None:\n\t\t\treturn sql, params\n\t\tif isinstance(params, dict):\n\t\t\tparams = collections.OrderedDict(sorted(params.items()))\n\t\telse:\n\t\t\tparams = tuple(params)\n\t\treturn self._converter.convert(sql, params)\n\n\treturn _format(sql, params)", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params, True)\n\treturn sql, params", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self.converter.convert(sql, params, self.placeholder)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert(sql, params)\n\treturn converted_sql, converted_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql_out = self._converter.convert(sql)\n\tparams_out = self._converter.convert_many(params)\n\treturn sql_out, params_out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\treturn sql, params", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params, self._escape_style)\n\tif self._escape_style == 'named':\n\t\tparams = self._format_named_params(params)\n\telse:\n\t\tparams = self._format_pyformat_params(params)\n\treturn sql, params\n```\n\n```python", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tparams, sql = self._converter.convert_many(sql, many_params)\n\treturn sql, params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert_many(sql, many_params)\n\treturn converted_sql, converted_params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\n\tif isinstance(many_params[0], dict):\n\t\tparams = many_params\n\t\tmany_params = [tuple(p[name] for name in self._converter.field_names) for p in params]\n\telse:\n\t\tparams = [dict(zip(self._converter.field_names, p)) for p in many_params]\n\n\treturn sql, many_params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert_many(sql, many_params)\n\treturn converted_sql, converted_params\n\n#sql/__init__.py\nfrom .sql import sql\nfrom .sql_converter import sql_converter\nfrom .sql_converters.sql_converters import *\n\n#tests/sql_tests.py\nimport unittest\nimport sql\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = list(many_params)\n\tconverted_many_params = self._converter.convert_many(many_params)\n\treturn sql, converted_many_params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = list(many_params)\n\tif many_params:\n\t\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tparams = []\n\tfor param in many_params:\n\t\tparams.append(param)\n\treturn sql, params", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted = self._converter.convert_many(many_params)\n\treturn sql, converted", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = list(many_params)\n\tmany_params = [self._converter.convert_many(parameters=p) for p in many_params]\n\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted_params = self._converter.convert_many(many_params)\n\tconverted_sql = self._converter.convert_many_sql(sql)\n\n\treturn converted_sql, converted_params\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tparams, sql = self._converter.convert_many(sql, many_params)\n\treturn sql, params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert_many(sql, many_params)\n\treturn converted_sql, converted_params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\n\tif isinstance(many_params[0], dict):\n\t\tparams = many_params\n\t\tmany_params = [tuple(p[name] for name in self._converter.field_names) for p in params]\n\telse:\n\t\tparams = [dict(zip(self._converter.field_names, p)) for p in many_params]\n\n\treturn sql, many_params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert_many(sql, many_params)\n\treturn converted_sql, converted_params\n\n#sql/__init__.py\nfrom .sql import sql\nfrom .sql_converter import sql_converter\nfrom .sql_converters.sql_converters import *\n\n#tests/sql_tests.py\nimport unittest\nimport sql\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = list(many_params)\n\tconverted_many_params = self._converter.convert_many(many_params)\n\treturn sql, converted_many_params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = list(many_params)\n\tif many_params:\n\t\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tparams = []\n\tfor param in many_params:\n\t\tparams.append(param)\n\treturn sql, params", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted = self._converter.convert_many(many_params)\n\treturn sql, converted", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = list(many_params)\n\tmany_params = [self._converter.convert_many(parameters=p) for p in many_params]\n\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tconverted_params = self._converter.convert_many(many_params)\n\tconverted_sql = self._converter.convert_many_sql(sql)\n\n\treturn converted_sql, converted_params\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\n\ttry:\n\t\tobj = self.OCFLObject(path)\n\texcept ValueError:\n\t\treturn False\n\t\t\n\tif obj.object_type not in self.allowed_types:\n\t\treturn False\n\t\t\n\treturn True\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.read_root(path)\n\texcept Exception:\n\t\treturn False\n\treturn True\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.root.tree.get(path)\n\texcept KeyError:\n\t\treturn False\n\treturn True\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tobj = self.fs.getinfo(path, namespaces=('ocfl',))\n\t\tif not obj.is_dir():\n\t\t\treturn False\n\t\tif 'ocfl' not in obj.metadata:\n\t\t\treturn False\n\t\tif 'version' not in obj.metadata['ocfl']:\n\t\t\treturn False\n\t\tif 'inventory' not in obj.metadata['ocfl']:\n\t\t\treturn False\n\t\treturn True\n\texcept FileNotFoundError:\n\t\treturn False", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\timport os\n\timport ocfl\n\n\tif not os.path.isdir(path):\n\t\treturn False\n\tif not os.path.isfile(os.path.join(path, \"bag-info.txt\")):\n\t\treturn False\n\tif not os.path.isfile(os.path.join(path, \"manifest-sha512.txt\")):\n\t\treturn False\n\tif not os.path.isfile(os.path.join(path, \"tagmanifest-sha512.txt\")):\n\t\treturn False\n\n\t# Validate bag-info.txt\n\tbaginfo_txt = ocfl.io.read_file(os.path.join(path, \"bag-info.txt\"))\n\trequired_fields = [\"External-Identifier\",\n\t\t\t\t\t   \"Bagging-Date\",\n\t\t\t\t\t   \"Payload-Oxum\"]\n\tbaginfo_dict = self._parse_baginfo(baginfo_txt)\n\thas_required_fields = all([field in baginfo_dict for field in required_fields])\n\tif not has_required_fields:\n\t\treturn False\n\n\t# Validate manifest-sha512.txt\n\tmanifest_txt = ocfl.io.read_file(os.path.join(path, \"manifest-sha512.txt\"))\n\tmanifest_dict", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tpass\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\traise NotImplementedError()", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tocflv1.validate(path)\n\texcept Exception:\n\t\treturn False\n\treturn True", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tocfl = OCFL(path)\n\texcept OCFLNotFoundError:\n\t\treturn False\n\texcept OCFLVersionError:\n\t\treturn False\n\texcept OCFLValidationError:\n\t\treturn False\n\treturn True", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\traise NotImplementedError()"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\n\ttry:\n\t\tobj = self.OCFLObject(path)\n\texcept ValueError:\n\t\treturn False\n\t\t\n\tif obj.object_type not in self.allowed_types:\n\t\treturn False\n\t\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.read_root(path)\n\texcept Exception:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.root.tree.get(path)\n\texcept KeyError:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tobj = self.fs.getinfo(path, namespaces=('ocfl',))\n\t\tif not obj.is_dir():\n\t\t\treturn False\n\t\tif 'ocfl' not in obj.metadata:\n\t\t\treturn False\n\t\tif 'version' not in obj.metadata['ocfl']:\n\t\t\treturn False\n\t\tif 'inventory' not in obj.metadata['ocfl']:\n\t\t\treturn False\n\t\treturn True\n\texcept FileNotFoundError:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\timport os\n\timport ocfl\n\n\tif not os.path.isdir(path):\n\t\treturn False\n\tif not os.path.isfile(os.path.join(path, \"bag-info.txt\")):\n\t\treturn False\n\tif not os.path.isfile(os.path.join(path, \"manifest-sha512.txt\")):\n\t\treturn False\n\tif not os.path.isfile(os.path.join(path, \"tagmanifest-sha512.txt\")):\n\t\treturn False\n\n\t# Validate bag-info.txt\n\tbaginfo_txt = ocfl.io.read_file(os.path.join(path, \"bag-info.txt\"))\n\trequired_fields = [\"External-Identifier\",\n\t\t\t\t\t   \"Bagging-Date\",\n\t\t\t\t\t   \"Payload-Oxum\"]\n\tbaginfo_dict = self._parse_baginfo(baginfo_txt)\n\thas_required_fields = all([field in baginfo_dict for field in required_fields])\n\tif not has_required_fields:\n\t\treturn False\n\n\t# Validate manifest-sha512.txt\n\tmanifest_txt = ocfl.io.read_file(os.path.join(path, \"manifest-sha512.txt\"))\n\tmanifest_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tocflv1.validate(path)\n\texcept Exception:\n\t\treturn False\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tocfl = OCFL(path)\n\texcept OCFLNotFoundError:\n\t\treturn False\n\texcept OCFLVersionError:\n\t\treturn False\n\texcept OCFLValidationError:\n\t\treturn False\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tprefix_str = ''\n\toutput_str = ''\n\tfor element in self.messages:\n\t\tprefix_str += prefix\n\t\toutput_str += prefix_str + str(element) + '\\n'\n\treturn output_str\n\nif __name__ == '__main__':\n\td = {1: 'a', 2: 'b', 3: 'c', 4: 'd'}\n\tprint d.items()\n\tprint sorted(d.items())\n\tprint status_str(d, '  ')", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus_str = ''\n\tfor s in sorted(self.messages):\n\t\tstatus_str += prefix + s + ' '\n\treturn status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus = ''\n\tfor s in sorted(self.messages):\n\t\tstatus += prefix + s + '\\n'\n\treturn status\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = ''\n\tfor m in sorted(self.messages):\n\t\tresult += prefix + str(m) + '\\n'\n\treturn result\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tself.messages.sort()\n\tret = ''\n\tfor message in self.messages:\n\t\tret += prefix + str(message) + '\\n'\n\treturn ret\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tret = ''\n\tfor msg in sorted(self.messages, key=lambda x: x.timestamp):\n\t\tret += '{}{}'.format(prefix, msg)\n\t\tprefix = '\\n'\n\treturn ret\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tself.msg_list.sort()\n\tstatus = ''\n\tfor i in self.msg_list:\n\t\tstatus += str(prefix) + str(i) + '\\n'\n\treturn status\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus = ''\n\tfor i in sorted(list(self.messages.keys())):\n\t\tstatus += prefix + ' ' + str(self.messages[i]) + '\\n'\n\treturn status\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstr_status = prefix\n\tself.messages.sort()\n\tfor message in self.messages:\n\t\tstr_status = str_status + str(message)\n\treturn str_status\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tret = ''\n\tret += prefix + 'Messages:\\n'\n\tfor msg in sorted(self.messages):\n\t\tret += prefix + str(msg) + '\\n'\n\treturn ret"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tprefix_str = ''\n\toutput_str = ''\n\tfor element in self.messages:\n\t\tprefix_str += prefix\n\t\toutput_str += prefix_str + str(element) + '\\n'\n\treturn output_str\n\nif __name__ == '__main__':\n\td = {1: 'a', 2: 'b', 3: 'c', 4: 'd'}\n\tprint d.items()\n\tprint sorted(d.items())\n\tprint status_str(d, '  ')", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus_str = ''\n\tfor s in sorted(self.messages):\n\t\tstatus_str += prefix + s + ' '\n\treturn status_str\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus = ''\n\tfor s in sorted(self.messages):\n\t\tstatus += prefix + s + '\\n'\n\treturn status\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = ''\n\tfor m in sorted(self.messages):\n\t\tresult += prefix + str(m) + '\\n'\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tself.messages.sort()\n\tret = ''\n\tfor message in self.messages:\n\t\tret += prefix + str(message) + '\\n'\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tret = ''\n\tfor msg in sorted(self.messages, key=lambda x: x.timestamp):\n\t\tret += '{}{}'.format(prefix, msg)\n\t\tprefix = '\\n'\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tself.msg_list.sort()\n\tstatus = ''\n\tfor i in self.msg_list:\n\t\tstatus += str(prefix) + str(i) + '\\n'\n\treturn status\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus = ''\n\tfor i in sorted(list(self.messages.keys())):\n\t\tstatus += prefix + ' ' + str(self.messages[i]) + '\\n'\n\treturn status\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstr_status = prefix\n\tself.messages.sort()\n\tfor message in self.messages:\n\t\tstr_status = str_status + str(message)\n\treturn str_status\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tret = ''\n\tret += prefix + 'Messages:\\n'\n\tfor msg in sorted(self.messages):\n\t\tret += prefix + str(msg) + '\\n'\n\treturn ret", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tprefix = prefix or ''\n\treturn '%s%s' % (prefix, self.log.status_str)\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tlog = self.log\n\tstatus_str = log.status_str\n\tif prefix:\n\t\tstatus_str = prefix + ' ' + status_str\n\treturn status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\traise NotImplementedError\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.log.status_str()\n\tif prefix:\n\t\tstatus_str = '%s: %s' % (prefix, status_str)\n\treturn status_str", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str()\n\treturn '{}{}'.format(prefix, status)\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.log.status_str()\n\tif status_str is None:\n\t\treturn None\n\tif prefix:\n\t\tstatus_str = f\"{prefix}: {status_str}\"\n\treturn status_str", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str()\n\treturn '\\n'.join(prefix + line for line in status.split('\\n'))", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tfrom pymatgen.util.string import get_str_with_symbol\n\treturn prefix + get_str_with_symbol(self.log.status_str)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str()\n\tif prefix:\n\t\tstatus = prefix + status\n\treturn status", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tlog_str = self.log.status_str()\n\treturn '{0}{1}'.format(prefix, log_str)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tprefix = prefix or ''\n\treturn '%s%s' % (prefix, self.log.status_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tlog = self.log\n\tstatus_str = log.status_str\n\tif prefix:\n\t\tstatus_str = prefix + ' ' + status_str\n\treturn status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.log.status_str()\n\tif prefix:\n\t\tstatus_str = '%s: %s' % (prefix, status_str)\n\treturn status_str", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str()\n\treturn '{}{}'.format(prefix, status)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.log.status_str()\n\tif status_str is None:\n\t\treturn None\n\tif prefix:\n\t\tstatus_str = f\"{prefix}: {status_str}\"\n\treturn status_str", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str()\n\treturn '\\n'.join(prefix + line for line in status.split('\\n'))", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tfrom pymatgen.util.string import get_str_with_symbol\n\treturn prefix + get_str_with_symbol(self.log.status_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str()\n\tif prefix:\n\t\tstatus = prefix + status\n\treturn status", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tlog_str = self.log.status_str()\n\treturn '{0}{1}'.format(prefix, log_str)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n\t\n\t", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tprint(\"is_valid\")\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\traise NotImplementedError()\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\traise NotImplementedError\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tprint \"is_valid called with %s\" % identifier\n\treturn True"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n\t\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tprint(\"is_valid\")\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\traise NotImplementedError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tprint \"is_valid called with %s\" % identifier\n\treturn True", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif extract_spec_version:\n\t\ttry:\n\t\t\tspec_version = inventory['type']\n\t\texcept KeyError:\n\t\t\tpass\n\t\n\t# Check for required keys\n\trequired_keys = [\n\t\t'id',\n\t\t'name',\n\t\t'description',\n\t\t'tags',\n\t\t'notes',\n\t\t'inventory_hosts',\n\t\t'inventory_sources',\n\t\t'related',\n\t\t'custom_fields',\n\t\t'created',\n\t\t'modified',\n\t\t'type',\n\t\t'controller',\n\t]\n\tfor key in required_keys:\n\t\tif key not in inventory:\n\t\t\traise KeyError(\"Inventory is missing required key: {}\".format(key))\n\t\n\t# Validate the type value\n\tif spec_version == '0.1':\n\t\trequired_keys = [\n\t\t\t'hosts',\n\t\t\t'sources',\n\t\t\t'related',\n\t\t\t'custom_fields',\n\t\t\t'created',\n\t\t\t'modified',\n\t\t]\n\t\tfor key in required_keys:\n\t\t\tif key not in inventory:\n\t\t\t\traise KeyError(\"Inventory is missing required key: {}\".format(key))\n\t\n\t# Validate the id value\n\ttry:\n\t\tvalidate_uuid(invent", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\traise NotImplementedError", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tfrom .. import utils\n\n\tinventory_type = inventory.get('type')\n\tinventory_version = inventory.get('version')\n\n\tif inventory_type:\n\t\tif inventory_type in ('inventory', 'inventory_list'):\n\t\t\tif inventory_version:\n\t\t\t\tif inventory_version in ('0.1', '0.2'):\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\traise utils.InvalidInventoryTypeVersion(\"Invalid inventory version: {}. Valid versions are ['0.1', '0.2']\".format(inventory_version))\n\t\t\telse:\n\t\t\t\traise utils.InvalidInventoryTypeVersion(\"No version defined for inventory type: {}\".format(inventory_type))\n\t\telif inventory_type in ('node', 'nodes'):\n\t\t\tif inventory_version:\n\t\t\t\tif inventory_version in ('0.1', '0.2'):\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\traise utils.InvalidInventoryTypeVersion(\"Invalid nodes version: {}. Valid versions are ['0.1', '0.2']\".format(inventory_version))\n\t\t\telse:\n\t\t\t\traise utils.InvalidInventoryTypeVersion(\"No version defined for nodes type: {}\".format(inventory_type))\n\t\telse:\n\t\t\t", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tself.spec_version = self.spec_version or \"1.0\"\n\t\n\t# Validate the inventory\n\tself.validate_inventory_type(inventory)\n\t\n\t# If extract_spec_version is True, then we'll look at inventory['type'] to determine the spec version\n\tif extract_spec_version:\n\t\tspec_version = self.get_spec_version(inventory)\n\t\t\n\t\tif spec_version is None:\n\t\t\tspec_version = self.spec_version\n\t\t\t\n\t\tself.spec_version = spec_version\n\t\t\n\t# Now we'll actually validate the inventory based on the spec version\n\tself.validate_inventory(inventory)\n\t\n\t# If the spec version is 1.0, then we'll validate the inventory for the 1.0 spec\n\tif self.spec_version == \"1.0\":\n\t\tself.validate_inventory_1_0(inventory)\n\t\t\n\t# If the spec version is 1.1, then we'll validate the inventory for the 1.1 spec\n\telif self.spec_version == \"1.1\":\n\t\tself.validate_inventory_1_1(inventory)\n\t\t\n\t# If the spec version is 1.", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tspec_version = None\n\tif extract_spec_version:\n\t\tif inventory.type:\n\t\t\tspec_version = self.determine_spec_version(inventory.type)\n\t\t\n\t\tif spec_version == None:\n\t\t\tspec_version = self.spec_version\n\t\t\t\n\tself.spec_version = spec_version\n\t\n\tvalid = True\n\t\n\tif self.spec_version == 1:\n\t\tfor inv in inventory.inventory:\n\t\t\tif inv.type == None:\n\t\t\t\tself.errors.append(Error(inv, 'type is required'))\n\t\t\t\tvalid = False\n\t\t\telif inv.type != 'file':\n\t\t\t\tself.errors.append(Error(inv, 'type must be file'))\n\t\t\t\tvalid = False\n\t\t\t\n\t\t\tif inv.name == None:\n\t\t\t\tself.errors.append(Error(inv, 'name is required'))\n\t\t\t\tvalid = False\n\t\t\telif len(inv.name) > 255:\n\t\t\t\tself.errors.append(Error(inv, 'name must be less than 256 characters'))\n\t\t\t\tvalid = False\n\t\t\t\n\t\t\tif inv.path == None:\n\t\t\t\tself.errors.append(Error(inv, 'path is required'))\n\t\t\t\tvalid = False\n\t\t", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\ttry:\n\t\tinventory = inventory.get('inventory', None)\n\t\tif inventory is None:\n\t\t\traise ValueError('A valid inventory is required')\n\n\t\tinventory_type = inventory.get('type', None)\n\n\t\tif extract_spec_version and inventory_type:\n\t\t\tself.spec_version = inventory_type\n\n\t\tif not self.spec_version:\n\t\t\traise ValueError('No valid specification version given')\n\n\t\tself._validate_fields(inventory)\n\n\t\treturn True\n\texcept Exception as e:\n\t\traise", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif extract_spec_version:\n\t\ttry:\n\t\t\tspec_version = inventory['type']\n\t\texcept KeyError:\n\t\t\tpass\n\t\n\t# Check for required keys\n\trequired_keys = [\n\t\t'id',\n\t\t'name',\n\t\t'description',\n\t\t'tags',\n\t\t'notes',\n\t\t'inventory_hosts',\n\t\t'inventory_sources',\n\t\t'related',\n\t\t'custom_fields',\n\t\t'created',\n\t\t'modified',\n\t\t'type',\n\t\t'controller',\n\t]\n\tfor key in required_keys:\n\t\tif key not in inventory:\n\t\t\traise KeyError(\"Inventory is missing required key: {}\".format(key))\n\t\n\t# Validate the type value\n\tif spec_version == '0.1':\n\t\trequired_keys = [\n\t\t\t'hosts',\n\t\t\t'sources',\n\t\t\t'related',\n\t\t\t'custom_fields',\n\t\t\t'created',\n\t\t\t'modified',\n\t\t]\n\t\tfor key in required_keys:\n\t\t\tif key not in inventory:\n\t\t\t\traise KeyError(\"Inventory is missing required key: {}\".format(key))\n\t\n\t# Validate the id value\n\ttry:\n\t\tvalidate_uuid(invent", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tfrom .. import utils\n\n\tinventory_type = inventory.get('type')\n\tinventory_version = inventory.get('version')\n\n\tif inventory_type:\n\t\tif inventory_type in ('inventory', 'inventory_list'):\n\t\t\tif inventory_version:\n\t\t\t\tif inventory_version in ('0.1', '0.2'):\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\traise utils.InvalidInventoryTypeVersion(\"Invalid inventory version: {}. Valid versions are ['0.1', '0.2']\".format(inventory_version))\n\t\t\telse:\n\t\t\t\traise utils.InvalidInventoryTypeVersion(\"No version defined for inventory type: {}\".format(inventory_type))\n\t\telif inventory_type in ('node', 'nodes'):\n\t\t\tif inventory_version:\n\t\t\t\tif inventory_version in ('0.1', '0.2'):\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\traise utils.InvalidInventoryTypeVersion(\"Invalid nodes version: {}. Valid versions are ['0.1', '0.2']\".format(inventory_version))\n\t\t\telse:\n\t\t\t\traise utils.InvalidInventoryTypeVersion(\"No version defined for nodes type: {}\".format(inventory_type))\n\t\telse:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tself.spec_version = self.spec_version or \"1.0\"\n\t\n\t# Validate the inventory\n\tself.validate_inventory_type(inventory)\n\t\n\t# If extract_spec_version is True, then we'll look at inventory['type'] to determine the spec version\n\tif extract_spec_version:\n\t\tspec_version = self.get_spec_version(inventory)\n\t\t\n\t\tif spec_version is None:\n\t\t\tspec_version = self.spec_version\n\t\t\t\n\t\tself.spec_version = spec_version\n\t\t\n\t# Now we'll actually validate the inventory based on the spec version\n\tself.validate_inventory(inventory)\n\t\n\t# If the spec version is 1.0, then we'll validate the inventory for the 1.0 spec\n\tif self.spec_version == \"1.0\":\n\t\tself.validate_inventory_1_0(inventory)\n\t\t\n\t# If the spec version is 1.1, then we'll validate the inventory for the 1.1 spec\n\telif self.spec_version == \"1.1\":\n\t\tself.validate_inventory_1_1(inventory)\n\t\t\n\t# If the spec version is 1.", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tspec_version = None\n\tif extract_spec_version:\n\t\tif inventory.type:\n\t\t\tspec_version = self.determine_spec_version(inventory.type)\n\t\t\n\t\tif spec_version == None:\n\t\t\tspec_version = self.spec_version\n\t\t\t\n\tself.spec_version = spec_version\n\t\n\tvalid = True\n\t\n\tif self.spec_version == 1:\n\t\tfor inv in inventory.inventory:\n\t\t\tif inv.type == None:\n\t\t\t\tself.errors.append(Error(inv, 'type is required'))\n\t\t\t\tvalid = False\n\t\t\telif inv.type != 'file':\n\t\t\t\tself.errors.append(Error(inv, 'type must be file'))\n\t\t\t\tvalid = False\n\t\t\t\n\t\t\tif inv.name == None:\n\t\t\t\tself.errors.append(Error(inv, 'name is required'))\n\t\t\t\tvalid = False\n\t\t\telif len(inv.name) > 255:\n\t\t\t\tself.errors.append(Error(inv, 'name must be less than 256 characters'))\n\t\t\t\tvalid = False\n\t\t\t\n\t\t\tif inv.path == None:\n\t\t\t\tself.errors.append(Error(inv, 'path is required'))\n\t\t\t\tvalid = False\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\ttry:\n\t\tinventory = inventory.get('inventory', None)\n\t\tif inventory is None:\n\t\t\traise ValueError('A valid inventory is required')\n\n\t\tinventory_type = inventory.get('type', None)\n\n\t\tif extract_spec_version and inventory_type:\n\t\t\tself.spec_version = inventory_type\n\n\t\tif not self.spec_version:\n\t\t\traise ValueError('No valid specification version given')\n\n\t\tself._validate_fields(inventory)\n\n\t\treturn True\n\texcept Exception as e:\n\t\traise", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_missing = [digest for digest in digests_used if digest not in self.digests.keys()]\n\tif digests_missing:\n\t\tdigests_missing = \", \".join(digests_missing)\n\t\tself.error(f\"Digests {digests_missing} are not present in the manifest, and are needed for one of the following files:\\n{manifest_files}\")", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present_and_used = {digest for digest, used in digests_used.items() if used}\n\tdigests_missing = set(digests_used.keys()) - digests_present_and_used\n\tif digests_missing:\n\t\tself.error(\"The following digests are present in the manifest but not used by any of the images: {0}\".format(\", \".join(digests_missing)))", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_list = []\n\tfor manifest_file in manifest_files:\n\t\twith open(manifest_file, 'r') as f:\n\t\t\tyaml = yaml_load(f)\n\t\t\tfor key, value in yaml.items():\n\t\t\t\tif 'digest' in value:\n\t\t\t\t\tdigests_list.append(value['digest'])\n\n\tif not set(digests_list).issubset(set(digests_used)):\n\t\treturn error(\n\t\t\t'manifest %s: digest %s is not present and used in digests %s' % (\n\t\t\t\tmanifest_file,\n\t\t\t\tset(digests_list) - set(digests_used),\n\t\t\t\tset(digests_used)\n\t\t\t)\n\t\t)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_used_in_manifest = {digest for manifest in manifest_files for digest in manifest.digests}\n\tdigests_not_used = digests_used_in_manifest - digests_used\n\tif digests_not_used:\n\t\tself.error(f'The following digests were not used in any of the manifest files: {digests_not_used}')\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests = {}\n\tfor manifest in manifest_files:\n\t\tdigests.update(manifest.digests)\n\tfor digest in digests:\n\t\tif digest not in digests_used:\n\t\t\treturn error(\"Digest %s is not used in any layers\" % digest)\n\t\tif digest not in digests:\n\t\t\treturn error(\"Digest %s is not present in the manifest\" % digest)\n\treturn True", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present_and_used = self.get_digests_present_and_used(manifest_files, digests_used)\n\n\t# Check digests are present and used\n\tfor digest in digests_used:\n\t\tif digest not in digests_present_and_used:\n\t\t\tself.error(\"Digest \" + digest + \" is not present in any of the manifest files.\")\n\tdigests_present_and_used = []\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests = self.manifest_get_digests(manifest_files)\n\n\tmissing_digests = self.manifest_check_digests_present(manifest_digests, digests_used)\n\n\tif missing_digests:\n\t\tself.error(\"Digest not found: {}\".format(missing_digests))\n\t\treturn False\n\n\treturn True", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tself.log(3, \"Checking digests are present and used\")\n\tdigests_present = {}\n\tfor manifest_file in manifest_files:\n\t\tself.log(1, \"Checking digests in %s\" % manifest_file)\n\t\tdigests = self.get_digests_from_manifest(manifest_file)\n\t\tfor digest in digests:\n\t\t\tif digest not in digests_present:\n\t\t\t\tdigests_present[digest] = [manifest_file]\n\t\t\telse:\n\t\t\t\tdigests_present[digest].append(manifest_file)\n\tfor digest in digests_used:\n\t\tif digest not in digests_present:\n\t\t\tself.error(\"Digest %s not present in any manifest file\" % digest)\n\t\telif len(digests_present[digest]) > 1:\n\t\t\tself.error(\"Digest %s present in multiple manifest files (%s)\" % (digest, \", \".join(digests_present[digest])))\n\t\telse:\n\t\t\tself.log(2, \"Digest %s present in %s\" % (digest, digests_present[digest][0]))\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = set()\n\tfor manifest_file in manifest_files:\n\t\tdigests = manifest_file.get_digests()\n\t\tif not digests:\n\t\t\tcontinue\n\t\tdigests_present.update(set(digests))\n\tif not digests_present.issuperset(digests_used):\n\t\tself.error('Digests not found: {}'.format(', '.join(sorted(digests_used - digests_present))))\n\t\tself.error('Digests found: {}'.format(', '.join(sorted(digests_present))))\n\t\treturn False\n\treturn True", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_missing = []\n\tdigests_present = []\n\tfor digest in digests_used:\n\t\tif digest not in digests_present:\n\t\t\tdigests_present.append(digest)\n\t\telif digest in digests_present:\n\t\t\tdigests_missing.append(digest)\n\n\tif len(digests_missing) > 0:\n\t\terror(\"Digests missing for manifest files: {0}\".format(digests_missing))\n\n\tif len(digests_present) != len(manifest_files):\n\t\terror(\"Digests present but not used for manifest files: {0}\".format(digests_present))\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_missing = [digest for digest in digests_used if digest not in self.digests.keys()]\n\tif digests_missing:\n\t\tdigests_missing = \", \".join(digests_missing)\n\t\tself.error(f\"Digests {digests_missing} are not present in the manifest, and are needed for one of the following files:\\n{manifest_files}\")", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present_and_used = {digest for digest, used in digests_used.items() if used}\n\tdigests_missing = set(digests_used.keys()) - digests_present_and_used\n\tif digests_missing:\n\t\tself.error(\"The following digests are present in the manifest but not used by any of the images: {0}\".format(\", \".join(digests_missing)))", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_list = []\n\tfor manifest_file in manifest_files:\n\t\twith open(manifest_file, 'r') as f:\n\t\t\tyaml = yaml_load(f)\n\t\t\tfor key, value in yaml.items():\n\t\t\t\tif 'digest' in value:\n\t\t\t\t\tdigests_list.append(value['digest'])\n\n\tif not set(digests_list).issubset(set(digests_used)):\n\t\treturn error(\n\t\t\t'manifest %s: digest %s is not present and used in digests %s' % (\n\t\t\t\tmanifest_file,\n\t\t\t\tset(digests_list) - set(digests_used),\n\t\t\t\tset(digests_used)\n\t\t\t)\n\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_used_in_manifest = {digest for manifest in manifest_files for digest in manifest.digests}\n\tdigests_not_used = digests_used_in_manifest - digests_used\n\tif digests_not_used:\n\t\tself.error(f'The following digests were not used in any of the manifest files: {digests_not_used}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests = {}\n\tfor manifest in manifest_files:\n\t\tdigests.update(manifest.digests)\n\tfor digest in digests:\n\t\tif digest not in digests_used:\n\t\t\treturn error(\"Digest %s is not used in any layers\" % digest)\n\t\tif digest not in digests:\n\t\t\treturn error(\"Digest %s is not present in the manifest\" % digest)\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present_and_used = self.get_digests_present_and_used(manifest_files, digests_used)\n\n\t# Check digests are present and used\n\tfor digest in digests_used:\n\t\tif digest not in digests_present_and_used:\n\t\t\tself.error(\"Digest \" + digest + \" is not present in any of the manifest files.\")\n\tdigests_present_and_used = []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests = self.manifest_get_digests(manifest_files)\n\n\tmissing_digests = self.manifest_check_digests_present(manifest_digests, digests_used)\n\n\tif missing_digests:\n\t\tself.error(\"Digest not found: {}\".format(missing_digests))\n\t\treturn False\n\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tself.log(3, \"Checking digests are present and used\")\n\tdigests_present = {}\n\tfor manifest_file in manifest_files:\n\t\tself.log(1, \"Checking digests in %s\" % manifest_file)\n\t\tdigests = self.get_digests_from_manifest(manifest_file)\n\t\tfor digest in digests:\n\t\t\tif digest not in digests_present:\n\t\t\t\tdigests_present[digest] = [manifest_file]\n\t\t\telse:\n\t\t\t\tdigests_present[digest].append(manifest_file)\n\tfor digest in digests_used:\n\t\tif digest not in digests_present:\n\t\t\tself.error(\"Digest %s not present in any manifest file\" % digest)\n\t\telif len(digests_present[digest]) > 1:\n\t\t\tself.error(\"Digest %s present in multiple manifest files (%s)\" % (digest, \", \".join(digests_present[digest])))\n\t\telse:\n\t\t\tself.log(2, \"Digest %s present in %s\" % (digest, digests_present[digest][0]))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = set()\n\tfor manifest_file in manifest_files:\n\t\tdigests = manifest_file.get_digests()\n\t\tif not digests:\n\t\t\tcontinue\n\t\tdigests_present.update(set(digests))\n\tif not digests_present.issuperset(digests_used):\n\t\tself.error('Digests not found: {}'.format(', '.join(sorted(digests_used - digests_present))))\n\t\tself.error('Digests found: {}'.format(', '.join(sorted(digests_present))))\n\t\treturn False\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_missing = []\n\tdigests_present = []\n\tfor digest in digests_used:\n\t\tif digest not in digests_present:\n\t\t\tdigests_present.append(digest)\n\t\telif digest in digests_present:\n\t\t\tdigests_missing.append(digest)\n\n\tif len(digests_missing) > 0:\n\t\terror(\"Digests missing for manifest files: {0}\".format(digests_missing))\n\n\tif len(digests_present) != len(manifest_files):\n\t\terror(\"Digests present but not used for manifest files: {0}\".format(digests_present))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tprior_data = prior.get_data_dict()\n\tcurrent_data = self.get_data_dict()\n\tif prior.get_type() != self.get_type() :\n\t\treturn error(\"Prior version is of different type.\")\n\tif prior_data.keys() != current_data.keys() :\n\t\treturn error(\"Prior version has different data.\")\n\tfor key in prior_data.keys() :\n\t\tif prior_data[key] != current_data[key] :\n\t\t\treturn error(\"Prior version has different data for key %s.\" % key)\n\treturn 0\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror = None\n\tfor i in range(len(self.items)):\n\t\tif self.items[i].name != prior.items[i].name or self.items[i].quantity < prior.items[i].quantity or self.items[i].price != prior.items[i].price:\n\t\t\terror = error + 'Inventory item ' + str(i) + ' is not a valid prior version of the inventory item in the current version.\\n'\n\treturn error", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror()\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror = None\n\t# Ensure that prior is a valid prior version of the current inventory\n\tif prior.inventory_id != self.inventory_id:\n\t\terror = \"inventory_id should be the same for both inventories\"\n\telif prior.inventory_date > self.inventory_date:\n\t\terror = \"prior inventory date should be older than the current one\"\n\telif prior.inventory_date == self.inventory_date:\n\t\terror = \"prior inventory date should be older than the current one\"\n\telif prior.inventory_source != self.inventory_source:\n\t\terror = \"inventory_source should be the same for both inventories\"\n\telif prior.inventory_version + 1 != self.inventory_version:\n\t\terror = \"prior version should be one less than the current one\"\n\telif prior.inventory_type != self.inventory_type:\n\t\terror = \"inventory_type should be the same for both inventories\"\n\n\treturn error\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror()\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\tdef validate_as_prior_version(self, prior):\n\t\tif not self.compare(prior):\n\t\t\treturn self.error(\"inventory doesn't match prior version\")\n\t\tif self.type != prior.type:\n\t\t\treturn self.error(\"inventory type doesn't match prior version\")\n\t\tif self.inventory != prior.inventory:\n\t\t\treturn self.error(\"inventory doesn't match prior version\")\n\n\t# The following checks are not part of the specification, but they are needed to\n\t# ensure that the code in the specification is always invoked.\n\tdef validate_as_prior_version(self, prior):\n\t\tif not self.is_valid():\n\t\t\treturn self.error(\"inventory is not valid\")\n\t\tif not prior.is_valid():\n\t\t\treturn self.error(\"prior inventory is not valid\")\n\t\tif self.type != prior.type:\n\t\t\treturn self.error(\"inventory type doesn't match prior version\")\n\t\tif self.inventory != prior.inventory:\n\t\t\treturn self.error(\"inventory doesn't match prior version\")\n\n\t# The following checks are not part of the specification, but they are needed to\n\t# ensure that the code in the specification is always invoked.\n\tdef validate_as"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tprior_data = prior.get_data_dict()\n\tcurrent_data = self.get_data_dict()\n\tif prior.get_type() != self.get_type() :\n\t\treturn error(\"Prior version is of different type.\")\n\tif prior_data.keys() != current_data.keys() :\n\t\treturn error(\"Prior version has different data.\")\n\tfor key in prior_data.keys() :\n\t\tif prior_data[key] != current_data[key] :\n\t\t\treturn error(\"Prior version has different data for key %s.\" % key)\n\treturn 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror = None\n\tfor i in range(len(self.items)):\n\t\tif self.items[i].name != prior.items[i].name or self.items[i].quantity < prior.items[i].quantity or self.items[i].price != prior.items[i].price:\n\t\t\terror = error + 'Inventory item ' + str(i) + ' is not a valid prior version of the inventory item in the current version.\\n'\n\treturn error", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror = None\n\t# Ensure that prior is a valid prior version of the current inventory\n\tif prior.inventory_id != self.inventory_id:\n\t\terror = \"inventory_id should be the same for both inventories\"\n\telif prior.inventory_date > self.inventory_date:\n\t\terror = \"prior inventory date should be older than the current one\"\n\telif prior.inventory_date == self.inventory_date:\n\t\terror = \"prior inventory date should be older than the current one\"\n\telif prior.inventory_source != self.inventory_source:\n\t\terror = \"inventory_source should be the same for both inventories\"\n\telif prior.inventory_version + 1 != self.inventory_version:\n\t\terror = \"prior version should be one less than the current one\"\n\telif prior.inventory_type != self.inventory_type:\n\t\terror = \"inventory_type should be the same for both inventories\"\n\n\treturn error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\terror()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\tdef validate_as_prior_version(self, prior):\n\t\tif not self.compare(prior):\n\t\t\treturn self.error(\"inventory doesn't match prior version\")\n\t\tif self.type != prior.type:\n\t\t\treturn self.error(\"inventory type doesn't match prior version\")\n\t\tif self.inventory != prior.inventory:\n\t\t\treturn self.error(\"inventory doesn't match prior version\")\n\n\t# The following checks are not part of the specification, but they are needed to\n\t# ensure that the code in the specification is always invoked.\n\tdef validate_as_prior_version(self, prior):\n\t\tif not self.is_valid():\n\t\t\treturn self.error(\"inventory is not valid\")\n\t\tif not prior.is_valid():\n\t\t\treturn self.error(\"prior inventory is not valid\")\n\t\tif self.type != prior.type:\n\t\t\treturn self.error(\"inventory type doesn't match prior version\")\n\t\tif self.inventory != prior.inventory:\n\t\t\treturn self.error(\"inventory doesn't match prior version\")\n\n\t# The following checks are not part of the specification, but they are needed to\n\t# ensure that the code in the specification is always invoked.\n\tdef validate_as", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\twith open(inventory, 'r') as f:\n\t\tlines = f.readlines()\n\t\tlines = [line.strip() for line in lines]\n\t\tfor line in lines:\n\t\t\tlogical_path_map[line] = \"{}/{}\".format(version,line)\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor key, value in inventory.items():\n\t\tlogical_path_map[key] = {\n\t\t\t\"path\": value['path'] + '/' + version,\n\t\t\t\"last_updated\": value['last_updated']\n\t\t}\n\treturn logical_path_map", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor host in inventory:\n\t\tfor state in inventory[host]:\n\t\t\tlogical_path_map[state] = inventory[host][state][version]\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path in inventory:\n\t\tlogical_path_map[logical_path] = inventory[logical_path]['path']\n\tif version:\n\t\tlogical_path_map['version'] = version\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path, path_map in inventory.items():\n\t\tif version in path_map:\n\t\t\tlogical_path_map[logical_path] = path_map[version]\n\t\telse:\n\t\t\traise Exception('No state found for version: %s' % version)\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor module in inventory:\n\t\tfor file_path in inventory[module]:\n\t\t\tpath_map[file_path] = version\n\treturn path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state in inventory.get_states(version):\n\t\tlogical_path_map[state] = inventory.get_state_file_path(state, version)\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path in inventory:\n\t\tlogical_path_map[logical_path] = inventory[logical_path][version]\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path, file_path in inventory[version]['file_path'].items():\n\t\tlogical_path_map[logical_path] = file_path\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor file_path, file_name in inventory[f'{version}']['files'].items():\n\t\tfor state_name, state in file_name.items():\n\t\t\tlogical_path_map[state_name] = file_path\n\treturn logical_path_map\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\twith open(inventory, 'r') as f:\n\t\tlines = f.readlines()\n\t\tlines = [line.strip() for line in lines]\n\t\tfor line in lines:\n\t\t\tlogical_path_map[line] = \"{}/{}\".format(version,line)\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor key, value in inventory.items():\n\t\tlogical_path_map[key] = {\n\t\t\t\"path\": value['path'] + '/' + version,\n\t\t\t\"last_updated\": value['last_updated']\n\t\t}\n\treturn logical_path_map", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor host in inventory:\n\t\tfor state in inventory[host]:\n\t\t\tlogical_path_map[state] = inventory[host][state][version]\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path in inventory:\n\t\tlogical_path_map[logical_path] = inventory[logical_path]['path']\n\tif version:\n\t\tlogical_path_map['version'] = version\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path, path_map in inventory.items():\n\t\tif version in path_map:\n\t\t\tlogical_path_map[logical_path] = path_map[version]\n\t\telse:\n\t\t\traise Exception('No state found for version: %s' % version)\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor module in inventory:\n\t\tfor file_path in inventory[module]:\n\t\t\tpath_map[file_path] = version\n\treturn path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state in inventory.get_states(version):\n\t\tlogical_path_map[state] = inventory.get_state_file_path(state, version)\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path in inventory:\n\t\tlogical_path_map[logical_path] = inventory[logical_path][version]\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor logical_path, file_path in inventory[version]['file_path'].items():\n\t\tlogical_path_map[logical_path] = file_path\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor file_path, file_name in inventory[f'{version}']['files'].items():\n\t\tfor state_name, state in file_name.items():\n\t\t\tlogical_path_map[state_name] = file_path\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tfixity_file_list = []\n\tfor fixity_file in fixity:\n\t\tfixity_file_list.append(fixity_file['path'])\n\t\tself.validate_file_structure(fixity_file)\n\t\tself.validate_file_exists(fixity_file['path'], manifest_files)\n\t\n\tif len(fixity_file_list) != len(set(fixity_file_list)):\n\t\treturn self.error('The same file is listed more than once in the fixity block.')\n\t\n\treturn None", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tcheck_fixity_structure(fixity, manifest_files)\n\treturn validate_fixity_block(fixity, manifest_files)\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\tif fixity is None:\n\t\t\tself.error(\"Inventory does not contain any fixity block.\")\n\t\t\treturn False\n\t\tif not isinstance(fixity, list):\n\t\t\tself.error(\"Fixity block is not a list.\")\n\t\t\treturn False\n\t\tfor fixity_block in fixity:\n\t\t\tif not isinstance(fixity_block, dict):\n\t\t\t\tself.error(\"Fixity block is not a dictionary.\")\n\t\t\t\treturn False\n\t\t\tif \"file\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain a file key.\")\n\t\t\t\treturn False\n\t\t\tif \"md5\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain an md5 key.\")\n\t\t\t\treturn False\n\t\t\tif \"sha1\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain a sha1 key.\")\n\t\t\t\treturn False\n\t\t\tif \"sha256\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain a sha256 key.\")\n\t\t\t\treturn False\n\t\t\tif fixity_block[\"file\"] not in manifest_files:\n\t\t\t\tself.error(\"Fixity block references file that is not listed in manifest.\")\n\t\t\t\treturn False\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\t#Check that fixity block has one element\n\t\tassert(len(fixity) == 1)\n\t\t#Check that the fixity block is named 'Fixity'\n\t\tassert(fixity[0].tag == \"{http://www.openarchives.org/OAI/2.0/}fixity\")\n\texcept AssertionError:\n\t\treturn error(\"FAIL: Fixity block is improperly structured\")\n\n\tfor file_fixity in fixity[0]:\n\t\t#Check that a checksum is defined for each file\n\t\tassert(file_fixity.find('{http://www.openarchives.org/OAI/2.0/}checksum') is not None)\n\t\t#Check that a checksum algorithm is defined for each file\n\t\tassert(file_fixity.find('{http://www.openarchives.org/OAI/2.0/}algorithm') is not None)\n\t\t#Check that the file listed in the manifest is referenced\n\t\tassert(file_fixity.get('identifier') in manifest_files)", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tmanifest_files = set(manifest_files)\n\t\n\tfor file, fixity_value in fixity.items():\n\t\tif file not in manifest_files:\n\t\t\treturn error('fixity block contains file not listed in manifest', file)\n\t\tif not fixity_value.startswith('sha256:'):\n\t\t\treturn error('fixity value is not sha256', file)\n\t\tif len(fixity_value) != 65:\n\t\t\treturn error('fixity value does not have expected length of 65', file)\n\t\n\treturn None", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.log.info(\"Validating fixity block in inventory\")\n\n\tif not isinstance(fixity, dict):\n\t\treturn self.error(\"fixity block must be a dictionary\")\n\n\tfixity_keys = list(fixity.keys())\n\tif \"files\" not in fixity_keys:\n\t\treturn self.error(\"fixity block must specify files\")\n\n\tif \"checksums\" not in fixity_keys:\n\t\treturn self.error(\"fixity block must specify checksums\")\n\n\tif not isinstance(fixity[\"files\"], list) or not isinstance(fixity[\"checksums\"], list):\n\t\treturn self.error(\"fixity block files and checksums must be lists\")\n\n\tif len(fixity[\"files\"]) != len(fixity[\"checksums\"]):\n\t\treturn self.error(\"fixity block files and checksums must have the same length\")\n\n\tfor index, file in enumerate(fixity[\"files\"]):\n\t\tif file not in manifest_files:\n\t\t\treturn self.error(f\"{file} listed in fixity block but not in manifest\")\n\t\tif not isinstance(fixity[\"checksums\"][index], str):\n\t\t\treturn self.error(f\"fixity block checksum for {file} must be string\")\n\n\treturn True", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tpass", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.log.info(\"Validate fixity block in inventory\")\n\tmanifest_files = [f.strip() for f in manifest_files]\n\tfor k, v in fixity.items():\n\t\tif k not in manifest_files:\n\t\t\treturn self.error(\"Fixity block references file that is not listed in manifest: %s\" % k)\n\n\t\tif len(v) > 1:\n\t\t\treturn self.error(\"Multiple fixity values for single file: %s\" % k)\n\n\t\tif len(v[0]) != 64:\n\t\t\treturn self.error(\"Invalid fixity value for %s: %s\" % (k, v[0]))\n\t\n\treturn self.success()", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.log.info('Validating fixity block')\n\tfixity_block_keys = fixity.keys()\n\t\n\tcheck_keys = ['algorithm', 'reference', 'value']\n\t\n\tif sorted(check_keys) != sorted(fixity_block_keys):\n\t\tself.log.error('Fixity block is missing keys.')\n\t\treturn error()\n\t\n\tvalues = fixity.values()\n\treference = values[1]\n\t\n\tfixity_files = reference.keys()\n\t\n\treference_files = [file for file in fixity_files if file in manifest_files]\n\t\n\tif reference_files != fixity_files:\n\t\tself.log.error('Fixity block contains files that are not included in the manifest.')\n\t\treturn error()\n\t\n\treturn True\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tvalid = True\n\tif not fixity:\n\t\tself.error(\"No fixity block found\")\n\t\tvalid = False\n\telif not isinstance(fixity, dict):\n\t\tself.error(\"Fixity block is not a dictionary\")\n\t\tvalid = False\n\telse:\n\t\t# Validate the fixity dictionary structure\n\t\tfixity_structure = {\"md5\": {}}\n\t\tfor key, value in fixity.items():\n\t\t\tif key not in fixity_structure:\n\t\t\t\tself.error(f\"{key} is an invalid fixity key\")\n\t\t\t\tvalid = False\n\t\t\telif isinstance(value, dict):\n\t\t\t\tif not all(file_name in manifest_files for file_name in value):\n\t\t\t\t\tself.error(f\"{value} is not included in the manifest\")\n\t\t\t\t\tvalid = False\n\t\t\telse:\n\t\t\t\tself.error(f\"{value} is not a dictionary\")\n\t\t\t\tvalid = False\n\n\treturn valid\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tfixity_file_list = []\n\tfor fixity_file in fixity:\n\t\tfixity_file_list.append(fixity_file['path'])\n\t\tself.validate_file_structure(fixity_file)\n\t\tself.validate_file_exists(fixity_file['path'], manifest_files)\n\t\n\tif len(fixity_file_list) != len(set(fixity_file_list)):\n\t\treturn self.error('The same file is listed more than once in the fixity block.')\n\t\n\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tcheck_fixity_structure(fixity, manifest_files)\n\treturn validate_fixity_block(fixity, manifest_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\tif fixity is None:\n\t\t\tself.error(\"Inventory does not contain any fixity block.\")\n\t\t\treturn False\n\t\tif not isinstance(fixity, list):\n\t\t\tself.error(\"Fixity block is not a list.\")\n\t\t\treturn False\n\t\tfor fixity_block in fixity:\n\t\t\tif not isinstance(fixity_block, dict):\n\t\t\t\tself.error(\"Fixity block is not a dictionary.\")\n\t\t\t\treturn False\n\t\t\tif \"file\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain a file key.\")\n\t\t\t\treturn False\n\t\t\tif \"md5\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain an md5 key.\")\n\t\t\t\treturn False\n\t\t\tif \"sha1\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain a sha1 key.\")\n\t\t\t\treturn False\n\t\t\tif \"sha256\" not in fixity_block.keys():\n\t\t\t\tself.error(\"Fixity block does not contain a sha256 key.\")\n\t\t\t\treturn False\n\t\t\tif fixity_block[\"file\"] not in manifest_files:\n\t\t\t\tself.error(\"Fixity block references file that is not listed in manifest.\")\n\t\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\t#Check that fixity block has one element\n\t\tassert(len(fixity) == 1)\n\t\t#Check that the fixity block is named 'Fixity'\n\t\tassert(fixity[0].tag == \"{http://www.openarchives.org/OAI/2.0/}fixity\")\n\texcept AssertionError:\n\t\treturn error(\"FAIL: Fixity block is improperly structured\")\n\n\tfor file_fixity in fixity[0]:\n\t\t#Check that a checksum is defined for each file\n\t\tassert(file_fixity.find('{http://www.openarchives.org/OAI/2.0/}checksum') is not None)\n\t\t#Check that a checksum algorithm is defined for each file\n\t\tassert(file_fixity.find('{http://www.openarchives.org/OAI/2.0/}algorithm') is not None)\n\t\t#Check that the file listed in the manifest is referenced\n\t\tassert(file_fixity.get('identifier') in manifest_files)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tmanifest_files = set(manifest_files)\n\t\n\tfor file, fixity_value in fixity.items():\n\t\tif file not in manifest_files:\n\t\t\treturn error('fixity block contains file not listed in manifest', file)\n\t\tif not fixity_value.startswith('sha256:'):\n\t\t\treturn error('fixity value is not sha256', file)\n\t\tif len(fixity_value) != 65:\n\t\t\treturn error('fixity value does not have expected length of 65', file)\n\t\n\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.log.info(\"Validating fixity block in inventory\")\n\n\tif not isinstance(fixity, dict):\n\t\treturn self.error(\"fixity block must be a dictionary\")\n\n\tfixity_keys = list(fixity.keys())\n\tif \"files\" not in fixity_keys:\n\t\treturn self.error(\"fixity block must specify files\")\n\n\tif \"checksums\" not in fixity_keys:\n\t\treturn self.error(\"fixity block must specify checksums\")\n\n\tif not isinstance(fixity[\"files\"], list) or not isinstance(fixity[\"checksums\"], list):\n\t\treturn self.error(\"fixity block files and checksums must be lists\")\n\n\tif len(fixity[\"files\"]) != len(fixity[\"checksums\"]):\n\t\treturn self.error(\"fixity block files and checksums must have the same length\")\n\n\tfor index, file in enumerate(fixity[\"files\"]):\n\t\tif file not in manifest_files:\n\t\t\treturn self.error(f\"{file} listed in fixity block but not in manifest\")\n\t\tif not isinstance(fixity[\"checksums\"][index], str):\n\t\t\treturn self.error(f\"fixity block checksum for {file} must be string\")\n\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.log.info(\"Validate fixity block in inventory\")\n\tmanifest_files = [f.strip() for f in manifest_files]\n\tfor k, v in fixity.items():\n\t\tif k not in manifest_files:\n\t\t\treturn self.error(\"Fixity block references file that is not listed in manifest: %s\" % k)\n\n\t\tif len(v) > 1:\n\t\t\treturn self.error(\"Multiple fixity values for single file: %s\" % k)\n\n\t\tif len(v[0]) != 64:\n\t\t\treturn self.error(\"Invalid fixity value for %s: %s\" % (k, v[0]))\n\t\n\treturn self.success()", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.log.info('Validating fixity block')\n\tfixity_block_keys = fixity.keys()\n\t\n\tcheck_keys = ['algorithm', 'reference', 'value']\n\t\n\tif sorted(check_keys) != sorted(fixity_block_keys):\n\t\tself.log.error('Fixity block is missing keys.')\n\t\treturn error()\n\t\n\tvalues = fixity.values()\n\treference = values[1]\n\t\n\tfixity_files = reference.keys()\n\t\n\treference_files = [file for file in fixity_files if file in manifest_files]\n\t\n\tif reference_files != fixity_files:\n\t\tself.log.error('Fixity block contains files that are not included in the manifest.')\n\t\treturn error()\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tvalid = True\n\tif not fixity:\n\t\tself.error(\"No fixity block found\")\n\t\tvalid = False\n\telif not isinstance(fixity, dict):\n\t\tself.error(\"Fixity block is not a dictionary\")\n\t\tvalid = False\n\telse:\n\t\t# Validate the fixity dictionary structure\n\t\tfixity_structure = {\"md5\": {}}\n\t\tfor key, value in fixity.items():\n\t\t\tif key not in fixity_structure:\n\t\t\t\tself.error(f\"{key} is an invalid fixity key\")\n\t\t\t\tvalid = False\n\t\t\telif isinstance(value, dict):\n\t\t\t\tif not all(file_name in manifest_files for file_name in value):\n\t\t\t\t\tself.error(f\"{value} is not included in the manifest\")\n\t\t\t\t\tvalid = False\n\t\t\telse:\n\t\t\t\tself.error(f\"{value} is not a dictionary\")\n\t\t\t\tvalid = False\n\n\treturn valid\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "_id": "62b463153879012d19481498", "code": ["def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor current_file in os.listdir(path):\n\t\tfiles.append(current_file)\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn os.listdir(path)\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\n\tfor (dirpath, dirnames, filenames) in os.walk(path):\n\t\tfiles.extend(filenames)\n\t\tbreak\n\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\timport re\n\tfiles = os.listdir(path)\n\tfiles = [f for f in files if re.search(r'\\.md$', f)]\n\tfiles.sort()\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\n\tdir_list = os.listdir(path)\n\tfiles_list = [os.path.join(path, file) for file in dir_list if os.path.isfile(os.path.join(path, file))]\n\n\treturn files_list", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = os.listdir(path)\n\treturn files", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\n\ttry:\n\t\treturn os.listdir(path)\n\texcept OSError as e:\n\t\tprint(e)\n\n\treturn []", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor current_file in os.listdir(path):\n\t\tfiles.append(current_file)\n\treturn files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn os.listdir(path)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\n\tfor (dirpath, dirnames, filenames) in os.walk(path):\n\t\tfiles.extend(filenames)\n\t\tbreak\n\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\timport re\n\tfiles = os.listdir(path)\n\tfiles = [f for f in files if re.search(r'\\.md$', f)]\n\tfiles.sort()\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\n\tdir_list = os.listdir(path)\n\tfiles_list = [os.path.join(path, file) for file in dir_list if os.path.isfile(os.path.join(path, file))]\n\n\treturn files_list", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = os.listdir(path)\n\treturn files", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\n\ttry:\n\t\treturn os.listdir(path)\n\texcept OSError as e:\n\t\tprint(e)\n\n\treturn []", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tdata[xml.name] = []\n\tfor file in files:\n\t\tdata[file.xml.name].append(file)\n\treturn data\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tfiles_by_xml = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tif xml in file:\n\t\t\t\tfiles_by_xml[xml] = files_by_xml.get(xml, []) + [os.path.join(source, file)]\n\treturn files_by_xml\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor file in files:\n\t\tfilename = file.split('/')[-1]\n\t\tif filename in xmls:\n\t\t\txml = xmls[filename]\n\t\t\tkey = xml.split('.')[0]\n\t\t\tif key not in data:\n\t\t\t\tdata[key] = []\n\t\t\tdata[key].append(file)\n\treturn data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txml_dict = {}\n\tfile_dict = {}\n\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tfilename = os.path.basename(file)\n\t\t\tif filename.startswith(os.path.splitext(os.path.basename(xml))[0]):\n\t\t\t\txml_dict.setdefault(xml, []).append(file)\n\n\tfor xml, xml_files in xml_dict.items():\n\t\tfor xml_file in xml_files:\n\t\t\tfilename = os.path.basename(xml_file)\n\t\t\tfile_dict.setdefault(filename, []).append(xml_file)\n\n\treturn xml_dict, file_dict\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txml_files = dict()\n\tfor xml in xmls:\n\t\txml_files[xml] = list()\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif file.startswith(xml):\n\t\t\t\txml_files[xml].append(file)\n\treturn xml_files\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor xml_filename in xmls:\n\t\tresult[xml_filename] = []\n\n\tfor file in files:\n\t\txml_filename = file['xml_filename']\n\t\tresult[xml_filename].append(file)\n\n\treturn result\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tif xml in file:\n\t\t\t\tdata[xml] = file\n\treturn data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgroups = {}\n\tfor f in files:\n\t\txml_filename = source.get_xml_filename(f)\n\t\tif xml_filename not in xmls:\n\t\t\txmls[xml_filename] = [f]\n\t\telse:\n\t\t\txmls[xml_filename].append(f)\n\tgroups[\"xmls\"] = xmls\n\tgroups[\"files\"] = files\n\treturn groups\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgrouped = {}\n\tfor file in files:\n\t\tfile_name = file.split(\"\\\\\")[-1]\n\t\tgrouped[file_name] = []\n\t\tfor xml in xmls:\n\t\t\tif file_name.startswith(xml):\n\t\t\t\tgrouped[xml].append(file)\n\treturn grouped\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\n\tfor xml in xmls:\n\t\txml_filename = os.path.basename(xml)\n\t\txml_group_name = xml_filename.split(\".\")[0]\n\t\tresult[xml_group_name] = []\n\n\t\tif xml_group_name not in files:\n\t\t\tcontinue\n\n\t\tfor file in files[xml_group_name]:\n\t\t\tresult[xml_group_name].append(file)\n\n\treturn result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tdata[xml.name] = []\n\tfor file in files:\n\t\tdata[file.xml.name].append(file)\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tfiles_by_xml = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tif xml in file:\n\t\t\t\tfiles_by_xml[xml] = files_by_xml.get(xml, []) + [os.path.join(source, file)]\n\treturn files_by_xml\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor file in files:\n\t\tfilename = file.split('/')[-1]\n\t\tif filename in xmls:\n\t\t\txml = xmls[filename]\n\t\t\tkey = xml.split('.')[0]\n\t\t\tif key not in data:\n\t\t\t\tdata[key] = []\n\t\t\tdata[key].append(file)\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txml_dict = {}\n\tfile_dict = {}\n\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tfilename = os.path.basename(file)\n\t\t\tif filename.startswith(os.path.splitext(os.path.basename(xml))[0]):\n\t\t\t\txml_dict.setdefault(xml, []).append(file)\n\n\tfor xml, xml_files in xml_dict.items():\n\t\tfor xml_file in xml_files:\n\t\t\tfilename = os.path.basename(xml_file)\n\t\t\tfile_dict.setdefault(filename, []).append(xml_file)\n\n\treturn xml_dict, file_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txml_files = dict()\n\tfor xml in xmls:\n\t\txml_files[xml] = list()\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif file.startswith(xml):\n\t\t\t\txml_files[xml].append(file)\n\treturn xml_files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor xml_filename in xmls:\n\t\tresult[xml_filename] = []\n\n\tfor file in files:\n\t\txml_filename = file['xml_filename']\n\t\tresult[xml_filename].append(file)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tif xml in file:\n\t\t\t\tdata[xml] = file\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgroups = {}\n\tfor f in files:\n\t\txml_filename = source.get_xml_filename(f)\n\t\tif xml_filename not in xmls:\n\t\t\txmls[xml_filename] = [f]\n\t\telse:\n\t\t\txmls[xml_filename].append(f)\n\tgroups[\"xmls\"] = xmls\n\tgroups[\"files\"] = files\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgrouped = {}\n\tfor file in files:\n\t\tfile_name = file.split(\"\\\\\")[-1]\n\t\tgrouped[file_name] = []\n\t\tfor xml in xmls:\n\t\t\tif file_name.startswith(xml):\n\t\t\t\tgrouped[xml].append(file)\n\treturn grouped\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\n\tfor xml in xmls:\n\t\txml_filename = os.path.basename(xml)\n\t\txml_group_name = xml_filename.split(\".\")[0]\n\t\tresult[xml_group_name] = []\n\n\t\tif xml_group_name not in files:\n\t\t\tcontinue\n\n\t\tfor file in files[xml_group_name]:\n\t\t\tresult[xml_group_name].append(file)\n\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tpass\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_basename = os.path.basename(file_path)\n\tmatches = [file_basename.startswith(prefix + \"-\"), file_basename.startswith(prefix + \".\")]\n\treturn any(matches)\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_path = os.path.basename(file_path)\n\treturn file_path.startswith(prefix+\"-\") or file_path.startswith(prefix+\".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\timport os\n\n\tbase_name = os.path.basename(file_path)\n\tif base_name.startswith(prefix + '-') or base_name.startswith(prefix + '.'):\n\t\treturn True\n\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfrom os.path import basename\n\tbasename = basename(file_path)\n\tprefix = prefix.lower()\n\tprefix_dash = prefix + \"-\"\n\tprefix_dot = prefix + \".\"\n\treturn basename.lower().startswith(prefix_dash) or basename.lower().startswith(prefix_dot)", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\timport os\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tpath = file_path.split(\"/\")[-1]\n\treturn path.startswith(prefix) or path.startswith(prefix + \"-\") or path.startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_basename = os.path.basename(file_path)\n\n\tif file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\tbasename = os.path.basename(file_path)\n\t\n\tif basename.startswith(prefix + \"-\"):\n\t\treturn True\n\t\n\tif basename.startswith(prefix + \".\"):\n\t\treturn True\n\t\n\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\timport os\n\tbasename = os.path.basename(file_path)\n\tprefix_dot = prefix + \".\"\n\tprefix_dash = prefix + \"-\"\n\treturn basename.startswith(prefix_dot) or basename.startswith(prefix_dash)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_basename = os.path.basename(file_path)\n\tmatches = [file_basename.startswith(prefix + \"-\"), file_basename.startswith(prefix + \".\")]\n\treturn any(matches)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_path = os.path.basename(file_path)\n\treturn file_path.startswith(prefix+\"-\") or file_path.startswith(prefix+\".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\timport os\n\n\tbase_name = os.path.basename(file_path)\n\tif base_name.startswith(prefix + '-') or base_name.startswith(prefix + '.'):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfrom os.path import basename\n\tbasename = basename(file_path)\n\tprefix = prefix.lower()\n\tprefix_dash = prefix + \"-\"\n\tprefix_dot = prefix + \".\"\n\treturn basename.lower().startswith(prefix_dash) or basename.lower().startswith(prefix_dot)", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\timport os\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tpath = file_path.split(\"/\")[-1]\n\treturn path.startswith(prefix) or path.startswith(prefix + \"-\") or path.startswith(prefix + \".\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_basename = os.path.basename(file_path)\n\n\tif file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\tbasename = os.path.basename(file_path)\n\t\n\tif basename.startswith(prefix + \"-\"):\n\t\treturn True\n\t\n\tif basename.startswith(prefix + \".\"):\n\t\treturn True\n\t\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\timport os\n\tbasename = os.path.basename(file_path)\n\tprefix_dot = prefix + \".\"\n\tprefix_dash = prefix + \"-\"\n\treturn basename.startswith(prefix_dot) or basename.startswith(prefix_dash)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatching_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatching_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\t\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\t\n\treturn filenames", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif prefix in file:\n\t\t\tfilenames.append(file)\n\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfiltered = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfiltered.append(f)\n\treturn filtered\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f[:len(prefix)] == prefix:\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfiltered_files = []\n\tfor file_name in files:\n\t\tif file_name.startswith(prefix):\n\t\t\tfiltered_files.append(file_name)\n\treturn filtered_files\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatching_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatching_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\t\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\t\n\treturn filenames", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif prefix in file:\n\t\t\tfilenames.append(file)\n\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfiltered = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfiltered.append(f)\n\treturn filtered\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f[:len(prefix)] == prefix:\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfiltered_files = []\n\tfor file_name in files:\n\t\tif file_name.startswith(prefix):\n\t\t\tfiltered_files.append(file_name)\n\treturn filtered_files\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport os\n\timport re\n\n\tfiles = os.listdir(folder)\n\tfiles = [f for f in files if f.endswith(\".xml\")]\n\tfiles = sorted(files)\n\n\tgrouped_files = _group_files_by_xml_filename(files)\n\n\treturn grouped_files\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroups = {}\n\tfor file in folder.files:\n\t\tfile_group = _group_files_by_xml_filename(file)\n\t\tif file_group not in groups:\n\t\t\tgroups[file_group] = [file]\n\t\telse:\n\t\t\tgroups[file_group].append(file)\n\n\treturn groups\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfolders = []\n\tfor entry in folder.get_entries():\n\t\tif entry.is_file():\n\t\t\tfolders.append(entry)\n\t\telse:\n\t\t\tfolders.extend(_explore_folder(entry))\n\treturn folders\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = os.listdir(folder)\n\tfiles.sort()\n\tgrouped_files = []\n\tfor f in files:\n\t\tif f.endswith(\".xml\"):\n\t\t\tgrouped_files.append(_group_files_by_xml_filename(f, folder))\n\treturn grouped_files\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint(folder)\n\n\t# Get the file list\n\tfile_list = os.listdir(folder)\n\n\t# Remove all hidden files\n\tfile_list = [f for f in file_list if not f.startswith('.')]\n\n\t# Get all grouped files\n\tgrouped_files = _group_files_by_xml_filename(file_list)\n\n\t# Explore each group\n\tfor group in grouped_files:\n\t\tgroup_path = os.path.join(folder, group)\n\t\t_explore_group(group_path)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = folder.list_files()\n\tgrouped_files = _group_files_by_xml_filename(files)\n\n\treturn [(key, grouped_files[key]) for key in grouped_files.keys()]\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport os, re\n\n\tprint('Exploring folder: ' + folder)\n\n\txml_files = []\n\tpdf_files = []\n\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor file in files:\n\t\t\tif file.endswith('.xml'):\n\t\t\t\txml_files.append(os.path.join(root, file))\n\t\t\tif file.endswith('.pdf'):\n\t\t\t\tpdf_files.append(os.path.join(root, file))\n\n\txml_files.sort()\n\tpdf_files.sort()\n\n\tgrouped_files = _group_files_by_xml_filename(xml_files, pdf_files)\n\n\tprint('Grouped files: ' + str(grouped_files))\n\n\tfor group in grouped_files:\n\t\tgroup_name = re.search('(?<=\\\\\\\\)(.*?)(?=\\\\\\\\)', group[0]).group(0)\n\t\tgroup_folder = os.path.join(folder, group_name)\n\t\t_make_folder(group_folder)\n\t\tprint('Making folder: ' + group_folder)\n\t\tfor filename in group:\n\t\t\t_move_file(filename, group_folder)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint('Exploring folder: ' + folder)\n\tgroup_by_filename, group_by_date = _group_files_by_xml_filename(folder)\n\tprint('Found ' + str(len(group_by_filename)) + ' files.')\n\tprint('Found ' + str(len(group_by_date)) + ' different dates.')\n\n\tprint('Exploring group by filename...')\n\tfor group_name, group in group_by_filename.items():\n\t\t_explore_group(group)\n\n\tprint('Exploring group by date...')\n\tfor group_name, group in group_by_date.items():\n\t\t_explore_group(group)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfile_groups = {}\n\tfor path, dirs, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tabs_path = path + os.sep + filename\n\t\t\txml_filename = _get_xml_filename(abs_path)\n\t\t\tif xml_filename:\n\t\t\t\tif xml_filename not in file_groups:\n\t\t\t\t\tfile_groups[xml_filename] = []\n\t\t\t\tfile_groups[xml_filename].append(abs_path)\n\treturn file_groups\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfile_groups = {}\n\tfor file in folder.files:\n\t\tfiles_in_group = _group_files_by_xml_filename(file)\n\t\tfile_groups.update(files_in_group)\n\treturn file_groups\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport os\n\timport re\n\n\tfiles = os.listdir(folder)\n\tfiles = [f for f in files if f.endswith(\".xml\")]\n\tfiles = sorted(files)\n\n\tgrouped_files = _group_files_by_xml_filename(files)\n\n\treturn grouped_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroups = {}\n\tfor file in folder.files:\n\t\tfile_group = _group_files_by_xml_filename(file)\n\t\tif file_group not in groups:\n\t\t\tgroups[file_group] = [file]\n\t\telse:\n\t\t\tgroups[file_group].append(file)\n\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfolders = []\n\tfor entry in folder.get_entries():\n\t\tif entry.is_file():\n\t\t\tfolders.append(entry)\n\t\telse:\n\t\t\tfolders.extend(_explore_folder(entry))\n\treturn folders\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = os.listdir(folder)\n\tfiles.sort()\n\tgrouped_files = []\n\tfor f in files:\n\t\tif f.endswith(\".xml\"):\n\t\t\tgrouped_files.append(_group_files_by_xml_filename(f, folder))\n\treturn grouped_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint(folder)\n\n\t# Get the file list\n\tfile_list = os.listdir(folder)\n\n\t# Remove all hidden files\n\tfile_list = [f for f in file_list if not f.startswith('.')]\n\n\t# Get all grouped files\n\tgrouped_files = _group_files_by_xml_filename(file_list)\n\n\t# Explore each group\n\tfor group in grouped_files:\n\t\tgroup_path = os.path.join(folder, group)\n\t\t_explore_group(group_path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = folder.list_files()\n\tgrouped_files = _group_files_by_xml_filename(files)\n\n\treturn [(key, grouped_files[key]) for key in grouped_files.keys()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport os, re\n\n\tprint('Exploring folder: ' + folder)\n\n\txml_files = []\n\tpdf_files = []\n\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor file in files:\n\t\t\tif file.endswith('.xml'):\n\t\t\t\txml_files.append(os.path.join(root, file))\n\t\t\tif file.endswith('.pdf'):\n\t\t\t\tpdf_files.append(os.path.join(root, file))\n\n\txml_files.sort()\n\tpdf_files.sort()\n\n\tgrouped_files = _group_files_by_xml_filename(xml_files, pdf_files)\n\n\tprint('Grouped files: ' + str(grouped_files))\n\n\tfor group in grouped_files:\n\t\tgroup_name = re.search('(?<=\\\\\\\\)(.*?)(?=\\\\\\\\)', group[0]).group(0)\n\t\tgroup_folder = os.path.join(folder, group_name)\n\t\t_make_folder(group_folder)\n\t\tprint('Making folder: ' + group_folder)\n\t\tfor filename in group:\n\t\t\t_move_file(filename, group_folder)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint('Exploring folder: ' + folder)\n\tgroup_by_filename, group_by_date = _group_files_by_xml_filename(folder)\n\tprint('Found ' + str(len(group_by_filename)) + ' files.')\n\tprint('Found ' + str(len(group_by_date)) + ' different dates.')\n\n\tprint('Exploring group by filename...')\n\tfor group_name, group in group_by_filename.items():\n\t\t_explore_group(group)\n\n\tprint('Exploring group by date...')\n\tfor group_name, group in group_by_date.items():\n\t\t_explore_group(group)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfile_groups = {}\n\tfor path, dirs, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tabs_path = path + os.sep + filename\n\t\t\txml_filename = _get_xml_filename(abs_path)\n\t\t\tif xml_filename:\n\t\t\t\tif xml_filename not in file_groups:\n\t\t\t\t\tfile_groups[xml_filename] = []\n\t\t\t\tfile_groups[xml_filename].append(abs_path)\n\treturn file_groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfile_groups = {}\n\tfor file in folder.files:\n\t\tfiles_in_group = _group_files_by_xml_filename(file)\n\t\tfile_groups.update(files_in_group)\n\treturn file_groups\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\tfrom datetime import datetime\n\tfrom xml.etree import ElementTree as ET\n\tfrom .pdf_reader import PDFReader\n\tfrom ..util import get_file_list\n\n\tfile_name = os.path.basename(file_path)\n\tfile_name, file_ext = os.path.splitext(file_name)\n\tfile_name = file_name.lower()\n\tfile_prefix = file_name[:len(prefix)]\n\n\tif file_prefix != prefix:\n\t\treturn None\n\n\tfile_id = file_name[len(prefix):]\n\n\tif file_ext == \".pdf\":\n\t\treturn dict(component_id=file_id, file_path=file_path)\n\n\tif file_ext in [\".xml\", \".txt\"]:\n\t\ttry:\n\t\t\troot = ET.parse(file_path).getroot()\n\t\t\tftype = root.attrib[\"ftype\"]\n\t\texcept:\n\t\t\tftype = None\n\n\t\treturn dict(component_id=file_id, file_path=file_path, ftype=ftype, file_path=file_path)\n\n\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\t\n\tftype = None\n\t\n\tcomponent_id = os.path.basename(file_path).split('.')[0]\n\n\tif prefix.lower() == \"pdf\":\n\t\tftype = prefix\n\telse:\n\t\tftype = os.path.basename(file_path).split('.')[1]\n\n\tif ftype == \"xml\":\n\t\treturn None\n\telif ftype == \"pdf\":\n\t\treturn {component_id: file_path}\n\telse:\n\t\treturn {component_id: [file_path, ftype, file_path]}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport re\n\timport zipfile\n\n\tftype = file_path.split(\".\")\n\tftype = ftype[len(ftype) - 1]\n\n\tif ftype == \"pdf\":\n\t\treturn {\n\t\t\t\"component_id\": prefix,\n\t\t\t\"file_path\": file_path\n\t\t}\n\n\tif ftype != \"xml\":\n\t\treturn None\n\n\txml_file = open(file_path, \"r\")\n\tlines = xml_file.readlines()\n\txml_file.close()\n\n\tlines = [line.strip() for line in lines]\n\n\tmatch = re.search(r\"(<component_id>)(.*)(</component_id>)\", lines[0])\n\n\tif match:\n\t\treturn {\n\t\t\t\"component_id\": match.group(2),\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": ftype,\n\t\t\t\"file_path\": file_path\n\t\t}\n\n\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\tfrom xml.etree import ElementTree\n\n\tftype = None\n\tbasename = os.path.basename(file_path)\n\tif basename.startswith(prefix):\n\t\tcomponent_id = basename[len(prefix):]\n\t\tif not component_id.endswith('.xml'):\n\t\t\tcomponent_id = component_id.replace('_', '-')\n\t\t\txml_path = os.path.join(os.path.dirname(file_path), component_id + '.xml')\n\t\t\tif os.path.exists(xml_path):\n\t\t\t\tftype = 'pdf'\n\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport re\n\n\tresult = None\n\tftype = os.path.basename(file_path).split(\".\")[-1]\n\tif ftype.lower() == \"pdf\":\n\t\tresult = {\n\t\t\t\"component_id\": prefix,\n\t\t\t\"file_path\": file_path,\n\t\t}\n\telse:\n\t\tresult = {\n\t\t\t\"component_id\": prefix,\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": ftype,\n\t\t\t\"file_path\": file_path,\n\t\t}\n\treturn result\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfrom os import path\n\tfile_name = path.basename(file_path)\n\tif not file_name.startswith(prefix):\n\t\treturn None\n\ttry:\n\t\timport xml.etree.ElementTree as ET\n\t\ttree = ET.parse(file_path)\n\t\troot = tree.getroot()\n\t\tif root.tag != \"pdf\":\n\t\t\treturn None\n\texcept:\n\t\treturn {\"component_id\": file_name.split(\"_\")[0], \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": file_name.split(\"_\")[0], \"file_path\": file_path, \"ftype\": root.attrib[\"ftype\"], \"title\": root.attrib[\"title\"]}\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_path = file_path.lower()\n\tcomponents = file_path.split('/')\n\tif components[0] != prefix:\n\t\treturn None\n\tfile_name = components[-1]\n\tif file_name[-4:] == '.pdf':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:])}\n\telif file_name[-3:] == '.md':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:]), 'ftype': 'markdown'}\n\telif file_name[-3:] == '.ht':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:]), 'ftype': 'html'}\n\telif file_name[-3:] == '.rt':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:]), 'ftype': 'rst'}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfrom pathlib import Path\n\timport logging\n\tlogger = logging.getLogger()\n\n\tfile_path = Path(file_path)\n\tstem = file_path.stem\n\tsuffix = file_path.suffix\n\n\tftype = None\n\tif stem.startswith(prefix) and suffix == \".pdf\":\n\t\tcomponent_id = stem[len(prefix):]\n\t\treturn component_id\n\telif stem.startswith(prefix) and suffix != \".pdf\":\n\t\tcomponent_id = stem[len(prefix):]\n\t\tftype = suffix\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\telse:\n\t\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport re\n\tfile_type = None\n\tfile_name = os.path.basename(file_path)\n\tif file_name.startswith(prefix):\n\t\tfile_type = \"pdf\"\n\t\tcomponent_id = file_name[len(prefix):]\n\t\tif component_id.startswith(\"_\") or component_id.endswith(\"_\"):\n\t\t\tcomponent_id = None\n\t\telse:\n\t\t\tcomponent_id = component_id.split(\"_\")[0]\n\telse:\n\t\tmatch = re.search(prefix + r\"([^_]+)_?([^_]*).*\\.xml\", file_name)\n\t\tif match:\n\t\t\tfile_type = \"xml\"\n\t\t\tcomponent_id = match.group(1)\n\t\t\tftype = match.group(2)\n\n\tif file_type is None:\n\t\treturn None\n\n\tresult = {\n\t\t\"component_id\": component_id,\n\t\t\"file_path\": file_path,\n\t}\n\tif file_type == \"pdf\":\n\t\treturn result\n\telse:\n\t\tresult[\"ftype\"] = ftype\n\t\treturn result\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tpass\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\tfrom datetime import datetime\n\tfrom xml.etree import ElementTree as ET\n\tfrom .pdf_reader import PDFReader\n\tfrom ..util import get_file_list\n\n\tfile_name = os.path.basename(file_path)\n\tfile_name, file_ext = os.path.splitext(file_name)\n\tfile_name = file_name.lower()\n\tfile_prefix = file_name[:len(prefix)]\n\n\tif file_prefix != prefix:\n\t\treturn None\n\n\tfile_id = file_name[len(prefix):]\n\n\tif file_ext == \".pdf\":\n\t\treturn dict(component_id=file_id, file_path=file_path)\n\n\tif file_ext in [\".xml\", \".txt\"]:\n\t\ttry:\n\t\t\troot = ET.parse(file_path).getroot()\n\t\t\tftype = root.attrib[\"ftype\"]\n\t\texcept:\n\t\t\tftype = None\n\n\t\treturn dict(component_id=file_id, file_path=file_path, ftype=ftype, file_path=file_path)\n\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\t\n\tftype = None\n\t\n\tcomponent_id = os.path.basename(file_path).split('.')[0]\n\n\tif prefix.lower() == \"pdf\":\n\t\tftype = prefix\n\telse:\n\t\tftype = os.path.basename(file_path).split('.')[1]\n\n\tif ftype == \"xml\":\n\t\treturn None\n\telif ftype == \"pdf\":\n\t\treturn {component_id: file_path}\n\telse:\n\t\treturn {component_id: [file_path, ftype, file_path]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport re\n\timport zipfile\n\n\tftype = file_path.split(\".\")\n\tftype = ftype[len(ftype) - 1]\n\n\tif ftype == \"pdf\":\n\t\treturn {\n\t\t\t\"component_id\": prefix,\n\t\t\t\"file_path\": file_path\n\t\t}\n\n\tif ftype != \"xml\":\n\t\treturn None\n\n\txml_file = open(file_path, \"r\")\n\tlines = xml_file.readlines()\n\txml_file.close()\n\n\tlines = [line.strip() for line in lines]\n\n\tmatch = re.search(r\"(<component_id>)(.*)(</component_id>)\", lines[0])\n\n\tif match:\n\t\treturn {\n\t\t\t\"component_id\": match.group(2),\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": ftype,\n\t\t\t\"file_path\": file_path\n\t\t}\n\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\tfrom xml.etree import ElementTree\n\n\tftype = None\n\tbasename = os.path.basename(file_path)\n\tif basename.startswith(prefix):\n\t\tcomponent_id = basename[len(prefix):]\n\t\tif not component_id.endswith('.xml'):\n\t\t\tcomponent_id = component_id.replace('_', '-')\n\t\t\txml_path = os.path.join(os.path.dirname(file_path), component_id + '.xml')\n\t\t\tif os.path.exists(xml_path):\n\t\t\t\tftype = 'pdf'\n\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport re\n\n\tresult = None\n\tftype = os.path.basename(file_path).split(\".\")[-1]\n\tif ftype.lower() == \"pdf\":\n\t\tresult = {\n\t\t\t\"component_id\": prefix,\n\t\t\t\"file_path\": file_path,\n\t\t}\n\telse:\n\t\tresult = {\n\t\t\t\"component_id\": prefix,\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": ftype,\n\t\t\t\"file_path\": file_path,\n\t\t}\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfrom os import path\n\tfile_name = path.basename(file_path)\n\tif not file_name.startswith(prefix):\n\t\treturn None\n\ttry:\n\t\timport xml.etree.ElementTree as ET\n\t\ttree = ET.parse(file_path)\n\t\troot = tree.getroot()\n\t\tif root.tag != \"pdf\":\n\t\t\treturn None\n\texcept:\n\t\treturn {\"component_id\": file_name.split(\"_\")[0], \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": file_name.split(\"_\")[0], \"file_path\": file_path, \"ftype\": root.attrib[\"ftype\"], \"title\": root.attrib[\"title\"]}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_path = file_path.lower()\n\tcomponents = file_path.split('/')\n\tif components[0] != prefix:\n\t\treturn None\n\tfile_name = components[-1]\n\tif file_name[-4:] == '.pdf':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:])}\n\telif file_name[-3:] == '.md':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:]), 'ftype': 'markdown'}\n\telif file_name[-3:] == '.ht':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:]), 'ftype': 'html'}\n\telif file_name[-3:] == '.rt':\n\t\treturn {'component_id': components[1], 'file_path': '/'.join(components[1:]), 'ftype': 'rst'}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfrom pathlib import Path\n\timport logging\n\tlogger = logging.getLogger()\n\n\tfile_path = Path(file_path)\n\tstem = file_path.stem\n\tsuffix = file_path.suffix\n\n\tftype = None\n\tif stem.startswith(prefix) and suffix == \".pdf\":\n\t\tcomponent_id = stem[len(prefix):]\n\t\treturn component_id\n\telif stem.startswith(prefix) and suffix != \".pdf\":\n\t\tcomponent_id = stem[len(prefix):]\n\t\tftype = suffix\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport re\n\tfile_type = None\n\tfile_name = os.path.basename(file_path)\n\tif file_name.startswith(prefix):\n\t\tfile_type = \"pdf\"\n\t\tcomponent_id = file_name[len(prefix):]\n\t\tif component_id.startswith(\"_\") or component_id.endswith(\"_\"):\n\t\t\tcomponent_id = None\n\t\telse:\n\t\t\tcomponent_id = component_id.split(\"_\")[0]\n\telse:\n\t\tmatch = re.search(prefix + r\"([^_]+)_?([^_]*).*\\.xml\", file_name)\n\t\tif match:\n\t\t\tfile_type = \"xml\"\n\t\t\tcomponent_id = match.group(1)\n\t\t\tftype = match.group(2)\n\n\tif file_type is None:\n\t\treturn None\n\n\tresult = {\n\t\t\"component_id\": component_id,\n\t\t\"file_path\": file_path,\n\t}\n\tif file_type == \"pdf\":\n\t\treturn result\n\telse:\n\t\tresult[\"ftype\"] = ftype\n\t\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions.update({lang: file_path})", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions.update({lang: file_path})", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets.update({basename: file_path})\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tfilename, file_extension = os.path.splitext(basename)\n\tself._assets[basename] = file_path\n\tself._asset_types[basename] = file_extension\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets.update({basename: file_path})\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tfilename, file_extension = os.path.splitext(basename)\n\tself._assets[basename] = file_path\n\tself._asset_types[basename] = file_extension\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport zipfile\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfiles_by_xml_filename = _group_files_by_xml_filename(zip_file)\n\t\txml_filename_by_file = {file: xml_filename for xml_filename, files in files_by_xml_filename.items() for file in files}\n\t\tzip_file.seek(0, 0) # Reset the zip file pointer.\n\t\treturn zip_file, files_by_xml_filename, xml_filename_by_file\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\txml_paths = []\n\tzip_file = ZipFile(zip_path)\n\tfor file_name in zip_file.namelist():\n\t\txml_paths = _group_files_by_xml_filename(file_name, xml_paths)\n\treturn xml_paths\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\tfrom os.path import dirname\n\tfrom os import makedirs\n\tfrom itertools import chain\n\n\tfrom ._group_files_by_xml_filename import _group_files_by_xml_filename\n\n\tzip_path = str(zip_path)\n\tzip_dir = dirname(zip_path)\n\n\twith ZipFile(zip_path) as zip_file:\n\t\tfile_names = zip_file.namelist()\n\t\tfile_name_groups = _group_files_by_xml_filename(file_names)\n\n\t\timport_groups = []\n\t\tfor file_name_group in file_name_groups:\n\t\t\timport_group = []\n\t\t\tfor file_name in file_name_group:\n\t\t\t\tfile_path = zip_file.extract(file_name, path=zip_dir)\n\t\t\t\timport_group.append(file_path)\n\t\t\timport_groups.append(import_group)\n\n\treturn import_groups\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(zip_path)\n\n\tzip_object = ZipFile(zip_path)\n\tzip_object.close()\n\n\treturn files\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgrouped_by_filename = _group_files_by_xml_filename(zip_path)\n\t\n\t_explore_grouped_by_filename(grouped_by_filename)\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tdef explore(zip_path, xml_filename_group):\n\t\tfrom zipfile import ZipFile\n\t\twith ZipFile(zip_path) as zf:\n\t\t\tfor name in zf.namelist():\n\t\t\t\txml_filename_group.setdefault(name, []).append(zip_path)\n\t\n\txml_filename_group = {}\n\texplore(zip_path, xml_filename_group)\n\treturn xml_filename_group\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfrom .explore import explore\n\tfrom .explore import _group_files_by_xml_filename\n\tfrom .explore import _group_files_by_xml_filename_and_date\n\n\txml_files_by_filename, xml_files_by_filename_and_date = _group_files_by_xml_filename(zip_path)\n\n\tgrouped_files = [\n\t\t{\n\t\t\t\"files\": xml_files,\n\t\t\t\"title\": '{filename} ({number_of_files})'.format(\n\t\t\t\tfilename=key,\n\t\t\t\tnumber_of_files=len(xml_files)\n\t\t\t)\n\t\t}\n\t\tfor key, xml_files in xml_files_by_filename.items()\n\t]\n\n\tgrouped_files += [\n\t\t{\n\t\t\t\"files\": xml_files,\n\t\t\t\"title\": '{filename} ({number_of_files})'.format(\n\t\t\t\tfilename=key,\n\t\t\t\tnumber_of_files=len(xml_files)\n\t\t\t)\n\t\t}\n\t\tfor key, xml_files in xml_files_by_filename_and_date.items()\n\t]\n\n\treturn explore(grouped_files)\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\tfile_groups = _group_files_by_xml_filename(zip_file)\n\treturn file_groups\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\twith ZipFile(zip_path) as zf:\n\t\tfor name in zf.filelist:\n\t\t\ttry:\n\t\t\t\tyield _group_files_by_xml_filename(zf, name)\n\t\t\texcept Exception as e:\n\t\t\t\tprint(e)\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_files = _group_files_by_xml_filename(zip_path)\n\treturn set(_explore_zip_files(zip_files))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport zipfile\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfiles_by_xml_filename = _group_files_by_xml_filename(zip_file)\n\t\txml_filename_by_file = {file: xml_filename for xml_filename, files in files_by_xml_filename.items() for file in files}\n\t\tzip_file.seek(0, 0) # Reset the zip file pointer.\n\t\treturn zip_file, files_by_xml_filename, xml_filename_by_file\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\txml_paths = []\n\tzip_file = ZipFile(zip_path)\n\tfor file_name in zip_file.namelist():\n\t\txml_paths = _group_files_by_xml_filename(file_name, xml_paths)\n\treturn xml_paths\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\tfrom os.path import dirname\n\tfrom os import makedirs\n\tfrom itertools import chain\n\n\tfrom ._group_files_by_xml_filename import _group_files_by_xml_filename\n\n\tzip_path = str(zip_path)\n\tzip_dir = dirname(zip_path)\n\n\twith ZipFile(zip_path) as zip_file:\n\t\tfile_names = zip_file.namelist()\n\t\tfile_name_groups = _group_files_by_xml_filename(file_names)\n\n\t\timport_groups = []\n\t\tfor file_name_group in file_name_groups:\n\t\t\timport_group = []\n\t\t\tfor file_name in file_name_group:\n\t\t\t\tfile_path = zip_file.extract(file_name, path=zip_dir)\n\t\t\t\timport_group.append(file_path)\n\t\t\timport_groups.append(import_group)\n\n\treturn import_groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(zip_path)\n\n\tzip_object = ZipFile(zip_path)\n\tzip_object.close()\n\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgrouped_by_filename = _group_files_by_xml_filename(zip_path)\n\t\n\t_explore_grouped_by_filename(grouped_by_filename)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tdef explore(zip_path, xml_filename_group):\n\t\tfrom zipfile import ZipFile\n\t\twith ZipFile(zip_path) as zf:\n\t\t\tfor name in zf.namelist():\n\t\t\t\txml_filename_group.setdefault(name, []).append(zip_path)\n\t\n\txml_filename_group = {}\n\texplore(zip_path, xml_filename_group)\n\treturn xml_filename_group\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfrom .explore import explore\n\tfrom .explore import _group_files_by_xml_filename\n\tfrom .explore import _group_files_by_xml_filename_and_date\n\n\txml_files_by_filename, xml_files_by_filename_and_date = _group_files_by_xml_filename(zip_path)\n\n\tgrouped_files = [\n\t\t{\n\t\t\t\"files\": xml_files,\n\t\t\t\"title\": '{filename} ({number_of_files})'.format(\n\t\t\t\tfilename=key,\n\t\t\t\tnumber_of_files=len(xml_files)\n\t\t\t)\n\t\t}\n\t\tfor key, xml_files in xml_files_by_filename.items()\n\t]\n\n\tgrouped_files += [\n\t\t{\n\t\t\t\"files\": xml_files,\n\t\t\t\"title\": '{filename} ({number_of_files})'.format(\n\t\t\t\tfilename=key,\n\t\t\t\tnumber_of_files=len(xml_files)\n\t\t\t)\n\t\t}\n\t\tfor key, xml_files in xml_files_by_filename_and_date.items()\n\t]\n\n\treturn explore(grouped_files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\tfile_groups = _group_files_by_xml_filename(zip_file)\n\treturn file_groups\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\twith ZipFile(zip_path) as zf:\n\t\tfor name in zf.filelist:\n\t\t\ttry:\n\t\t\t\tyield _group_files_by_xml_filename(zf, name)\n\t\t\texcept Exception as e:\n\t\t\t\tprint(e)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_files = _group_files_by_xml_filename(zip_path)\n\treturn set(_explore_zip_files(zip_files))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\treturn zip_ref.namelist()\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\treturn [f for f in zipfile.ZipFile(zip_path, 'r').namelist()]\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\twith ZipFile(zip_path, 'r') as zip_handle:\n\t\tfiles = zip_handle.namelist()\n\treturn files\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\ttry:\n\t\tz = zipfile.ZipFile(zip_path)\n\texcept IOError:\n\t\tprint 'IOError: %s' % zip_path\n\t\treturn []\n\treturn z.namelist()\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\n\tzf = ZipFile(zip_path)\n\treturn zf.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\tzip_file = ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\twith ZipFile(zip_path, 'r') as zip_file:\n\t\tzip_files = zip_file.namelist()\n\treturn zip_files\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzf = zipfile.ZipFile(zip_path)\n\tnames = zf.namelist()\n\treturn names\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzip_file = zipfile.ZipFile(zip_path)\n\tzip_file.extractall()\n\treturn zip_file.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\n\tlist_files = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tlist_files = zip_ref.namelist()\n\n\tlist_files = [os.path.basename(x) for x in list_files]\n\treturn list_files\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\treturn zip_ref.namelist()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\treturn [f for f in zipfile.ZipFile(zip_path, 'r').namelist()]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\twith ZipFile(zip_path, 'r') as zip_handle:\n\t\tfiles = zip_handle.namelist()\n\treturn files\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\ttry:\n\t\tz = zipfile.ZipFile(zip_path)\n\texcept IOError:\n\t\tprint 'IOError: %s' % zip_path\n\t\treturn []\n\treturn z.namelist()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\n\tzf = ZipFile(zip_path)\n\treturn zf.namelist()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\tzip_file = ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\twith ZipFile(zip_path, 'r') as zip_file:\n\t\tzip_files = zip_file.namelist()\n\treturn zip_files\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzf = zipfile.ZipFile(zip_path)\n\tnames = zf.namelist()\n\treturn names\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzip_file = zipfile.ZipFile(zip_path)\n\tzip_file.extractall()\n\treturn zip_file.namelist()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\n\tlist_files = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tlist_files = zip_ref.namelist()\n\n\tlist_files = [os.path.basename(x) for x in list_files]\n\treturn list_files\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tnew_content = \"\"\n\tin_tag = False\n\tfor c in content:\n\t\tif c == '<':\n\t\t\tin_tag = True\n\t\t\tnew_content += c\n\t\telif c == '>':\n\t\t\tin_tag = False\n\t\t\tnew_content += c\n\t\telif in_tag:\n\t\t\tif c == 'w':\n\t\t\t\tnew_content += 'w-'\n\t\t\telse:\n\t\t\t\tnew_content += c\n\t\telse:\n\t\t\tnew_content += c\n\treturn new_content", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tnew_content = re.sub(r'w:st=\"([^\"]+)\"', r'w-st=\"\\1\"', content)\n\treturn new_content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tdef replace_st(match):\n\t\treturn match.group(1) + \"-st=\" + match.group(3)\n\treturn content.replace('w:st=', replace_st)\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\timport re\n\tcontent = re.sub(r'w:st=\"([^\"]*)\"', r'w-st=\"\\1\"', content)\n\tcontent = re.sub(r'w:st=\"([^\"]*)\"', r'w-st=\"\\1\"', content)\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(' w:st=\"', ' w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tprefix = \"w:st=\"\n\tsuffix = \"w-st=\"\n\tindex = content.find(prefix)\n\twhile index != -1:\n\t\tcontent = content[:index] + suffix + content[index + len(prefix):]\n\t\tindex = content.find(prefix, index + len(suffix))\n\treturn content", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\tcontent = content.replace('w:stb=\"', 'w-stb=\"')\n\treturn content"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tnew_content = \"\"\n\tin_tag = False\n\tfor c in content:\n\t\tif c == '<':\n\t\t\tin_tag = True\n\t\t\tnew_content += c\n\t\telif c == '>':\n\t\t\tin_tag = False\n\t\t\tnew_content += c\n\t\telif in_tag:\n\t\t\tif c == 'w':\n\t\t\t\tnew_content += 'w-'\n\t\t\telse:\n\t\t\t\tnew_content += c\n\t\telse:\n\t\t\tnew_content += c\n\treturn new_content", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tnew_content = re.sub(r'w:st=\"([^\"]+)\"', r'w-st=\"\\1\"', content)\n\treturn new_content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tdef replace_st(match):\n\t\treturn match.group(1) + \"-st=\" + match.group(3)\n\treturn content.replace('w:st=', replace_st)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\timport re\n\tcontent = re.sub(r'w:st=\"([^\"]*)\"', r'w-st=\"\\1\"', content)\n\tcontent = re.sub(r'w:st=\"([^\"]*)\"', r'w-st=\"\\1\"', content)\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(' w:st=\"', ' w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tprefix = \"w:st=\"\n\tsuffix = \"w-st=\"\n\tindex = content.find(prefix)\n\twhile index != -1:\n\t\tcontent = content[:index] + suffix + content[index + len(prefix):]\n\t\tindex = content.find(prefix, index + len(suffix))\n\treturn content", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\tcontent = content.replace('w:stb=\"', 'w-stb=\"')\n\treturn content", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfrom lxml import etree\n\timport re\n\t\n\tpubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate_node = node.xpath(xpath)\n\t\tif pubdate_node:\n\t\t\tpubdate = pubdate_node[0]\n\t\t\tbreak\n\t\t\t\n\tif pubdate:\n\t\tpubdate_text = etree.tostring(pubdate, method='text')\n\t\tpubdate_text = pubdate_text.strip()\n\t\t# Remove any characters at the end.\n\t\tpubdate_text = re.sub(r'\\s*[a-zA-Z]+$', '', pubdate_text)\n\t\t# Make sure we're returning a string, not unicode.\n\t\tpubdate_text = str(pubdate_text)\n\t\t# Remove comma, but not if it's in the middle of a number.\n\t\tpubdate_text = re.sub(r'(?<=\\d),(?!\\d)', '', pubdate_text)\n\t\t# Remove leading zero.\n\t\tpubdate_text = re.sub(r'^0+', '', pubdate_text)\n\t\t# Remove leading space.\n\t\tpubdate_text = re.sub(r'^\\s+', '', pubdate_text)", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\t\n\tfor xpath in pubdate_xpaths:\n\t\tresult = node.xpath(xpath)\n\t\t\n\t\tif len(result) > 0:\n\t\t\tpubdate = result[0].text\n\t\t\tbreak\n\t\n\treturn pubdate\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatches = [node.xpath(xpath) for xpath in pubdate_xpaths]\n\treturn [m[0] for m in matches if m][0].text\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn pubdate\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\tbreak\n\treturn pubdate\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif match:\n\t\t\treturn match[0].strip()\n\t\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatch = None\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif len(match) > 0:\n\t\t\tbreak\n\treturn match\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif len(pubdate) > 0:\n\t\t\tpubdate = pubdate[0]\n\t\t\tbreak\n\treturn pubdate\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatches = []\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif match:\n\t\t\tmatches.append(match[0])\n\tif matches:\n\t\treturn matches[0]\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\t\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\t\n\t\tif pubdate:\n\t\t\tbreak\n\t\n\tif pubdate:\n\t\treturn pubdate[0]\n\telse:\n\t\treturn None\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfrom lxml import etree\n\timport re\n\t\n\tpubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate_node = node.xpath(xpath)\n\t\tif pubdate_node:\n\t\t\tpubdate = pubdate_node[0]\n\t\t\tbreak\n\t\t\t\n\tif pubdate:\n\t\tpubdate_text = etree.tostring(pubdate, method='text')\n\t\tpubdate_text = pubdate_text.strip()\n\t\t# Remove any characters at the end.\n\t\tpubdate_text = re.sub(r'\\s*[a-zA-Z]+$', '', pubdate_text)\n\t\t# Make sure we're returning a string, not unicode.\n\t\tpubdate_text = str(pubdate_text)\n\t\t# Remove comma, but not if it's in the middle of a number.\n\t\tpubdate_text = re.sub(r'(?<=\\d),(?!\\d)', '', pubdate_text)\n\t\t# Remove leading zero.\n\t\tpubdate_text = re.sub(r'^0+', '', pubdate_text)\n\t\t# Remove leading space.\n\t\tpubdate_text = re.sub(r'^\\s+', '', pubdate_text)", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\t\n\tfor xpath in pubdate_xpaths:\n\t\tresult = node.xpath(xpath)\n\t\t\n\t\tif len(result) > 0:\n\t\t\tpubdate = result[0].text\n\t\t\tbreak\n\t\n\treturn pubdate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatches = [node.xpath(xpath) for xpath in pubdate_xpaths]\n\treturn [m[0] for m in matches if m][0].text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn pubdate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\tbreak\n\treturn pubdate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif match:\n\t\t\treturn match[0].strip()\n\t\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatch = None\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif len(match) > 0:\n\t\t\tbreak\n\treturn match\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif len(pubdate) > 0:\n\t\t\tpubdate = pubdate[0]\n\t\t\tbreak\n\treturn pubdate\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatches = []\n\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif match:\n\t\t\tmatches.append(match[0])\n\tif matches:\n\t\treturn matches[0]\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\t\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\t\n\t\tif pubdate:\n\t\t\tbreak\n\t\n\tif pubdate:\n\t\treturn pubdate[0]\n\telse:\n\t\treturn None\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tnumber_and_sup = []\n\tissue_split = issue.split('-')\n\tif len(issue_split) == 1:\n\t\tnumber = issue\n\telif len(issue_split) == 2:\n\t\tnumber = issue_split[0]\n\t\tsup = issue_split[1]\n\telif len(issue_split) == 3:\n\t\tnumber = issue_split[0]\n\t\tsup = issue_split[2]\n\telif len(issue_split) > 3:\n\t\tnumber = issue_split[0]\n\t\tfor i in range(1, len(issue_split) - 1):\n\t\t\tsup = issue_split[i]\n\tif number is not None:\n\t\tnumber_and_sup.append(number)\n\tif sup is not None:\n\t\tnumber_and_sup.append(sup)\n\treturn number_and_sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumbers = [\n\t\tissue.get('number'),\n\t\tissue.get('number', '')\n\t]\n\tsupplements = [\n\t\tissue.get('sup'),\n\t\tissue.get('sup', ''),\n\t]\n\treturn numbers, supplements\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsupplement = None\n\n\tif issue is None:\n\t\treturn number, supplement\n\n\tparts = issue.split(' ')\n\n\tif len(parts) == 1:\n\t\tnumber = parts[0]\n\telif len(parts) == 2:\n\t\tnumber = parts[0].strip(', ')\n\t\tsupplement = parts[1].strip(', ')\n\n\treturn number, supplement\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\t\n\tfor a in issue.contents:\n\t\tif a.name == 'sup':\n\t\t\tsup = a.string\n\t\telif a.name == 'a':\n\t\t\tif not a.string:\n\t\t\t\tcontinue\n\t\t\tif ' ' in a.string:\n\t\t\t\tnumber, sup = a.string.split(' ')\n\t\t\telse:\n\t\t\t\tnumber = a.string\n\t\n\treturn number, sup\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\n\tif issue.tag == 'issue':\n\t\tnumber = issue.get('number')\n\t\tsup = issue.get('sup')\n\telif issue.tag == 'sup':\n\t\tsup = issue.get('number')\n\telif issue.tag == 'number':\n\t\tnumber = issue.text\n\n\treturn number, sup\n\n\n#TODO: make this more robust", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tif issue.text:\n\t\tmatch = re.match(r'^(?P<number>\\d+)(?P<sup>\\([^()]*\\))?$', issue.text)\n\t\tif match:\n\t\t\tnumber = match.group('number')\n\t\t\tsup = match.group('sup')\n\treturn number, sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_sup = []\n\t\n\tnumber = issue.findtext(\"number\")\n\tif number is not None:\n\t\tnumber_and_sup.append((number, \"\"))\n\t\n\tsup = issue.findtext(\"sup\")\n\tif sup is not None:\n\t\tnumber_and_sup.append((number, sup))\n\t\n\treturn number_and_sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_supplment = []\n\n\tnumber = issue.xpath('string(./h4[contains(@class, \"ticket-title\")])').strip()\n\tif number:\n\t\tnumber_and_supplment.append((number, ''))\n\n\t# TODO: The following is currently not used. It's only here in case we need it later\n\t# sup = issue.xpath('string(./span[contains(@class, \"ticket-suppl\")])').strip()\n\t# if sup:\n\t# \tnumber_and_supplment.append((number, sup))\n\n\treturn number_and_supplment\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tnumber_match = re.search(r'<b>([A-Z]*)</b>', issue)\n\tif number_match:\n\t\tnumber = number_match.group(1)\n\tsup_match = re.search(r'<sup>([0-9]*)</sup>', issue)\n\tif sup_match:\n\t\tsup = sup_match.group(1)\n\treturn number, sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = []\n\tsup = []\n\n\tfor c in issue.contents:\n\t\ttry:\n\t\t\tnumber.append(c.get_text())\n\t\texcept AttributeError:\n\t\t\tpass\n\n\t\ttry:\n\t\t\tsup.append(c.sup.get_text())\n\t\texcept AttributeError:\n\t\t\tpass\n\n\treturn number, sup\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tnumber_and_sup = []\n\tissue_split = issue.split('-')\n\tif len(issue_split) == 1:\n\t\tnumber = issue\n\telif len(issue_split) == 2:\n\t\tnumber = issue_split[0]\n\t\tsup = issue_split[1]\n\telif len(issue_split) == 3:\n\t\tnumber = issue_split[0]\n\t\tsup = issue_split[2]\n\telif len(issue_split) > 3:\n\t\tnumber = issue_split[0]\n\t\tfor i in range(1, len(issue_split) - 1):\n\t\t\tsup = issue_split[i]\n\tif number is not None:\n\t\tnumber_and_sup.append(number)\n\tif sup is not None:\n\t\tnumber_and_sup.append(sup)\n\treturn number_and_sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumbers = [\n\t\tissue.get('number'),\n\t\tissue.get('number', '')\n\t]\n\tsupplements = [\n\t\tissue.get('sup'),\n\t\tissue.get('sup', ''),\n\t]\n\treturn numbers, supplements\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsupplement = None\n\n\tif issue is None:\n\t\treturn number, supplement\n\n\tparts = issue.split(' ')\n\n\tif len(parts) == 1:\n\t\tnumber = parts[0]\n\telif len(parts) == 2:\n\t\tnumber = parts[0].strip(', ')\n\t\tsupplement = parts[1].strip(', ')\n\n\treturn number, supplement\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\t\n\tfor a in issue.contents:\n\t\tif a.name == 'sup':\n\t\t\tsup = a.string\n\t\telif a.name == 'a':\n\t\t\tif not a.string:\n\t\t\t\tcontinue\n\t\t\tif ' ' in a.string:\n\t\t\t\tnumber, sup = a.string.split(' ')\n\t\t\telse:\n\t\t\t\tnumber = a.string\n\t\n\treturn number, sup\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\n\tif issue.tag == 'issue':\n\t\tnumber = issue.get('number')\n\t\tsup = issue.get('sup')\n\telif issue.tag == 'sup':\n\t\tsup = issue.get('number')\n\telif issue.tag == 'number':\n\t\tnumber = issue.text\n\n\treturn number, sup\n\n\n#TODO: make this more robust", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tif issue.text:\n\t\tmatch = re.match(r'^(?P<number>\\d+)(?P<sup>\\([^()]*\\))?$', issue.text)\n\t\tif match:\n\t\t\tnumber = match.group('number')\n\t\t\tsup = match.group('sup')\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_sup = []\n\t\n\tnumber = issue.findtext(\"number\")\n\tif number is not None:\n\t\tnumber_and_sup.append((number, \"\"))\n\t\n\tsup = issue.findtext(\"sup\")\n\tif sup is not None:\n\t\tnumber_and_sup.append((number, sup))\n\t\n\treturn number_and_sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_supplment = []\n\n\tnumber = issue.xpath('string(./h4[contains(@class, \"ticket-title\")])').strip()\n\tif number:\n\t\tnumber_and_supplment.append((number, ''))\n\n\t# TODO: The following is currently not used. It's only here in case we need it later\n\t# sup = issue.xpath('string(./span[contains(@class, \"ticket-suppl\")])').strip()\n\t# if sup:\n\t# \tnumber_and_supplment.append((number, sup))\n\n\treturn number_and_supplment\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tnumber_match = re.search(r'<b>([A-Z]*)</b>', issue)\n\tif number_match:\n\t\tnumber = number_match.group(1)\n\tsup_match = re.search(r'<sup>([0-9]*)</sup>', issue)\n\tif sup_match:\n\t\tsup = sup_match.group(1)\n\treturn number, sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = []\n\tsup = []\n\n\tfor c in issue.contents:\n\t\ttry:\n\t\t\tnumber.append(c.get_text())\n\t\texcept AttributeError:\n\t\t\tpass\n\n\t\ttry:\n\t\t\tsup.append(c.sup.get_text())\n\t\texcept AttributeError:\n\t\t\tpass\n\n\treturn number, sup\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdetails = \"\"\n\tif debug:\n\t\tdetails = f\", {self.debug_details()}\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({details})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\n\t\t\" \" * indent\n\t\t+ f\"{self.__class__.__name__}({self.debug_details}{obj})\"\n\t)", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.obj_type}: {self.obj_name}, \" if debug else \"\"\n\treturn f\"{' ' * indent}{self.__class__.__name__}({debug_details}{obj})\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.type}, {self.tag}, \" if debug else \"\"\n\treturn (f\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.tag}, {self.value}, {self.pos}, {self.line}, {self.column}, {self.file_name}, \" if debug else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tif debug:\n\t\tdebug_details = f\"obj={obj}, \"\n\telse:\n\t\tdebug_details = \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.type}, \" if debug else \"\"\n\treturn f\"{' ' * indent}{self.__class__.__name__}({debug_details}{obj})\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = f\"debug={debug}, \" if debug else \"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\n\t\t\" \" * indent\n\t\t+ f\"{self.__class__.__name__}({debug_details}obj={obj})\"\n\t\t+ f\"\\n\"\n\t\t+ \"\\n\".join(\n\t\t\tp.pretty(indent=indent + 2, debug=debug) for p in self.props\n\t\t)\n\t)\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"debug={self.debug}, \" if debug else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.__class__.__name__}({self.obj}, {self.location})\" if debug else \"\"\n\treturn (\n\t\t\" \" * indent\n\t\t+ f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t)\n\n\nif __name__ == \"__main__\":\n\tprint(pretty(1))\n\tprint(pretty(\"hello\"))\n\tprint(pretty(True))\n\tprint(pretty(None))\n\tprint(pretty([\"hello\", \"world\"]))"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdetails = \"\"\n\tif debug:\n\t\tdetails = f\", {self.debug_details()}\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({details})\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\n\t\t\" \" * indent\n\t\t+ f\"{self.__class__.__name__}({self.debug_details}{obj})\"\n\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.obj_type}: {self.obj_name}, \" if debug else \"\"\n\treturn f\"{' ' * indent}{self.__class__.__name__}({debug_details}{obj})\"", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.type}, {self.tag}, \" if debug else \"\"\n\treturn (f\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.tag}, {self.value}, {self.pos}, {self.line}, {self.column}, {self.file_name}, \" if debug else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tif debug:\n\t\tdebug_details = f\"obj={obj}, \"\n\telse:\n\t\tdebug_details = \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.type}, \" if debug else \"\"\n\treturn f\"{' ' * indent}{self.__class__.__name__}({debug_details}{obj})\"", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = f\"debug={debug}, \" if debug else \"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\n\t\t\" \" * indent\n\t\t+ f\"{self.__class__.__name__}({debug_details}obj={obj})\"\n\t\t+ f\"\\n\"\n\t\t+ \"\\n\".join(\n\t\t\tp.pretty(indent=indent + 2, debug=debug) for p in self.props\n\t\t)\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"debug={self.debug}, \" if debug else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.__class__.__name__}({self.obj}, {self.location})\" if debug else \"\"\n\treturn (\n\t\t\" \" * indent\n\t\t+ f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t)\n\n\nif __name__ == \"__main__\":\n\tprint(pretty(1))\n\tprint(pretty(\"hello\"))\n\tprint(pretty(True))\n\tprint(pretty(None))\n\tprint(pretty([\"hello\", \"world\"]))", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tx = self.absorb(args)\n\tself.set(x)\n\treturn x\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tdef absorb_impl(args):\n\t\tif len(args) < 1:\n\t\t\treturn []\n\t\telif len(args) == 1:\n\t\t\treturn args\n\t\telse:\n\t\t\tx, xs = args[0], args[1:]\n\t\t\treturn [x] + absorb_impl([y for y in xs if y != x])\n\n\treturn absorb_impl(args)", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = args.copy()\n\n\t# If the first argument is a list, then we're absorbing a list\n\tif args[0] is not None and args[0].type == 'list':\n\t\treturn self.absorb_list(args)\n\n\t# If the first argument is a list, then we're absorbing a dictionary\n\tif args[0] is not None and args[0].type == 'dict':\n\t\treturn self.absorb_dict(args)\n\n\t# Otherwise we're absorbing a tuple\n\treturn self.absorb_tuple(args)\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif hasattr(arg, 'absorb'):\n\t\t\targ = arg.absorb()\n\t\tnew_args.append(arg)\n\treturn new_args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tresult = []\n\ti = 0\n\twhile i < len(args):\n\t\tresult.append(args[i])\n\t\ti += 1\n\t\tif i < len(args) and args[i].is_absorption():\n\t\t\tresult.append(args[i])\n\t\t\ti += 1\n\treturn result\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tl = []\n\tfor x in args:\n\t\tl.append(self.absorb(x))\n\treturn l\n\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = args.split(',')\n\targs = [self.args_to_expression(arg) for arg in args]\n\n\tif len(args) == 2:\n\t\tself.args = args\n\t\treturn self._absorb_2()\n\n\telif len(args) == 3:\n\t\tself.args = args\n\t\treturn self._absorb_3()\n\n\telif len(args) == 4:\n\t\tself.args = args\n\t\treturn self._absorb_4()\n\n\telif len(args) == 5:\n\t\tself.args = args\n\t\treturn self._absorb_5()\n\n\telif len(args) == 6:\n\t\tself.args = args\n\t\treturn self._absorb_6()\n\n\telif len(args) == 7:\n\t\tself.args = args\n\t\treturn self._absorb_7()\n\n\telif len(args) == 8:\n\t\tself.args = args\n\t\treturn self._absorb_8()\n\n\telif len(args) == 9:\n\t\tself.args = args\n\t\treturn self._absorb_9()\n\n\telif len(args) == 10:\n\t\tself.args = args\n\t\treturn self._absorb_10()\n\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tl = []\n\n\tfor a in args:\n\t\tif not l:\n\t\t\tl.append(a)\n\t\telse:\n\t\t\tl[-1] = l[-1].absorb(a)\n\n\treturn l\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\t# If the arguments are empty, return self.\n\tif not args:\n\t\treturn self\n\t# Otherwise, absorb the first argument.\n\tfirst = args[0]\n\targs = args[1:]\n\t# If the first argument is not a list, return a new list containing the first argument and the remaining arguments.\n\tif not first.isList():\n\t\treturn List([first] + args)\n\t# Otherwise, absorb the first argument and return the result.\n\treturn first.absorb(args)\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tfrom .. import Expr\n\treturn Expr([self.absorb(a) for a in args])\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tx = self.absorb(args)\n\tself.set(x)\n\treturn x\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tdef absorb_impl(args):\n\t\tif len(args) < 1:\n\t\t\treturn []\n\t\telif len(args) == 1:\n\t\t\treturn args\n\t\telse:\n\t\t\tx, xs = args[0], args[1:]\n\t\t\treturn [x] + absorb_impl([y for y in xs if y != x])\n\n\treturn absorb_impl(args)", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = args.copy()\n\n\t# If the first argument is a list, then we're absorbing a list\n\tif args[0] is not None and args[0].type == 'list':\n\t\treturn self.absorb_list(args)\n\n\t# If the first argument is a list, then we're absorbing a dictionary\n\tif args[0] is not None and args[0].type == 'dict':\n\t\treturn self.absorb_dict(args)\n\n\t# Otherwise we're absorbing a tuple\n\treturn self.absorb_tuple(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif hasattr(arg, 'absorb'):\n\t\t\targ = arg.absorb()\n\t\tnew_args.append(arg)\n\treturn new_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tresult = []\n\ti = 0\n\twhile i < len(args):\n\t\tresult.append(args[i])\n\t\ti += 1\n\t\tif i < len(args) and args[i].is_absorption():\n\t\t\tresult.append(args[i])\n\t\t\ti += 1\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tl = []\n\tfor x in args:\n\t\tl.append(self.absorb(x))\n\treturn l\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = args.split(',')\n\targs = [self.args_to_expression(arg) for arg in args]\n\n\tif len(args) == 2:\n\t\tself.args = args\n\t\treturn self._absorb_2()\n\n\telif len(args) == 3:\n\t\tself.args = args\n\t\treturn self._absorb_3()\n\n\telif len(args) == 4:\n\t\tself.args = args\n\t\treturn self._absorb_4()\n\n\telif len(args) == 5:\n\t\tself.args = args\n\t\treturn self._absorb_5()\n\n\telif len(args) == 6:\n\t\tself.args = args\n\t\treturn self._absorb_6()\n\n\telif len(args) == 7:\n\t\tself.args = args\n\t\treturn self._absorb_7()\n\n\telif len(args) == 8:\n\t\tself.args = args\n\t\treturn self._absorb_8()\n\n\telif len(args) == 9:\n\t\tself.args = args\n\t\treturn self._absorb_9()\n\n\telif len(args) == 10:\n\t\tself.args = args\n\t\treturn self._absorb_10()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tl = []\n\n\tfor a in args:\n\t\tif not l:\n\t\t\tl.append(a)\n\t\telse:\n\t\t\tl[-1] = l[-1].absorb(a)\n\n\treturn l\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\t# If the arguments are empty, return self.\n\tif not args:\n\t\treturn self\n\t# Otherwise, absorb the first argument.\n\tfirst = args[0]\n\targs = args[1:]\n\t# If the first argument is not a list, return a new list containing the first argument and the remaining arguments.\n\tif not first.isList():\n\t\treturn List([first] + args)\n\t# Otherwise, absorb the first argument and return the result.\n\treturn first.absorb(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tfrom .. import Expr\n\treturn Expr([self.absorb(a) for a in args])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.registry.register(func, hook)\n\n\t\treturn func\n\n\treturn decorator", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif not hasattr(self, \"_hooks\"):\n\t\t\tself._hooks = {}\n\n\t\tif hook not in self._hooks:\n\t\t\tself._hooks[hook] = []\n\n\t\tself._hooks[hook].append(func)\n\n\t\treturn func\n\n\treturn decorator\n\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef add_handler(func):\n\t\tself.hooks[hook].append(func)\n\t\treturn func\n\treturn add_handler\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks.setdefault(hook, []).append(func)\n\t\treturn func\n\treturn decorator", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.registry[hook] = func\n\t\treturn func\n\treturn decorator", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.add_handler(hook, func)\n\t\treturn func\n\treturn decorator", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef _decorator(fn):\n\t\tself._registry[hook].append(fn)\n\t\treturn fn\n\treturn _decorator", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].add(func)\n\t\treturn func\n\treturn decorator\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.registry.register(func, hook)\n\n\t\treturn func\n\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif not hasattr(self, \"_hooks\"):\n\t\t\tself._hooks = {}\n\n\t\tif hook not in self._hooks:\n\t\t\tself._hooks[hook] = []\n\n\t\tself._hooks[hook].append(func)\n\n\t\treturn func\n\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef add_handler(func):\n\t\tself.hooks[hook].append(func)\n\t\treturn func\n\treturn add_handler\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks.setdefault(hook, []).append(func)\n\t\treturn func\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.registry[hook] = func\n\t\treturn func\n\treturn decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.add_handler(hook, func)\n\t\treturn func\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef _decorator(fn):\n\t\tself._registry[hook].append(fn)\n\t\treturn fn\n\treturn _decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].add(func)\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"cert_file\": \"cert.pem\",\n\t\t\t\"key_file\": \"key.pem\",\n\t\t\t\"client_cert_auth\": True,\n\t\t\t\"client_ca_file\": \"ca.pem\",\n\t\t\t\"client_cert_file\": \"client.pem\",\n\t\t\t\"client_key_file\": \"client-key.pem\",\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"user\": user,\n\t\t\t\"password\": \"password\",\n\t\t\t\"admin_user\": \"root\",\n\t\t\t\"admin_password\": \"password\",\n\t\t\t\"enable_auth\": True,\n\t\t\t\"enable_auth_token\": True,\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enable_authz\": True,\n\t\t\t\"enable_authz_basic\": True,\n\t\t\t\"enable_authz_token\": True,\n\t\t\t\"enable_authz_key\": True,\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"listen_client_urls\": \"https://0.0.0.0:2379\",\n\t\t\t\"listen_peer_urls\": \"https://0.0.0.0:2380\",\n\t\t\t\"initial_advertise_peer_urls\": \"https://0.0.", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"port\": 9000,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"port\": 9001,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"port\": 9002,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"port\": 8080,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"file\": \"auth.log\",\n\t\t\t\"max_size\": 10,\n\t\t\t\"max_backups\": 10,\n\t\t\t\"max_age\": 1", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"INFO\",\n\t\t\t\"file\": \"/var/log/kimono.log\"\n\t\t}\n\t}\n\t\n\tif user.tls.enabled:\n\t\tconfig[\"tls\"] = {\n\t\t\t\"enabled\": True,\n\t\t\t\"cert_file\": user.tls.cert_file,\n\t\t\t\"key_file\": user.tls.key_file,\n\t\t\t\"client_ca_file\": user.tls.client_ca_file,\n\t\t\t\"server_ca_file\": user.tls.server_ca_file\n\t\t}\n\t\n\tif user.authentication.enabled:\n\t\tconfig[\"authentication\"] = {\n\t\t\t\"enabled\": True,\n\t\t\t\"users\": user.authentication.users\n\t\t}\n\t\n\tif user.authorization.enabled:\n\t\tconfig[\"authorization\"] = {\n\t\t\t\"enabled", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert_file\": \"\",\n\t\t\t\"key_file\": \"\",\n\t\t\t\"ca_file\": \"\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"token\": \"\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"roles\": [\n\t\t\t\t{\n\t\t\t\t\t\"role\": \"admin\",\n\t\t\t\t\t\"permission\": \"*\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8000\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"file\": \"logs/debug.log\"\n\t\t}\n\t}\n\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"certfile\": \"/path/to/cert.pem\",\n\t\t\t\"keyfile\": \"/path/to/key.pem\",\n\t\t\t\"ca\": \"/path/to/ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"user\": user,\n\t\t\t\"password\": \"password\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"admin\": \"password\",\n\t\t\t\"user\": \"password\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"port\": 8080\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\"\n\t\t}\n\t}\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"docs\": {\n\t\t\t\"url\": \"http://localhost:8080\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"url\": f\"http://{etcd_host}:{etcd_port}\",\n\t\t\t\"user\": user\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"admin_user\": user\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"user\": user\n\t\t},\n\t\t\"tls\": {\n\t\t\t\"cert_file\": \"cert.pem\",\n\t\t\t\"key_file\": \"key.pem\",\n\t\t\t\"ca_file\": \"ca.pem\"\n\t\t}\n\t}\n\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"/etc/kube-apiserver/pki/apiserver.crt\",\n\t\t\t\"key\": \"/etc/kube-apiserver/pki/apiserver.key\",\n\t\t\t\"ca\": \"/etc/kube-apiserver/pki/ca.crt\",\n\t\t\t\"client_ca\": \"/etc/kube-apiserver/pki/ca.crt\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"anonymous\": {\"enabled\": False},\n\t\t\t\"x509\": {\"clientCAFile\": \"/etc/kube-apiserver/pki/ca.crt\"}\n\t\t},\n\t\t\"authorization\": {\"mode\": \"Node\"},\n\t\t\"etcd\": {\n\t\t\t\"endpoints\": [\"https://%s:%d\" % (etcd_host, etcd_port)],\n\t\t\t\"cafile\": \"/etc/kube-apiserver/pki/ca.crt\",\n\t\t\t\"certfile\": \"/etc/kube-apiserver/pki/apiserver-etcd-client.crt\",\n\t\t\t\"keyfile\": \"/etc/kube-apiserver/pki/apiserver-etcd-client.key\"\n\t\t},\n\t", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {}\n\tconfig[\"tls\"] = {}\n\tconfig[\"tls\"][\"enable\"] = False\n\tconfig[\"tls\"][\"cert_file\"] = \"\"\n\tconfig[\"tls\"][\"key_file\"] = \"\"\n\tconfig[\"tls\"][\"ca_file\"] = \"\"\n\tconfig[\"tls\"][\"client_cert_auth\"] = False\n\tconfig[\"tls\"][\"server_name\"] = \"\"\n\n\tconfig[\"authentication\"] = {}\n\tconfig[\"authentication\"][\"method\"] = \"basic\"\n\tconfig[\"authentication\"][\"username\"] = \"admin\"\n\tconfig[\"authentication\"][\"password\"] = \"admin\"\n\tconfig[\"authorization\"] = {}\n\tconfig[\"authorization\"][\"enable\"] = False\n\n\tconfig[\"etcd\"] = {}\n\tconfig[\"etcd\"][\"host\"] = etcd_host\n\tconfig[\"etcd\"][\"port\"] = etcd_port\n\tconfig[\"docs\"] = {}\n\tconfig[\"docs\"][\"enable\"] = True\n\tconfig[\"docs\"][\"dir\"] = \"docs\"\n\tconfig[\"log\"] = {}\n\tconfig[\"log\"][\"level\"] = \"info\"\n\tconfig[\"log\"][\"access_log_format\"] = \"json\"\n\tconfig[\"log\"][\"filename\"] = \"access.log\"\n\tconfig[\"log\"]", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconf = dict()\n\tconf[\"tls\"] = dict()\n\tconf[\"authentication\"] = dict()\n\tconf[\"authorization\"] = dict()\n\tconf[\"etcd\"] = dict()\n\tconf[\"log\"] = dict()\n\tconf[\"docs\"] = dict()\n\n\tconf[\"tls\"][\"enabled\"] = False\n\tconf[\"authentication\"][\"enabled\"] = False\n\tconf[\"authorization\"][\"enabled\"] = False\n\tconf[\"etcd\"][\"endpoints\"] = [f\"http://{etcd_host}:{etcd_port}\"]\n\tconf[\"etcd\"][\"watch_interval\"] = 15\n\tconf[\"log\"][\"level\"] = \"info\"\n\tconf[\"docs\"][\"enabled\"] = False\n\tconf[\"docs\"][\"url\"] = \"/docs\"\n\tconf[\"docs\"][\"title\"] = \"API documentation\"\n\tconf[\"docs\"][\"description\"] = \"The API documentation is provided by Swagger UI.\"\n\tconf[\"docs\"][\"version\"] = \"1.0.0\"\n\tconf[\"docs\"][\"termsOfService\"] = \"http://swagger.io/terms/\"\n\tconf[\"docs\"][\"contact\"] = dict()\n\tconf[\"docs\"][\"contact\"][\"name\"] = \"Support\"\n\tconf[\"docs\"][\"contact\"][\"url\"] = \"http://www", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tbase = {\n\t\t\"tls\": {\n\t\t\t\"tls_cert\": \"\",\n\t\t\t\"tls_key\": \"\",\n\t\t\t\"tls_ca\": \"\",\n\t\t\t\"tls_verify\": False,\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"mode\": \"off\",\n\t\t\t\"cert\": \"tls_cert\",\n\t\t\t\"key\": \"tls_key\",\n\t\t\t\"client_cert_auth\": False,\n\t\t\t\"allowed_cn\": [],\n\t\t\t\"allowed_ou\": [],\n\t\t\t\"allowed_uri_sans\": [],\n\t\t\t\"allowed_names\": [],\n\t\t\t\"allowed_groups\": [],\n\t\t\t\"allowed_dns_sans\": [],\n\t\t\t\"allowed_email_sans\": [],\n\t\t\t\"allowed_uri_domains\": [],\n\t\t\t\"ttl\": \"15m\",\n\t\t},\n\t\t\"authorization\": {\"mode\": \"off\"},\n\t\t\"etcd\": {\"host\": etcd_host, \"port\": etcd_port, \"prefix\": \"/v3/kv\", \"version\": \"v3\"},\n\t\t\"docs\": {\n\t\t\t\"endpoint\": \"http://0.0.0.0:8000/docs\",\n\t\t\t\"prefix\": \"/v3/kv\",\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\","], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"cert_file\": \"cert.pem\",\n\t\t\t\"key_file\": \"key.pem\",\n\t\t\t\"client_cert_auth\": True,\n\t\t\t\"client_ca_file\": \"ca.pem\",\n\t\t\t\"client_cert_file\": \"client.pem\",\n\t\t\t\"client_key_file\": \"client-key.pem\",\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"user\": user,\n\t\t\t\"password\": \"password\",\n\t\t\t\"admin_user\": \"root\",\n\t\t\t\"admin_password\": \"password\",\n\t\t\t\"enable_auth\": True,\n\t\t\t\"enable_auth_token\": True,\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enable_authz\": True,\n\t\t\t\"enable_authz_basic\": True,\n\t\t\t\"enable_authz_token\": True,\n\t\t\t\"enable_authz_key\": True,\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"listen_client_urls\": \"https://0.0.0.0:2379\",\n\t\t\t\"listen_peer_urls\": \"https://0.0.0.0:2380\",\n\t\t\t\"initial_advertise_peer_urls\": \"https://0.0.", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"port\": 9000,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"port\": 9001,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"port\": 9002,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"port\": 8080,\n\t\t\t\"certificate\": \"./certificate/server.crt\",\n\t\t\t\"private_key\": \"./certificate/server.key\",\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"file\": \"auth.log\",\n\t\t\t\"max_size\": 10,\n\t\t\t\"max_backups\": 10,\n\t\t\t\"max_age\": 1", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"INFO\",\n\t\t\t\"file\": \"/var/log/kimono.log\"\n\t\t}\n\t}\n\t\n\tif user.tls.enabled:\n\t\tconfig[\"tls\"] = {\n\t\t\t\"enabled\": True,\n\t\t\t\"cert_file\": user.tls.cert_file,\n\t\t\t\"key_file\": user.tls.key_file,\n\t\t\t\"client_ca_file\": user.tls.client_ca_file,\n\t\t\t\"server_ca_file\": user.tls.server_ca_file\n\t\t}\n\t\n\tif user.authentication.enabled:\n\t\tconfig[\"authentication\"] = {\n\t\t\t\"enabled\": True,\n\t\t\t\"users\": user.authentication.users\n\t\t}\n\t\n\tif user.authorization.enabled:\n\t\tconfig[\"authorization\"] = {\n\t\t\t\"enabled", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert_file\": \"\",\n\t\t\t\"key_file\": \"\",\n\t\t\t\"ca_file\": \"\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"token\": \"\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"roles\": [\n\t\t\t\t{\n\t\t\t\t\t\"role\": \"admin\",\n\t\t\t\t\t\"permission\": \"*\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8000\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"file\": \"logs/debug.log\"\n\t\t}\n\t}\n\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"certfile\": \"/path/to/cert.pem\",\n\t\t\t\"keyfile\": \"/path/to/key.pem\",\n\t\t\t\"ca\": \"/path/to/ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"user\": user,\n\t\t\t\"password\": \"password\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"admin\": \"password\",\n\t\t\t\"user\": \"password\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"port\": 8080\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\"\n\t\t}\n\t}\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"docs\": {\n\t\t\t\"url\": \"http://localhost:8080\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"url\": f\"http://{etcd_host}:{etcd_port}\",\n\t\t\t\"user\": user\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"admin_user\": user\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"user\": user\n\t\t},\n\t\t\"tls\": {\n\t\t\t\"cert_file\": \"cert.pem\",\n\t\t\t\"key_file\": \"key.pem\",\n\t\t\t\"ca_file\": \"ca.pem\"\n\t\t}\n\t}\n\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"/etc/kube-apiserver/pki/apiserver.crt\",\n\t\t\t\"key\": \"/etc/kube-apiserver/pki/apiserver.key\",\n\t\t\t\"ca\": \"/etc/kube-apiserver/pki/ca.crt\",\n\t\t\t\"client_ca\": \"/etc/kube-apiserver/pki/ca.crt\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"anonymous\": {\"enabled\": False},\n\t\t\t\"x509\": {\"clientCAFile\": \"/etc/kube-apiserver/pki/ca.crt\"}\n\t\t},\n\t\t\"authorization\": {\"mode\": \"Node\"},\n\t\t\"etcd\": {\n\t\t\t\"endpoints\": [\"https://%s:%d\" % (etcd_host, etcd_port)],\n\t\t\t\"cafile\": \"/etc/kube-apiserver/pki/ca.crt\",\n\t\t\t\"certfile\": \"/etc/kube-apiserver/pki/apiserver-etcd-client.crt\",\n\t\t\t\"keyfile\": \"/etc/kube-apiserver/pki/apiserver-etcd-client.key\"\n\t\t},\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {}\n\tconfig[\"tls\"] = {}\n\tconfig[\"tls\"][\"enable\"] = False\n\tconfig[\"tls\"][\"cert_file\"] = \"\"\n\tconfig[\"tls\"][\"key_file\"] = \"\"\n\tconfig[\"tls\"][\"ca_file\"] = \"\"\n\tconfig[\"tls\"][\"client_cert_auth\"] = False\n\tconfig[\"tls\"][\"server_name\"] = \"\"\n\n\tconfig[\"authentication\"] = {}\n\tconfig[\"authentication\"][\"method\"] = \"basic\"\n\tconfig[\"authentication\"][\"username\"] = \"admin\"\n\tconfig[\"authentication\"][\"password\"] = \"admin\"\n\tconfig[\"authorization\"] = {}\n\tconfig[\"authorization\"][\"enable\"] = False\n\n\tconfig[\"etcd\"] = {}\n\tconfig[\"etcd\"][\"host\"] = etcd_host\n\tconfig[\"etcd\"][\"port\"] = etcd_port\n\tconfig[\"docs\"] = {}\n\tconfig[\"docs\"][\"enable\"] = True\n\tconfig[\"docs\"][\"dir\"] = \"docs\"\n\tconfig[\"log\"] = {}\n\tconfig[\"log\"][\"level\"] = \"info\"\n\tconfig[\"log\"][\"access_log_format\"] = \"json\"\n\tconfig[\"log\"][\"filename\"] = \"access.log\"\n\tconfig[\"log\"]", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconf = dict()\n\tconf[\"tls\"] = dict()\n\tconf[\"authentication\"] = dict()\n\tconf[\"authorization\"] = dict()\n\tconf[\"etcd\"] = dict()\n\tconf[\"log\"] = dict()\n\tconf[\"docs\"] = dict()\n\n\tconf[\"tls\"][\"enabled\"] = False\n\tconf[\"authentication\"][\"enabled\"] = False\n\tconf[\"authorization\"][\"enabled\"] = False\n\tconf[\"etcd\"][\"endpoints\"] = [f\"http://{etcd_host}:{etcd_port}\"]\n\tconf[\"etcd\"][\"watch_interval\"] = 15\n\tconf[\"log\"][\"level\"] = \"info\"\n\tconf[\"docs\"][\"enabled\"] = False\n\tconf[\"docs\"][\"url\"] = \"/docs\"\n\tconf[\"docs\"][\"title\"] = \"API documentation\"\n\tconf[\"docs\"][\"description\"] = \"The API documentation is provided by Swagger UI.\"\n\tconf[\"docs\"][\"version\"] = \"1.0.0\"\n\tconf[\"docs\"][\"termsOfService\"] = \"http://swagger.io/terms/\"\n\tconf[\"docs\"][\"contact\"] = dict()\n\tconf[\"docs\"][\"contact\"][\"name\"] = \"Support\"\n\tconf[\"docs\"][\"contact\"][\"url\"] = \"http://www", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tbase = {\n\t\t\"tls\": {\n\t\t\t\"tls_cert\": \"\",\n\t\t\t\"tls_key\": \"\",\n\t\t\t\"tls_ca\": \"\",\n\t\t\t\"tls_verify\": False,\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"mode\": \"off\",\n\t\t\t\"cert\": \"tls_cert\",\n\t\t\t\"key\": \"tls_key\",\n\t\t\t\"client_cert_auth\": False,\n\t\t\t\"allowed_cn\": [],\n\t\t\t\"allowed_ou\": [],\n\t\t\t\"allowed_uri_sans\": [],\n\t\t\t\"allowed_names\": [],\n\t\t\t\"allowed_groups\": [],\n\t\t\t\"allowed_dns_sans\": [],\n\t\t\t\"allowed_email_sans\": [],\n\t\t\t\"allowed_uri_domains\": [],\n\t\t\t\"ttl\": \"15m\",\n\t\t},\n\t\t\"authorization\": {\"mode\": \"off\"},\n\t\t\"etcd\": {\"host\": etcd_host, \"port\": etcd_port, \"prefix\": \"/v3/kv\", \"version\": \"v3\"},\n\t\t\"docs\": {\n\t\t\t\"endpoint\": \"http://0.0.0.0:8000/docs\",\n\t\t\t\"prefix\": \"/v3/kv\",\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttry:\n\t\tzone = self.utcoffset(dt)\n\texcept NotImplementedError:\n\t\traise ValueError(\"fromutc() cannot convert naive timezone aware datetimes\")\n\treturn dt - zone\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz is None:\n\t\traise ValueError(\"datetime must have a timezone\")\n\n\tutc = dt.astimezone(pytz.utc)\n\n\treturn utc.replace(tzinfo=self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tres = dt.tzinfo.fromutc(dt)\n\tres = res.replace(tzinfo=self)\n\treturn res\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tutc = self._getutc(dt)\n\ttz = self._gettz(dt)\n\tif tz is not None:\n\t\tutc = utc.replace(tzinfo=tz)\n\treturn utc\n\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\told_tzinfo = dt.tzinfo\n\tnew_tzinfo = self._tzinfo\n\n\ttry:\n\t\tdt = dt.astimezone(None)\n\texcept TypeError:\n\t\tdt = dt.replace(tzinfo=None)\n\tdt = dt.replace(tzinfo=None)\n\n\tdt = new_tzinfo.localize(dt)\n\tdt = dt.astimezone(old_tzinfo)\n\n\treturn dt", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo\n\tdt = tz.localize(dt)\n\treturn dt.astimezone(self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tutcoffset = self.utcoffset(dt)\n\treturn dt + utcoffset", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\traise NotImplementedError(\"fromutc() is not implemented.\")\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tdttz = self.localize(dt, is_dst=None)\n\treturn dttz.astimezone(self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz_new = Timezone(self.tzname)\n\tdelta = tz_new.utcoffset(dt) - self.utcoffset(dt)\n\treturn dt + delta"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttry:\n\t\tzone = self.utcoffset(dt)\n\texcept NotImplementedError:\n\t\traise ValueError(\"fromutc() cannot convert naive timezone aware datetimes\")\n\treturn dt - zone\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz is None:\n\t\traise ValueError(\"datetime must have a timezone\")\n\n\tutc = dt.astimezone(pytz.utc)\n\n\treturn utc.replace(tzinfo=self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tres = dt.tzinfo.fromutc(dt)\n\tres = res.replace(tzinfo=self)\n\treturn res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tutc = self._getutc(dt)\n\ttz = self._gettz(dt)\n\tif tz is not None:\n\t\tutc = utc.replace(tzinfo=tz)\n\treturn utc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\told_tzinfo = dt.tzinfo\n\tnew_tzinfo = self._tzinfo\n\n\ttry:\n\t\tdt = dt.astimezone(None)\n\texcept TypeError:\n\t\tdt = dt.replace(tzinfo=None)\n\tdt = dt.replace(tzinfo=None)\n\n\tdt = new_tzinfo.localize(dt)\n\tdt = dt.astimezone(old_tzinfo)\n\n\treturn dt", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo\n\tdt = tz.localize(dt)\n\treturn dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tutcoffset = self.utcoffset(dt)\n\treturn dt + utcoffset", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\traise NotImplementedError(\"fromutc() is not implemented.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tdttz = self.localize(dt, is_dst=None)\n\treturn dttz.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz_new = Timezone(self.tzname)\n\tdelta = tz_new.utcoffset(dt) - self.utcoffset(dt)\n\treturn dt + delta", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n\tself.decades = int(self.decades)\n\tself.centuries = int(self.centuries)\n\tself.millennia = int(self.millennia)\n\tself.eras = int(self.eras)\n\tself.babylon = int(self.babylon)\n\tself.christ = int(self.christ)\n\tself.julius = int(self.julius)\n\tself.greek = int(self.greek)\n\tself.roman = int(self.roman)\n\tself.middle = int(self.middle)\n\tself.modern = int(self.modern)\n\tself.postmodern = int(self.postmodern)\n\tself.contemporary = int(self.contemporary)\n\tself.future = int(self.future)\n\tself.distant = int(self.distant)", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tdays = self.days\n\tseconds = self.seconds % 60\n\tminutes = (self.seconds // 60) % 60\n\thours = (self.seconds // 3600) % 24\n\treturn TimeDelta(days=days, hours=hours, minutes=minutes, seconds=seconds)", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tpass", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.ms = int(self.ms)\n\tself.s = int(self.s)\n\tself.m = int(self.m)\n\tself.h = int(self.h)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tnormalized_self = self\n\n\tfor field in self.fields:\n\t\tif field in ['days', 'hours', 'minutes', 'seconds']:\n\t\t\tnormalized_self[field] = self[field] % self[field].max\n\n\treturn normalized_self\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tfrom .. import core\n\n\tself._normalize = True\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(Normalize, self).__init__(*args, **kwargs)", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.normalize_days()\n\tself.normalize_hours()\n\tself.normalize_minutes()\n\tself.normalize_seconds()\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.years = self.years % 100\n\tself.months = self.months % 12\n\tself.weeks = self.weeks % 52\n\tself.days = self.days % 7\n\tself.hours = self.hours % 24\n\tself.minutes = self.minutes % 60\n\tself.seconds = self.seconds % 60", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tmultiplier = self.unit_multiplier()\n\tmodified = self % multiplier\n\treturn self - modified + (multiplier if modified > multiplier / 2 else 0)", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.normalize_months()\n\tself.normalize_years()\n\tself.normalize_days()\n\tself.normalize_hours()\n\tself.normalize_minutes()\n\tself.normalize_seconds()\n\treturn self\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n\tself.decades = int(self.decades)\n\tself.centuries = int(self.centuries)\n\tself.millennia = int(self.millennia)\n\tself.eras = int(self.eras)\n\tself.babylon = int(self.babylon)\n\tself.christ = int(self.christ)\n\tself.julius = int(self.julius)\n\tself.greek = int(self.greek)\n\tself.roman = int(self.roman)\n\tself.middle = int(self.middle)\n\tself.modern = int(self.modern)\n\tself.postmodern = int(self.postmodern)\n\tself.contemporary = int(self.contemporary)\n\tself.future = int(self.future)\n\tself.distant = int(self.distant)", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tdays = self.days\n\tseconds = self.seconds % 60\n\tminutes = (self.seconds // 60) % 60\n\thours = (self.seconds // 3600) % 24\n\treturn TimeDelta(days=days, hours=hours, minutes=minutes, seconds=seconds)", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.ms = int(self.ms)\n\tself.s = int(self.s)\n\tself.m = int(self.m)\n\tself.h = int(self.h)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tnormalized_self = self\n\n\tfor field in self.fields:\n\t\tif field in ['days', 'hours', 'minutes', 'seconds']:\n\t\t\tnormalized_self[field] = self[field] % self[field].max\n\n\treturn normalized_self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tfrom .. import core\n\n\tself._normalize = True\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(Normalize, self).__init__(*args, **kwargs)", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.normalize_days()\n\tself.normalize_hours()\n\tself.normalize_minutes()\n\tself.normalize_seconds()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.years = self.years % 100\n\tself.months = self.months % 12\n\tself.weeks = self.weeks % 52\n\tself.days = self.days % 7\n\tself.hours = self.hours % 24\n\tself.minutes = self.minutes % 60\n\tself.seconds = self.seconds % 60", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tmultiplier = self.unit_multiplier()\n\tmodified = self % multiplier\n\treturn self - modified + (multiplier if modified > multiplier / 2 else 0)", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.normalize_months()\n\tself.normalize_years()\n\tself.normalize_days()\n\tself.normalize_hours()\n\tself.normalize_minutes()\n\tself.normalize_seconds()\n\treturn self\n", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef _new_func(*args, **kwargs):\n\t\tres = namefunc(*args, **kwargs)\n\t\tif isinstance(res, unicode):\n\t\t\tres = res.encode('utf-8')\n\t\treturn res\n\treturn _new_func\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef func(*args, **kwargs):\n\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\treturn func", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(self, tzinfo):\n\t\tname = namefunc(self, tzinfo)\n\t\tif sys.version_info[0] == 2:\n\t\t\tname = name.encode('utf-8')\n\t\treturn name\n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef _wrapper(self, *args, **kwargs):\n\t\tname = namefunc(self, *args, **kwargs)\n\t\tif sys.version_info[:2] == (2, 5):\n\t\t\treturn name.encode('utf8')\n\t\treturn name\n\treturn _wrapper", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tname = namefunc(*args, **kwargs)\n\t\tif isinstance(name, unicode):\n\t\t\tname = name.encode('utf-8')\n\t\treturn name\n\treturn wrapper", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(self, dt, **kwargs):\n\t\tpytz_name = namefunc(self, dt, **kwargs)\n\t\tif not isinstance(pytz_name, str):\n\t\t\t# Python 2\n\t\t\tpytz_name = pytz_name.encode('ascii')\n\t\treturn pytz_name\n\n\twrapper.__name__ = namefunc.__name__\n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tif sys.version_info[0] == 2:\n\t\t\treturn unicode_to_bytestring(\n\t\t\t\tnamefunc(*args, **kwargs),\n\t\t\t\tencoding=\"UTF-8\"\n\t\t\t)\n\t\telse:\n\t\t\treturn namefunc(*args, **kwargs)\n\treturn wrapper\n\n\ntzname_in_python2.__doc__ = tzname_in_python2.__doc__.format(\n\tunicode_to_bytestring=unicode_to_bytestring.__doc__\n)\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\timport sys\n\t\tif sys.version_info.major == 2:\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(*args, **kwargs)\n\treturn wrapper\n\n\n@tzname_in_python2", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef inner(*args, **kwargs):\n\t\ttry:\n\t\t\tresult = namefunc(*args, **kwargs)\n\t\t\tif isinstance(result, unicode):\n\t\t\t\tresult = result.encode('ascii')\n\t\t\treturn result\n\t\texcept UnicodeDecodeError:\n\t\t\treturn namefunc(*args, **kwargs)\n\treturn inner\n\nfrom pytz import timezone", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif sys.version_info[0] < 3:\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\ntzname_in_python2.__name__ = tzname_in_python2.__wrapped__.__name__\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef _new_func(*args, **kwargs):\n\t\tres = namefunc(*args, **kwargs)\n\t\tif isinstance(res, unicode):\n\t\t\tres = res.encode('utf-8')\n\t\treturn res\n\treturn _new_func\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef func(*args, **kwargs):\n\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\treturn func", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(self, tzinfo):\n\t\tname = namefunc(self, tzinfo)\n\t\tif sys.version_info[0] == 2:\n\t\t\tname = name.encode('utf-8')\n\t\treturn name\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef _wrapper(self, *args, **kwargs):\n\t\tname = namefunc(self, *args, **kwargs)\n\t\tif sys.version_info[:2] == (2, 5):\n\t\t\treturn name.encode('utf8')\n\t\treturn name\n\treturn _wrapper", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tname = namefunc(*args, **kwargs)\n\t\tif isinstance(name, unicode):\n\t\t\tname = name.encode('utf-8')\n\t\treturn name\n\treturn wrapper", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(self, dt, **kwargs):\n\t\tpytz_name = namefunc(self, dt, **kwargs)\n\t\tif not isinstance(pytz_name, str):\n\t\t\t# Python 2\n\t\t\tpytz_name = pytz_name.encode('ascii')\n\t\treturn pytz_name\n\n\twrapper.__name__ = namefunc.__name__\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tif sys.version_info[0] == 2:\n\t\t\treturn unicode_to_bytestring(\n\t\t\t\tnamefunc(*args, **kwargs),\n\t\t\t\tencoding=\"UTF-8\"\n\t\t\t)\n\t\telse:\n\t\t\treturn namefunc(*args, **kwargs)\n\treturn wrapper\n\n\ntzname_in_python2.__doc__ = tzname_in_python2.__doc__.format(\n\tunicode_to_bytestring=unicode_to_bytestring.__doc__\n)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\timport sys\n\t\tif sys.version_info.major == 2:\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(*args, **kwargs)\n\treturn wrapper\n\n\n@tzname_in_python2", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef inner(*args, **kwargs):\n\t\ttry:\n\t\t\tresult = namefunc(*args, **kwargs)\n\t\t\tif isinstance(result, unicode):\n\t\t\t\tresult = result.encode('ascii')\n\t\t\treturn result\n\t\texcept UnicodeDecodeError:\n\t\t\treturn namefunc(*args, **kwargs)\n\treturn inner\n\nfrom pytz import timezone", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif sys.version_info[0] < 3:\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\ntzname_in_python2.__name__ = tzname_in_python2.__wrapped__.__name__\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tversion = open('version.txt').read().strip()\n\texcept Exception:\n\t\tversion = '0.0.1'\n\treturn version\n\n\n__version__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\tversions = {}\n\ttry:\n\t\tfrom ._version import version as _version\n\t\tversions[\"version\"] = _version\n\texcept:\n\t\tversions[\"version\"] = \"0.0.1\"\n\n\ttry:\n\t\tfrom ._git_revision import git_revision as _git_revision\n\t\tversions[\"git_revision\"] = _git_revision\n\texcept:\n\t\tversions[\"git_revision\"] = None\n\n\treturn versions", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\tversion = pkg_resources.require(\"talon\")[0].version\n\texcept Exception:\n\t\tversion = \"0.1\"\n\treturn version\n\n__version__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom setuptools_scm import get_version\n\t\tfrom setuptools_scm.version import get_local_dirty_tag\n\t\tfrom setuptools_scm.git import parse\n\n\t\tdef clean_scheme(version):\n\t\t\t\"\"\"\n\t\t\tRemove dirty tag (+dirty) from the version string.\n\t\t\t\"\"\"\n\t\t\treturn parse(version).format_with(\"{tag}\")\n\n\t\tdef local_scheme(version):\n\t\t\t\"\"\"\n\t\t\tUsed to determine whether a dirty tag is appended to the version.\n\n\t\t\tThis returns the local version if it's not dirty, otherwise an empty string.\n\t\t\t\"\"\"\n\t\t\tif get_local_dirty_tag():\n\t\t\t\treturn \"\"\n\t\t\telse:\n\t\t\t\treturn version\n\n\t\tversion = get_version(\n\t\t\troot=\"../..\",\n\t\t\trelative_to=__file__,\n\t\t\tversion_scheme=\"guess-next-dev\",\n\t\t\tlocal_scheme=local_scheme,\n\t\t\tclean_scheme=clean_scheme,\n\t\t)\n\texcept Exception:\n\t\tversion = \"0.0.0\"\n\n\treturn version\n\n\n__version__ = get_versions()\ndel get_versions", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom ._version import version as __version__\n\texcept ImportError:\n\t\t__version__ = \"0.0.0\"\n\n\treturn __version__\n\n\n__version__ = get_versions()\ndel get_versions", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\timport os\n\n\tdefault_version = \"1.0.0\"\n\tversion_file = os.path.join(os.path.dirname(__file__), \"VERSION\")\n\ttry:\n\t\twith open(version_file, \"r\") as ver_file:\n\t\t\tversion = ver_file.readline().strip()\n\texcept IOError:\n\t\tversion = default_version\n\treturn version\n\n\n__version__ = get_versions()\n\ndel get_versions", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport importlib.metadata as importlib_metadata\n\texcept ImportError:\n\t\timport importlib_metadata\n\n\t# Read the version information from the metadata of the package\n\tversion = importlib_metadata.version(\"pyqt5-widgets\")\n\tversion_data = version.split(\"+\")\n\n\t# If there are more than one parts, then it is a development version.\n\t# If there are only two parts, then it is a release version.\n\tif len(version_data) > 2:\n\t\tversion = \".\".join(version_data[:-1])\n\t\tversion_build = version_data[-1]\n\telse:\n\t\tversion = version_data[0]\n\t\tversion_build = None\n\n\treturn version, version_build\n\n\n__version__, __version_build__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\t# TODO: Return the version information from the file.\n\t\n\treturn {\n\t\t\"game_version\": \"1.0\",\n\t\t\"revision_hash\": \"0000000000000000000000000000000000000000\",\n\t\t\"revision_date\": \"0000-00-00\",\n\t\t\"revision_time\": \"00:00:00\"\n\t}\n\t\n\n# Gets the version information.\nVERSIONS = get_versions()\n\n\n# Prints the version information.\nif __name__ == \"__main__\":\n\tprint \"Game version:\", VERSIONS[\"game_version\"]\n\tprint \"Revision hash:\", VERSIONS[\"revision_hash\"]\n\tprint \"Revision date:\", VERSIONS[\"revision_date\"]\n\tprint \"Revision time:\", VERSIONS[\"revision_time\"]", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\tversion_file = pkg_resources.resource_string(__name__, 'VERSION')\n\t\tversion = version_file.decode('utf-8')\n\texcept:\n\t\tversion = '0.0.0'\n\t\n\treturn version\n\n__version__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom setuptools_scm import get_version\n\t\tversion = get_version(\n\t\t\troot='..',\n\t\t\trelative_to=__file__,\n\t\t\tversion_scheme='no-guess-dev',\n\t\t\tlocal_scheme='node-and-date'\n\t\t)\n\texcept (ImportError, LookupError):\n\t\tversion = '0.0.0'\n\n\treturn version\n\n\n__version__ = get_versions()"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tversion = open('version.txt').read().strip()\n\texcept Exception:\n\t\tversion = '0.0.1'\n\treturn version\n\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\tversions = {}\n\ttry:\n\t\tfrom ._version import version as _version\n\t\tversions[\"version\"] = _version\n\texcept:\n\t\tversions[\"version\"] = \"0.0.1\"\n\n\ttry:\n\t\tfrom ._git_revision import git_revision as _git_revision\n\t\tversions[\"git_revision\"] = _git_revision\n\texcept:\n\t\tversions[\"git_revision\"] = None\n\n\treturn versions", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\tversion = pkg_resources.require(\"talon\")[0].version\n\texcept Exception:\n\t\tversion = \"0.1\"\n\treturn version\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom setuptools_scm import get_version\n\t\tfrom setuptools_scm.version import get_local_dirty_tag\n\t\tfrom setuptools_scm.git import parse\n\n\t\tdef clean_scheme(version):\n\t\t\t\"\"\"\n\t\t\tRemove dirty tag (+dirty) from the version string.\n\t\t\t\"\"\"\n\t\t\treturn parse(version).format_with(\"{tag}\")\n\n\t\tdef local_scheme(version):\n\t\t\t\"\"\"\n\t\t\tUsed to determine whether a dirty tag is appended to the version.\n\n\t\t\tThis returns the local version if it's not dirty, otherwise an empty string.\n\t\t\t\"\"\"\n\t\t\tif get_local_dirty_tag():\n\t\t\t\treturn \"\"\n\t\t\telse:\n\t\t\t\treturn version\n\n\t\tversion = get_version(\n\t\t\troot=\"../..\",\n\t\t\trelative_to=__file__,\n\t\t\tversion_scheme=\"guess-next-dev\",\n\t\t\tlocal_scheme=local_scheme,\n\t\t\tclean_scheme=clean_scheme,\n\t\t)\n\texcept Exception:\n\t\tversion = \"0.0.0\"\n\n\treturn version\n\n\n__version__ = get_versions()\ndel get_versions", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom ._version import version as __version__\n\texcept ImportError:\n\t\t__version__ = \"0.0.0\"\n\n\treturn __version__\n\n\n__version__ = get_versions()\ndel get_versions", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\timport os\n\n\tdefault_version = \"1.0.0\"\n\tversion_file = os.path.join(os.path.dirname(__file__), \"VERSION\")\n\ttry:\n\t\twith open(version_file, \"r\") as ver_file:\n\t\t\tversion = ver_file.readline().strip()\n\texcept IOError:\n\t\tversion = default_version\n\treturn version\n\n\n__version__ = get_versions()\n\ndel get_versions", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport importlib.metadata as importlib_metadata\n\texcept ImportError:\n\t\timport importlib_metadata\n\n\t# Read the version information from the metadata of the package\n\tversion = importlib_metadata.version(\"pyqt5-widgets\")\n\tversion_data = version.split(\"+\")\n\n\t# If there are more than one parts, then it is a development version.\n\t# If there are only two parts, then it is a release version.\n\tif len(version_data) > 2:\n\t\tversion = \".\".join(version_data[:-1])\n\t\tversion_build = version_data[-1]\n\telse:\n\t\tversion = version_data[0]\n\t\tversion_build = None\n\n\treturn version, version_build\n\n\n__version__, __version_build__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\t# TODO: Return the version information from the file.\n\t\n\treturn {\n\t\t\"game_version\": \"1.0\",\n\t\t\"revision_hash\": \"0000000000000000000000000000000000000000\",\n\t\t\"revision_date\": \"0000-00-00\",\n\t\t\"revision_time\": \"00:00:00\"\n\t}\n\t\n\n# Gets the version information.\nVERSIONS = get_versions()\n\n\n# Prints the version information.\nif __name__ == \"__main__\":\n\tprint \"Game version:\", VERSIONS[\"game_version\"]\n\tprint \"Revision hash:\", VERSIONS[\"revision_hash\"]\n\tprint \"Revision date:\", VERSIONS[\"revision_date\"]\n\tprint \"Revision time:\", VERSIONS[\"revision_time\"]", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\tversion_file = pkg_resources.resource_string(__name__, 'VERSION')\n\t\tversion = version_file.decode('utf-8')\n\texcept:\n\t\tversion = '0.0.0'\n\t\n\treturn version\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom setuptools_scm import get_version\n\t\tversion = get_version(\n\t\t\troot='..',\n\t\t\trelative_to=__file__,\n\t\t\tversion_scheme='no-guess-dev',\n\t\t\tlocal_scheme='node-and-date'\n\t\t)\n\texcept (ImportError, LookupError):\n\t\tversion = '0.0.0'\n\n\treturn version\n\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1]\n\tif closet_tag.contains(\"+\"):\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[0][-1]\n\treturn \".\" if \"+\" in closet_tag else \"+\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset = pieces[0].split(\"/\")[1]\n\tdot = \".\" if \"+\" not in closet else \"+\"\n\treturn dot\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\ttag_pieces = pieces[0].split(\" \")[0]\n\n\tif \"+\" in tag_pieces:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1][0]\n\tif closet_tag.find(\"+\") >= 0:\n\t\treturn \".\"\n\treturn \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\tlast = pieces[-1]\n\tif \"+\" in last.split(\"-\")[0]:\n\t\treturn \".\"\n\treturn \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1][1]\n\tif closet_tag.find(\"+\") == -1:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1]\n\tplus_or_dot = \".\"\n\tif \"+\" in closet_tag:\n\t\tplus_or_dot = \"+\"\n\treturn plus_or_dot\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset = pieces[-1]\n\tif \"+\" in closet:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1]\n\tif pieces[-1].count(\"+\") > 0:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1]\n\tif closet_tag.contains(\"+\"):\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[0][-1]\n\treturn \".\" if \"+\" in closet_tag else \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset = pieces[0].split(\"/\")[1]\n\tdot = \".\" if \"+\" not in closet else \"+\"\n\treturn dot\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\ttag_pieces = pieces[0].split(\" \")[0]\n\n\tif \"+\" in tag_pieces:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1][0]\n\tif closet_tag.find(\"+\") >= 0:\n\t\treturn \".\"\n\treturn \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\tlast = pieces[-1]\n\tif \"+\" in last.split(\"-\")[0]:\n\t\treturn \".\"\n\treturn \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1][1]\n\tif closet_tag.find(\"+\") == -1:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1]\n\tplus_or_dot = \".\"\n\tif \"+\" in closet_tag:\n\t\tplus_or_dot = \"+\"\n\treturn plus_or_dot\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset = pieces[-1]\n\tif \"+\" in closet:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[-1]\n\tif pieces[-1].count(\"+\") > 0:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tfrom sys import stderr\n\n\tdef f(*args):\n\t\tPopen(*args, **{\n\t\t\t'stdout': PIPE,\n\t\t\t'stderr': PIPE if hide_stderr else stderr,\n\t\t\t'stdin': PIPE,\n\t\t\t'cwd': cwd,\n\t\t\t'env': env,\n\t\t}).wait()\n\n\tdef echo(*args):\n\t\tif verbose:\n\t\t\tprint(*args)\n\n\techo('$', *commands, *args)\n\tf(commands, args)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef _run_command(commands, args, cwd, env):\n\t\timport subprocess\n\t\tproc = subprocess.Popen(\n\t\t\tcommands,\n\t\t\targs,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t\tcwd=cwd,\n\t\t\tenv=env\n\t\t)\n\t\tstdout, stderr = proc.communicate()\n\t\tif proc.returncode != 0:\n\t\t\traise subprocess.CalledProcessError(proc.returncode, commands, stdout, stderr)\n\t\treturn stdout\n\n\timport os\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\timport subprocess\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\ttry:\n\t\tstdout = _run_command(commands, args, cwd, env)\n\texcept subprocess.CalledProcessError as e:\n\t\tstdout = e.output\n\t\tif hide_stderr:\n\t\t\tprint(e.output)\n\t\telse:\n\t\t\tprint(e.output)\n\t\t\traise\n\tif verbose:\n\t\tprint(stdout)\n\treturn stdout", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom distutils import spawn\n\timport os\n\timport subprocess\n\n\tdef run_command_verbose(command, args, cwd=None, env=None):\n\t\tprint(\"+\", command, \" \".join(args))\n\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env)\n\n\tdef run_command_quiet(command, args, cwd=None, env=None):\n\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env, stderr=subprocess.STDOUT)\n\n\tif verbose:\n\t\trun_command = run_command_verbose\n\telse:\n\t\trun_command = run_command_quiet\n\n\tif type(commands) is str:\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tcommand = spawn.find_executable(command)\n\t\tif not command:\n\t\t\traise Exception(\"command not found: \" + command)\n\t\trun_command(command, args, cwd, env)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommands = commands.split()\n\targs = args.split()\n\tenv = env or {}\n\targs.extend(commands)\n\tenv.update({\"PATH\": os.getenv(\"PATH\")})\n\tif verbose:\n\t\tprint(\"Running command: %s\" % \" \".join(args))\n\tprocess = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tstdout, stderr = process.communicate()\n\tret = process.returncode\n\tif ret != 0:\n\t\tif hide_stderr:\n\t\t\traise Exception(\"Command %s failed: %s (%s)\" % (\" \".join(args), stderr, ret))\n\t\telse:\n\t\t\traise Exception(\"Command %s failed: %s (%s) (stderr: %s)\" % (\" \".join(args), stdout, ret, stderr))\n\tif stdout:\n\t\tprint(stdout)\n\treturn stdout", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs = ' '.join(args)\n\tprint(\">>>\", args)\n\tfrom subprocess import Popen, PIPE\n\tprocess = Popen(commands + args,\n\t\tshell=True, stdout=PIPE, stdin=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\toutput, error = process.communicate()\n\tif verbose:\n\t\tprint \"Command:\", commands + args\n\t\tprint \"Output:\", output\n\t\tprint \"Error:\", error\n\tif process.returncode != 0:\n\t\traise Exception(\"Command %s failed with exit code %d\" % (commands + args, process.returncode))\n\tif hide_stderr:\n\t\treturn output\n\telse:\n\t\treturn output, error\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport logging\n\timport shlex\n\t\n\tlogging.debug('run_command: %s' % commands)\n\tlogging.debug('run_command: args: %s' % args)\n\tlogging.debug('run_command: cwd: %s' % cwd)\n\tlogging.debug('run_command: verbose: %s' % verbose)\n\tlogging.debug('run_command: hide_stderr: %s' % hide_stderr)\n\tlogging.debug('run_command: env: %s' % env)\n\t\n\tcommands = shlex.split(commands)\n\targs = shlex.split(args)\n\targs.extend(commands)\n\t\n\tlogging.debug('run_command: args: %s' % args)\n\t\n\tif cwd:\n\t\tlogging.debug('run_command: cwd: %s' % cwd)\n\t\tlogging.debug('run_command: command: %s %s' % (cwd, commands))\n\t\tlogging.debug('run_command: args: %s' % args)\n\t\tprocess = subprocess.Popen(args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\telse:\n\t\t", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tcommands = [commands] if type(commands) is str else commands\n\tstdout = PIPE if verbose else PIPE if args else None\n\tstderr = None if hide_stderr else PIPE\n\tkwargs = {\n\t\t'stdout': stdout,\n\t\t'stderr': stderr,\n\t\t'cwd': cwd,\n\t\t'env': env,\n\t}\n\tif args:\n\t\tkwargs['stdin'] = PIPE\n\t\tkwargs['input'] = args if type(args) is str else '\\n'.join(args)\n\tkwargs = {k: v for k, v in kwargs.items() if v is not None}\n\tprocess = Popen(\n\t\tcommands,\n\t\tshell=isinstance(commands, str),\n\t\t**kwargs,\n\t)\n\toutput = ''\n\twhile process.poll() is None:\n\t\toutput += process.stdout.readline()\n\toutput += process.stdout.read()\n\tretcode = process.poll()\n\toutput = output.strip()\n\tif retcode:\n\t\traise RuntimeError(output)\n\treturn output", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\toutput = []\n\n\tdef call(cmd):\n\t\ttry:\n\t\t\tpipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\t\tout, err = pipe.communicate()\n\n\t\t\tif sys.version_info[0] > 2:\n\t\t\t\tout = out.decode(\"utf-8\")\n\t\t\t\terr = err.decode(\"utf-8\")\n\n\t\t\toutput.append(out)\n\t\t\tif err and not hide_stderr:\n\t\t\t\toutput.append(err)\n\t\t\tif verbose:\n\t\t\t\tsys.stdout.write(out)\n\t\t\t\tsys.stdout.flush()\n\t\t\treturn pipe.returncode\n\t\texcept KeyboardInterrupt:\n\t\t\treturn 1\n\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tcommands = [cmd.format(**args) for cmd in commands]\n\n\tif verbose:\n\t\tprint(' '.join(commands[0].split()))\n\n\tfor cmd in commands:\n\t\tif call(cmd.split()) != 0:\n\t\t\treturn output", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\ttry:\n\t\tif verbose:\n\t\t\tprint('Running command: {0}'.format(commands))\n\t\tprocess = subprocess.Popen(commands,\n\t\t\tcwd=cwd,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t\tenv=env)\n\n\t\tstdout, stderr = process.communicate()\n\texcept:\n\t\tprint('Failed to run command: {0}'.format(commands))\n\t\traise\n\n\tif verbose:\n\t\tprint('Command output:\\n{0}'.format(stdout))\n\t\tif not sys.stdout.isatty():\n\t\t\tsys.stdout.write(stdout)\n\n\tif hide_stderr:\n\t\treturn stdout\n\telse:\n\t\treturn stdout, stderr\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\tif isinstance(commands, basestring):\n\t\tcommands = [commands]\n\tif isinstance(args, basestring):\n\t\targs = [args]\n\targs = ' '.join(args)\n\tif verbose:\n\t\tprint(commands[0], args)\n\tif 1:\n\t\tif hide_stderr:\n\t\t\toutput = subprocess.check_output(\n\t\t\t\tcommands,\n\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\tshell=True,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env)\n\t\telse:\n\t\t\toutput = subprocess.check_output(\n\t\t\t\tcommands,\n\t\t\t\tshell=True,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env)\n\t\toutput = output.decode('utf-8')\n\t\treturn output\n\telse:\n\t\tp = subprocess.Popen(\n\t\t\tcommands,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t\tshell=True,\n\t\t\tcwd=cwd,\n\t\t\tenv=env)\n\t\tout, err = p.communicate()\n\t\tout = out.decode('utf-8')\n\t\terr = err.decode"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tfrom sys import stderr\n\n\tdef f(*args):\n\t\tPopen(*args, **{\n\t\t\t'stdout': PIPE,\n\t\t\t'stderr': PIPE if hide_stderr else stderr,\n\t\t\t'stdin': PIPE,\n\t\t\t'cwd': cwd,\n\t\t\t'env': env,\n\t\t}).wait()\n\n\tdef echo(*args):\n\t\tif verbose:\n\t\t\tprint(*args)\n\n\techo('$', *commands, *args)\n\tf(commands, args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef _run_command(commands, args, cwd, env):\n\t\timport subprocess\n\t\tproc = subprocess.Popen(\n\t\t\tcommands,\n\t\t\targs,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t\tcwd=cwd,\n\t\t\tenv=env\n\t\t)\n\t\tstdout, stderr = proc.communicate()\n\t\tif proc.returncode != 0:\n\t\t\traise subprocess.CalledProcessError(proc.returncode, commands, stdout, stderr)\n\t\treturn stdout\n\n\timport os\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\timport subprocess\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\ttry:\n\t\tstdout = _run_command(commands, args, cwd, env)\n\texcept subprocess.CalledProcessError as e:\n\t\tstdout = e.output\n\t\tif hide_stderr:\n\t\t\tprint(e.output)\n\t\telse:\n\t\t\tprint(e.output)\n\t\t\traise\n\tif verbose:\n\t\tprint(stdout)\n\treturn stdout", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom distutils import spawn\n\timport os\n\timport subprocess\n\n\tdef run_command_verbose(command, args, cwd=None, env=None):\n\t\tprint(\"+\", command, \" \".join(args))\n\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env)\n\n\tdef run_command_quiet(command, args, cwd=None, env=None):\n\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env, stderr=subprocess.STDOUT)\n\n\tif verbose:\n\t\trun_command = run_command_verbose\n\telse:\n\t\trun_command = run_command_quiet\n\n\tif type(commands) is str:\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tcommand = spawn.find_executable(command)\n\t\tif not command:\n\t\t\traise Exception(\"command not found: \" + command)\n\t\trun_command(command, args, cwd, env)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommands = commands.split()\n\targs = args.split()\n\tenv = env or {}\n\targs.extend(commands)\n\tenv.update({\"PATH\": os.getenv(\"PATH\")})\n\tif verbose:\n\t\tprint(\"Running command: %s\" % \" \".join(args))\n\tprocess = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tstdout, stderr = process.communicate()\n\tret = process.returncode\n\tif ret != 0:\n\t\tif hide_stderr:\n\t\t\traise Exception(\"Command %s failed: %s (%s)\" % (\" \".join(args), stderr, ret))\n\t\telse:\n\t\t\traise Exception(\"Command %s failed: %s (%s) (stderr: %s)\" % (\" \".join(args), stdout, ret, stderr))\n\tif stdout:\n\t\tprint(stdout)\n\treturn stdout", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs = ' '.join(args)\n\tprint(\">>>\", args)\n\tfrom subprocess import Popen, PIPE\n\tprocess = Popen(commands + args,\n\t\tshell=True, stdout=PIPE, stdin=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\toutput, error = process.communicate()\n\tif verbose:\n\t\tprint \"Command:\", commands + args\n\t\tprint \"Output:\", output\n\t\tprint \"Error:\", error\n\tif process.returncode != 0:\n\t\traise Exception(\"Command %s failed with exit code %d\" % (commands + args, process.returncode))\n\tif hide_stderr:\n\t\treturn output\n\telse:\n\t\treturn output, error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport logging\n\timport shlex\n\t\n\tlogging.debug('run_command: %s' % commands)\n\tlogging.debug('run_command: args: %s' % args)\n\tlogging.debug('run_command: cwd: %s' % cwd)\n\tlogging.debug('run_command: verbose: %s' % verbose)\n\tlogging.debug('run_command: hide_stderr: %s' % hide_stderr)\n\tlogging.debug('run_command: env: %s' % env)\n\t\n\tcommands = shlex.split(commands)\n\targs = shlex.split(args)\n\targs.extend(commands)\n\t\n\tlogging.debug('run_command: args: %s' % args)\n\t\n\tif cwd:\n\t\tlogging.debug('run_command: cwd: %s' % cwd)\n\t\tlogging.debug('run_command: command: %s %s' % (cwd, commands))\n\t\tlogging.debug('run_command: args: %s' % args)\n\t\tprocess = subprocess.Popen(args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\telse:\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tcommands = [commands] if type(commands) is str else commands\n\tstdout = PIPE if verbose else PIPE if args else None\n\tstderr = None if hide_stderr else PIPE\n\tkwargs = {\n\t\t'stdout': stdout,\n\t\t'stderr': stderr,\n\t\t'cwd': cwd,\n\t\t'env': env,\n\t}\n\tif args:\n\t\tkwargs['stdin'] = PIPE\n\t\tkwargs['input'] = args if type(args) is str else '\\n'.join(args)\n\tkwargs = {k: v for k, v in kwargs.items() if v is not None}\n\tprocess = Popen(\n\t\tcommands,\n\t\tshell=isinstance(commands, str),\n\t\t**kwargs,\n\t)\n\toutput = ''\n\twhile process.poll() is None:\n\t\toutput += process.stdout.readline()\n\toutput += process.stdout.read()\n\tretcode = process.poll()\n\toutput = output.strip()\n\tif retcode:\n\t\traise RuntimeError(output)\n\treturn output", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\toutput = []\n\n\tdef call(cmd):\n\t\ttry:\n\t\t\tpipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\t\tout, err = pipe.communicate()\n\n\t\t\tif sys.version_info[0] > 2:\n\t\t\t\tout = out.decode(\"utf-8\")\n\t\t\t\terr = err.decode(\"utf-8\")\n\n\t\t\toutput.append(out)\n\t\t\tif err and not hide_stderr:\n\t\t\t\toutput.append(err)\n\t\t\tif verbose:\n\t\t\t\tsys.stdout.write(out)\n\t\t\t\tsys.stdout.flush()\n\t\t\treturn pipe.returncode\n\t\texcept KeyboardInterrupt:\n\t\t\treturn 1\n\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tcommands = [cmd.format(**args) for cmd in commands]\n\n\tif verbose:\n\t\tprint(' '.join(commands[0].split()))\n\n\tfor cmd in commands:\n\t\tif call(cmd.split()) != 0:\n\t\t\treturn output", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\ttry:\n\t\tif verbose:\n\t\t\tprint('Running command: {0}'.format(commands))\n\t\tprocess = subprocess.Popen(commands,\n\t\t\tcwd=cwd,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t\tenv=env)\n\n\t\tstdout, stderr = process.communicate()\n\texcept:\n\t\tprint('Failed to run command: {0}'.format(commands))\n\t\traise\n\n\tif verbose:\n\t\tprint('Command output:\\n{0}'.format(stdout))\n\t\tif not sys.stdout.isatty():\n\t\t\tsys.stdout.write(stdout)\n\n\tif hide_stderr:\n\t\treturn stdout\n\telse:\n\t\treturn stdout, stderr\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\tif isinstance(commands, basestring):\n\t\tcommands = [commands]\n\tif isinstance(args, basestring):\n\t\targs = [args]\n\targs = ' '.join(args)\n\tif verbose:\n\t\tprint(commands[0], args)\n\tif 1:\n\t\tif hide_stderr:\n\t\t\toutput = subprocess.check_output(\n\t\t\t\tcommands,\n\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\tshell=True,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env)\n\t\telse:\n\t\t\toutput = subprocess.check_output(\n\t\t\t\tcommands,\n\t\t\t\tshell=True,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env)\n\t\toutput = output.decode('utf-8')\n\t\treturn output\n\telse:\n\t\tp = subprocess.Popen(\n\t\t\tcommands,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE,\n\t\t\tshell=True,\n\t\t\tcwd=cwd,\n\t\t\tenv=env)\n\t\tout, err = p.communicate()\n\t\tout = out.decode('utf-8')\n\t\terr = err.decode", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom ._versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'pymoose/__init__.py'\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = 'pymoose-'\n\treturn config\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer import VersioneerConfig\n\tcfg = VersioneerConfig()\n\tcfg.version_file_source = 'setup.py'\n\tcfg.version_script = 'setup.py'\n\tcfg.parentdir_prefix_version_dev_template = '''\\\n# This file is automatically generated by versioneer.\n# Please don't edit this file directly.\n\nfrom __future__ import print_function\nimport sys\n\n__version__ = '{version}'\n'''\n\tcfg.tag_prefix = ''\n\tcfg.parentdir_prefix = 'GitPython-'\n\tcfg.version_dev_template = '''\\\n# This file is automatically generated by versioneer.\n# Please don't edit this file directly.\n\nfrom __future__ import print_function\nimport sys\n\nversion = '{version}'\nfull_version = '{full_version}'\ngit_revision = '{git_revision}'\ngit_branch = '{git_branch}'\n\n\nif not sys.version_info >= (2, 7):\n    raise RuntimeError(\"GitPython requires Python 2.7 or higher.\")\nif not sys.version_info >= (3, 3):\n    raise RuntimeError(\"GitPython requires Python 3.3 or higher.\")\n'''\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.version_file_source = \"versioneer/_version.py\"\n\tconfig.version_file = \"_version.py\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"py-\"\n\treturn config", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer.versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.v_file = \"versioneer.py\"\n\tconfig.v_keywords = [\"version\"]\n\tconfig.parentdir_prefix_sep = \".\"\n\tconfig.style = \"pep440\"\n\tconfig.versionfile_source = \"pyproject.toml\"\n\tconfig.versionfile_build = \"pyproject.toml\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"moban\"\n\treturn config", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\timport os\n\timport sys\n\tfrom versioneer.versioneer import VersioneerConfig\n\n\tconfig = VersioneerConfig()\n\tconfig.parentdir_prefix_sep = '__prefix__'\n\tconfig.parentdir_prefix = 'VERSIONEER'\n\n\t# Note that this is the git repo root, not the python package root\n\tconfig.root = os.path.abspath(\n\t\tos.path.join(\n\t\t\tos.path.dirname(__file__),\n\t\t\t'..'))\n\n\t# The name of the attribute to set in each package's __init__.py\n\tconfig.tag_attr = '__version__'\n\n\t# These are the files to search to find version information\n\tconfig.version_files = [\"setup.py\"]\n\n\t# When versioneer.get_versions() is called, if this is non-empty,\n\t# then versioneer will only search for version information in the\n\t# specified directories.\n\tconfig.search_dirs = []\n\n\t# List any patterns to exclude when searching for files\n\tconfig.exclude_files = []\n\n\t# List any files that should be ignored in the git repo\n\tconfig.ignored_files = []\n\n\t# If this key is set to a list of strings, then versioneer will only\n\t# search for version", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\timport os\n\tfrom versioneer import VersioneerConfig\n\tv_config = VersioneerConfig()\n\tv_config.VCS = 'git'\n\tv_config.style = 'pep440'\n\tv_config.tag_prefix = ''\n\tv_config.parentdir_prefix = 'python-'\n\tv_config.versionfile_source = 'python/_version.py'\n\tv_config.verbose = True\n\treturn v_config\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom .config import VersioneerConfig\n\tcfg = VersioneerConfig()\n\n\t# Set versioneer config options\n\tcfg.VCS = 'git'\n\tcfg.style = 'pep440'\n\tcfg.tag_prefix = ''\n\tcfg.parentdir_prefix = '.'\n\tcfg.versionfile_source = 'versioneer/versionfile_source.py'\n\tcfg.verbose = False\n\n\treturn cfg", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tcfg = VersioneerConfig()\n\tcfg.version_file_source = 'versioneer/_version.py'\n\tcfg.version_script = 'versioneer/version.py'\n\tcfg.tag_prefix = ''\n\tcfg.parentdir_prefix = 'myproject-'\n\tcfg.versionfile_kwargs = {'template': '{tag}'}\n\treturn cfg\n\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\timport os\n\tfrom versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.version_file = os.path.join(os.path.dirname(__file__), 'version.py')\n\tconfig.version_file_encoding = 'utf-8'\n\tconfig.tag_prefix = '' # tags are like 1.2.0\n\tconfig.parentdir_prefix = 'python-'\n\tconfig.version_dev_template = '''# this file should be overwritten with the current git HEAD when a build is performed\nrelease = False\n__version__ = \"{tag}\"'''\n\tconfig.version_dev_template_lines = ['release = False',\n\t\t\t\t\t\t\t\t\t\t'__version__ = \"{tag}\"']\n\tconfig.version_tag_prefix = ''\n\tconfig.parentdir_prefix = 'python-'\n\tconfig.verbose = False\n\treturn config", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.version_file_source = 'wc_tape/_version.py'\n\tconfig.version_file_build = 'wc_tape/_version.py'\n\tconfig.parentdir_prefix_files = 'wc_tape/'\n\tconfig.tag_prefix = 'wc_tape-'\n\tconfig.parentdir_prefix = 'wc_tape-'\n\tconfig.version_file_source_template = '''# coding: utf-8\n# file generated by setuptools_scm\n# don't change, don't track in version control\n__version__ = '{version}'\n'''\n\tconfig.version_file_build_template = '''# coding: utf-8\n# file generated by setuptools_scm\n# don't change, don't track in version control\n__version__ = '{version}'\n'''\n\treturn config"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom ._versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'pymoose/__init__.py'\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = 'pymoose-'\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer import VersioneerConfig\n\tcfg = VersioneerConfig()\n\tcfg.version_file_source = 'setup.py'\n\tcfg.version_script = 'setup.py'\n\tcfg.parentdir_prefix_version_dev_template = '''\\\n# This file is automatically generated by versioneer.\n# Please don't edit this file directly.\n\nfrom __future__ import print_function\nimport sys\n\n__version__ = '{version}'\n'''\n\tcfg.tag_prefix = ''\n\tcfg.parentdir_prefix = 'GitPython-'\n\tcfg.version_dev_template = '''\\\n# This file is automatically generated by versioneer.\n# Please don't edit this file directly.\n\nfrom __future__ import print_function\nimport sys\n\nversion = '{version}'\nfull_version = '{full_version}'\ngit_revision = '{git_revision}'\ngit_branch = '{git_branch}'\n\n\nif not sys.version_info >= (2, 7):\n    raise RuntimeError(\"GitPython requires Python 2.7 or higher.\")\nif not sys.version_info >= (3, 3):\n    raise RuntimeError(\"GitPython requires Python 3.3 or higher.\")\n'''\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.version_file_source = \"versioneer/_version.py\"\n\tconfig.version_file = \"_version.py\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"py-\"\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer.versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.v_file = \"versioneer.py\"\n\tconfig.v_keywords = [\"version\"]\n\tconfig.parentdir_prefix_sep = \".\"\n\tconfig.style = \"pep440\"\n\tconfig.versionfile_source = \"pyproject.toml\"\n\tconfig.versionfile_build = \"pyproject.toml\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"moban\"\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\timport os\n\timport sys\n\tfrom versioneer.versioneer import VersioneerConfig\n\n\tconfig = VersioneerConfig()\n\tconfig.parentdir_prefix_sep = '__prefix__'\n\tconfig.parentdir_prefix = 'VERSIONEER'\n\n\t# Note that this is the git repo root, not the python package root\n\tconfig.root = os.path.abspath(\n\t\tos.path.join(\n\t\t\tos.path.dirname(__file__),\n\t\t\t'..'))\n\n\t# The name of the attribute to set in each package's __init__.py\n\tconfig.tag_attr = '__version__'\n\n\t# These are the files to search to find version information\n\tconfig.version_files = [\"setup.py\"]\n\n\t# When versioneer.get_versions() is called, if this is non-empty,\n\t# then versioneer will only search for version information in the\n\t# specified directories.\n\tconfig.search_dirs = []\n\n\t# List any patterns to exclude when searching for files\n\tconfig.exclude_files = []\n\n\t# List any files that should be ignored in the git repo\n\tconfig.ignored_files = []\n\n\t# If this key is set to a list of strings, then versioneer will only\n\t# search for version", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\timport os\n\tfrom versioneer import VersioneerConfig\n\tv_config = VersioneerConfig()\n\tv_config.VCS = 'git'\n\tv_config.style = 'pep440'\n\tv_config.tag_prefix = ''\n\tv_config.parentdir_prefix = 'python-'\n\tv_config.versionfile_source = 'python/_version.py'\n\tv_config.verbose = True\n\treturn v_config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom .config import VersioneerConfig\n\tcfg = VersioneerConfig()\n\n\t# Set versioneer config options\n\tcfg.VCS = 'git'\n\tcfg.style = 'pep440'\n\tcfg.tag_prefix = ''\n\tcfg.parentdir_prefix = '.'\n\tcfg.versionfile_source = 'versioneer/versionfile_source.py'\n\tcfg.verbose = False\n\n\treturn cfg", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tcfg = VersioneerConfig()\n\tcfg.version_file_source = 'versioneer/_version.py'\n\tcfg.version_script = 'versioneer/version.py'\n\tcfg.tag_prefix = ''\n\tcfg.parentdir_prefix = 'myproject-'\n\tcfg.versionfile_kwargs = {'template': '{tag}'}\n\treturn cfg\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\timport os\n\tfrom versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.version_file = os.path.join(os.path.dirname(__file__), 'version.py')\n\tconfig.version_file_encoding = 'utf-8'\n\tconfig.tag_prefix = '' # tags are like 1.2.0\n\tconfig.parentdir_prefix = 'python-'\n\tconfig.version_dev_template = '''# this file should be overwritten with the current git HEAD when a build is performed\nrelease = False\n__version__ = \"{tag}\"'''\n\tconfig.version_dev_template_lines = ['release = False',\n\t\t\t\t\t\t\t\t\t\t'__version__ = \"{tag}\"']\n\tconfig.version_tag_prefix = ''\n\tconfig.parentdir_prefix = 'python-'\n\tconfig.verbose = False\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom versioneer import VersioneerConfig\n\tconfig = VersioneerConfig()\n\tconfig.version_file_source = 'wc_tape/_version.py'\n\tconfig.version_file_build = 'wc_tape/_version.py'\n\tconfig.parentdir_prefix_files = 'wc_tape/'\n\tconfig.tag_prefix = 'wc_tape-'\n\tconfig.parentdir_prefix = 'wc_tape-'\n\tconfig.version_file_source_template = '''# coding: utf-8\n# file generated by setuptools_scm\n# don't change, don't track in version control\n__version__ = '{version}'\n'''\n\tconfig.version_file_build_template = '''# coding: utf-8\n# file generated by setuptools_scm\n# don't change, don't track in version control\n__version__ = '{version}'\n'''\n\treturn config", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tmethod.vcs_handler = True\n\tmethod.vcs = vcs\n\treturn method\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn.vcs_handler = method\n\t\tvcs.add_handler(fn)\n\t\treturn fn\n\treturn decorator", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(method):\n\t\tvcs.handlers[method.__name__] = method\n\t\treturn method\n\treturn decorator", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tsetattr(vcs, f.__name__, f)\n\t\treturn f\n\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef _decorator(func):\n\t\tfunc.vcs_handler = method\n\t\tfunc.vcs = vcs\n\t\treturn func\n\treturn _decorator\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs.register_method(method, func)\n\t\treturn func\n\treturn decorator\n\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\treturn decorator", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.vcs_handler = (vcs, method)\n\t\treturn func\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.vcs = vcs\n\t\twrapper.method = method\n\t\treturn wrapper\n\treturn decorator", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn._vcs_handler = (vcs, method)\n\t\treturn fn\n\treturn decorator\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tmethod.vcs_handler = True\n\tmethod.vcs = vcs\n\treturn method\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn.vcs_handler = method\n\t\tvcs.add_handler(fn)\n\t\treturn fn\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(method):\n\t\tvcs.handlers[method.__name__] = method\n\t\treturn method\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tsetattr(vcs, f.__name__, f)\n\t\treturn f\n\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef _decorator(func):\n\t\tfunc.vcs_handler = method\n\t\tfunc.vcs = vcs\n\t\treturn func\n\treturn _decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs.register_method(method, func)\n\t\treturn func\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.vcs_handler = (vcs, method)\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.vcs = vcs\n\t\twrapper.method = method\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn._vcs_handler = (vcs, method)\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects, good_objects, bad_objects = 0, 0, 0\n\n\tobjects = self.get_objects()\n\tfor object in objects:\n\t\ttry:\n\t\t\tif validate_objects:\n\t\t\t\tobject.validate()\n\t\t\tif check_digests:\n\t\t\t\tobject.check_digests()\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tprint(f\"Error validating object {object.get_name()}: {e}\")\n\t\t\tbad_objects += 1\n\t\tnum_objects += 1\n\n\tprint(f\"Checked {num_objects} objects ({good_objects} good, {bad_objects} bad)\")\n\treturn num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\terrors = []\n\n\tfor root_obj in self.root_objects:\n\t\troot_obj.validate_hierarchy(\n\t\t\tvalidate_objects=validate_objects,\n\t\t\tcheck_digests=check_digests,\n\t\t\tshow_warnings=show_warnings,\n\t\t\terrors=errors)\n\n\treturn num_objects, good_objects, errors", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tdef _validate_object(path, object_data):\n\t\tif not object_data:\n\t\t\tprint('Warning: Object not found: {}'.format(path))\n\t\t\treturn\n\n\t\tif object_data['digest'] != hashlib.sha256(object_data['data']).digest():\n\t\t\tprint('Warning: Invalid digest for object: {}'.format(path))\n\t\t\treturn\n\n\t\tif not check_digests:\n\t\t\treturn True\n\n\t\ttry:\n\t\t\tzip_file = zipfile.ZipFile(object_data['data'])\n\t\texcept zipfile.BadZipfile:\n\t\t\tprint('Warning: Not a valid zipfile: {}'.format(path))\n\t\t\treturn\n\n\t\tzip_file.testzip()\n\t\tif zip_file.testzip():\n\t\t\tprint('Warning: Zipfile has errors: {}'.format(path))\n\t\t\treturn\n\n\t\tif show_warnings:\n\t\t\tzip_file.printdir()\n\n\t\treturn True\n\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor path, object_data in self.hierarchy.items():\n\t\tnum_objects += 1\n\t\tif _validate_object(path, object_data):\n\t\t\tgood_objects += 1\n\n\treturn num_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects, good_objects = self.validate_directory(\n\t\tself.root_directory_file,\n\t\tvalidate_objects=validate_objects,\n\t\tcheck_digests=check_digests,\n\t\tshow_warnings=show_warnings,\n\t)\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj.validate(validate_objects=validate_objects, check_digests=check_digests)\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"Object {} failed validation: {}\".format(obj.path, e))\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor key in self.keys():\n\t\tif key.endswith('_object'):\n\t\t\tnum_objects += 1\n\t\t\ttry:\n\t\t\t\tobj = self.load_object(key)\n\t\t\t\tif validate_objects:\n\t\t\t\t\tobj.validate(check_digests)\n\t\t\t\tif obj.object_type != 'storage_root':\n\t\t\t\t\tgood_objects += 1\n\t\t\texcept Exception as e:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(str(e))\n\t\t\t\tprint('key:', key)\n\treturn num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor object_path in self.walk_storage_root():\n\t\tnum_objects += 1\n\t\tobject_path = os.path.join(self.storage_root, object_path)\n\t\tif check_digests:\n\t\t\tobject_digest = self.get_object_digest(object_path)\n\t\t\tif object_digest != self.get_object_digest_from_path(object_path):\n\t\t\t\tprint('Bad digest for:', object_path)\n\t\t\t\tif show_warnings:\n\t\t\t\t\twarnings.warn('Bad digest for: {}'.format(object_path))\n\t\t\t\tself.delete_object(object_path)\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\t\tif validate_objects:\n\t\t\tself.validate_object_file(object_path)\n\treturn num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects:\n\t\tnum_objects += 1\n\t\tif check_digests:\n\t\t\tif not self.check_digest(obj):\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(f\"\\tWarning: object {obj.name} has an invalid digest\")\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\t\tif validate_objects:\n\t\t\tif not obj.validate():\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(f\"\\tWarning: object {obj.name} failed validation\")\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\treturn num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tif validate_objects:\n\t\tfor object_id in self.objects:\n\t\t\tnum_objects += 1\n\t\t\tobject = self.objects[object_id]\n\t\t\tif object.validate(show_warnings=show_warnings, check_digests=check_digests):\n\t\t\t\tgood_objects += 1\n\n\tif self.validate(check_digests=check_digests):\n\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor file_name in sorted(os.listdir(self.storage_root)):\n\t\tfile_path = os.path.join(self.storage_root, file_name)\n\t\tif file_name.upper().endswith('.DIGEST'):\n\t\t\tdigest_path = file_path\n\t\t\tdigest_name = file_name\n\t\t\tdigest = Digest.load_from_file(digest_path)\n\t\t\tif digest.digest_name != \"SHA1\":\n\t\t\t\traise Exception(\"Unsupported digest type: \" + digest.digest_name)\n\t\t\tfile_path = file_path[:-len('.digest')]\n\t\telse:\n\t\t\tdigest = None\n\t\tif not os.path.isfile(file_path):\n\t\t\tcontinue\n\t\tkey = file_name[:-len('.dat')]\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\tgood_objects += self.validate_object(key, file_path, digest)\n\t\telse:\n\t\t\tgood_objects += 1\n\tif check_digests:\n\t\tgood_digests = self.validate_digests()\n\t\tif good_digests != num_objects:\n\t\t\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects, good_objects, bad_objects = 0, 0, 0\n\n\tobjects = self.get_objects()\n\tfor object in objects:\n\t\ttry:\n\t\t\tif validate_objects:\n\t\t\t\tobject.validate()\n\t\t\tif check_digests:\n\t\t\t\tobject.check_digests()\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tprint(f\"Error validating object {object.get_name()}: {e}\")\n\t\t\tbad_objects += 1\n\t\tnum_objects += 1\n\n\tprint(f\"Checked {num_objects} objects ({good_objects} good, {bad_objects} bad)\")\n\treturn num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\terrors = []\n\n\tfor root_obj in self.root_objects:\n\t\troot_obj.validate_hierarchy(\n\t\t\tvalidate_objects=validate_objects,\n\t\t\tcheck_digests=check_digests,\n\t\t\tshow_warnings=show_warnings,\n\t\t\terrors=errors)\n\n\treturn num_objects, good_objects, errors", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tdef _validate_object(path, object_data):\n\t\tif not object_data:\n\t\t\tprint('Warning: Object not found: {}'.format(path))\n\t\t\treturn\n\n\t\tif object_data['digest'] != hashlib.sha256(object_data['data']).digest():\n\t\t\tprint('Warning: Invalid digest for object: {}'.format(path))\n\t\t\treturn\n\n\t\tif not check_digests:\n\t\t\treturn True\n\n\t\ttry:\n\t\t\tzip_file = zipfile.ZipFile(object_data['data'])\n\t\texcept zipfile.BadZipfile:\n\t\t\tprint('Warning: Not a valid zipfile: {}'.format(path))\n\t\t\treturn\n\n\t\tzip_file.testzip()\n\t\tif zip_file.testzip():\n\t\t\tprint('Warning: Zipfile has errors: {}'.format(path))\n\t\t\treturn\n\n\t\tif show_warnings:\n\t\t\tzip_file.printdir()\n\n\t\treturn True\n\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor path, object_data in self.hierarchy.items():\n\t\tnum_objects += 1\n\t\tif _validate_object(path, object_data):\n\t\t\tgood_objects += 1\n\n\treturn num_objects", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects, good_objects = self.validate_directory(\n\t\tself.root_directory_file,\n\t\tvalidate_objects=validate_objects,\n\t\tcheck_digests=check_digests,\n\t\tshow_warnings=show_warnings,\n\t)\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj.validate(validate_objects=validate_objects, check_digests=check_digests)\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"Object {} failed validation: {}\".format(obj.path, e))\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor key in self.keys():\n\t\tif key.endswith('_object'):\n\t\t\tnum_objects += 1\n\t\t\ttry:\n\t\t\t\tobj = self.load_object(key)\n\t\t\t\tif validate_objects:\n\t\t\t\t\tobj.validate(check_digests)\n\t\t\t\tif obj.object_type != 'storage_root':\n\t\t\t\t\tgood_objects += 1\n\t\t\texcept Exception as e:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(str(e))\n\t\t\t\tprint('key:', key)\n\treturn num_objects, good_objects", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor object_path in self.walk_storage_root():\n\t\tnum_objects += 1\n\t\tobject_path = os.path.join(self.storage_root, object_path)\n\t\tif check_digests:\n\t\t\tobject_digest = self.get_object_digest(object_path)\n\t\t\tif object_digest != self.get_object_digest_from_path(object_path):\n\t\t\t\tprint('Bad digest for:', object_path)\n\t\t\t\tif show_warnings:\n\t\t\t\t\twarnings.warn('Bad digest for: {}'.format(object_path))\n\t\t\t\tself.delete_object(object_path)\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\t\tif validate_objects:\n\t\t\tself.validate_object_file(object_path)\n\treturn num_objects, good_objects", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects:\n\t\tnum_objects += 1\n\t\tif check_digests:\n\t\t\tif not self.check_digest(obj):\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(f\"\\tWarning: object {obj.name} has an invalid digest\")\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\t\tif validate_objects:\n\t\t\tif not obj.validate():\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(f\"\\tWarning: object {obj.name} failed validation\")\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\treturn num_objects, good_objects", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tif validate_objects:\n\t\tfor object_id in self.objects:\n\t\t\tnum_objects += 1\n\t\t\tobject = self.objects[object_id]\n\t\t\tif object.validate(show_warnings=show_warnings, check_digests=check_digests):\n\t\t\t\tgood_objects += 1\n\n\tif self.validate(check_digests=check_digests):\n\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor file_name in sorted(os.listdir(self.storage_root)):\n\t\tfile_path = os.path.join(self.storage_root, file_name)\n\t\tif file_name.upper().endswith('.DIGEST'):\n\t\t\tdigest_path = file_path\n\t\t\tdigest_name = file_name\n\t\t\tdigest = Digest.load_from_file(digest_path)\n\t\t\tif digest.digest_name != \"SHA1\":\n\t\t\t\traise Exception(\"Unsupported digest type: \" + digest.digest_name)\n\t\t\tfile_path = file_path[:-len('.digest')]\n\t\telse:\n\t\t\tdigest = None\n\t\tif not os.path.isfile(file_path):\n\t\t\tcontinue\n\t\tkey = file_name[:-len('.dat')]\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\tgood_objects += self.validate_object(key, file_path, digest)\n\t\telse:\n\t\t\tgood_objects += 1\n\tif check_digests:\n\t\tgood_digests = self.validate_digests()\n\t\tif good_digests != num_objects:\n\t\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.metadata = {\n\t\t'ocflVersion': 'draft-01',\n\t\t'contentDigest': 'sha256:d6a770ba38583ed4bb4525bd96e50461655d27599e3490fda98c6339d4e60f',\n\t\t'head': 'version-000001',\n\t}\n\tself.inventory = {\n\t\t'versions': [\n\t\t\t{\n\t\t\t\t'id': 'version-000001',\n\t\t\t\t'created': '2022-11-09T14:22:00Z',\n\t\t\t\t'base': True,\n\t\t\t\t'inventory': {},\n\t\t\t},\n\t\t],\n\t}\n\tself.digest = 'sha256:d6a770ba38583ed4bb4525bd96e50461655d27599e3490fda98c6339d4e60f'\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.initialize_storage_root()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.set_root_properties()\n\tself.set_inventory()\n\tself.set_version_control()\n\tself.set_object_control()\n\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create_dir(\"objects\")\n\tself.create_dir(\"metadata\")\n\tself.create_dir(\"versions\")\n\tself.create_dir(\"inventories\")\n\tself.create_dir(\"profiles\")\n\n\tself.create_file(\"version.txt\", \"0.1.0\")\n\tself.create_file(\"inventory.txt\", \"version=1\")\n\tself.create_file(\"profiles.txt\", \"version=1\")\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.version = '1.0'\n\n\tself.ocfl_version = '1.0'\n\tself.object_layout = 'flat'\n\n\tself.root_object_id = 'root'\n\tself.objects = {}\n\tself.children = {}\n\n\tself.add_object(self.root_object_id, 'dir')\n\n\tself.add_child(self.root_object_id, 'objects')\n\tself.add_child(self.root_object_id, 'spec')\n\tself.add_child(self.root_object_id, 'auth')\n\n\tself.objects[self.root_object_id].mimetype = 'application/vnd.ocfl.root+json'\n\tself.objects[self.root_object_id].content = {\n\t\t'version': self.version,\n\t\t'ocflVersion': self.ocfl_version,\n\t\t'objectLayout': self.object_layout,\n\t\t'rootObjectId': self.root_object_id,\n\t\t'objects': {},\n\t\t'children': {},\n\t}\n\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create_directories()\n\tself.create_objects_directory()\n\tself.create_metadata_directory()\n\tself.write_version_file()\n\tself.write_inventory_file()\n\tself.write_objects_directory_manifest()\n\tself.write_metadata_directory_manifest()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.identifier = 'root'\n\tself.metadata = {}\n\tself.objects = {}\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.object_ids = {}\n\tself.object_manifest_ids = {}\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = OCFLStorageRoot.create(root_path=self.root_path)\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root_version = '00000001'\n\tself.storage_root_dir = os.path.join(self.root_dir, 'root')\n\n\tos.mkdir(self.root_dir)\n\tos.mkdir(self.storage_root_dir)\n\tself._init_storage_root()\n\tself._init_metadata()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.metadata = {\n\t\t'ocflVersion': 'draft-01',\n\t\t'contentDigest': 'sha256:d6a770ba38583ed4bb4525bd96e50461655d27599e3490fda98c6339d4e60f',\n\t\t'head': 'version-000001',\n\t}\n\tself.inventory = {\n\t\t'versions': [\n\t\t\t{\n\t\t\t\t'id': 'version-000001',\n\t\t\t\t'created': '2022-11-09T14:22:00Z',\n\t\t\t\t'base': True,\n\t\t\t\t'inventory': {},\n\t\t\t},\n\t\t],\n\t}\n\tself.digest = 'sha256:d6a770ba38583ed4bb4525bd96e50461655d27599e3490fda98c6339d4e60f'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.initialize_storage_root()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.set_root_properties()\n\tself.set_inventory()\n\tself.set_version_control()\n\tself.set_object_control()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create_dir(\"objects\")\n\tself.create_dir(\"metadata\")\n\tself.create_dir(\"versions\")\n\tself.create_dir(\"inventories\")\n\tself.create_dir(\"profiles\")\n\n\tself.create_file(\"version.txt\", \"0.1.0\")\n\tself.create_file(\"inventory.txt\", \"version=1\")\n\tself.create_file(\"profiles.txt\", \"version=1\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.version = '1.0'\n\n\tself.ocfl_version = '1.0'\n\tself.object_layout = 'flat'\n\n\tself.root_object_id = 'root'\n\tself.objects = {}\n\tself.children = {}\n\n\tself.add_object(self.root_object_id, 'dir')\n\n\tself.add_child(self.root_object_id, 'objects')\n\tself.add_child(self.root_object_id, 'spec')\n\tself.add_child(self.root_object_id, 'auth')\n\n\tself.objects[self.root_object_id].mimetype = 'application/vnd.ocfl.root+json'\n\tself.objects[self.root_object_id].content = {\n\t\t'version': self.version,\n\t\t'ocflVersion': self.ocfl_version,\n\t\t'objectLayout': self.object_layout,\n\t\t'rootObjectId': self.root_object_id,\n\t\t'objects': {},\n\t\t'children': {},\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create_directories()\n\tself.create_objects_directory()\n\tself.create_metadata_directory()\n\tself.write_version_file()\n\tself.write_inventory_file()\n\tself.write_objects_directory_manifest()\n\tself.write_metadata_directory_manifest()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.identifier = 'root'\n\tself.metadata = {}\n\tself.objects = {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.object_ids = {}\n\tself.object_manifest_ids = {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = OCFLStorageRoot.create(root_path=self.root_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root_version = '00000001'\n\tself.storage_root_dir = os.path.join(self.root_dir, 'root')\n\n\tos.mkdir(self.root_dir)\n\tos.mkdir(self.storage_root_dir)\n\tself._init_storage_root()\n\tself._init_metadata()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor,minor,patch = version.split('.')\n\tmajor = int(major)\n\tminor = int(minor)\n\tpatch = int(patch)\n\n\tpatch += 1\n\tif patch == 10:\n\t\tpatch = 0\n\t\tminor +=1\n\t\tif minor == 10:\n\t\t\tminor = 0\n\t\t\tmajor += 1\n\n\treturn \"%d.%d.%d\"%(major,minor,patch)", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tnew_version = version.split('.')\n\tnew_version[-1] = str(int(new_version[-1]) + 1)\n\treturn '.'.join(new_version)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion = [int(i) for i in version]\n\n\tversion[-1] += 1\n\tversion[1] += 1\n\n\tversion = [str(i) for i in version]\n\tversion = '.'.join(version)\n\n\treturn version\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\tmajor, minor, patch = version.split('.')\n\t\n\tnew_patch = int(patch) + 1\n\tnew_minor = int(minor)\n\tnew_major = int(major)\n\t\n\tif new_patch >= 10:\n\t\tnew_patch = 0\n\t\tnew_minor += 1\n\t\tif new_minor >= 10:\n\t\t\tnew_minor = 0\n\t\t\tnew_major += 1\n\t\n\treturn '.'.join([str(new_major), str(new_minor), str(new_patch)])\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch, _ = version.split('.')\n\tmajor = int(major)\n\tminor = int(minor)\n\tpatch = int(patch)\n\treturn \"{}.{}.{}\".format(major, minor, patch + 1)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[-1] = str(int(version[-1])+1)\n\treturn '.'.join(version)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch = version.split('.')\n\tpatch = str(int(patch) + 1)\n\treturn '.'.join([major, minor, patch])\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_list = version.split('.')\n\tversion_list[-1] = str(int(version_list[-1]) + 1)\n\treturn '.'.join(version_list)", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\n\tif len(version_parts) == 3:\n\t\tversion_parts = [int(x) for x in version_parts]\n\t\tversion_parts[2] += 1\n\n\telif len(version_parts) == 2:\n\t\tversion_parts = [int(x) for x in version_parts]\n\t\tversion_parts[1] += 1\n\t\tversion_parts[2] = 0\n\n\telif len(version_parts) == 1:\n\t\tversion_parts = [int(x) for x in version_parts]\n\t\tversion_parts[0] += 1\n\t\tversion_parts[1] = 1\n\t\tversion_parts[2] = 0\n\n\tversion = '.'.join([str(x) for x in version_parts])\n\n\treturn version", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\tversion_parts[-1] = str(int(version_parts[-1]) + 1)\n\treturn '.'.join(version_parts)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor,minor,patch = version.split('.')\n\tmajor = int(major)\n\tminor = int(minor)\n\tpatch = int(patch)\n\n\tpatch += 1\n\tif patch == 10:\n\t\tpatch = 0\n\t\tminor +=1\n\t\tif minor == 10:\n\t\t\tminor = 0\n\t\t\tmajor += 1\n\n\treturn \"%d.%d.%d\"%(major,minor,patch)", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tnew_version = version.split('.')\n\tnew_version[-1] = str(int(new_version[-1]) + 1)\n\treturn '.'.join(new_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion = [int(i) for i in version]\n\n\tversion[-1] += 1\n\tversion[1] += 1\n\n\tversion = [str(i) for i in version]\n\tversion = '.'.join(version)\n\n\treturn version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\tmajor, minor, patch = version.split('.')\n\t\n\tnew_patch = int(patch) + 1\n\tnew_minor = int(minor)\n\tnew_major = int(major)\n\t\n\tif new_patch >= 10:\n\t\tnew_patch = 0\n\t\tnew_minor += 1\n\t\tif new_minor >= 10:\n\t\t\tnew_minor = 0\n\t\t\tnew_major += 1\n\t\n\treturn '.'.join([str(new_major), str(new_minor), str(new_patch)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch, _ = version.split('.')\n\tmajor = int(major)\n\tminor = int(minor)\n\tpatch = int(patch)\n\treturn \"{}.{}.{}\".format(major, minor, patch + 1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[-1] = str(int(version[-1])+1)\n\treturn '.'.join(version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch = version.split('.')\n\tpatch = str(int(patch) + 1)\n\treturn '.'.join([major, minor, patch])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_list = version.split('.')\n\tversion_list[-1] = str(int(version_list[-1]) + 1)\n\treturn '.'.join(version_list)", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\n\tif len(version_parts) == 3:\n\t\tversion_parts = [int(x) for x in version_parts]\n\t\tversion_parts[2] += 1\n\n\telif len(version_parts) == 2:\n\t\tversion_parts = [int(x) for x in version_parts]\n\t\tversion_parts[1] += 1\n\t\tversion_parts[2] = 0\n\n\telif len(version_parts) == 1:\n\t\tversion_parts = [int(x) for x in version_parts]\n\t\tversion_parts[0] += 1\n\t\tversion_parts[1] = 1\n\t\tversion_parts[2] = 0\n\n\tversion = '.'.join([str(x) for x in version_parts])\n\n\treturn version", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\tversion_parts[-1] = str(int(version_parts[-1]) + 1)\n\treturn '.'.join(version_parts)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_digests = {}\n\tfor version_dir in version_dirs:\n\t\t# get the inventory for this version\n\t\tversion_inventory = self.get_inventory(version_dir)\n\t\t# check the version inventory is complete\n\t\tself.validate_inventory(version_inventory)\n\t\t# check the inventory digests are the same as the root inventory\n\t\tfor inventory_record in version_inventory:\n\t\t\tinventory_digests[inventory_record[\"name\"]] = inventory_record[\"digest\"]\n\treturn inventory_digests\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_file_path = os.path.join(self.content_path, 'root_inventory.json')\n\twith open(inventory_file_path) as inventory_file:\n\t\tinventory_file_dict = json.load(inventory_file)\n\n\tcontent_digest_dict = {}\n\tversion_dir_list = []\n\tfor version_dir in version_dirs:\n\t\tversion_digest_dict = {\n\t\t\t'inventory': {},\n\t\t\t'content_digest': {}\n\t\t}\n\t\tversion_dir_list.append(version_dir)\n\n\t\t# get the inventory\n\t\tinventory_file_path = os.path.join(self.content_path, version_dir, 'inventory.json')\n\t\twith open(inventory_file_path) as inventory_file:\n\t\t\tversion_digest_dict['inventory'] = json.load(inventory_file)\n\n\t\t# get the content_digest\n\t\tcontent_digest_file_path = os.path.join(self.content_path, version_dir, 'content_digest.json')\n\t\twith open(content_digest_file_path) as content_digest_file:\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tpass\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventory_dirs = []\n\tfor v in version_dirs:\n\t\tversion_inventory_dirs.append(os.path.join(v, \"inventory\"))\n\n\tversion_inventory_dirs.append(os.path.join(self.version_dir, \"inventory\"))\n\n\tversion_inventories = {}\n\tfor v in version_inventory_dirs:\n\t\tversion_inventories[v] = Inventory(os.path.join(v, \"inventory\"))\n\n\tprevious_version_inventory = version_inventories[version_dirs[0]]\n\tprevious_version_inventory.load()\n\tprevious_version_inventory.version = version_dirs[0]\n\n\tfor i, v in enumerate(version_dirs[1:]):\n\t\tversion_inventory = version_inventories[os.path.join(v, \"inventory\")]\n\t\tversion_inventory.load()\n\t\tversion_inventory.version = v\n\n\t\tnew_version_inventory_content_digests = {}\n\t\tfor c in version_inventory.content_digests:\n\t\t\tif not c in previous_version_inventory", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdigests = {}\n\tversion_prefix = self.version_prefix\n\tfor v in version_dirs:\n\t\tinventory_path = os.path.join(self.version_dir, v, 'inventory.json')\n\t\twith open(inventory_path, 'r') as inventory_file:\n\t\t\tinventory = json.load(inventory_file)\n\t\t\tfor path, details in inventory.items():\n\t\t\t\tdigest = details['digest']\n\t\t\t\tdigests[digest] = path\n\n\tinventory_path = os.path.join(self.version_dir, version_prefix, 'inventory.json')\n\twith open(inventory_path, 'r') as inventory_file:\n\t\tinventory = json.load(inventory_file)\n\t\tfor path, details in inventory.items():\n\t\t\tdigest = details['digest']\n\t\t\tif digest in digests:\n\t\t\t\tdigests.pop(digest)\n\n\tif len(digests) > 0:\n\t\traise RuntimeError('The root inventory version ' + version_prefix +\n\t\t\t' has more content than the inventories for the previous versions.')\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = []\n\tversion_content_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion = self.version_dir_to_version(version_dir)\n\t\tversion_inventory_path = os.path.join(self.root_dir, version_dir, \"inventory\")\n\t\tversion_inventory_data = self.load_json_file(version_inventory_path)\n\t\tversion_inventories.append(version_inventory_data)\n\t\tversion_content_digests[version] = version_inventory_data[\"content_digests\"]\n\n\t# Make sure the inventories are valid.\n\tfor i in range(len(version_inventories)):\n\t\tself.validate_inventory(version_inventories[i])\n\n\t# Make sure the content digests are valid.\n\tfor version, content_digests in version_content_digests.items():\n\t\tfor content_digest in content_digests:\n\t\t\tself.validate_content_digest(content_digest)\n\n\t# Make sure the inventories are in sequence.\n\tprevious_version_inventory = version_inventories[0]\n\tprevious_version = self.version_dir_to_", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.logger.info(\"Validating the version inventories.\")\n\n\t# we'll need to create a root inventory for the first time\n\tself.root_inventory = Inventory(self.root_dir)\n\n\tfor version_dir in version_dirs:\n\t\tversion = version_dir.split('/')[0]\n\t\tself.logger.info(\"Validating version %s\" % version)\n\n\t\t# make sure the version directory exists\n\t\tif not os.path.isdir(version_dir):\n\t\t\tself.logger.error(\"Version %s does not exist.\" % version)\n\t\t\treturn False\n\n\t\t# create the inventory for the version\n\t\tversion_inventory = Inventory(version_dir)\n\n\t\t# iterate through the files and validate them\n\t\tfor file in version_inventory.files:\n\t\t\tfile_path = os.path.join(version_dir, file)\n\t\t\tself.validate_file(file_path)\n\n\t\t\t# check the content hash\n\t\t\tfile_content_hash = version_inventory.file_content_hashes[file]\n\t\t\troot_content_hash = self.root_inventory.file_content_hashes[file]\n\t\t\tif file_content_hash != root_content_hash:", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventory_dirs = [f for f in self.listdir(self.version_dir) if f.startswith('inventory-')]\n\tif len(version_inventory_dirs) != len(version_dirs):\n\t\traise Exception(\"version inventory directories don't match version directories\")\n\n\tinventory_dirs = []\n\n\tfor version_dir, version_inventory_dir in zip(version_dirs, version_inventory_dirs):\n\t\tversion_inventory_path = self.version_dir + '/' + version_dir + '/' + version_inventory_dir\n\t\ttry:\n\t\t\tinventory = Inventory(version_inventory_path)\n\t\texcept:\n\t\t\traise Exception(\"Invalid inventory file: %s\" % version_inventory_path)\n\n\t\tfor filename in inventory.files:\n\t\t\tversion_file_path = self.version_dir + '/' + version_dir + '/' + filename\n\t\t\tself.validate_file(version_file_path, inventory.files[filename])\n\n\t\tinventory_dirs.append(self.version_dir + '/' + version_dir + '/' + version_inventory_dir)\n\n\tif", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_paths = [os.path.join(self.path, d, 'inventory.json') for d in version_dirs]\n\tcontent_digests = {}\n\tfor i, path in enumerate(inventory_paths):\n\t\ttry:\n\t\t\twith open(path) as fp:\n\t\t\t\tinventory = json.load(fp)\n\t\t\t\tversion = int(os.path.basename(path).split('-')[0])\n\t\t\t\tif 'content_digests' in inventory:\n\t\t\t\t\tfor key, value in inventory['content_digests'].items():\n\t\t\t\t\t\tcontent_digests[key] = value\n\t\t\t\tif i == 0:\n\t\t\t\t\tcontinue\n\t\t\t\tprev_version = int(os.path.basename(inventory_paths[i - 1]).split('-')[0])\n\t\t\t\tprev_inventory = self.read_inventory(prev_version)\n\t\t\t\tif not self.validate_inventory(prev_inventory, inventory):\n\t\t\t\t\traise Exception(\"Inventory at version %d is different from version %d\" % (version, prev_version))\n\t\texcept Exception as e:\n\t\t\traise Exception(\"Failed to validate inventory at version %d: %s\" % (version, e))\n\n\t\"\"\"", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = [self.load_version_inventory(dir) for dir in version_dirs]\n\n\tfor i in range(1, len(version_dirs)):\n\t\tprev_version_inventory = version_inventories[i-1]\n\t\tcurrent_version_inventory = version_inventories[i]\n\n\t\tself.validate_version_inventory(prev_version_inventory, current_version_inventory)\n\n\t\tcurrent_version_inventory_digests = {}\n\t\tfor x in current_version_inventory[\"content\"]:\n\t\t\tcurrent_version_inventory_digests[x[\"digest\"]] = x[\"digest\"]\n\n\t\tfor x in prev_version_inventory[\"content\"]:\n\t\t\tif x[\"digest\"] not in current_version_inventory_digests:\n\t\t\t\tlog.error(\"digest %s missing from version %s inventory\" %\n\t\t\t\t\t(x[\"digest\"], version_dirs[i]))\n\t\t\t\tsys.exit(1)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_digests = {}\n\tfor version_dir in version_dirs:\n\t\t# get the inventory for this version\n\t\tversion_inventory = self.get_inventory(version_dir)\n\t\t# check the version inventory is complete\n\t\tself.validate_inventory(version_inventory)\n\t\t# check the inventory digests are the same as the root inventory\n\t\tfor inventory_record in version_inventory:\n\t\t\tinventory_digests[inventory_record[\"name\"]] = inventory_record[\"digest\"]\n\treturn inventory_digests\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_file_path = os.path.join(self.content_path, 'root_inventory.json')\n\twith open(inventory_file_path) as inventory_file:\n\t\tinventory_file_dict = json.load(inventory_file)\n\n\tcontent_digest_dict = {}\n\tversion_dir_list = []\n\tfor version_dir in version_dirs:\n\t\tversion_digest_dict = {\n\t\t\t'inventory': {},\n\t\t\t'content_digest': {}\n\t\t}\n\t\tversion_dir_list.append(version_dir)\n\n\t\t# get the inventory\n\t\tinventory_file_path = os.path.join(self.content_path, version_dir, 'inventory.json')\n\t\twith open(inventory_file_path) as inventory_file:\n\t\t\tversion_digest_dict['inventory'] = json.load(inventory_file)\n\n\t\t# get the content_digest\n\t\tcontent_digest_file_path = os.path.join(self.content_path, version_dir, 'content_digest.json')\n\t\twith open(content_digest_file_path) as content_digest_file:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventory_dirs = []\n\tfor v in version_dirs:\n\t\tversion_inventory_dirs.append(os.path.join(v, \"inventory\"))\n\n\tversion_inventory_dirs.append(os.path.join(self.version_dir, \"inventory\"))\n\n\tversion_inventories = {}\n\tfor v in version_inventory_dirs:\n\t\tversion_inventories[v] = Inventory(os.path.join(v, \"inventory\"))\n\n\tprevious_version_inventory = version_inventories[version_dirs[0]]\n\tprevious_version_inventory.load()\n\tprevious_version_inventory.version = version_dirs[0]\n\n\tfor i, v in enumerate(version_dirs[1:]):\n\t\tversion_inventory = version_inventories[os.path.join(v, \"inventory\")]\n\t\tversion_inventory.load()\n\t\tversion_inventory.version = v\n\n\t\tnew_version_inventory_content_digests = {}\n\t\tfor c in version_inventory.content_digests:\n\t\t\tif not c in previous_version_inventory", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdigests = {}\n\tversion_prefix = self.version_prefix\n\tfor v in version_dirs:\n\t\tinventory_path = os.path.join(self.version_dir, v, 'inventory.json')\n\t\twith open(inventory_path, 'r') as inventory_file:\n\t\t\tinventory = json.load(inventory_file)\n\t\t\tfor path, details in inventory.items():\n\t\t\t\tdigest = details['digest']\n\t\t\t\tdigests[digest] = path\n\n\tinventory_path = os.path.join(self.version_dir, version_prefix, 'inventory.json')\n\twith open(inventory_path, 'r') as inventory_file:\n\t\tinventory = json.load(inventory_file)\n\t\tfor path, details in inventory.items():\n\t\t\tdigest = details['digest']\n\t\t\tif digest in digests:\n\t\t\t\tdigests.pop(digest)\n\n\tif len(digests) > 0:\n\t\traise RuntimeError('The root inventory version ' + version_prefix +\n\t\t\t' has more content than the inventories for the previous versions.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = []\n\tversion_content_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion = self.version_dir_to_version(version_dir)\n\t\tversion_inventory_path = os.path.join(self.root_dir, version_dir, \"inventory\")\n\t\tversion_inventory_data = self.load_json_file(version_inventory_path)\n\t\tversion_inventories.append(version_inventory_data)\n\t\tversion_content_digests[version] = version_inventory_data[\"content_digests\"]\n\n\t# Make sure the inventories are valid.\n\tfor i in range(len(version_inventories)):\n\t\tself.validate_inventory(version_inventories[i])\n\n\t# Make sure the content digests are valid.\n\tfor version, content_digests in version_content_digests.items():\n\t\tfor content_digest in content_digests:\n\t\t\tself.validate_content_digest(content_digest)\n\n\t# Make sure the inventories are in sequence.\n\tprevious_version_inventory = version_inventories[0]\n\tprevious_version = self.version_dir_to_", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.logger.info(\"Validating the version inventories.\")\n\n\t# we'll need to create a root inventory for the first time\n\tself.root_inventory = Inventory(self.root_dir)\n\n\tfor version_dir in version_dirs:\n\t\tversion = version_dir.split('/')[0]\n\t\tself.logger.info(\"Validating version %s\" % version)\n\n\t\t# make sure the version directory exists\n\t\tif not os.path.isdir(version_dir):\n\t\t\tself.logger.error(\"Version %s does not exist.\" % version)\n\t\t\treturn False\n\n\t\t# create the inventory for the version\n\t\tversion_inventory = Inventory(version_dir)\n\n\t\t# iterate through the files and validate them\n\t\tfor file in version_inventory.files:\n\t\t\tfile_path = os.path.join(version_dir, file)\n\t\t\tself.validate_file(file_path)\n\n\t\t\t# check the content hash\n\t\t\tfile_content_hash = version_inventory.file_content_hashes[file]\n\t\t\troot_content_hash = self.root_inventory.file_content_hashes[file]\n\t\t\tif file_content_hash != root_content_hash:", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventory_dirs = [f for f in self.listdir(self.version_dir) if f.startswith('inventory-')]\n\tif len(version_inventory_dirs) != len(version_dirs):\n\t\traise Exception(\"version inventory directories don't match version directories\")\n\n\tinventory_dirs = []\n\n\tfor version_dir, version_inventory_dir in zip(version_dirs, version_inventory_dirs):\n\t\tversion_inventory_path = self.version_dir + '/' + version_dir + '/' + version_inventory_dir\n\t\ttry:\n\t\t\tinventory = Inventory(version_inventory_path)\n\t\texcept:\n\t\t\traise Exception(\"Invalid inventory file: %s\" % version_inventory_path)\n\n\t\tfor filename in inventory.files:\n\t\t\tversion_file_path = self.version_dir + '/' + version_dir + '/' + filename\n\t\t\tself.validate_file(version_file_path, inventory.files[filename])\n\n\t\tinventory_dirs.append(self.version_dir + '/' + version_dir + '/' + version_inventory_dir)\n\n\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_paths = [os.path.join(self.path, d, 'inventory.json') for d in version_dirs]\n\tcontent_digests = {}\n\tfor i, path in enumerate(inventory_paths):\n\t\ttry:\n\t\t\twith open(path) as fp:\n\t\t\t\tinventory = json.load(fp)\n\t\t\t\tversion = int(os.path.basename(path).split('-')[0])\n\t\t\t\tif 'content_digests' in inventory:\n\t\t\t\t\tfor key, value in inventory['content_digests'].items():\n\t\t\t\t\t\tcontent_digests[key] = value\n\t\t\t\tif i == 0:\n\t\t\t\t\tcontinue\n\t\t\t\tprev_version = int(os.path.basename(inventory_paths[i - 1]).split('-')[0])\n\t\t\t\tprev_inventory = self.read_inventory(prev_version)\n\t\t\t\tif not self.validate_inventory(prev_inventory, inventory):\n\t\t\t\t\traise Exception(\"Inventory at version %d is different from version %d\" % (version, prev_version))\n\t\texcept Exception as e:\n\t\t\traise Exception(\"Failed to validate inventory at version %d: %s\" % (version, e))\n\n\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = [self.load_version_inventory(dir) for dir in version_dirs]\n\n\tfor i in range(1, len(version_dirs)):\n\t\tprev_version_inventory = version_inventories[i-1]\n\t\tcurrent_version_inventory = version_inventories[i]\n\n\t\tself.validate_version_inventory(prev_version_inventory, current_version_inventory)\n\n\t\tcurrent_version_inventory_digests = {}\n\t\tfor x in current_version_inventory[\"content\"]:\n\t\t\tcurrent_version_inventory_digests[x[\"digest\"]] = x[\"digest\"]\n\n\t\tfor x in prev_version_inventory[\"content\"]:\n\t\t\tif x[\"digest\"] not in current_version_inventory_digests:\n\t\t\t\tlog.error(\"digest %s missing from version %s inventory\" %\n\t\t\t\t\t(x[\"digest\"], version_dirs[i]))\n\t\t\t\tsys.exit(1)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tfrom os.path import exists, isfile, isdir\n\n\tif not exists(path):\n\t\treturn 'nonexistent'\n\tif isfile(path):\n\t\treturn 'file'\n\tif isdir(path):\n\t\treturn 'folder'\n\treturn 'unknown'\n\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"link\"\n\telse:\n\t\treturn \"unknown\"\n\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = str(path)\n\tpath_type = 'file'\n\tif os.path.isdir(path):\n\t\tpath_type = 'dir'\n\telif os.path.islink(path):\n\t\tpath_type = 'link'\n\treturn path_type\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\timport stat\n\n\tmode = os.lstat(path).st_mode\n\tif stat.S_ISDIR(mode):\n\t\treturn \"dir\"\n\telif stat.S_ISREG(mode):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"unknown\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\ttype = \"\"\n\ttry:\n\t\ttype = os.path.splitext(path)[1][1:].lower()\n\texcept OSError:\n\t\tpass\n\treturn type\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os, mimetypes\n\t\n\tif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\tfile_type, encoding = mimetypes.guess_type(path)\n\t\treturn file_type\n\telse:\n\t\treturn None\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\n\tif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.islink(path):\n\t\treturn 'symlink'\n\telse:\n\t\treturn None\n\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\n\tif os.path.isdir(path):\n\t\treturn \"folder\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telse:\n\t\treturn None\n\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = \"unknown\"\n\tif os.path.isdir(path):\n\t\tpath_type = \"directory\"\n\telif os.path.isfile(path):\n\t\tpath_type = \"file\"\n\telif os.path.islink(path):\n\t\tpath_type = \"link\"\n\treturn path_type", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\timport stat\n\n\tif os.path.exists(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn \"directory\"\n\t\telif os.path.isfile(path):\n\t\t\treturn \"file\"\n\t\telif os.path.islink(path):\n\t\t\treturn \"link\"\n\telif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"link\"\n\telse:\n\t\treturn \"unknown\"\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tfrom os.path import exists, isfile, isdir\n\n\tif not exists(path):\n\t\treturn 'nonexistent'\n\tif isfile(path):\n\t\treturn 'file'\n\tif isdir(path):\n\t\treturn 'folder'\n\treturn 'unknown'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"link\"\n\telse:\n\t\treturn \"unknown\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = str(path)\n\tpath_type = 'file'\n\tif os.path.isdir(path):\n\t\tpath_type = 'dir'\n\telif os.path.islink(path):\n\t\tpath_type = 'link'\n\treturn path_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\timport stat\n\n\tmode = os.lstat(path).st_mode\n\tif stat.S_ISDIR(mode):\n\t\treturn \"dir\"\n\telif stat.S_ISREG(mode):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\ttype = \"\"\n\ttry:\n\t\ttype = os.path.splitext(path)[1][1:].lower()\n\texcept OSError:\n\t\tpass\n\treturn type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os, mimetypes\n\t\n\tif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\tfile_type, encoding = mimetypes.guess_type(path)\n\t\treturn file_type\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\n\tif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.islink(path):\n\t\treturn 'symlink'\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\n\tif os.path.isdir(path):\n\t\treturn \"folder\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = \"unknown\"\n\tif os.path.isdir(path):\n\t\tpath_type = \"directory\"\n\telif os.path.isfile(path):\n\t\tpath_type = \"file\"\n\telif os.path.islink(path):\n\t\tpath_type = \"link\"\n\treturn path_type", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\timport os\n\timport stat\n\n\tif os.path.exists(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn \"directory\"\n\t\telif os.path.isfile(path):\n\t\t\treturn \"file\"\n\t\telif os.path.islink(path):\n\t\t\treturn \"link\"\n\telif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"link\"\n\telse:\n\t\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams_to_amend = {\n\t\t'include_fields': 'id,summary,status,priority,component,target_milestone,resolution,flags,assigned_to,creator,creation_ts,last_change_time,delta_ts,blocks,depends_on,depends_on_resolution',\n\t\t'max_results': 100\n\t}\n\tfor bug_id in bug_ids:\n\t\tparams_to_amend['bug_id'] = bug_id\n\t\tparams.update(params_to_amend)\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbz_params = {}\n\tif bug_ids:\n\t\tbz_params['bug_id'] = ','.join(bug_ids)\n\tbz_params.update(params)\n\treturn bz_params\n\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams.update({\n\t\t\"bug_id\": bug_ids,\n\t\t\"include_fields\": [\"id\", \"summary\", \"priority\", \"status\", \"importance\", \"component\", \"version\", \"platform\", \"op_sys\", \"whiteboard\", \"keywords\", \"bug_group\"],\n\t})\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['id'] = bug_ids\n\tparams['include_fields'] = 'id,summary,short_desc,creation_ts,assigned_to,status,priority,resolution,op_sys,version,platform'\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams[\"bug_id\"] = bug_ids\n\tparams[\"include_fields\"] = \"id,summary,priority,severity,product,component,assigned_to\"\n\tparams[\"include_fields\"] += \",reporter,status,resolution,last_change_time,creation_ts,longdesc\"\n\n\tparams[\"include_fields\"] += \",keywords,blocks,depends_on,depends_on_bugs\"\n\tparams[\"include_fields\"] += \",whiteboard,longdesc,assignee,qa_contact\"\n\n\tparams[\"include_fields\"] += \",cf_test_plan,cf_test_execution,cf_test_results\"\n\n\treturn params", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tpass", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_ids'] = ','.join(bug_ids)\n\treturn params\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get('include_fields'):\n\t\tparams['include_fields'] = 'id,summary,status,resolution'\n\tif not params.get", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams_to_amend = {\n\t\t'include_fields': 'id,summary,status,priority,component,target_milestone,resolution,flags,assigned_to,creator,creation_ts,last_change_time,delta_ts,blocks,depends_on,depends_on_resolution',\n\t\t'max_results': 100\n\t}\n\tfor bug_id in bug_ids:\n\t\tparams_to_amend['bug_id'] = bug_id\n\t\tparams.update(params_to_amend)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbz_params = {}\n\tif bug_ids:\n\t\tbz_params['bug_id'] = ','.join(bug_ids)\n\tbz_params.update(params)\n\treturn bz_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams.update({\n\t\t\"bug_id\": bug_ids,\n\t\t\"include_fields\": [\"id\", \"summary\", \"priority\", \"status\", \"importance\", \"component\", \"version\", \"platform\", \"op_sys\", \"whiteboard\", \"keywords\", \"bug_group\"],\n\t})\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['id'] = bug_ids\n\tparams['include_fields'] = 'id,summary,short_desc,creation_ts,assigned_to,status,priority,resolution,op_sys,version,platform'\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams[\"bug_id\"] = bug_ids\n\tparams[\"include_fields\"] = \"id,summary,priority,severity,product,component,assigned_to\"\n\tparams[\"include_fields\"] += \",reporter,status,resolution,last_change_time,creation_ts,longdesc\"\n\n\tparams[\"include_fields\"] += \",keywords,blocks,depends_on,depends_on_bugs\"\n\tparams[\"include_fields\"] += \",whiteboard,longdesc,assignee,qa_contact\"\n\n\tparams[\"include_fields\"] += \",cf_test_plan,cf_test_execution,cf_test_results\"\n\n\treturn params", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_ids'] = ','.join(bug_ids)\n\treturn params\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = dict()\n\tfor node in nodes:\n\t\tnode.merge_to_dict(merged_nodes)\n\treturn merged_nodes", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tmerged_nodes = dict_merge(node, merged_nodes)\n\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn MappingNode(result)\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdef merge_values(node1, node2):\n\t\tfor key in node2.keys():\n\t\t\tvalue = node2.get(key)\n\t\t\tif key in node1:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tnode1[key].merge(value)\n\t\t\t\telse:\n\t\t\t\t\tnode1[key] = value\n\t\t\telse:\n\t\t\t\tnode1[key] = value\n\t\treturn node1\n\t\n\tnode = nodes[0]\n\tfor node in nodes[1:]:\n\t\tmerge_values(node, node)\n\treturn node", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tmerged_nodes.setdefault(key, []).append(value)\n\treturn {key: value[-1] if isinstance(value[-1], MappingNode) else value for key, value in merged_nodes.items()}\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult_nodes = dict()\n\tfor node_path, node in nodes.items():\n\t\tresult_nodes = deep_merge_nodes_path(result_nodes, node_path, node)\n\treturn result_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tkeys = set()\n\t\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in keys:\n\t\t\t\tkeys.add(key)\n\t\t\t\tmerged_nodes[key] = value\n\t\t\telif isinstance(merged_nodes[key], MappingNode):\n\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\t\n\treturn merged_nodes", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdef _deep_merge_nodes(nodes):\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tresult.update(node.to_dict())\n\t\t\tfor key, value in node.items():\n\t\t\t\tif key in result and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key].update(value)\n\t\treturn result\n\n\tif nodes:\n\t\tresult = {\n\t\t\tkey: _deep_merge_nodes(value)\n\t\t\tfor key, value in groupby(nodes, lambda node: node.key)\n\t\t}\n\t\tfor key, value in result.items():\n\t\t\tif len(value) == 1:\n\t\t\t\tresult[key] = value[0]\n\t\treturn result\n\n\treturn {}\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult_dict = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tresult_dict[key] = value\n\treturn result_dict\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = dict()\n\tfor node in nodes:\n\t\tnode.merge_to_dict(merged_nodes)\n\treturn merged_nodes", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tmerged_nodes = dict_merge(node, merged_nodes)\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn MappingNode(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdef merge_values(node1, node2):\n\t\tfor key in node2.keys():\n\t\t\tvalue = node2.get(key)\n\t\t\tif key in node1:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tnode1[key].merge(value)\n\t\t\t\telse:\n\t\t\t\t\tnode1[key] = value\n\t\t\telse:\n\t\t\t\tnode1[key] = value\n\t\treturn node1\n\t\n\tnode = nodes[0]\n\tfor node in nodes[1:]:\n\t\tmerge_values(node, node)\n\treturn node", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tmerged_nodes.setdefault(key, []).append(value)\n\treturn {key: value[-1] if isinstance(value[-1], MappingNode) else value for key, value in merged_nodes.items()}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult_nodes = dict()\n\tfor node_path, node in nodes.items():\n\t\tresult_nodes = deep_merge_nodes_path(result_nodes, node_path, node)\n\treturn result_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tkeys = set()\n\t\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in keys:\n\t\t\t\tkeys.add(key)\n\t\t\t\tmerged_nodes[key] = value\n\t\t\telif isinstance(merged_nodes[key], MappingNode):\n\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\t\n\treturn merged_nodes", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdef _deep_merge_nodes(nodes):\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tresult.update(node.to_dict())\n\t\t\tfor key, value in node.items():\n\t\t\t\tif key in result and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key].update(value)\n\t\treturn result\n\n\tif nodes:\n\t\tresult = {\n\t\t\tkey: _deep_merge_nodes(value)\n\t\t\tfor key, value in groupby(nodes, lambda node: node.key)\n\t\t}\n\t\tfor key, value in result.items():\n\t\t\tif len(value) == 1:\n\t\t\t\tresult[key] = value[0]\n\t\treturn result\n\n\treturn {}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult_dict = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tresult_dict[key] = value\n\treturn result_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Blazing fast, multi-threaded, command-line file search utility\")\n\n\tparser.add_argument('-a', '--all', help='Search all files', action='store_true')\n\tparser.add_argument('-d', '--dir', help='Search directories as well as files', action='store_true')\n\tparser.add_argument('-p', '--print', help='Print the matched files instead of copying them to the destination', action='store_true')\n\tparser.add_argument('-r', '--recursive', help='Search directories recursively', action='store_true')\n\tparser.add_argument('-m', '--multiple', help='Copy multiple files to destination', action='store_true')\n\tparser.add_argument('-V', '--version', help='Print version number and exit', action='store_true')\n\tparser.add_argument('-v', '--verbose', help='Print verbose output', action='store_true')\n\tparser.add_argument('-i', '--ignore', help='Ignore the specified files and directories', nargs='*')\n\n\tparser.add_argument('src", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(*argument[0], **argument[1])\n\targs = parser.parse_args()\n\treturn args\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Convert a csv file to a json file\")\n\tparser.add_argument(\"csv_file\", help=\"The name of the csv file that should be converted to json\")\n\tparser.add_argument(\"json_file\", help=\"The name of the json file that the csv file should be converted to\")\n\treturn parser.parse_args(arguments)", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Generate a CSV file containing all the files in a directory and its subdirectories.')\n\tparser.add_argument('-d', '--directory', type=str, required=True, help='The directory from which to generate a CSV list')\n\tparser.add_argument('-o', '--output', type=str, required=True, help='The path to the CSV file to be generated')\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-d', '--directory', help='directory to search for files to compress', required=True)\n\tparser.add_argument('-c', '--config', help='config file', required=True)\n\tparser.add_argument('-o', '--out-dir', help='output directory', required=True)\n\tparser.add_argument('-s', '--size', help='size of the data to compress', required=True)\n\tparser.add_argument('-l', '--log-dir', help='directory to place log files', required=True)\n\tparser.add_argument('-z', '--zstd', help='zstd executable', default='zstd')\n\tparser.add_argument('-p', '--parallel', help='number of threads to use', default=8)\n\tparser.add_argument('-m', '--memory', help='maximum memory per worker, in GB', default=4)\n\tparser.add_argument('-x', '--compress', help='compression level', default=19)\n\tparser.add_argument('--no-compress', help='do not compress', action='store_true', default=", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')\n\tparser.add_argument('--sum', dest='accumulate', action='store_const', const=sum, default=max, help='sum the integers')\n\t\n\treturn parser.parse_args(arguments)", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('-i', '--input', help='Path to input file', required=True)\n\tparser.add_argument('-o', '--output', help='Path to output file', required=True)\n\n\treturn parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"This script will run a series of tests on the provided input file. It will report if any of the tests fail and the line number of the input file where they failed.\")\n\tparser.add_argument('-i', '--input', help=\"The input file to test\")\n\tparser.add_argument('-v', '--verbose', help=\"Print out all the tests that are being run\", action='store_true')\n\tparser.add_argument('-s', '--silent', help=\"Do not print the results of each test\", action='store_true')\n\tparser.add_argument('-o', '--output', help=\"The output file to write the results to\")\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"A simple script to take a file or directory and rename each file in it to a sequential number, optionally with leading zeros\",\n\t\tprog=\"rename\")\n\tparser.add_argument(\"-d\", \"--dir\", type=str, help=\"The directory, or path, to rename the files in\")\n\tparser.add_argument(\"-f\", \"--file\", type=str, help=\"The file to rename\")\n\tparser.add_argument(\"-s\", \"--start\", type=int, help=\"The number to start renaming the files at\")\n\tparser.add_argument(\"-p\", \"--padding\", type=int, help=\"The number of leading zeros to add\")\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='')\n\tparser.add_argument('-l', '--language', dest='language', help='The language to use', default='en')\n\tparser.add_argument('-v', '--verbose', dest='verbose', help='Whether to print verbose output', action='store_true')\n\tparser.add_argument('-c', '--csv', dest='csv', help='Whether to use CSV files to store data')\n\tparser.add_argument('-e', '--elasticsearch', dest='elasticsearch', help='Whether to store data in ElasticSearch')\n\tparser.add_argument('-k', '--kibana', dest='kibana', help='Whether to use Kibana to visualize the data')\n\tparser.add_argument('-f', '--filter', dest='filter', help='The filter to apply to the Wikipedia pages', default='all')\n\tparser.add_argument('-s', '--start', dest='start', help='The starting page in a category', default='')\n\tparser.add_argument('-t', '--time', dest='time', help='The number of seconds to wait between requests', default="], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Blazing fast, multi-threaded, command-line file search utility\")\n\n\tparser.add_argument('-a', '--all', help='Search all files', action='store_true')\n\tparser.add_argument('-d', '--dir', help='Search directories as well as files', action='store_true')\n\tparser.add_argument('-p', '--print', help='Print the matched files instead of copying them to the destination', action='store_true')\n\tparser.add_argument('-r', '--recursive', help='Search directories recursively', action='store_true')\n\tparser.add_argument('-m', '--multiple', help='Copy multiple files to destination', action='store_true')\n\tparser.add_argument('-V', '--version', help='Print version number and exit', action='store_true')\n\tparser.add_argument('-v', '--verbose', help='Print verbose output', action='store_true')\n\tparser.add_argument('-i', '--ignore', help='Ignore the specified files and directories', nargs='*')\n\n\tparser.add_argument('src", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(*argument[0], **argument[1])\n\targs = parser.parse_args()\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Convert a csv file to a json file\")\n\tparser.add_argument(\"csv_file\", help=\"The name of the csv file that should be converted to json\")\n\tparser.add_argument(\"json_file\", help=\"The name of the json file that the csv file should be converted to\")\n\treturn parser.parse_args(arguments)", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Generate a CSV file containing all the files in a directory and its subdirectories.')\n\tparser.add_argument('-d', '--directory', type=str, required=True, help='The directory from which to generate a CSV list')\n\tparser.add_argument('-o', '--output', type=str, required=True, help='The path to the CSV file to be generated')\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-d', '--directory', help='directory to search for files to compress', required=True)\n\tparser.add_argument('-c', '--config', help='config file', required=True)\n\tparser.add_argument('-o', '--out-dir', help='output directory', required=True)\n\tparser.add_argument('-s', '--size', help='size of the data to compress', required=True)\n\tparser.add_argument('-l', '--log-dir', help='directory to place log files', required=True)\n\tparser.add_argument('-z', '--zstd', help='zstd executable', default='zstd')\n\tparser.add_argument('-p', '--parallel', help='number of threads to use', default=8)\n\tparser.add_argument('-m', '--memory', help='maximum memory per worker, in GB', default=4)\n\tparser.add_argument('-x', '--compress', help='compression level', default=19)\n\tparser.add_argument('--no-compress', help='do not compress', action='store_true', default=", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')\n\tparser.add_argument('--sum', dest='accumulate', action='store_const', const=sum, default=max, help='sum the integers')\n\t\n\treturn parser.parse_args(arguments)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('-i', '--input', help='Path to input file', required=True)\n\tparser.add_argument('-o', '--output', help='Path to output file', required=True)\n\n\treturn parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"This script will run a series of tests on the provided input file. It will report if any of the tests fail and the line number of the input file where they failed.\")\n\tparser.add_argument('-i', '--input', help=\"The input file to test\")\n\tparser.add_argument('-v', '--verbose', help=\"Print out all the tests that are being run\", action='store_true')\n\tparser.add_argument('-s', '--silent', help=\"Do not print the results of each test\", action='store_true')\n\tparser.add_argument('-o', '--output', help=\"The output file to write the results to\")\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"A simple script to take a file or directory and rename each file in it to a sequential number, optionally with leading zeros\",\n\t\tprog=\"rename\")\n\tparser.add_argument(\"-d\", \"--dir\", type=str, help=\"The directory, or path, to rename the files in\")\n\tparser.add_argument(\"-f\", \"--file\", type=str, help=\"The file to rename\")\n\tparser.add_argument(\"-s\", \"--start\", type=int, help=\"The number to start renaming the files at\")\n\tparser.add_argument(\"-p\", \"--padding\", type=int, help=\"The number of leading zeros to add\")\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='')\n\tparser.add_argument('-l', '--language', dest='language', help='The language to use', default='en')\n\tparser.add_argument('-v', '--verbose', dest='verbose', help='Whether to print verbose output', action='store_true')\n\tparser.add_argument('-c', '--csv', dest='csv', help='Whether to use CSV files to store data')\n\tparser.add_argument('-e', '--elasticsearch', dest='elasticsearch', help='Whether to store data in ElasticSearch')\n\tparser.add_argument('-k', '--kibana', dest='kibana', help='Whether to use Kibana to visualize the data')\n\tparser.add_argument('-f', '--filter', dest='filter', help='The filter to apply to the Wikipedia pages', default='all')\n\tparser.add_argument('-s', '--start', dest='start', help='The starting page in a category', default='')\n\tparser.add_argument('-t', '--time', dest='time', help='The number of seconds to wait between requests', default=", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_usage().split('[--')[1].split(']')[0].replace(',', '')\n\treturn ' '.join(flag.split() for flag in flags.split('--') if flag)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_help()\n\tflags = flags.split(\"positional arguments:\")[-1]\n\tflags = flags.replace(\"optional arguments:\", \"\")\n\tflags = flags.replace(\"\\n\\n\", \"\")\n\tflags = flags.replace(\"-h, --help\", \"\")\n\tflags = flags.replace(\"show this help message and exit\", \"\")\n\tflags = flags.replace(\"  \", \" \")\n\tflags = flags.replace(\"\\n\", \" \")\n\tflags = flags.replace(\"  \", \" \")\n\tflags = \" \".join(flags.split())\n\treturn flags.strip()\n\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor action in parser._actions:\n\t\tflag = \"--\" if action.option_strings else \"-\"\n\t\tflag += \"\".join(action.option_strings)\n\t\t# if flag.endswith('-'):\n\t\t# \tflag = flag[:-1]\n\t\tflags += flag + \" \"\n\treturn flags\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor action in parser._actions:\n\t\tflags += \" \" + \" \".join([tuple(t)[0] for t in action.option_strings])\n\n\treturn flags\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_usage().split()\n\tflags = [flag for flag in flags if flag.startswith(\"-\")]\n\treturn \" \".join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn \" \".join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_help()\n\tflags = flags[flags.index('options:')+8:]\n\tflags = flags[:flags.index('\\n')]\n\treturn flags", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tflags.append(action.option_strings[0].replace(\"--\", \"-\"))\n\treturn \" \".join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\ttry:\n\t\treturn \" \".join(parser.format_usage().split()[2:])\n\texcept AttributeError:\n\t\treturn \" \".join(parser.format_help().split()[2:])", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn ' '.join(flags)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_usage().split('[--')[1].split(']')[0].replace(',', '')\n\treturn ' '.join(flag.split() for flag in flags.split('--') if flag)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_help()\n\tflags = flags.split(\"positional arguments:\")[-1]\n\tflags = flags.replace(\"optional arguments:\", \"\")\n\tflags = flags.replace(\"\\n\\n\", \"\")\n\tflags = flags.replace(\"-h, --help\", \"\")\n\tflags = flags.replace(\"show this help message and exit\", \"\")\n\tflags = flags.replace(\"  \", \" \")\n\tflags = flags.replace(\"\\n\", \" \")\n\tflags = flags.replace(\"  \", \" \")\n\tflags = \" \".join(flags.split())\n\treturn flags.strip()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor action in parser._actions:\n\t\tflag = \"--\" if action.option_strings else \"-\"\n\t\tflag += \"\".join(action.option_strings)\n\t\t# if flag.endswith('-'):\n\t\t# \tflag = flag[:-1]\n\t\tflags += flag + \" \"\n\treturn flags\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor action in parser._actions:\n\t\tflags += \" \" + \" \".join([tuple(t)[0] for t in action.option_strings])\n\n\treturn flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_usage().split()\n\tflags = [flag for flag in flags if flag.startswith(\"-\")]\n\treturn \" \".join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn \" \".join(flags)", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_help()\n\tflags = flags[flags.index('options:')+8:]\n\tflags = flags[:flags.index('\\n')]\n\treturn flags", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tflags.append(action.option_strings[0].replace(\"--\", \"-\"))\n\treturn \" \".join(flags)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\ttry:\n\t\treturn \" \".join(parser.format_usage().split()[2:])\n\texcept AttributeError:\n\t\treturn \" \".join(parser.format_help().split()[2:])", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn ' '.join(flags)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\tfor argument in unparsed_arguments:\n\t\tkey_value = argument.split(\"=\")\n\t\targuments[key_value[0]] = key_value[1]\n\treturn arguments", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\t\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif unparsed_argument.startswith('--'):\n\t\t\targument, value = unparsed_argument.split('=')\n\t\t\targument = argument[2:]\n\t\t\targuments[argument] = value\n\t\n\treturn arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\tcounter = 0\n\twhile counter < len(unparsed_arguments):\n\t\tcurrent_argument = unparsed_arguments[counter]\n\t\tif current_argument[0] == \"-\":\n\t\t\targuments[current_argument] = unparsed_arguments[counter + 1]\n\t\t\tcounter += 1\n\t\tcounter += 1\n\treturn arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\n\tfor argument in unparsed_arguments:\n\t\tkey, value = argument.split('=')\n\t\targuments[key] = value\n\n\treturn arguments\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparameter_values = {}\n\tfor raw_argument in unparsed_arguments:\n\t\targument = raw_argument.split(':')\n\t\tparameter_values[argument[0]] = argument[1]\n\treturn parameter_values\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = dict()\n\n\t# Go through all arguments\n\tfor argument in unparsed_arguments:\n\t\t# Check if argument is key-value pair\n\t\tsplitted_argument = argument.split(\"=\")\n\n\t\tif len(splitted_argument) == 2:\n\t\t\t# Add key-value pair to arguments map\n\t\t\targuments[splitted_argument[0]] = splitted_argument[1]\n\n\treturn arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments_dict = {}\n\tfor argument in unparsed_arguments:\n\t\targument = argument.split(\"=\", 1)\n\t\targuments_dict[argument[0]] = argument[1]\n\treturn arguments_dict\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparsed_arguments = {}\n\t\n\tfor argument in unparsed_arguments:\n\t\targument_split = argument.split('=')\n\t\tparsed_arguments[argument_split[0]] = argument_split[1]\n\t\t\n\treturn parsed_arguments\n\t", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tfrom collections import defaultdict\n\t\n\targuments = defaultdict(list)\n\t\n\tfor arg in unparsed_arguments:\n\t\tkey, value = arg.split('=')\n\t\targuments[key.lower()].append(value)\n\t\n\treturn arguments", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparams = {}\n\tprevious_key = None\n\tfor argument in unparsed_arguments:\n\t\tif argument.startswith('--'):\n\t\t\tkey = argument.split('=', 1)[0][2:]\n\t\t\tparams[key] = True\n\t\t\tprevious_key = key\n\t\telif previous_key is not None:\n\t\t\tparams[previous_key] = argument\n\t\t\tprevious_key = None\n\t\telse:\n\t\t\traise ValueError('Invalid argument: {}'.format(argument))\n\treturn params\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\tfor argument in unparsed_arguments:\n\t\tkey_value = argument.split(\"=\")\n\t\targuments[key_value[0]] = key_value[1]\n\treturn arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\t\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif unparsed_argument.startswith('--'):\n\t\t\targument, value = unparsed_argument.split('=')\n\t\t\targument = argument[2:]\n\t\t\targuments[argument] = value\n\t\n\treturn arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\tcounter = 0\n\twhile counter < len(unparsed_arguments):\n\t\tcurrent_argument = unparsed_arguments[counter]\n\t\tif current_argument[0] == \"-\":\n\t\t\targuments[current_argument] = unparsed_arguments[counter + 1]\n\t\t\tcounter += 1\n\t\tcounter += 1\n\treturn arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\n\tfor argument in unparsed_arguments:\n\t\tkey, value = argument.split('=')\n\t\targuments[key] = value\n\n\treturn arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparameter_values = {}\n\tfor raw_argument in unparsed_arguments:\n\t\targument = raw_argument.split(':')\n\t\tparameter_values[argument[0]] = argument[1]\n\treturn parameter_values\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = dict()\n\n\t# Go through all arguments\n\tfor argument in unparsed_arguments:\n\t\t# Check if argument is key-value pair\n\t\tsplitted_argument = argument.split(\"=\")\n\n\t\tif len(splitted_argument) == 2:\n\t\t\t# Add key-value pair to arguments map\n\t\t\targuments[splitted_argument[0]] = splitted_argument[1]\n\n\treturn arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments_dict = {}\n\tfor argument in unparsed_arguments:\n\t\targument = argument.split(\"=\", 1)\n\t\targuments_dict[argument[0]] = argument[1]\n\treturn arguments_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparsed_arguments = {}\n\t\n\tfor argument in unparsed_arguments:\n\t\targument_split = argument.split('=')\n\t\tparsed_arguments[argument_split[0]] = argument_split[1]\n\t\t\n\treturn parsed_arguments\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tfrom collections import defaultdict\n\t\n\targuments = defaultdict(list)\n\t\n\tfor arg in unparsed_arguments:\n\t\tkey, value = arg.split('=')\n\t\targuments[key.lower()].append(value)\n\t\n\treturn arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparams = {}\n\tprevious_key = None\n\tfor argument in unparsed_arguments:\n\t\tif argument.startswith('--'):\n\t\t\tkey = argument.split('=', 1)[0][2:]\n\t\t\tparams[key] = True\n\t\t\tprevious_key = key\n\t\telif previous_key is not None:\n\t\t\tparams[previous_key] = argument\n\t\t\tprevious_key = None\n\t\telse:\n\t\t\traise ValueError('Invalid argument: {}'.format(argument))\n\treturn params\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = unparsed_arguments[:]\n\n\tfor subparser_name, parser in subparsers.items():\n\t\ttry:\n\t\t\tnamespace, unparsed_arguments = parser.parse_known_args(unparsed_arguments)\n\t\t\tparsed_arguments[subparser_name] = namespace\n\t\texcept SystemExit:\n\t\t\tpass\n\n\treturn parsed_arguments, unparsed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_subparser_names = subparsers.keys()\n\tsuccessful_subparsers = []\n\tremaining_arguments = unparsed_arguments\n\tfor subparser_name in all_subparser_names:\n\t\tcurrent_subparser = subparsers[subparser_name]\n\t\ttry:\n\t\t\tcurrent_namespace, remaining_arguments = current_subparser.parse_known_args(remaining_arguments)\n\t\t\tsuccessful_subparsers.append((subparser_name, current_namespace))\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn successful_subparsers, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tif argument.startswith(\"--\"):\n\t\t\tremaining_arguments.append(argument)\n\t\telse:\n\t\t\tparsed_argument = False\n\t\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\t\ttry:\n\t\t\t\t\tparsed_argument = subparser.parse_args([argument])\n\t\t\t\texcept SystemExit:\n\t\t\t\t\tpass\n\t\t\t\tif parsed_argument:\n\t\t\t\t\tparsed_arguments[subparser_name] = parsed_argument\n\t\t\t\t\tbreak\n\t\t\tif not parsed_argument:\n\t\t\t\tremaining_arguments.append(argument)\n\treturn parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = set(subparsers.keys())\n\tclaimed = set()\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\n\twhile len(unparsed_arguments) > 0:\n\t\targument = unparsed_arguments[0]\n\t\tif argument in subparser_names:\n\t\t\tsubparser = subparsers[argument]\n\t\t\tparsed, remaining = subparser.parse_known_args(unparsed_arguments)\n\t\t\tparsed_arguments[argument] = parsed\n\t\t\tclaimed.add(argument)\n\t\t\tunparsed_arguments = remaining\n\t\telse:\n\t\t\tremaining_arguments.append(argument)\n\t\t\tunparsed_arguments = unparsed_arguments[1:]\n\n\treturn parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunclaimed_arguments = unparsed_arguments[:]\n\tfor action in subparsers:\n\t\tparser = subparsers[action]\n\t\ttry:\n\t\t\targs, unclaimed_arguments = parser.parse_known_args(unclaimed_arguments)\n\t\t\tparsed_arguments[action] = args\n\t\texcept SystemExit:\n\t\t\t# Catch failures from the subparser, likely due to missing arguments\n\t\t\tpass\n\treturn parsed_arguments, unclaimed_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunclaimed_arguments = unparsed_arguments[:]\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tparsed_arguments[subparser_name], unclaimed_arguments = subparser.parse_known_args(unclaimed_arguments)\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn parsed_arguments, unclaimed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsers = {}\n\tfor name, parser in subparsers.items():\n\t\tparsers[name] = parser.parse_known_args(unparsed_arguments)\n\n\tremaining_arguments = []\n\tfor index in range(len(unparsed_arguments)):\n\t\targument = unparsed_arguments[index]\n\t\tparsers_that_claim_this_argument = []\n\t\tfor name, (parsed_arguments, remaining_arguments) in parsers.items():\n\t\t\tparsers_that_claim_this_argument.append((name, remaining_arguments))\n\n\t\tif len(parsers_that_claim_this_argument) == 1:\n\t\t\tname, remaining_arguments = parsers_that_claim_this_argument[0]\n\t\t\tif not remaining_arguments:\n\t\t\t\tremaining_arguments.append(argument)\n\t\t\t\tparsers[name] = (parsed_arguments, remaining_arguments)\n\t\t\telse:\n\t\t\t\tremaining_arguments.append(argument)\n\t\telse:\n\t\t\tremaining_arguments.append(argument)\n\n\treturn parsers, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_argument_sets = {\n\t\tname: subparser.parse_known_args(unparsed_arguments)\n\t\tfor name, subparser in subparsers.items()\n\t}\n\tsubparser_names_and_arguments = {\n\t\tname: subparser_result[0]\n\t\tfor name, subparser_result in subparser_argument_sets.items()\n\t}\n\t# Only one subparser can claim an argument.\n\tsubparser_name = next(\n\t\t(\n\t\t\tname\n\t\t\tfor name, arguments in subparser_argument_sets.items()\n\t\t\tif arguments[1]\n\t\t),\n\t\tNone,\n\t)\n\tremaining_arguments = (\n\t\tsubparser_argument_sets[subparser_name][1]\n\t\tif subparser_name is not None\n\t\telse []\n\t)\n\treturn subparser_names_and_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tunparsed_for_subparsers = {}\n\tremaining_arguments = []\n\tfor name in subparsers:\n\t\tsubparser = subparsers[name]\n\t\tparser = subparser.get_parser()\n\t\ttry:\n\t\t\tunparsed_for_subparsers[name] = parser.parse_args(unparsed_arguments)\n\t\texcept SystemExit:\n\t\t\tpass\n\t\telse:\n\t\t\tremaining_arguments = []\n\t\t\tbreak\n\t\telse:\n\t\t\tremaining_arguments = unparsed_arguments[len(sys.argv) - len(unparsed_for_subparsers[name]):]\n\treturn unparsed_for_subparsers, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tclaimed_arguments = set()\n\tclaimed_subparsers = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\tclaimed_subparsers.append(subparser_name)\n\t\t\tclaimed_arguments.update(parsed_arguments[1])\n\t\texcept SystemExit:\n\t\t\t# We don't need to handle any exceptions; argparse will have already printed\n\t\t\t# any error message and exited with status 2.\n\t\t\tpass\n\n\tremaining_arguments = [arg for arg in unparsed_arguments if arg not in claimed_arguments]\n\treturn claimed_subparsers, remaining_arguments\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = unparsed_arguments[:]\n\n\tfor subparser_name, parser in subparsers.items():\n\t\ttry:\n\t\t\tnamespace, unparsed_arguments = parser.parse_known_args(unparsed_arguments)\n\t\t\tparsed_arguments[subparser_name] = namespace\n\t\texcept SystemExit:\n\t\t\tpass\n\n\treturn parsed_arguments, unparsed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_subparser_names = subparsers.keys()\n\tsuccessful_subparsers = []\n\tremaining_arguments = unparsed_arguments\n\tfor subparser_name in all_subparser_names:\n\t\tcurrent_subparser = subparsers[subparser_name]\n\t\ttry:\n\t\t\tcurrent_namespace, remaining_arguments = current_subparser.parse_known_args(remaining_arguments)\n\t\t\tsuccessful_subparsers.append((subparser_name, current_namespace))\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn successful_subparsers, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tif argument.startswith(\"--\"):\n\t\t\tremaining_arguments.append(argument)\n\t\telse:\n\t\t\tparsed_argument = False\n\t\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\t\ttry:\n\t\t\t\t\tparsed_argument = subparser.parse_args([argument])\n\t\t\t\texcept SystemExit:\n\t\t\t\t\tpass\n\t\t\t\tif parsed_argument:\n\t\t\t\t\tparsed_arguments[subparser_name] = parsed_argument\n\t\t\t\t\tbreak\n\t\t\tif not parsed_argument:\n\t\t\t\tremaining_arguments.append(argument)\n\treturn parsed_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunclaimed_arguments = unparsed_arguments[:]\n\tfor action in subparsers:\n\t\tparser = subparsers[action]\n\t\ttry:\n\t\t\targs, unclaimed_arguments = parser.parse_known_args(unclaimed_arguments)\n\t\t\tparsed_arguments[action] = args\n\t\texcept SystemExit:\n\t\t\t# Catch failures from the subparser, likely due to missing arguments\n\t\t\tpass\n\treturn parsed_arguments, unclaimed_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunclaimed_arguments = unparsed_arguments[:]\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tparsed_arguments[subparser_name], unclaimed_arguments = subparser.parse_known_args(unclaimed_arguments)\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn parsed_arguments, unclaimed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsers = {}\n\tfor name, parser in subparsers.items():\n\t\tparsers[name] = parser.parse_known_args(unparsed_arguments)\n\n\tremaining_arguments = []\n\tfor index in range(len(unparsed_arguments)):\n\t\targument = unparsed_arguments[index]\n\t\tparsers_that_claim_this_argument = []\n\t\tfor name, (parsed_arguments, remaining_arguments) in parsers.items():\n\t\t\tparsers_that_claim_this_argument.append((name, remaining_arguments))\n\n\t\tif len(parsers_that_claim_this_argument) == 1:\n\t\t\tname, remaining_arguments = parsers_that_claim_this_argument[0]\n\t\t\tif not remaining_arguments:\n\t\t\t\tremaining_arguments.append(argument)\n\t\t\t\tparsers[name] = (parsed_arguments, remaining_arguments)\n\t\t\telse:\n\t\t\t\tremaining_arguments.append(argument)\n\t\telse:\n\t\t\tremaining_arguments.append(argument)\n\n\treturn parsers, remaining_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_argument_sets = {\n\t\tname: subparser.parse_known_args(unparsed_arguments)\n\t\tfor name, subparser in subparsers.items()\n\t}\n\tsubparser_names_and_arguments = {\n\t\tname: subparser_result[0]\n\t\tfor name, subparser_result in subparser_argument_sets.items()\n\t}\n\t# Only one subparser can claim an argument.\n\tsubparser_name = next(\n\t\t(\n\t\t\tname\n\t\t\tfor name, arguments in subparser_argument_sets.items()\n\t\t\tif arguments[1]\n\t\t),\n\t\tNone,\n\t)\n\tremaining_arguments = (\n\t\tsubparser_argument_sets[subparser_name][1]\n\t\tif subparser_name is not None\n\t\telse []\n\t)\n\treturn subparser_names_and_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tunparsed_for_subparsers = {}\n\tremaining_arguments = []\n\tfor name in subparsers:\n\t\tsubparser = subparsers[name]\n\t\tparser = subparser.get_parser()\n\t\ttry:\n\t\t\tunparsed_for_subparsers[name] = parser.parse_args(unparsed_arguments)\n\t\texcept SystemExit:\n\t\t\tpass\n\t\telse:\n\t\t\tremaining_arguments = []\n\t\t\tbreak\n\t\telse:\n\t\t\tremaining_arguments = unparsed_arguments[len(sys.argv) - len(unparsed_for_subparsers[name]):]\n\treturn unparsed_for_subparsers, remaining_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tclaimed_arguments = set()\n\tclaimed_subparsers = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\tclaimed_subparsers.append(subparser_name)\n\t\t\tclaimed_arguments.update(parsed_arguments[1])\n\t\texcept SystemExit:\n\t\t\t# We don't need to handle any exceptions; argparse will have already printed\n\t\t\t# any error message and exited with status 2.\n\t\t\tpass\n\n\tremaining_arguments = [arg for arg in unparsed_arguments if arg not in claimed_arguments]\n\treturn claimed_subparsers, remaining_arguments\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"A command line tool to help with the development of the Hackathon Challenge.\")\n\tsubparsers = parser.add_subparsers()\n\n\tmake_train_parser(subparsers)\n\tmake_test_parser(subparsers)\n\tmake_predict_parser(subparsers)\n\n\tparser.add_argument('--debug', action='store_true', help='Print debug information to the console.')\n\n\treturn parser\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\timport textwrap\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A script to find and replace text in files.\",\n\t\tepilog=textwrap.dedent(\"\"\"\n\t\t\tCopyright 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023 William N. Braswell\n\t\t\tThis program is released under the terms of the GNU General Public License v3.0.\n\t\t\"\"\")\n\t)\n\n\tparser.add_argument(\n\t\t\"-v\", \"--version\",\n\t\thelp=\"display version information\",\n\t\taction=\"version\",\n\t\tversion=f\"{parser.prog} 1.0.10\"\n\t)\n\n\tparser.add_argument(\n\t\t\"-b\", \"--binary\",\n\t\thelp=\"open files in binary mode\",\n\t\taction=\"store_true\"\n\t)\n\n\tparser.add_argument(\n\t\t\"-e\", \"--encoding\",\n\t\thelp=\"specify encoding for text files\",\n\t\tdefault=\"utf-8\"\n\t)\n\n\tparser.add_argument(", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"Convert a csv file to a sqlite database.\")\n\tparser.add_argument('--db', type=str, default='db.sqlite',\n\t\t\t\t\t\thelp=\"SQLite database to store data in.\")\n\tparser.add_argument('--csv', type=str, default='data.csv',\n\t\t\t\t\t\thelp=\"CSV file to convert to SQLite\")\n\tparser.add_argument('--table', type=str, default='data',\n\t\t\t\t\t\thelp=\"Name of SQLite table to store data in.\")\n\tparser.add_argument('--delim', type=str, default=',',\n\t\t\t\t\t\thelp=\"Delimiter to use when parsing CSV file.\")\n\tparser.add_argument('--quote', type=str, default='\"',\n\t\t\t\t\t\thelp=\"Quote character to use when parsing CSV file.\")\n\tparser.add_argument('--header', type=str, default='first',\n\t\t\t\t\t\thelp=\"Whether the CSV file has a header row.\")\n\treturn parser\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('integers', metavar='N', type=int, nargs='+',\n\t                    help='an integer for the accumulator')\n\tparser.add_argument('--sum', dest='accumulate', action='store_const',\n\t                    const=sum, default=max,\n\t                    help='sum the integers (default: find the max)')\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_a = subparsers.add_parser('a')\n\tparser_a.add_argument('x', type=int, help='the base')\n\tparser_a.add_argument('y', type=int, help='the exponent')\n\tparser_a.set_defaults(func=print_a)\n\n\tparser_b = subparsers.add_parser('b')\n\tparser_b.add_argument('x', type=int, help='the base')\n\tparser_b.add_argument('y', type=int, help='the exponent')\n\tparser_b.set_defaults(func=print_b)\n\n\treturn parser, parser_a, parser_b", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description='Generate a random number')\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s 1.0')\n\tparser.add_argument('-n', '--num', type=int, help='number of random numbers to generate')\n\tparser.add_argument('-f', '--file', help='write output to FILE', metavar='FILE')\n\tparser.add_argument('-d', '--delimiter', help='use DELIMITER instead of TAB for field delimiter', metavar='DELIMITER')\n\tparser.add_argument('-s', '--seed', type=int, help='random number generator seed')\n\tparser.add_argument('-m', '--min', type=int, help='generate integers at least MIN', metavar='MIN')\n\tparser.add_argument('-M', '--max', type=int, help='generate integers at most MAX', metavar='MAX')\n\tparser.add_argument('-c', '--chars', type=int, help='generate CHARS random characters', metavar='CHARS')\n\tparser.", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Generate a 10-minute video from a given image, sound, and a video editor.\")\n\n\tsubparsers = parser.add_subparsers(dest=\"command\", help=\"command to run\")\n\tsubparsers.required = True\n\n\tparser_process = subparsers.add_parser(\"process\", help=\"process a folder of images and sounds\")\n\tparser_process.add_argument(\"--dir\", type=str, help=\"process a folder of images and sounds\", required=True)\n\n\tparser_generate = subparsers.add_parser(\"generate\", help=\"generate a 10-minute video from a folder of images and sounds\")\n\tparser_generate.add_argument(\"--dir\", type=str, help=\"generate a 10-minute video from a folder of images and sounds\", required=True)\n\n\tparser_convert = subparsers.add_parser(\"convert\", help=\"convert a video to a folder of images and sounds\")\n\tparser_convert.add_argument(\"--dir\", type=str, help=\"convert a video to a folder of images and sounds\", required=True)\n\n\treturn parser, parser_process, parser_generate, parser_convert\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Bump the version of a project.\"\n\t)\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_major = subparsers.add_parser(\"major\", help=\"Bump major version.\")\n\tparser_major.set_defaults(which=\"major\")\n\n\tparser_minor = subparsers.add_parser(\"minor\", help=\"Bump minor version.\")\n\tparser_minor.set_defaults(which=\"minor\")\n\n\tparser_patch = subparsers.add_parser(\"patch\", help=\"Bump patch version.\")\n\tparser_patch.set_defaults(which=\"patch\")\n\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription = \"A pipeline for the creation of a set of images to use for training a neural network\"\n\t)\n\tsubparsers = parser.add_subparsers()\n\n\tmake_parser = subparsers.add_parser(\"make\")\n\tmake_parser.add_argument(\"--output-dir\", type = str, required = True, help = \"The directory to store the output images in\")\n\tmake_parser.add_argument(\"--seed\", type = int, required = False, default = None, help = \"A seed for the random number generator\")\n\tmake_parser.add_argument(\"--num-images\", type = int, required = False, default = 1000, help = \"The number of images to generate\")\n\tmake_parser.add_argument(\"--pixel-width\", type = int, required = False, default = 50, help = \"The width (in pixels) of the output image\")\n\tmake_parser.add_argument(\"--pixel-height\", type = int, required = False, default = 50, help = \"The height (in pixels) of the output image\")\n\tmake_parser.add_argument(\"--image-format\", type = str, required = False, default = \"png\", help = \"The format to", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription='A tool for generating and managing an ECS deployment.',\n\t\tepilog='For more information, visit https://github.com/jeremy-miller/ecs-deploy',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter)\n\n\tsubparsers = parser.add_subparsers()\n\n\tsubparsers.required = True\n\tsubparsers.dest = 'subcommand'\n\n\tcmd_deploy = subparsers.add_parser('deploy', help='Deploy an ECS service.')\n\tcmd_deploy.add_argument('service', help='The name of the service to deploy.')\n\tcmd_deploy.add_argument('--cluster', help='The name of the ECS cluster to deploy to.')\n\tcmd_deploy.add_argument('--image', help='The image to deploy.')\n\tcmd_deploy.add_argument('--count',\n\t\ttype=int,\n\t\thelp='The number of tasks to deploy.')\n\tcmd_deploy.add_argument('--force-new-deployment',\n\t\taction='store_true',\n\t\thelp='", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom ._parsers import main_parser, subparsers\n\treturn main_parser, subparsers\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"A command line tool to help with the development of the Hackathon Challenge.\")\n\tsubparsers = parser.add_subparsers()\n\n\tmake_train_parser(subparsers)\n\tmake_test_parser(subparsers)\n\tmake_predict_parser(subparsers)\n\n\tparser.add_argument('--debug', action='store_true', help='Print debug information to the console.')\n\n\treturn parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\timport textwrap\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A script to find and replace text in files.\",\n\t\tepilog=textwrap.dedent(\"\"\"\n\t\t\tCopyright 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023 William N. Braswell\n\t\t\tThis program is released under the terms of the GNU General Public License v3.0.\n\t\t\"\"\")\n\t)\n\n\tparser.add_argument(\n\t\t\"-v\", \"--version\",\n\t\thelp=\"display version information\",\n\t\taction=\"version\",\n\t\tversion=f\"{parser.prog} 1.0.10\"\n\t)\n\n\tparser.add_argument(\n\t\t\"-b\", \"--binary\",\n\t\thelp=\"open files in binary mode\",\n\t\taction=\"store_true\"\n\t)\n\n\tparser.add_argument(\n\t\t\"-e\", \"--encoding\",\n\t\thelp=\"specify encoding for text files\",\n\t\tdefault=\"utf-8\"\n\t)\n\n\tparser.add_argument(", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"Convert a csv file to a sqlite database.\")\n\tparser.add_argument('--db', type=str, default='db.sqlite',\n\t\t\t\t\t\thelp=\"SQLite database to store data in.\")\n\tparser.add_argument('--csv', type=str, default='data.csv',\n\t\t\t\t\t\thelp=\"CSV file to convert to SQLite\")\n\tparser.add_argument('--table', type=str, default='data',\n\t\t\t\t\t\thelp=\"Name of SQLite table to store data in.\")\n\tparser.add_argument('--delim', type=str, default=',',\n\t\t\t\t\t\thelp=\"Delimiter to use when parsing CSV file.\")\n\tparser.add_argument('--quote', type=str, default='\"',\n\t\t\t\t\t\thelp=\"Quote character to use when parsing CSV file.\")\n\tparser.add_argument('--header', type=str, default='first',\n\t\t\t\t\t\thelp=\"Whether the CSV file has a header row.\")\n\treturn parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('integers', metavar='N', type=int, nargs='+',\n\t                    help='an integer for the accumulator')\n\tparser.add_argument('--sum', dest='accumulate', action='store_const',\n\t                    const=sum, default=max,\n\t                    help='sum the integers (default: find the max)')\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_a = subparsers.add_parser('a')\n\tparser_a.add_argument('x', type=int, help='the base')\n\tparser_a.add_argument('y', type=int, help='the exponent')\n\tparser_a.set_defaults(func=print_a)\n\n\tparser_b = subparsers.add_parser('b')\n\tparser_b.add_argument('x', type=int, help='the base')\n\tparser_b.add_argument('y', type=int, help='the exponent')\n\tparser_b.set_defaults(func=print_b)\n\n\treturn parser, parser_a, parser_b", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description='Generate a random number')\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s 1.0')\n\tparser.add_argument('-n', '--num', type=int, help='number of random numbers to generate')\n\tparser.add_argument('-f', '--file', help='write output to FILE', metavar='FILE')\n\tparser.add_argument('-d', '--delimiter', help='use DELIMITER instead of TAB for field delimiter', metavar='DELIMITER')\n\tparser.add_argument('-s', '--seed', type=int, help='random number generator seed')\n\tparser.add_argument('-m', '--min', type=int, help='generate integers at least MIN', metavar='MIN')\n\tparser.add_argument('-M', '--max', type=int, help='generate integers at most MAX', metavar='MAX')\n\tparser.add_argument('-c', '--chars', type=int, help='generate CHARS random characters', metavar='CHARS')\n\tparser.", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Generate a 10-minute video from a given image, sound, and a video editor.\")\n\n\tsubparsers = parser.add_subparsers(dest=\"command\", help=\"command to run\")\n\tsubparsers.required = True\n\n\tparser_process = subparsers.add_parser(\"process\", help=\"process a folder of images and sounds\")\n\tparser_process.add_argument(\"--dir\", type=str, help=\"process a folder of images and sounds\", required=True)\n\n\tparser_generate = subparsers.add_parser(\"generate\", help=\"generate a 10-minute video from a folder of images and sounds\")\n\tparser_generate.add_argument(\"--dir\", type=str, help=\"generate a 10-minute video from a folder of images and sounds\", required=True)\n\n\tparser_convert = subparsers.add_parser(\"convert\", help=\"convert a video to a folder of images and sounds\")\n\tparser_convert.add_argument(\"--dir\", type=str, help=\"convert a video to a folder of images and sounds\", required=True)\n\n\treturn parser, parser_process, parser_generate, parser_convert\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Bump the version of a project.\"\n\t)\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_major = subparsers.add_parser(\"major\", help=\"Bump major version.\")\n\tparser_major.set_defaults(which=\"major\")\n\n\tparser_minor = subparsers.add_parser(\"minor\", help=\"Bump minor version.\")\n\tparser_minor.set_defaults(which=\"minor\")\n\n\tparser_patch = subparsers.add_parser(\"patch\", help=\"Bump patch version.\")\n\tparser_patch.set_defaults(which=\"patch\")\n\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription = \"A pipeline for the creation of a set of images to use for training a neural network\"\n\t)\n\tsubparsers = parser.add_subparsers()\n\n\tmake_parser = subparsers.add_parser(\"make\")\n\tmake_parser.add_argument(\"--output-dir\", type = str, required = True, help = \"The directory to store the output images in\")\n\tmake_parser.add_argument(\"--seed\", type = int, required = False, default = None, help = \"A seed for the random number generator\")\n\tmake_parser.add_argument(\"--num-images\", type = int, required = False, default = 1000, help = \"The number of images to generate\")\n\tmake_parser.add_argument(\"--pixel-width\", type = int, required = False, default = 50, help = \"The width (in pixels) of the output image\")\n\tmake_parser.add_argument(\"--pixel-height\", type = int, required = False, default = 50, help = \"The height (in pixels) of the output image\")\n\tmake_parser.add_argument(\"--image-format\", type = str, required = False, default = \"png\", help = \"The format to", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription='A tool for generating and managing an ECS deployment.',\n\t\tepilog='For more information, visit https://github.com/jeremy-miller/ecs-deploy',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter)\n\n\tsubparsers = parser.add_subparsers()\n\n\tsubparsers.required = True\n\tsubparsers.dest = 'subcommand'\n\n\tcmd_deploy = subparsers.add_parser('deploy', help='Deploy an ECS service.')\n\tcmd_deploy.add_argument('service', help='The name of the service to deploy.')\n\tcmd_deploy.add_argument('--cluster', help='The name of the ECS cluster to deploy to.')\n\tcmd_deploy.add_argument('--image', help='The image to deploy.')\n\tcmd_deploy.add_argument('--count',\n\t\ttype=int,\n\t\thelp='The number of tasks to deploy.')\n\tcmd_deploy.add_argument('--force-new-deployment',\n\t\taction='store_true',\n\t\thelp='", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom ._parsers import main_parser, subparsers\n\treturn main_parser, subparsers\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tnode.deep_merge_into(result)\n\n\treturn result", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(value, MappingNode) and isinstance(result[key], MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tnode_dict = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in node_dict:\n\t\t\t\t# if the value is a MappingNode, merge it\n\t\t\t\tif isinstance(node_dict[key], MappingNode):\n\t\t\t\t\tnode_dict[key].merge(value)\n\t\t\t\t# if the value is a list, append it\n\t\t\t\telif isinstance(node_dict[key], list):\n\t\t\t\t\tnode_dict[key].append(value)\n\t\t\t\t# otherwise, replace it\n\t\t\t\telse:\n\t\t\t\t\tnode_dict[key] = value\n\t\t\telse:\n\t\t\t\tnode_dict[key] = value\n\n\treturn node_dict\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tdeep_merge_node(result, node)\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in merged_nodes:\n\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tif type(merged_nodes[key]) == type(value):\n\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes[key] = value\n\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode):\n\t\t\t\t\tresult[key].update(value)\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif not key in result:\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tif type(result[key]) == MappingNode and type(value) == MappingNode:\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tnode.update(result)\n\t\tresult = node\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tnew_nodes = {}\n\tfor node in nodes:\n\t\tnode.key_path = node.key_path if new_nodes.get(node.key) else node.key\n\t\tif new_nodes.get(node.key):\n\t\t\tnew_nodes[node.key] = deep_merge_nodes([new_nodes[node.key], node])\n\t\telse:\n\t\t\tnew_nodes[node.key] = node\n\treturn list(new_nodes.values())\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tnode.deep_merge_into(result)\n\n\treturn result", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(value, MappingNode) and isinstance(result[key], MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tnode_dict = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in node_dict:\n\t\t\t\t# if the value is a MappingNode, merge it\n\t\t\t\tif isinstance(node_dict[key], MappingNode):\n\t\t\t\t\tnode_dict[key].merge(value)\n\t\t\t\t# if the value is a list, append it\n\t\t\t\telif isinstance(node_dict[key], list):\n\t\t\t\t\tnode_dict[key].append(value)\n\t\t\t\t# otherwise, replace it\n\t\t\t\telse:\n\t\t\t\t\tnode_dict[key] = value\n\t\t\telse:\n\t\t\t\tnode_dict[key] = value\n\n\treturn node_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tdeep_merge_node(result, node)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in merged_nodes:\n\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tif type(merged_nodes[key]) == type(value):\n\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes[key] = value\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode):\n\t\t\t\t\tresult[key].update(value)\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif not key in result:\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tif type(result[key]) == MappingNode and type(value) == MappingNode:\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tnode.update(result)\n\t\tresult = node\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tnew_nodes = {}\n\tfor node in nodes:\n\t\tnode.key_path = node.key_path if new_nodes.get(node.key) else node.key\n\t\tif new_nodes.get(node.key):\n\t\t\tnew_nodes[node.key] = deep_merge_nodes([new_nodes[node.key], node])\n\t\telse:\n\t\t\tnew_nodes[node.key] = node\n\treturn list(new_nodes.values())\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Convert a CSV file into a list of Django ORM objects\")\n\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", required=True, help=\"The CSV file to parse\")\n\tparser.add_argument(\"-t\", \"--template\", dest=\"template\", required=True, help=\"The template file to use\")\n\tparser.add_argument(\"-m\", \"--model\", dest=\"model\", required=True, help=\"The model to use\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", required=True, help=\"The output file\")\n\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"A command-line tool for converting between .txt and .json files.\")\n\tparser.add_argument('input', metavar='input', nargs='?', help='The input file to be converted. If not specified, stdin will be used')\n\tparser.add_argument('output', metavar='output', nargs='?', help='The output file to be created. If not specified, stdout will be used')\n\tparser.add_argument('--json', action='store_true', help='Specifies that the input file is a .json file instead of a .txt file')\n\tparser.add_argument('--txt', action='store_true', help='Specifies that the input file is a .txt file instead of a .json file')\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='Parse command-line arguments')\n\tparser.add_argument('-d', '--debug', dest='debug', action='store_true', help='Debug mode')\n\tparser.add_argument('-t', '--token', dest='token', help='Token which will be used to authenticate with the API')\n\tparser.add_argument('-a', '--auth', dest='auth', help='Authenticate with the API', action='store_true')\n\tparser.add_argument('-l', '--list', dest='list', help='List available data sets and exit', action='store_true')\n\tparser.add_argument('-f', '--file', dest='file', help='File which should be written')\n\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help='Verbose mode')\n\tparser.add_argument('data_set', metavar='data_set', help='Name of the data set to download')\n\tparser.add_argument('column', metavar='column', help='Name of the column to download')\n\tparser.add_argument('-i',", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='A script to convert between different data types.')\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\", action=\"store_true\")\n\tparser.add_argument(\"-d\", \"--debug\", help=\"run in debug mode\", action=\"store_true\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"output file (default: stdout)\", type=str)\n\tparser.add_argument(\"-i\", \"--input\", help=\"input file (default: stdin)\", type=str)\n\tparser.add_argument(\"-j\", \"--json\", help=\"input is JSON\", action=\"store_true\")\n\tparser.add_argument(\"-b\", \"--binary\", help=\"input is binary\", action=\"store_true\")\n\tparser.add_argument(\"-t\", \"--text\", help=\"input is text\", action=\"store_true\")\n\tparser.add_argument(\"-x\", \"--xml\", help=\"input is XML\", action=\"store_true\")\n\tparser.add_argument(\"-p\", \"--python\", help=\"input is Python\", action=\"store_true\")\n\tparser.add_argument", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description='Merge the contents of two or more text files into one.')\n\tparser.add_argument('-i', '--input', action='store', nargs='+', help='Input files to merge. At least 2 required.', required=True)\n\tparser.add_argument('-o', '--output', action='store', help='Output file. Required.', required=True)\n\tparser.add_argument('-d', '--delimiter', action='store', help='Line delimiter. Optional.', default='\\n')\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(prog='python3 -m ' + __package__, description='%(prog)s ' + __doc__)\n\tparser.add_argument('--debug', action='store_true', help='Display debugging information')\n\tparser.add_argument('--verbose', action='store_true', help='Display verbose output')\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __package__ + ' ' + __version__)\n\treturn parser.parse_args(arguments)\n\t", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\t# We use a separate parser to parse command-line arguments.\n\t# This parser is used solely for the help message, the version message, and any errors that occur while parsing the arguments.\n\tparser = argparse.ArgumentParser(description = \"A script to perform a linear regression of the form y = ax + b, where a and b are constants.\")\n\tparser.add_argument(\"-x\", \"--x_data\", help = \"The data on the x-axis (the independent variable).\")\n\tparser.add_argument(\"-y\", \"--y_data\", help = \"The data on the y-axis (the dependent variable).\")\n\tparser.add_argument(\"-v\", \"--verbose\", help = \"Display extra information about what the script is doing.\", action = \"store_true\")\n\tparser.add_argument(\"-o\", \"--output\", help = \"The file to output the results to. The output file is in CSV format.\")\n\tparser.add_argument(\"-w\", \"--write\", help = \"Whether or not to write the results to the output file. If output does not exist, it will be created. If output exists, the data will be appended to it. If the data in the output file is not in CSV format, the output file will be overwritten.\", action =", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Parse the command-line arguments\")\n\n\t# Add arguments\n\tparser.add_argument(\"-a\", \"--a\", help=\"A help\", type=str, default=\"default\")\n\tparser.add_argument(\"-b\", \"--b\", help=\"B help\", type=str, default=\"default\")\n\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')\n\tparser.add_argument('--sum', dest='accumulate', action='store_const',\n\t\t\t\t\t\tconst=sum, default=max, help='sum the integers (default: find the max)')\n\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Script to convert a list of RNA sequences from one notation to another\")\n\tparser.add_argument(\"-i\", \"--input\", help=\"Input file\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file\", required=True)\n\tparser.add_argument(\"-f\", \"--from\", help=\"Notation to convert from\", required=True)\n\tparser.add_argument(\"-t\", \"--to\", help=\"Notation to convert to\", required=True)\n\tparser.add_argument(\"-l\", \"--log\", help=\"Log file\", required=True)\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"Print extra information\", action=\"store_true\")\n\tparser.add_argument(\"-d\", \"--debug\", help=\"Print debug information\", action=\"store_true\")\n\tparser.add_argument(\"-x\", \"--exclude\", help=\"Exclude sequences with the given pattern\", required=False)\n\tparser.add_argument(\"-c\", \"--count\", help=\"Don't convert sequences, only count them\", action=\"store_true\")\n\targs = parser.parse_args(arguments)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Convert a CSV file into a list of Django ORM objects\")\n\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", required=True, help=\"The CSV file to parse\")\n\tparser.add_argument(\"-t\", \"--template\", dest=\"template\", required=True, help=\"The template file to use\")\n\tparser.add_argument(\"-m\", \"--model\", dest=\"model\", required=True, help=\"The model to use\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", required=True, help=\"The output file\")\n\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"A command-line tool for converting between .txt and .json files.\")\n\tparser.add_argument('input', metavar='input', nargs='?', help='The input file to be converted. If not specified, stdin will be used')\n\tparser.add_argument('output', metavar='output', nargs='?', help='The output file to be created. If not specified, stdout will be used')\n\tparser.add_argument('--json', action='store_true', help='Specifies that the input file is a .json file instead of a .txt file')\n\tparser.add_argument('--txt', action='store_true', help='Specifies that the input file is a .txt file instead of a .json file')\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='Parse command-line arguments')\n\tparser.add_argument('-d', '--debug', dest='debug', action='store_true', help='Debug mode')\n\tparser.add_argument('-t', '--token', dest='token', help='Token which will be used to authenticate with the API')\n\tparser.add_argument('-a', '--auth', dest='auth', help='Authenticate with the API', action='store_true')\n\tparser.add_argument('-l', '--list', dest='list', help='List available data sets and exit', action='store_true')\n\tparser.add_argument('-f', '--file', dest='file', help='File which should be written')\n\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help='Verbose mode')\n\tparser.add_argument('data_set', metavar='data_set', help='Name of the data set to download')\n\tparser.add_argument('column', metavar='column', help='Name of the column to download')\n\tparser.add_argument('-i',", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='A script to convert between different data types.')\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\", action=\"store_true\")\n\tparser.add_argument(\"-d\", \"--debug\", help=\"run in debug mode\", action=\"store_true\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"output file (default: stdout)\", type=str)\n\tparser.add_argument(\"-i\", \"--input\", help=\"input file (default: stdin)\", type=str)\n\tparser.add_argument(\"-j\", \"--json\", help=\"input is JSON\", action=\"store_true\")\n\tparser.add_argument(\"-b\", \"--binary\", help=\"input is binary\", action=\"store_true\")\n\tparser.add_argument(\"-t\", \"--text\", help=\"input is text\", action=\"store_true\")\n\tparser.add_argument(\"-x\", \"--xml\", help=\"input is XML\", action=\"store_true\")\n\tparser.add_argument(\"-p\", \"--python\", help=\"input is Python\", action=\"store_true\")\n\tparser.add_argument", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description='Merge the contents of two or more text files into one.')\n\tparser.add_argument('-i', '--input', action='store', nargs='+', help='Input files to merge. At least 2 required.', required=True)\n\tparser.add_argument('-o', '--output', action='store', help='Output file. Required.', required=True)\n\tparser.add_argument('-d', '--delimiter', action='store', help='Line delimiter. Optional.', default='\\n')\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(prog='python3 -m ' + __package__, description='%(prog)s ' + __doc__)\n\tparser.add_argument('--debug', action='store_true', help='Display debugging information')\n\tparser.add_argument('--verbose', action='store_true', help='Display verbose output')\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __package__ + ' ' + __version__)\n\treturn parser.parse_args(arguments)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\t# We use a separate parser to parse command-line arguments.\n\t# This parser is used solely for the help message, the version message, and any errors that occur while parsing the arguments.\n\tparser = argparse.ArgumentParser(description = \"A script to perform a linear regression of the form y = ax + b, where a and b are constants.\")\n\tparser.add_argument(\"-x\", \"--x_data\", help = \"The data on the x-axis (the independent variable).\")\n\tparser.add_argument(\"-y\", \"--y_data\", help = \"The data on the y-axis (the dependent variable).\")\n\tparser.add_argument(\"-v\", \"--verbose\", help = \"Display extra information about what the script is doing.\", action = \"store_true\")\n\tparser.add_argument(\"-o\", \"--output\", help = \"The file to output the results to. The output file is in CSV format.\")\n\tparser.add_argument(\"-w\", \"--write\", help = \"Whether or not to write the results to the output file. If output does not exist, it will be created. If output exists, the data will be appended to it. If the data in the output file is not in CSV format, the output file will be overwritten.\", action =", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Parse the command-line arguments\")\n\n\t# Add arguments\n\tparser.add_argument(\"-a\", \"--a\", help=\"A help\", type=str, default=\"default\")\n\tparser.add_argument(\"-b\", \"--b\", help=\"B help\", type=str, default=\"default\")\n\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')\n\tparser.add_argument('--sum', dest='accumulate', action='store_const',\n\t\t\t\t\t\tconst=sum, default=max, help='sum the integers (default: find the max)')\n\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Script to convert a list of RNA sequences from one notation to another\")\n\tparser.add_argument(\"-i\", \"--input\", help=\"Input file\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file\", required=True)\n\tparser.add_argument(\"-f\", \"--from\", help=\"Notation to convert from\", required=True)\n\tparser.add_argument(\"-t\", \"--to\", help=\"Notation to convert to\", required=True)\n\tparser.add_argument(\"-l\", \"--log\", help=\"Log file\", required=True)\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"Print extra information\", action=\"store_true\")\n\tparser.add_argument(\"-d\", \"--debug\", help=\"Print debug information\", action=\"store_true\")\n\tparser.add_argument(\"-x\", \"--exclude\", help=\"Exclude sequences with the given pattern\", required=False)\n\tparser.add_argument(\"-c\", \"--count\", help=\"Don't convert sequences, only count them\", action=\"store_true\")\n\targs = parser.parse_args(arguments)\n", "is_pass": false, "return_code": 2}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\timport argparse\n\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._HelpAction):\n\t\t\tcontinue\n\t\tflag_strings = action.option_strings\n\t\tflags.extend(flag_strings)\n\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != \"help\":\n\t\t\tflags.append(\"--%s\" % action.dest.replace(\"_\", \"-\"))\n\tif flags:\n\t\treturn \" \".join(flags)\n\treturn \"\"", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = set()\n\tfor action in parser._actions:\n\t\tif action.dest != \"help\":\n\t\t\tif action.option_strings:\n\t\t\t\tflags.update(action.option_strings)\n\tif '-h' in flags:\n\t\tflags.remove('-h')\n\tif '--help' in flags:\n\t\tflags.remove('--help')\n\treturn \" \".join(sorted(flags))\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\ttry:\n\t\timport argparse\n\texcept ImportError:\n\t\traise RuntimeError(\"argparse not available\")\n\treturn \" \".join(\n\t\t\"%s%s\" % (\n\t\t\t(a.dest.replace(\"_\", \"-\"),) if a.option_strings else (),\n\t\t\t(\" %s\" % a.metavar,) if a.metavar else (),\n\t\t)\n\t\tfor a in parser._actions\n\t\tif a.dest != \"help\" and (not a.option_strings or a.option_strings[0] != \"-h\")\n\t)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest is None:\n\t\t\tcontinue\n\t\tflags.append('--' + action.dest.replace('_', '-'))\n\t\tif action.nargs == 0:\n\t\t\tflags.append(' ')\n\t\telif action.nargs == 1:\n\t\t\tflags.append(' ')\n\t\telif action.nargs == '?':\n\t\t\tflags.append('[')\n\t\t\tflags.append('] ')\n\t\telif action.nargs == '*':\n\t\t\tflags.append('[')\n\t\t\tflags.append('...')\n\t\t\tflags.append('] ')\n\t\telif action.nargs == '+':\n\t\t\tflags.append('<')\n\t\t\tflags.append('...')\n\t\t\tflags.append('> ')\n\treturn ''.join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_usage().split()\n\tflags = [\n\t\tflag for flag in flags\n\t\tif flag not in [\"-h\", \"--help\"]\n\t]\n\treturn \" \".join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_name = action.option_strings[0] if action.option_strings else ''\n\t\tif flag_name.startswith('--'):\n\t\t\tflags.append('{}'.format(flag_name))\n\tif len(flags) > 0:\n\t\treturn ' '.join(flags)\n\telse:\n\t\treturn ''", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn \" \".join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != 'help':\n\t\t\tflags.append('--' + action.dest)\n\t\t\tif action.nargs != None:\n\t\t\t\tflags.append(str(action.nargs))\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_str = '--' if action.option_strings else ''\n\t\tif flag_str and action.required:\n\t\t\tflag_str = flag_str + ' required '\n\t\tif flag_str:\n\t\t\tflags.append(flag_str)\n\t\t\tflags.extend(action.option_strings)\n\treturn ' '.join(flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\timport argparse\n\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._HelpAction):\n\t\t\tcontinue\n\t\tflag_strings = action.option_strings\n\t\tflags.extend(flag_strings)\n\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != \"help\":\n\t\t\tflags.append(\"--%s\" % action.dest.replace(\"_\", \"-\"))\n\tif flags:\n\t\treturn \" \".join(flags)\n\treturn \"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = set()\n\tfor action in parser._actions:\n\t\tif action.dest != \"help\":\n\t\t\tif action.option_strings:\n\t\t\t\tflags.update(action.option_strings)\n\tif '-h' in flags:\n\t\tflags.remove('-h')\n\tif '--help' in flags:\n\t\tflags.remove('--help')\n\treturn \" \".join(sorted(flags))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\ttry:\n\t\timport argparse\n\texcept ImportError:\n\t\traise RuntimeError(\"argparse not available\")\n\treturn \" \".join(\n\t\t\"%s%s\" % (\n\t\t\t(a.dest.replace(\"_\", \"-\"),) if a.option_strings else (),\n\t\t\t(\" %s\" % a.metavar,) if a.metavar else (),\n\t\t)\n\t\tfor a in parser._actions\n\t\tif a.dest != \"help\" and (not a.option_strings or a.option_strings[0] != \"-h\")\n\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest is None:\n\t\t\tcontinue\n\t\tflags.append('--' + action.dest.replace('_', '-'))\n\t\tif action.nargs == 0:\n\t\t\tflags.append(' ')\n\t\telif action.nargs == 1:\n\t\t\tflags.append(' ')\n\t\telif action.nargs == '?':\n\t\t\tflags.append('[')\n\t\t\tflags.append('] ')\n\t\telif action.nargs == '*':\n\t\t\tflags.append('[')\n\t\t\tflags.append('...')\n\t\t\tflags.append('] ')\n\t\telif action.nargs == '+':\n\t\t\tflags.append('<')\n\t\t\tflags.append('...')\n\t\t\tflags.append('> ')\n\treturn ''.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = parser.format_usage().split()\n\tflags = [\n\t\tflag for flag in flags\n\t\tif flag not in [\"-h\", \"--help\"]\n\t]\n\treturn \" \".join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_name = action.option_strings[0] if action.option_strings else ''\n\t\tif flag_name.startswith('--'):\n\t\t\tflags.append('{}'.format(flag_name))\n\tif len(flags) > 0:\n\t\treturn ' '.join(flags)\n\telse:\n\t\treturn ''", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn \" \".join(flags)", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != 'help':\n\t\t\tflags.append('--' + action.dest)\n\t\t\tif action.nargs != None:\n\t\t\t\tflags.append(str(action.nargs))\n\treturn ' '.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_str = '--' if action.option_strings else ''\n\t\tif flag_str and action.required:\n\t\t\tflag_str = flag_str + ' required '\n\t\tif flag_str:\n\t\t\tflags.append(flag_str)\n\t\t\tflags.extend(action.option_strings)\n\treturn ' '.join(flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tfrom borgmatic.commands import borgmatic\n\tfrom borgmatic.config import overrides\n\n\tborgmatic.add_subcommand_parsers(borgmatic)\n\n\tcompleter = argparse.ArgumentCompleter(\n\t\tborgmatic.get_subcommand_parsers(),\n\t\tcompleter_arg=borgmatic.get_subcommand_parsers().get_completer_arg(),\n\t\tignore_requires_value=[\"--debug\", \"--log-level\", \"--show-config\"],\n\t)\n\n\tcompleter.complete()\n\tsys.exit(0)\n\n", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\timport borgmatic\n\tfrom borgmatic.borg import environment as borg_environment\n\tfrom borgmatic.borg import flags as borg_flags\n\tfrom borgmatic.commands import arguments as command_arguments\n\tfrom borgmatic.commands import execute as command_execute\n\tfrom borgmatic.config import paths as config_paths\n\tfrom borgmatic.config import validate as config_validate\n\tfrom borgmatic.config import generate as config_generate\n\tfrom borgmatic.config import generate_detection as config_generate_detection\n\n\t# Generate a fake config file in case the user is trying to run this in a directory without a config.\n\tconfig_paths.CONFIG_FILE_PATHS = [config_paths.generate_filename(sys.argv[0])]\n\n\tparser = argparse.ArgumentParser(prog='borgmatic')\n\tcommand_arguments.add_arguments(parser)\n\tconfig_generate.add_generate_arguments(parser)\n\tconfig_generate_detection.add_generate_detection_arguments(parser)\n\tborg_flags.add_flags(parser)\n\tborg_environment.add_environment_arguments(parser)\n\n\tsubparsers = parser.add_subpar", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport borgmatic.commands\n\timport borgmatic.config.validate\n\timport borgmatic.hooks\n\timport borgmatic.logger\n\timport borgmatic.notifier\n\timport borgmatic.retention\n\timport borgmatic.status\n\timport borgmatic.storage\n\timport borgmatic.transfer\n\timport borgmatic.version\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--version\", action=\"store_true\", help=\"Print the version and exit\")\n\n\tsubparsers = parser.add_subparsers(dest=\"command\", title=\"Commands\")\n\tsubparsers.required = True\n\n\tfor command_name, command_class in borgmatic.commands.COMMANDS.items():\n\t\tcommand_parser = subparsers.add_parser(command_name, help=command_class.help)\n\t\tcommand_class.add_arguments(command_parser)\n\n\t# Add completion for config file paths\n\tparser.add_argument(\n\t\t\"--config\",\n\t\tmetavar=\"CONFIG\",\n\t\tcompleter=borgmatic.config.validate.complete_config_paths,\n\t)\n\n\t# Add completion for config file paths\n\t", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport functools\n\timport itertools\n\timport sys\n\n\tfrom borgmatic.commands import (\n\t\tborg,\n\t\tcheck,\n\t\tdelete,\n\t\tdiff,\n\t\texport_tar,\n\t\textract,\n\t\tinit,\n\t\tlist as list_command,\n\t\tmount,\n\t\trecover,\n\t\tshell,\n\t\ttransfer,\n\t)\n\tfrom borgmatic.config import validate as validate_config\n\tfrom borgmatic.config.collect import collect_config_and_merge_argv\n\n\ttry:\n\t\targv = sys.argv[1:]\n\t\tconfig = collect_config_and_merge_argv(argv)\n\t\tvalidate_config(config)\n\texcept (\n\t\targparse.ArgumentError,\n\t\targparse.ArgumentTypeError,\n\t\targparse.Namespace,\n\t\tvalidate_config.Error,\n\t) as exc:\n\t\tprint(exc, file=sys.stderr)\n\t\tsys.exit(1)\n\n\tcommands = [\n\t\tborg,\n\t\tcheck,\n\t\tdelete,\n\t\tdiff,\n\t\texport_tar,\n\t\textract,\n\t\tinit,\n", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tprint(\"\"\"\\\n# Bash completion for borgmatic\n\n_borgmatic_completions() {\n  local curr_word=${COMP_WORDS[COMP_CWORD]}\n\n  if [[ ${COMP_CWORD} == 1 ]]; then\n    COMPREPLY=($(compgen -W \"backup create check config convert delete export extract init list prune recover rebuild rebuild-index \\\n        repository-lifecycle rmlogs run self-update show version\" -- ${curr_word}))\n  else\n    COMPREPLY=($(compgen -W \"$(borgmatic --help-all 2> /dev/null | grep -E \"^ +[a-z-]+\" | awk '{ print $1 }')\" -- ${curr_word}))\n  fi\n}\n\ncomplete -F _borgmatic_completions borgmatic\n\"\"\")", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tfrom borgmatic.config import module\n\tfrom borgmatic.config import validate\n\tfrom borgmatic.hooks import execution\n\n\tconfig_paths = validate.find_config_paths(module.find_config_paths())\n\tconfig = validate.parse_global_arguments(config_paths)\n\n\tsubcommand_names = [\n\t\tsubcommand_name\n\t\tfor subcommand_name in sorted(config['subcommands'].keys())\n\t\tif config['subcommands'][subcommand_name].get('hidden', False) is False\n\t]\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('subcommand', choices=subcommand_names)\n\tparser.add_argument('--config', choices=config_paths)\n\targs = parser.parse_args(sys.argv[1:2])\n\n\tsubcommand = config['subcommands'][args.subcommand]\n\tsubcommand_argument_parser = execution.build_subcommand_argparse_arguments(\n\t\tsubcommand['arguments'], subcommand['defaults']\n\t)\n\tsys.stdout.write(subcommand_argument_parser.format_help())\n\n\nif __name__ == '__main__':\n\tbash_completion()", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom itertools import chain\n\tfrom shlex import quote\n\n\timport click\n\n\tfrom borgmatic.commands import command_parsers\n\tfrom borgmatic.config import validate as config_validate\n\n\tconfig_validate.validate_environment()\n\n\tcontext_settings = dict(ignore_unknown_options=True, allow_extra_args=True)\n\tcommands = [\n\t\tcommand_parser.command\n\t\tfor command_parser in chain(\n\t\t\tcommand_parsers.get_all_command_parsers(), command_parsers.get_all_archive_parsers()\n\t\t)\n\t\tif command_parser.command.name not in (\"config\", \"generate\")\n\t]\n\tcommands.append(command_parsers.get_global_parser())\n\n\tfor parser in commands:\n\t\tparser.command.context_settings = context_settings\n\t\tparser.command.help = parser.help\n\n\tbash_command = click.bash_completion.get_completion_script(commands, prog_name=\"borgmatic\")\n\n\t# Fix up the generated script so it works with our command-line argument parsers.\n\tbash_command = bash_command.replace(\"${COMP_CWORD}\", \"${COMP_WORDS", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import Namespace\n\tfrom pathlib import Path\n\tfrom typing import List\n\tfrom typing import Optional\n\n\timport borgmatic\n\tfrom borgmatic.config import generate\n\tfrom borgmatic.config import validate\n\tfrom borgmatic.config import validate_environment\n\tfrom borgmatic.config import validate_repository_config\n\tfrom borgmatic.config import validate_storage_config\n\n\tdef _complete_config_paths(prefix: str, parsed_args: Namespace) -> List[str]:\n\t\t\"\"\"\n\t\tComplete config file paths for shell completion.\n\t\t\"\"\"\n\t\tconfig_paths = generate.generate_config_paths(\n\t\t\tconfig_filename=parsed_args.config_filename,\n\t\t\tconfig_paths=parsed_args.config_paths,\n\t\t\tenvironment_overrides=parsed_args.environment_overrides,\n\t\t)\n\t\treturn [str(path) for path in config_paths if path.name.startswith(prefix)]\n\n\tdef _complete_config_keys(prefix: str, parsed_args: Namespace) -> List[str]:\n\t\t\"\"\"\n\t\tComplete config keys for shell completion.\n\t\t\"\"\"\n\t\tvalidate_environment(parsed_", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tfrom borgmatic.commands import command_table\n\n\tcommand_list = list(command_table.keys())\n\tcommand_list.sort()\n\n\tparser = argparse.ArgumentParser(description=\"Automate your backups\", add_help=False)\n\tparser.add_argument(\"command\", choices=command_list)\n\tparser.add_argument(\"-h\", \"--help\", action=\"help\", help=\"Show this message and exit\")\n\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=\"%(prog)s 1.0\")\n\tsubparsers = parser.add_subparsers()\n\n\tfor command_name in command_list:\n\t\tcommand_table[command_name].add_parser(subparsers)\n\n\targs, argv = parser.parse_known_args()\n\n\tcommand_table[args.command].execute_command(argv)", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\tfrom typing import Any, Dict, List, Optional, Sequence, cast\n\n\tfrom .borgmatic.commands import (\n\t\tadd_command_subparsers,\n\t\tadd_global_arguments,\n\t\tadd_verbosity_argument,\n\t\tborgmatic,\n\t\tget_command_names,\n\t)\n\tfrom .borgmatic.config import BorgmaticConfig\n\tfrom .borgmatic.config import validate as validate_config\n\tfrom .borgmatic.logger import LoggerManager\n\tfrom .borgmatic.util import module_name_to_import_path\n\n\tlogger_manager = LoggerManager()\n\tlogger = logger_manager.logger(__name__)\n\tlogger.debug(\"Loading borgmatic command completions\")\n\n\tparser = argparse.ArgumentParser(description=\"Produce shell completions for borgmatic\")\n\tparser.add_argument(\n\t\t\"--shell\",\n\t\tchoices=(\"bash\",),\n\t\thelp=\"The type of shell for which to produce completions\",\n\t\trequired=True,\n\t)\n\tadd_global_arguments(parser)\n\n\tsubparsers = parser.add_subparsers(\n\t\tdest=\"command\",\n\t\tdescription=\"The borg"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tfrom borgmatic.commands import borgmatic\n\tfrom borgmatic.config import overrides\n\n\tborgmatic.add_subcommand_parsers(borgmatic)\n\n\tcompleter = argparse.ArgumentCompleter(\n\t\tborgmatic.get_subcommand_parsers(),\n\t\tcompleter_arg=borgmatic.get_subcommand_parsers().get_completer_arg(),\n\t\tignore_requires_value=[\"--debug\", \"--log-level\", \"--show-config\"],\n\t)\n\n\tcompleter.complete()\n\tsys.exit(0)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\timport borgmatic\n\tfrom borgmatic.borg import environment as borg_environment\n\tfrom borgmatic.borg import flags as borg_flags\n\tfrom borgmatic.commands import arguments as command_arguments\n\tfrom borgmatic.commands import execute as command_execute\n\tfrom borgmatic.config import paths as config_paths\n\tfrom borgmatic.config import validate as config_validate\n\tfrom borgmatic.config import generate as config_generate\n\tfrom borgmatic.config import generate_detection as config_generate_detection\n\n\t# Generate a fake config file in case the user is trying to run this in a directory without a config.\n\tconfig_paths.CONFIG_FILE_PATHS = [config_paths.generate_filename(sys.argv[0])]\n\n\tparser = argparse.ArgumentParser(prog='borgmatic')\n\tcommand_arguments.add_arguments(parser)\n\tconfig_generate.add_generate_arguments(parser)\n\tconfig_generate_detection.add_generate_detection_arguments(parser)\n\tborg_flags.add_flags(parser)\n\tborg_environment.add_environment_arguments(parser)\n\n\tsubparsers = parser.add_subpar", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport borgmatic.commands\n\timport borgmatic.config.validate\n\timport borgmatic.hooks\n\timport borgmatic.logger\n\timport borgmatic.notifier\n\timport borgmatic.retention\n\timport borgmatic.status\n\timport borgmatic.storage\n\timport borgmatic.transfer\n\timport borgmatic.version\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--version\", action=\"store_true\", help=\"Print the version and exit\")\n\n\tsubparsers = parser.add_subparsers(dest=\"command\", title=\"Commands\")\n\tsubparsers.required = True\n\n\tfor command_name, command_class in borgmatic.commands.COMMANDS.items():\n\t\tcommand_parser = subparsers.add_parser(command_name, help=command_class.help)\n\t\tcommand_class.add_arguments(command_parser)\n\n\t# Add completion for config file paths\n\tparser.add_argument(\n\t\t\"--config\",\n\t\tmetavar=\"CONFIG\",\n\t\tcompleter=borgmatic.config.validate.complete_config_paths,\n\t)\n\n\t# Add completion for config file paths\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport functools\n\timport itertools\n\timport sys\n\n\tfrom borgmatic.commands import (\n\t\tborg,\n\t\tcheck,\n\t\tdelete,\n\t\tdiff,\n\t\texport_tar,\n\t\textract,\n\t\tinit,\n\t\tlist as list_command,\n\t\tmount,\n\t\trecover,\n\t\tshell,\n\t\ttransfer,\n\t)\n\tfrom borgmatic.config import validate as validate_config\n\tfrom borgmatic.config.collect import collect_config_and_merge_argv\n\n\ttry:\n\t\targv = sys.argv[1:]\n\t\tconfig = collect_config_and_merge_argv(argv)\n\t\tvalidate_config(config)\n\texcept (\n\t\targparse.ArgumentError,\n\t\targparse.ArgumentTypeError,\n\t\targparse.Namespace,\n\t\tvalidate_config.Error,\n\t) as exc:\n\t\tprint(exc, file=sys.stderr)\n\t\tsys.exit(1)\n\n\tcommands = [\n\t\tborg,\n\t\tcheck,\n\t\tdelete,\n\t\tdiff,\n\t\texport_tar,\n\t\textract,\n\t\tinit,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tprint(\"\"\"\\\n# Bash completion for borgmatic\n\n_borgmatic_completions() {\n  local curr_word=${COMP_WORDS[COMP_CWORD]}\n\n  if [[ ${COMP_CWORD} == 1 ]]; then\n    COMPREPLY=($(compgen -W \"backup create check config convert delete export extract init list prune recover rebuild rebuild-index \\\n        repository-lifecycle rmlogs run self-update show version\" -- ${curr_word}))\n  else\n    COMPREPLY=($(compgen -W \"$(borgmatic --help-all 2> /dev/null | grep -E \"^ +[a-z-]+\" | awk '{ print $1 }')\" -- ${curr_word}))\n  fi\n}\n\ncomplete -F _borgmatic_completions borgmatic\n\"\"\")", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tfrom borgmatic.config import module\n\tfrom borgmatic.config import validate\n\tfrom borgmatic.hooks import execution\n\n\tconfig_paths = validate.find_config_paths(module.find_config_paths())\n\tconfig = validate.parse_global_arguments(config_paths)\n\n\tsubcommand_names = [\n\t\tsubcommand_name\n\t\tfor subcommand_name in sorted(config['subcommands'].keys())\n\t\tif config['subcommands'][subcommand_name].get('hidden', False) is False\n\t]\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('subcommand', choices=subcommand_names)\n\tparser.add_argument('--config', choices=config_paths)\n\targs = parser.parse_args(sys.argv[1:2])\n\n\tsubcommand = config['subcommands'][args.subcommand]\n\tsubcommand_argument_parser = execution.build_subcommand_argparse_arguments(\n\t\tsubcommand['arguments'], subcommand['defaults']\n\t)\n\tsys.stdout.write(subcommand_argument_parser.format_help())\n\n\nif __name__ == '__main__':\n\tbash_completion()", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom itertools import chain\n\tfrom shlex import quote\n\n\timport click\n\n\tfrom borgmatic.commands import command_parsers\n\tfrom borgmatic.config import validate as config_validate\n\n\tconfig_validate.validate_environment()\n\n\tcontext_settings = dict(ignore_unknown_options=True, allow_extra_args=True)\n\tcommands = [\n\t\tcommand_parser.command\n\t\tfor command_parser in chain(\n\t\t\tcommand_parsers.get_all_command_parsers(), command_parsers.get_all_archive_parsers()\n\t\t)\n\t\tif command_parser.command.name not in (\"config\", \"generate\")\n\t]\n\tcommands.append(command_parsers.get_global_parser())\n\n\tfor parser in commands:\n\t\tparser.command.context_settings = context_settings\n\t\tparser.command.help = parser.help\n\n\tbash_command = click.bash_completion.get_completion_script(commands, prog_name=\"borgmatic\")\n\n\t# Fix up the generated script so it works with our command-line argument parsers.\n\tbash_command = bash_command.replace(\"${COMP_CWORD}\", \"${COMP_WORDS", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import Namespace\n\tfrom pathlib import Path\n\tfrom typing import List\n\tfrom typing import Optional\n\n\timport borgmatic\n\tfrom borgmatic.config import generate\n\tfrom borgmatic.config import validate\n\tfrom borgmatic.config import validate_environment\n\tfrom borgmatic.config import validate_repository_config\n\tfrom borgmatic.config import validate_storage_config\n\n\tdef _complete_config_paths(prefix: str, parsed_args: Namespace) -> List[str]:\n\t\t\"\"\"\n\t\tComplete config file paths for shell completion.\n\t\t\"\"\"\n\t\tconfig_paths = generate.generate_config_paths(\n\t\t\tconfig_filename=parsed_args.config_filename,\n\t\t\tconfig_paths=parsed_args.config_paths,\n\t\t\tenvironment_overrides=parsed_args.environment_overrides,\n\t\t)\n\t\treturn [str(path) for path in config_paths if path.name.startswith(prefix)]\n\n\tdef _complete_config_keys(prefix: str, parsed_args: Namespace) -> List[str]:\n\t\t\"\"\"\n\t\tComplete config keys for shell completion.\n\t\t\"\"\"\n\t\tvalidate_environment(parsed_", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tfrom borgmatic.commands import command_table\n\n\tcommand_list = list(command_table.keys())\n\tcommand_list.sort()\n\n\tparser = argparse.ArgumentParser(description=\"Automate your backups\", add_help=False)\n\tparser.add_argument(\"command\", choices=command_list)\n\tparser.add_argument(\"-h\", \"--help\", action=\"help\", help=\"Show this message and exit\")\n\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=\"%(prog)s 1.0\")\n\tsubparsers = parser.add_subparsers()\n\n\tfor command_name in command_list:\n\t\tcommand_table[command_name].add_parser(subparsers)\n\n\targs, argv = parser.parse_known_args()\n\n\tcommand_table[args.command].execute_command(argv)", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\tfrom typing import Any, Dict, List, Optional, Sequence, cast\n\n\tfrom .borgmatic.commands import (\n\t\tadd_command_subparsers,\n\t\tadd_global_arguments,\n\t\tadd_verbosity_argument,\n\t\tborgmatic,\n\t\tget_command_names,\n\t)\n\tfrom .borgmatic.config import BorgmaticConfig\n\tfrom .borgmatic.config import validate as validate_config\n\tfrom .borgmatic.logger import LoggerManager\n\tfrom .borgmatic.util import module_name_to_import_path\n\n\tlogger_manager = LoggerManager()\n\tlogger = logger_manager.logger(__name__)\n\tlogger.debug(\"Loading borgmatic command completions\")\n\n\tparser = argparse.ArgumentParser(description=\"Produce shell completions for borgmatic\")\n\tparser.add_argument(\n\t\t\"--shell\",\n\t\tchoices=(\"bash\",),\n\t\thelp=\"The type of shell for which to produce completions\",\n\t\trequired=True,\n\t)\n\tadd_global_arguments(parser)\n\n\tsubparsers = parser.add_subparsers(\n\t\tdest=\"command\",\n\t\tdescription=\"The borg", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"This program converts a .xlsx file to a .csv file\")\n\tparser.add_argument('-i', '--input', required=True, help='Path to input file')\n\tparser.add_argument('-o', '--output', required=True, help='Path to output file')\n\tparsed_arguments = parser.parse_args(unparsed_arguments)\n\treturn parsed_arguments\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description=\"\")\n\t\n\tparser.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"Enable debugging output\")\n\t\n\t# add subparsers for each subcommand\n\tsubparsers = parser.add_subparsers(title=\"Subcommands\", dest=\"subparser_name\")\n\t\n\t# subparser for the \"parse\" subcommand\n\tparse_parser = subparsers.add_parser(\"parse\", help=\"Parse a file\")\n\tparse_parser.add_argument(\"file\", type=str, help=\"File to parse\")\n\t\n\t# subparser for the \"diff\" subcommand\n\tdiff_parser = subparsers.add_parser(\"diff\", help=\"Compare two files\")\n\tdiff_parser.add_argument(\"file1\", type=str, help=\"First file\")\n\tdiff_parser.add_argument(\"file2\", type=str, help=\"Second file\")\n\t\n\t# subparser for the \"search\" subcommand\n\tsearch_parser = subparsers.add_parser(\"search\", help=\"Search for a string in a file\")\n\tsearch_parser.add_argument(\"file\", type=str, help=\"File to", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\t\n\tparser_a = subparsers.add_parser(\"a\", help=\"do something\")\n\tparser_a.add_argument(\"--foo\", type=str, help=\"the foo argument\")\n\t\n\tparser_b = subparsers.add_parser(\"b\", help=\"do something else\")\n\tparser_b.add_argument(\"--bar\", type=str, help=\"the bar argument\")\n\t\n\tparser_c = subparsers.add_parser(\"c\", help=\"do something else\")\n\tparser_c.add_argument(\"--baz\", type=str, help=\"the baz argument\")\n\t\n\targs = parser.parse_args(unparsed_arguments)\n\t\n\tsubparser_name = args.subparser_name\n\t\n\tif subparser_name == \"a\":\n\t\treturn {subparser_name: args}\n\telif subparser_name == \"b\":\n\t\treturn {subparser_name: args}\n\telif subparser_name == \"c\":\n\t\treturn {subparser_name: args}\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\t# We need to create an argument parser for the whole program. This will parse the global\n\t# arguments.\n\tglobal_parser = argparse.ArgumentParser(description=\"Run tests for the specified library\")\n\tglobal_parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Show more information on stderr\")\n\tglobal_parser.add_argument(\"--quiet\", \"-q\", action=\"store_true\", help=\"Show less information on stderr\")\n\tglobal_parser.add_argument(\"--json\", \"-j\", action=\"store_true\", help=\"Print output in JSON format\")\n\tglobal_parser.add_argument(\"--no-color\", action=\"store_true\", help=\"Print output without ANSI colors\")\n\tglobal_parser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\tglobal_parser.add_argument(\"--help\", action=\"help\", help=\"Show this help message and exit\")\n\t\n\t# We need to get a list of subparsers.\n\tsubparsers = global_parser.add_subparsers(dest=\"command\", title=\"Commands\", metavar=\"COMMAND\", description=\"Command to", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\tfrom pathlib import Path\n\n\t# Parse the command-line arguments.\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--input\", type=Path, required=True, help=\"Input path\")\n\tparser.add_argument(\"--output\", type=Path, required=True, help=\"Output path\")\n\tparser.add_argument(\"--model-path\", type=Path, required=True, help=\"Model path\")\n\tparser.add_argument(\"--model-name\", type=str, required=True, help=\"Model name\")\n\tparser.add_argument(\"--model-version\", type=str, required=True, help=\"Model version\")\n\tparser.add_argument(\"--model-signature-name\", type=str, required=True, help=\"Model signature name\")\n\tparser.add_argument(\"--model-input-name\", type=str, required=True, help=\"Model input name\")\n\tparser.add_argument(\"--model-output-name\", type=str, required=True, help=\"Model output name\")\n\targs, _ = parser.parse_known_args(unparsed_arguments)\n\n\t# Convert file paths to Paths.\n\targs.input = Path(args.input)\n\targs.output =", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport logging\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser(description=\"This script is used to run the training of the image classifier.\")\n\tparser_subparsers = parser.add_subparsers(help='sub-command help', dest='subparser_name')\n\n\t# global arguments\n\tparser.add_argument('--log-level', default=logging.INFO, type=str, help='logging level')\n\tparser.add_argument('--log-file', default=None, type=str, help='logging file')\n\tparser.add_argument('--log-format', default='%(asctime)s %(levelname)s %(message)s', type=str,\n\t                    help='logging format')\n\n\t# training\n\tparser_training = parser_subparsers.add_parser('training', help='parser for training the image classifier')\n\n\tparser_training.add_argument('--data-dir', type=str, help='the directory where the images for training are stored')\n\tparser_training.add_argument('--model-dir', type=str, help='the directory where to save the model')\n\tparser_training.add_argument('--model-name', type=str", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tparser = argparse.ArgumentParser(description=\"Generate a new Python project from a template.\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\")\n\n\t# Setup the arguments for the \"new\" subcommand.\n\tnew_parser = subparsers.add_parser(\"new\", help=\"Generate a new Python project from the template.\")\n\tnew_parser.add_argument(\"name\", type=str, help=\"The name of the new project.\")\n\tnew_parser.add_argument(\"--template-path\", type=str, help=\"The path to the template project.\")\n\tnew_parser.add_argument(\"--install-dependencies\", action=\"store_true\", help=\"Install dependencies when generating the project.\")\n\tnew_parser.add_argument(\"--force\", action=\"store_true\", help=\"Force overwrite of existing project.\")\n\n\t# Parse the arguments and return them as a dict mapping from subparser name to argparse.Namespace\n\t# instance.\n\targuments = vars(parser.parse_args(unparsed_arguments))\n\tsubcommand = arguments.pop(\"subcommand\")\n\treturn {subcommand: argparse.Namespace(**arguments)}\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\t\n\tdef add_subparser(name, description, func):\n\t\t\"\"\"\n\t\tAdd a subparser for the given function.\n\t\t\"\"\"\n\t\tsubparser = subparsers.add_parser(name, description=description)\n\t\tfunc(subparser)\n\t\n\t# Add subparsers for each function.\n\tadd_subparser(\"new\", \"Create a new project.\", new_project)\n\tadd_subparser(\"clean\", \"Clean up the project directory.\", clean_project)\n\tadd_subparser(\"compile\", \"Compile the project.\", compile_project)\n\tadd_subparser(\"test\", \"Test the project.\", test_project)\n\tadd_subparser(\"run\", \"Run the project.\", run_project)\n\tadd_subparser(\"package\", \"Package the project.\", package_project)\n\tadd_subparser(\"install\", \"Install the project.\", install_project)\n\tadd_subparser(\"uninstall\", \"Uninstall the project.\", uninstall_project)\n\tadd_subparser(\"debug\", \"Debug the project.\", debug_project)\n\tadd_subparser(\"list\", \"List the installed packages.\", list_packages)\n\tadd_", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(prog=\"python -m \" + __package__, description=\"This script is used to test the installation and configuration of the Python package.\")\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + _get_version())\n\t\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\", dest=\"command\")\n\t\n\ttest_installation_parser = subparsers.add_parser(\"test_installation\", help=\"Test installation and configuration.\")\n\ttest_installation_parser.add_argument(\"-v\", \"--verbose\", help=\"Show additional debugging output.\", action=\"store_true\")\n\t\n\targs, remaining_args = parser.parse_known_args(unparsed_arguments)\n\tif args.command == \"test_installation\":\n\t\tfrom . import test_installation\n\t\ttest_installation.main(remaining_args)\n\telse:\n\t\tparser.print_help(sys.stderr)", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport sys\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run one of the subcommands below:\")\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\")\n\n\tcreate_parser = subparsers.add_parser(\"create\", help=\"Create a new model.\")\n\tcreate_parser.add_argument(\"model_name\", type=str, help=\"The name of the model.\")\n\tcreate_parser.add_argument(\"--citation\", \"-c\", type=str, help=\"The citation for the model.\")\n\tcreate_parser.add_argument(\"--description\", \"-d\", type=str, help=\"A description of the model.\")\n\tcreate_parser.add_argument(\n\t\t\"--license\", \"-l\",\n\t\ttype=str,\n\t\thelp=\"The license for the model. See https://choosealicense.com/ for more info.\")\n\tcreate_parser.add_argument(\n\t\t\"--private\", \"-p\",\n\t\taction=\"store_true\",\n\t\thelp=\"Make the model private. Only people with the model's owner's access can use it.\")\n\n\tinfo_parser = subparsers.add_parser(\n\t\t\"info\",\n\t\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"This program converts a .xlsx file to a .csv file\")\n\tparser.add_argument('-i', '--input', required=True, help='Path to input file')\n\tparser.add_argument('-o', '--output', required=True, help='Path to output file')\n\tparsed_arguments = parser.parse_args(unparsed_arguments)\n\treturn parsed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description=\"\")\n\t\n\tparser.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"Enable debugging output\")\n\t\n\t# add subparsers for each subcommand\n\tsubparsers = parser.add_subparsers(title=\"Subcommands\", dest=\"subparser_name\")\n\t\n\t# subparser for the \"parse\" subcommand\n\tparse_parser = subparsers.add_parser(\"parse\", help=\"Parse a file\")\n\tparse_parser.add_argument(\"file\", type=str, help=\"File to parse\")\n\t\n\t# subparser for the \"diff\" subcommand\n\tdiff_parser = subparsers.add_parser(\"diff\", help=\"Compare two files\")\n\tdiff_parser.add_argument(\"file1\", type=str, help=\"First file\")\n\tdiff_parser.add_argument(\"file2\", type=str, help=\"Second file\")\n\t\n\t# subparser for the \"search\" subcommand\n\tsearch_parser = subparsers.add_parser(\"search\", help=\"Search for a string in a file\")\n\tsearch_parser.add_argument(\"file\", type=str, help=\"File to", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\t\n\tparser_a = subparsers.add_parser(\"a\", help=\"do something\")\n\tparser_a.add_argument(\"--foo\", type=str, help=\"the foo argument\")\n\t\n\tparser_b = subparsers.add_parser(\"b\", help=\"do something else\")\n\tparser_b.add_argument(\"--bar\", type=str, help=\"the bar argument\")\n\t\n\tparser_c = subparsers.add_parser(\"c\", help=\"do something else\")\n\tparser_c.add_argument(\"--baz\", type=str, help=\"the baz argument\")\n\t\n\targs = parser.parse_args(unparsed_arguments)\n\t\n\tsubparser_name = args.subparser_name\n\t\n\tif subparser_name == \"a\":\n\t\treturn {subparser_name: args}\n\telif subparser_name == \"b\":\n\t\treturn {subparser_name: args}\n\telif subparser_name == \"c\":\n\t\treturn {subparser_name: args}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\t# We need to create an argument parser for the whole program. This will parse the global\n\t# arguments.\n\tglobal_parser = argparse.ArgumentParser(description=\"Run tests for the specified library\")\n\tglobal_parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Show more information on stderr\")\n\tglobal_parser.add_argument(\"--quiet\", \"-q\", action=\"store_true\", help=\"Show less information on stderr\")\n\tglobal_parser.add_argument(\"--json\", \"-j\", action=\"store_true\", help=\"Print output in JSON format\")\n\tglobal_parser.add_argument(\"--no-color\", action=\"store_true\", help=\"Print output without ANSI colors\")\n\tglobal_parser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\tglobal_parser.add_argument(\"--help\", action=\"help\", help=\"Show this help message and exit\")\n\t\n\t# We need to get a list of subparsers.\n\tsubparsers = global_parser.add_subparsers(dest=\"command\", title=\"Commands\", metavar=\"COMMAND\", description=\"Command to", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\tfrom pathlib import Path\n\n\t# Parse the command-line arguments.\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--input\", type=Path, required=True, help=\"Input path\")\n\tparser.add_argument(\"--output\", type=Path, required=True, help=\"Output path\")\n\tparser.add_argument(\"--model-path\", type=Path, required=True, help=\"Model path\")\n\tparser.add_argument(\"--model-name\", type=str, required=True, help=\"Model name\")\n\tparser.add_argument(\"--model-version\", type=str, required=True, help=\"Model version\")\n\tparser.add_argument(\"--model-signature-name\", type=str, required=True, help=\"Model signature name\")\n\tparser.add_argument(\"--model-input-name\", type=str, required=True, help=\"Model input name\")\n\tparser.add_argument(\"--model-output-name\", type=str, required=True, help=\"Model output name\")\n\targs, _ = parser.parse_known_args(unparsed_arguments)\n\n\t# Convert file paths to Paths.\n\targs.input = Path(args.input)\n\targs.output =", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport logging\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser(description=\"This script is used to run the training of the image classifier.\")\n\tparser_subparsers = parser.add_subparsers(help='sub-command help', dest='subparser_name')\n\n\t# global arguments\n\tparser.add_argument('--log-level', default=logging.INFO, type=str, help='logging level')\n\tparser.add_argument('--log-file', default=None, type=str, help='logging file')\n\tparser.add_argument('--log-format', default='%(asctime)s %(levelname)s %(message)s', type=str,\n\t                    help='logging format')\n\n\t# training\n\tparser_training = parser_subparsers.add_parser('training', help='parser for training the image classifier')\n\n\tparser_training.add_argument('--data-dir', type=str, help='the directory where the images for training are stored')\n\tparser_training.add_argument('--model-dir', type=str, help='the directory where to save the model')\n\tparser_training.add_argument('--model-name', type=str", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tparser = argparse.ArgumentParser(description=\"Generate a new Python project from a template.\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\")\n\n\t# Setup the arguments for the \"new\" subcommand.\n\tnew_parser = subparsers.add_parser(\"new\", help=\"Generate a new Python project from the template.\")\n\tnew_parser.add_argument(\"name\", type=str, help=\"The name of the new project.\")\n\tnew_parser.add_argument(\"--template-path\", type=str, help=\"The path to the template project.\")\n\tnew_parser.add_argument(\"--install-dependencies\", action=\"store_true\", help=\"Install dependencies when generating the project.\")\n\tnew_parser.add_argument(\"--force\", action=\"store_true\", help=\"Force overwrite of existing project.\")\n\n\t# Parse the arguments and return them as a dict mapping from subparser name to argparse.Namespace\n\t# instance.\n\targuments = vars(parser.parse_args(unparsed_arguments))\n\tsubcommand = arguments.pop(\"subcommand\")\n\treturn {subcommand: argparse.Namespace(**arguments)}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\t\n\tdef add_subparser(name, description, func):\n\t\t\"\"\"\n\t\tAdd a subparser for the given function.\n\t\t\"\"\"\n\t\tsubparser = subparsers.add_parser(name, description=description)\n\t\tfunc(subparser)\n\t\n\t# Add subparsers for each function.\n\tadd_subparser(\"new\", \"Create a new project.\", new_project)\n\tadd_subparser(\"clean\", \"Clean up the project directory.\", clean_project)\n\tadd_subparser(\"compile\", \"Compile the project.\", compile_project)\n\tadd_subparser(\"test\", \"Test the project.\", test_project)\n\tadd_subparser(\"run\", \"Run the project.\", run_project)\n\tadd_subparser(\"package\", \"Package the project.\", package_project)\n\tadd_subparser(\"install\", \"Install the project.\", install_project)\n\tadd_subparser(\"uninstall\", \"Uninstall the project.\", uninstall_project)\n\tadd_subparser(\"debug\", \"Debug the project.\", debug_project)\n\tadd_subparser(\"list\", \"List the installed packages.\", list_packages)\n\tadd_", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(prog=\"python -m \" + __package__, description=\"This script is used to test the installation and configuration of the Python package.\")\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + _get_version())\n\t\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\", dest=\"command\")\n\t\n\ttest_installation_parser = subparsers.add_parser(\"test_installation\", help=\"Test installation and configuration.\")\n\ttest_installation_parser.add_argument(\"-v\", \"--verbose\", help=\"Show additional debugging output.\", action=\"store_true\")\n\t\n\targs, remaining_args = parser.parse_known_args(unparsed_arguments)\n\tif args.command == \"test_installation\":\n\t\tfrom . import test_installation\n\t\ttest_installation.main(remaining_args)\n\telse:\n\t\tparser.print_help(sys.stderr)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport sys\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run one of the subcommands below:\")\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\")\n\n\tcreate_parser = subparsers.add_parser(\"create\", help=\"Create a new model.\")\n\tcreate_parser.add_argument(\"model_name\", type=str, help=\"The name of the model.\")\n\tcreate_parser.add_argument(\"--citation\", \"-c\", type=str, help=\"The citation for the model.\")\n\tcreate_parser.add_argument(\"--description\", \"-d\", type=str, help=\"A description of the model.\")\n\tcreate_parser.add_argument(\n\t\t\"--license\", \"-l\",\n\t\ttype=str,\n\t\thelp=\"The license for the model. See https://choosealicense.com/ for more info.\")\n\tcreate_parser.add_argument(\n\t\t\"--private\", \"-p\",\n\t\taction=\"store_true\",\n\t\thelp=\"Make the model private. Only people with the model's owner's access can use it.\")\n\n\tinfo_parser = subparsers.add_parser(\n\t\t\"info\",\n\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tclaimed_arguments = set()\n\tsubparser_results = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tresult = subparser.parse_known_args(unparsed_arguments)\n\t\t\tsubparser_results[subparser_name] = result[0]\n\t\t\tclaimed_arguments.update(result[1])\n\t\texcept SystemExit:\n\t\t\tpass\n\tremaining_arguments = list(set(unparsed_arguments) - claimed_arguments)\n\treturn subparser_results, remaining_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\targuments, unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\tparsed_arguments[subparser_name] = arguments\n\t\texcept Exception as exception:\n\t\t\tpass\n\treturn parsed_arguments, unparsed_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = list(unparsed_arguments)\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments.pop(0)\n\t\tif argument in {'--repository', '--repository-file'}:\n\t\t\trepository_file_path = unparsed_arguments.pop(0)\n\t\t\twith open(repository_file_path) as repository_file:\n\t\t\t\trepository_lines = repository_file.readlines()\n\t\t\t\trepository_lines = [line.strip() for line in repository_lines]\n\t\telse:\n\t\t\trepository_lines = []\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\telse:\n\t\t\tparsed_arguments = {}\n\t\t\tunparsed_arguments.insert(0, argument)\n\t\t\tbreak\n\treturn parsed_arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_subparser_arguments = {}\n\tunparsed_arguments = unparsed_arguments[:]\n\n\tfor subparser_name in sorted(subparsers):\n\t\tsubparser = subparsers[subparser_name]\n\t\tsubparser_arguments = subparser.parse_args(unparsed_arguments)\n\n\t\tif subparser_arguments:\n\t\t\tparsed_subparser_arguments[subparser_name] = subparser_arguments\n\n\t\t\tunparsed_arguments = [arg for arg in unparsed_arguments if arg not in subparser_arguments._get_args()]\n\n\treturn parsed_subparser_arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {name: {} for name in subparsers}\n\tunclaimed_arguments = unparsed_arguments[:]\n\tfor name, parser in subparsers.items():\n\t\ttry:\n\t\t\targs, unclaimed_arguments = parser.parse_known_args(unclaimed_arguments)\n\t\t\tsubparser_arguments[name] = args\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn subparser_arguments, unclaimed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments[0]\n\t\tunparsed_arguments = unparsed_arguments[1:]\n\t\tfor subparser_name, subparser_parser in subparsers.items():\n\t\t\tif subparser_parser.prefix_chars[0] == argument[0]:\n\t\t\t\ttry:\n\t\t\t\t\tsubparser_arguments[subparser_name] = subparser_parser.parse_args(unparsed_arguments)\n\t\t\t\t\treturn subparser_arguments, unparsed_arguments\n\t\t\t\texcept SystemExit:\n\t\t\t\t\tpass\n\n\traise ValueError('could not parse arguments: ' + ' '.join(unparsed_arguments))\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\n\twhile unparsed_arguments:\n\t\tsubparser_found = False\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser.parse_args(unparsed_arguments, namespace=Namespace())\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tparsed_arguments[subparser_name] = unparsed_arguments\n\t\t\t\tunparsed_arguments = []\n\t\t\t\tsubparser_found = True\n\t\t\t\tbreak\n\n\t\tif not subparser_found:\n\t\t\tunparsed_arguments.pop(0)\n\n\treturn parsed_arguments, unparsed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_claimed_arguments = {}\n\tother_arguments = []\n\tfor unclaimed_argument in unparsed_arguments:\n\t\ttry:\n\t\t\tsubparsers_claimed_arguments[subparsers[unclaimed_argument].name] = subparsers[unclaimed_argument].parse_args(unparsed_arguments)\n\t\texcept:\n\t\t\tother_arguments.append(unclaimed_argument)\n\treturn subparsers_claimed_arguments, other_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_subparser_arguments = {}\n\n\twhile unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif subparser.parse_args(unparsed_arguments):\n\t\t\t\tparsed_subparser_arguments[subparser_name], unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\t\tbreak\n\t\telse:\n\t\t\t# If no subparser could parse any of the arguments, we must have hit an error.\n\t\t\tsubparser.error(\"unrecognized arguments: {}\".format(' '.join(unparsed_arguments)))\n\n\treturn parsed_subparser_arguments, unparsed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = unparsed_arguments[:]\n\tfor subparser in subparsers:\n\t\ttry:\n\t\t\tparsed_arguments[subparser], remaining_arguments = subparsers[subparser].parse_known_args(remaining_arguments)\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn parsed_arguments, remaining_arguments\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tclaimed_arguments = set()\n\tsubparser_results = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tresult = subparser.parse_known_args(unparsed_arguments)\n\t\t\tsubparser_results[subparser_name] = result[0]\n\t\t\tclaimed_arguments.update(result[1])\n\t\texcept SystemExit:\n\t\t\tpass\n\tremaining_arguments = list(set(unparsed_arguments) - claimed_arguments)\n\treturn subparser_results, remaining_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\targuments, unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\tparsed_arguments[subparser_name] = arguments\n\t\texcept Exception as exception:\n\t\t\tpass\n\treturn parsed_arguments, unparsed_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = list(unparsed_arguments)\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments.pop(0)\n\t\tif argument in {'--repository', '--repository-file'}:\n\t\t\trepository_file_path = unparsed_arguments.pop(0)\n\t\t\twith open(repository_file_path) as repository_file:\n\t\t\t\trepository_lines = repository_file.readlines()\n\t\t\t\trepository_lines = [line.strip() for line in repository_lines]\n\t\telse:\n\t\t\trepository_lines = []\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\telse:\n\t\t\tparsed_arguments = {}\n\t\t\tunparsed_arguments.insert(0, argument)\n\t\t\tbreak\n\treturn parsed_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_subparser_arguments = {}\n\tunparsed_arguments = unparsed_arguments[:]\n\n\tfor subparser_name in sorted(subparsers):\n\t\tsubparser = subparsers[subparser_name]\n\t\tsubparser_arguments = subparser.parse_args(unparsed_arguments)\n\n\t\tif subparser_arguments:\n\t\t\tparsed_subparser_arguments[subparser_name] = subparser_arguments\n\n\t\t\tunparsed_arguments = [arg for arg in unparsed_arguments if arg not in subparser_arguments._get_args()]\n\n\treturn parsed_subparser_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {name: {} for name in subparsers}\n\tunclaimed_arguments = unparsed_arguments[:]\n\tfor name, parser in subparsers.items():\n\t\ttry:\n\t\t\targs, unclaimed_arguments = parser.parse_known_args(unclaimed_arguments)\n\t\t\tsubparser_arguments[name] = args\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn subparser_arguments, unclaimed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments[0]\n\t\tunparsed_arguments = unparsed_arguments[1:]\n\t\tfor subparser_name, subparser_parser in subparsers.items():\n\t\t\tif subparser_parser.prefix_chars[0] == argument[0]:\n\t\t\t\ttry:\n\t\t\t\t\tsubparser_arguments[subparser_name] = subparser_parser.parse_args(unparsed_arguments)\n\t\t\t\t\treturn subparser_arguments, unparsed_arguments\n\t\t\t\texcept SystemExit:\n\t\t\t\t\tpass\n\n\traise ValueError('could not parse arguments: ' + ' '.join(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\n\twhile unparsed_arguments:\n\t\tsubparser_found = False\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser.parse_args(unparsed_arguments, namespace=Namespace())\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tparsed_arguments[subparser_name] = unparsed_arguments\n\t\t\t\tunparsed_arguments = []\n\t\t\t\tsubparser_found = True\n\t\t\t\tbreak\n\n\t\tif not subparser_found:\n\t\t\tunparsed_arguments.pop(0)\n\n\treturn parsed_arguments, unparsed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_claimed_arguments = {}\n\tother_arguments = []\n\tfor unclaimed_argument in unparsed_arguments:\n\t\ttry:\n\t\t\tsubparsers_claimed_arguments[subparsers[unclaimed_argument].name] = subparsers[unclaimed_argument].parse_args(unparsed_arguments)\n\t\texcept:\n\t\t\tother_arguments.append(unclaimed_argument)\n\treturn subparsers_claimed_arguments, other_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_subparser_arguments = {}\n\n\twhile unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif subparser.parse_args(unparsed_arguments):\n\t\t\t\tparsed_subparser_arguments[subparser_name], unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\t\tbreak\n\t\telse:\n\t\t\t# If no subparser could parse any of the arguments, we must have hit an error.\n\t\t\tsubparser.error(\"unrecognized arguments: {}\".format(' '.join(unparsed_arguments)))\n\n\treturn parsed_subparser_arguments, unparsed_arguments\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = unparsed_arguments[:]\n\tfor subparser in subparsers:\n\t\ttry:\n\t\t\tparsed_arguments[subparser], remaining_arguments = subparsers[subparser].parse_known_args(remaining_arguments)\n\t\texcept SystemExit:\n\t\t\tpass\n\treturn parsed_arguments, remaining_arguments\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Generate an AST from a list of tokens\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\",\n\t\tdescription=\"valid subcommands\", help=\"additional help\")\n\tmake_lex_parser(subparsers)\n\tmake_parse_parser(subparsers)\n\treturn parser, subparsers", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\tparser_one = subparsers.add_parser('one')\n\tparser_one.add_argument('--one-arg', type=str, default=\"default_one_arg\", help=\"an argument for the one subcommand\")\n\t\n\tparser_two = subparsers.add_parser('two')\n\tparser_two.add_argument('--two-arg', type=str, default=\"default_two_arg\", help=\"an argument for the two subcommand\")\n\t\n\treturn parser,parser_one,parser_two\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Generate a template.\")\n\tsubparsers = parser.add_subparsers(\n\t\t\ttitle=\"subcommands\",\n\t\t\tdescription=\"valid subcommands\",\n\t\t\thelp=\"use <subcommand> -h for additional help\"\n\t)\n\n\tparser_template = subparsers.add_parser(\"template\", help=\"Print a template.\")\n\tparser_template.add_argument(\n\t\t\t\"template\",\n\t\t\tchoices=[\"cpp\", \"python\", \"markdown\"],\n\t\t\thelp=\"The template to print.\"\n\t)\n\tparser_template.set_defaults(func=template)\n\n\tparser_hello = subparsers.add_parser(\"hello\", help=\"Print a greeting.\")\n\tparser_hello.add_argument(\"-n\", \"--name\", help=\"The name to greet.\")\n\tparser_hello.add_argument(\"-s\", \"--suffix\", help=\"The suffix to add.\")\n\tparser_hello.set_defaults(func=hello)\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\tmake_subparser_parser(subparsers)\n\tmake_download_parser(subparsers)\n\tmake_list_parser(subparsers)\n\tmake_upload_parser(subparsers)\n\tmake_delete_parser(subparsers)\n\tmake_rename_parser(subparsers)\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser(\n\t\tdescription=\"Run a Python script over a set of files.\"\n\t)\n\tparser.add_argument(\n\t\t\"script\",\n\t\thelp=\"The Python script to run.\"\n\t)\n\tparser.add_argument(\n\t\t\"files\",\n\t\thelp=\"The files to run the script over.\"\n\t)\n\n\tsubparsers = parser.add_subparsers(\n\t\thelp=\"The type of files to run the script over.\",\n\t\tdest=\"files_type\"\n\t)\n\tsubparsers.required = True\n\n\tfiles_by_extension = subparsers.add_parser(\n\t\t\"extension\",\n\t\thelp=\"Run the script over files with a specific type.\"\n\t)\n\tfiles_by_extension.add_argument(\n\t\t\"-e\", \"--extension\",\n\t\tdest=\"extensions\",\n\t\tnargs=\"+\",\n\t\trequired=True,\n\t\thelp=\"The type of files to run the script over.\"\n\t)\n\n\tfiles_by_name = subparsers.add_parser(\n\t\t\"name\",\n\t\thelp=\"Run the script over files with a specific name.\"\n\t)\n\t", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import RawTextHelpFormatter\n\n\tparser = ArgumentParser(description=\"A simple command line tool for creating, editing and managing your todo lists.\",\n\t                        formatter_class=RawTextHelpFormatter)\n\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tadd_subparsers(subparsers)\n\n\treturn (parser, subparsers)\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\t# The first subparser is for the \"train\" command\n\ttrain_parser = subparsers.add_parser(\"train\")\n\ttrain_parser.add_argument(\"data\", type=str, help=\"Path to the data file\")\n\ttrain_parser.add_argument(\"-o\", \"--output\", type=str, default=\"model.pkl\", help=\"Path to save the trained model\")\n\ttrain_parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"Number of training epochs\")\n\ttrain_parser.add_argument(\"-b\", \"--batch-size\", type=int, default=32, help=\"Batch size for training\")\n\ttrain_parser.add_argument(\"-l\", \"--learning-rate\", type=float, default=0.001, help=\"Learning rate for training\")\n\ttrain_parser.add_argument(\"-d\", \"--dropout\", type=float, default=0.2, help=\"Dropout rate for training\")\n\ttrain_parser.add_argument(\"-m\", \"--momentum\", type=float, default=0.9, help=\"", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description='A tool for manipulating the contents of a CSV file.')\n\tsubparsers = parser.add_subparsers(dest='command')\n\t\n\tfrom .get_parser import make_get_parser\n\tget_parser = make_get_parser(subparsers)\n\t\n\tfrom .set_parser import make_set_parser\n\tset_parser = make_set_parser(subparsers)\n\t\n\tfrom .print_parser import make_print_parser\n\tprint_parser = make_print_parser(subparsers)\n\t\n\tfrom .append_parser import make_append_parser\n\tappend_parser = make_append_parser(subparsers)\n\t\n\tfrom .delete_parser import make_delete_parser\n\tdelete_parser = make_delete_parser(subparsers)\n\t\n\tfrom .sort_parser import make_sort_parser\n\tsort_parser = make_sort_parser(subparsers)\n\t\n\treturn parser, get_parser, set_parser, print_parser, append_parser, delete_parser, sort_parser", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import SUPPRESS\n\tfrom . import __version__\n\n\tparser = ArgumentParser(\n\t\t\tprog='tinypypi',\n\t\t\tdescription='A tiny local PyPI server',\n\t\t\tepilog='For more details, see the README at '\n\t\t\t\t\t'https://github.com/vinta/tinypypi',\n\t\t\tadd_help=False,\n\t\t)\n\tparser.add_argument(\n\t\t\t'-v', '--version',\n\t\t\taction='version',\n\t\t\tversion='%(prog)s ' + __version__,\n\t\t)\n\tsubparsers = parser.add_subparsers(\n\t\t\ttitle='subcommands',\n\t\t\tmetavar='subcommand',\n\t\t)\n\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\timport sys\n\n\tparser = ArgumentParser(description=\"A simple text-based game\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", description=\"valid subcommands\")\n\t\n\tnew_parser = subparsers.add_parser(\"new\", help=\"start a new game\")\n\tnew_parser.add_argument(\"name\", nargs=\"?\", help=\"name of the game\")\n\t\n\tload_parser = subparsers.add_parser(\"load\", help=\"load a previously saved game\")\n\tload_parser.add_argument(\"name\", help=\"name of the game\")\n\t\n\tsave_parser = subparsers.add_parser(\"save\", help=\"save the current game\")\n\tsave_parser.add_argument(\"name\", help=\"name of the game\")\n\t\n\treturn parser, subparsers\n\n#game_state.py"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Generate an AST from a list of tokens\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\",\n\t\tdescription=\"valid subcommands\", help=\"additional help\")\n\tmake_lex_parser(subparsers)\n\tmake_parse_parser(subparsers)\n\treturn parser, subparsers", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\tparser_one = subparsers.add_parser('one')\n\tparser_one.add_argument('--one-arg', type=str, default=\"default_one_arg\", help=\"an argument for the one subcommand\")\n\t\n\tparser_two = subparsers.add_parser('two')\n\tparser_two.add_argument('--two-arg', type=str, default=\"default_two_arg\", help=\"an argument for the two subcommand\")\n\t\n\treturn parser,parser_one,parser_two\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Generate a template.\")\n\tsubparsers = parser.add_subparsers(\n\t\t\ttitle=\"subcommands\",\n\t\t\tdescription=\"valid subcommands\",\n\t\t\thelp=\"use <subcommand> -h for additional help\"\n\t)\n\n\tparser_template = subparsers.add_parser(\"template\", help=\"Print a template.\")\n\tparser_template.add_argument(\n\t\t\t\"template\",\n\t\t\tchoices=[\"cpp\", \"python\", \"markdown\"],\n\t\t\thelp=\"The template to print.\"\n\t)\n\tparser_template.set_defaults(func=template)\n\n\tparser_hello = subparsers.add_parser(\"hello\", help=\"Print a greeting.\")\n\tparser_hello.add_argument(\"-n\", \"--name\", help=\"The name to greet.\")\n\tparser_hello.add_argument(\"-s\", \"--suffix\", help=\"The suffix to add.\")\n\tparser_hello.set_defaults(func=hello)\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\tmake_subparser_parser(subparsers)\n\tmake_download_parser(subparsers)\n\tmake_list_parser(subparsers)\n\tmake_upload_parser(subparsers)\n\tmake_delete_parser(subparsers)\n\tmake_rename_parser(subparsers)\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser(\n\t\tdescription=\"Run a Python script over a set of files.\"\n\t)\n\tparser.add_argument(\n\t\t\"script\",\n\t\thelp=\"The Python script to run.\"\n\t)\n\tparser.add_argument(\n\t\t\"files\",\n\t\thelp=\"The files to run the script over.\"\n\t)\n\n\tsubparsers = parser.add_subparsers(\n\t\thelp=\"The type of files to run the script over.\",\n\t\tdest=\"files_type\"\n\t)\n\tsubparsers.required = True\n\n\tfiles_by_extension = subparsers.add_parser(\n\t\t\"extension\",\n\t\thelp=\"Run the script over files with a specific type.\"\n\t)\n\tfiles_by_extension.add_argument(\n\t\t\"-e\", \"--extension\",\n\t\tdest=\"extensions\",\n\t\tnargs=\"+\",\n\t\trequired=True,\n\t\thelp=\"The type of files to run the script over.\"\n\t)\n\n\tfiles_by_name = subparsers.add_parser(\n\t\t\"name\",\n\t\thelp=\"Run the script over files with a specific name.\"\n\t)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import RawTextHelpFormatter\n\n\tparser = ArgumentParser(description=\"A simple command line tool for creating, editing and managing your todo lists.\",\n\t                        formatter_class=RawTextHelpFormatter)\n\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tadd_subparsers(subparsers)\n\n\treturn (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\t# The first subparser is for the \"train\" command\n\ttrain_parser = subparsers.add_parser(\"train\")\n\ttrain_parser.add_argument(\"data\", type=str, help=\"Path to the data file\")\n\ttrain_parser.add_argument(\"-o\", \"--output\", type=str, default=\"model.pkl\", help=\"Path to save the trained model\")\n\ttrain_parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"Number of training epochs\")\n\ttrain_parser.add_argument(\"-b\", \"--batch-size\", type=int, default=32, help=\"Batch size for training\")\n\ttrain_parser.add_argument(\"-l\", \"--learning-rate\", type=float, default=0.001, help=\"Learning rate for training\")\n\ttrain_parser.add_argument(\"-d\", \"--dropout\", type=float, default=0.2, help=\"Dropout rate for training\")\n\ttrain_parser.add_argument(\"-m\", \"--momentum\", type=float, default=0.9, help=\"", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description='A tool for manipulating the contents of a CSV file.')\n\tsubparsers = parser.add_subparsers(dest='command')\n\t\n\tfrom .get_parser import make_get_parser\n\tget_parser = make_get_parser(subparsers)\n\t\n\tfrom .set_parser import make_set_parser\n\tset_parser = make_set_parser(subparsers)\n\t\n\tfrom .print_parser import make_print_parser\n\tprint_parser = make_print_parser(subparsers)\n\t\n\tfrom .append_parser import make_append_parser\n\tappend_parser = make_append_parser(subparsers)\n\t\n\tfrom .delete_parser import make_delete_parser\n\tdelete_parser = make_delete_parser(subparsers)\n\t\n\tfrom .sort_parser import make_sort_parser\n\tsort_parser = make_sort_parser(subparsers)\n\t\n\treturn parser, get_parser, set_parser, print_parser, append_parser, delete_parser, sort_parser", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom argparse import SUPPRESS\n\tfrom . import __version__\n\n\tparser = ArgumentParser(\n\t\t\tprog='tinypypi',\n\t\t\tdescription='A tiny local PyPI server',\n\t\t\tepilog='For more details, see the README at '\n\t\t\t\t\t'https://github.com/vinta/tinypypi',\n\t\t\tadd_help=False,\n\t\t)\n\tparser.add_argument(\n\t\t\t'-v', '--version',\n\t\t\taction='version',\n\t\t\tversion='%(prog)s ' + __version__,\n\t\t)\n\tsubparsers = parser.add_subparsers(\n\t\t\ttitle='subcommands',\n\t\t\tmetavar='subcommand',\n\t\t)\n\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\timport sys\n\n\tparser = ArgumentParser(description=\"A simple text-based game\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", description=\"valid subcommands\")\n\t\n\tnew_parser = subparsers.add_parser(\"new\", help=\"start a new game\")\n\tnew_parser.add_argument(\"name\", nargs=\"?\", help=\"name of the game\")\n\t\n\tload_parser = subparsers.add_parser(\"load\", help=\"load a previously saved game\")\n\tload_parser.add_argument(\"name\", help=\"name of the game\")\n\t\n\tsave_parser = subparsers.add_parser(\"save\", help=\"save the current game\")\n\tsave_parser.add_argument(\"name\", help=\"name of the game\")\n\t\n\treturn parser, subparsers\n\n#game_state.py", "is_pass": true, "return_code": 0}]}
{"file_path": "infoblox_client/utils.py", "project": "infobloxopen/infoblox-client", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tend = start + max_results\n\t\tyield response[start:end]\n\t\tstart = end", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_count = len(response)\n\tpage_count = response_count // max_results\n\tpage_rest = response_count % max_results\n\tstart_idx = 0\n\tfor idx in range(page_count):\n\t\tend_idx = start_idx + max_results\n\t\tyield response[start_idx:end_idx]\n\t\tstart_idx = end_idx\n\tif page_rest != 0:\n\t\tyield response[start_idx:]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\tend = max_results\n\twhile start < len(response):\n\t\tyield response[start:end]\n\t\tstart = end\n\t\tend += max_results\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti+=max_results\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tyield response[start:start+max_results]\n\t\tstart += max_results", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart_index = 0\n\tend_index = max_results\n\twhile True:\n\t\tif start_index < len(response):\n\t\t\tyield response[start_index:end_index]\n\t\t\tstart_index += max_results\n\t\t\tend_index += max_results\n\t\telse:\n\t\t\tbreak", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 0\n\twhile True:\n\t\tresponse_page = response[page*max_results:(page+1)*max_results]\n\t\tif response_page:\n\t\t\tyield response_page\n\t\t\tpage += 1\n\t\telse:\n\t\t\tbreak\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tyield response[start:start + max_results]\n\t\tstart += max_results\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tend = start + max_results\n\t\tyield response[start: end]\n\t\tstart = end", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tidx = 0\n\twhile idx < len(response):\n\t\tyield response[idx:idx+max_results]\n\t\tidx += max_results"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tend = start + max_results\n\t\tyield response[start:end]\n\t\tstart = end", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_count = len(response)\n\tpage_count = response_count // max_results\n\tpage_rest = response_count % max_results\n\tstart_idx = 0\n\tfor idx in range(page_count):\n\t\tend_idx = start_idx + max_results\n\t\tyield response[start_idx:end_idx]\n\t\tstart_idx = end_idx\n\tif page_rest != 0:\n\t\tyield response[start_idx:]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\tend = max_results\n\twhile start < len(response):\n\t\tyield response[start:end]\n\t\tstart = end\n\t\tend += max_results\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti+=max_results\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tyield response[start:start+max_results]\n\t\tstart += max_results", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart_index = 0\n\tend_index = max_results\n\twhile True:\n\t\tif start_index < len(response):\n\t\t\tyield response[start_index:end_index]\n\t\t\tstart_index += max_results\n\t\t\tend_index += max_results\n\t\telse:\n\t\t\tbreak", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 0\n\twhile True:\n\t\tresponse_page = response[page*max_results:(page+1)*max_results]\n\t\tif response_page:\n\t\t\tyield response_page\n\t\t\tpage += 1\n\t\telse:\n\t\t\tbreak\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tyield response[start:start + max_results]\n\t\tstart += max_results\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart = 0\n\twhile start < len(response):\n\t\tend = start + max_results\n\t\tyield response[start: end]\n\t\tstart = end", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tidx = 0\n\twhile idx < len(response):\n\t\tyield response[idx:idx+max_results]\n\t\tidx += max_results", "is_pass": true, "return_code": 0}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {\"B\": 1, \"K\": 2 ** 10, \"M\": 2 ** 20, \"G\": 2 ** 30}\n\tnumber, unit = int(size[:-1]), size[-1].upper()\n\treturn number * multiplier[unit]\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tunits = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024 * 1024,\n\t\t'G': 1024 * 1024 * 1024\n\t}\n\n\tsize = size.upper()\n\tsuffix = size[-1]\n\tif suffix in units:\n\t\tnum = float(size[:-1])\n\t\tsuffix = size[-1]\n\t\treturn int(num * units[suffix])\n\telse:\n\t\traise Exception('Unknown file size units')", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {'K':10**3, 'M':10**6, 'G':10**9, 'T':10**12}\n\tnumber = size[:-1]\n\tunit = size[-1]\n\n\treturn int(float(number) * multiplier[unit])\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultipliers = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024 * 1024,\n\t\t'G': 1024 * 1024 * 1024,\n\t\t'T': 1024 * 1024 * 1024 * 1024\n\t}\n\tmultiplier = multipliers[size[-1].upper()]\n\tvalue = int(size[:-1]) * multiplier\n\treturn value\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {\n\t\t'K': 1,\n\t\t'M': 1024,\n\t\t'G': 1024 * 1024,\n\t\t'T': 1024 * 1024 * 1024\n\t}\n\tnumber, letter = size[:-1], size[-1]\n\tif letter not in multiplier:\n\t\traise ValueError('Invalid file size')\n\treturn int(float(number) * multiplier[letter])", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = {\n\t\t'B': 1,\n\t\t'KB': 1024,\n\t\t'MB': 1024**2,\n\t\t'GB': 1024**3,\n\t\t'TB': 1024**4,\n\t\t'PB': 1024**5\n\t}\n\ttry:\n\t\tnumber, suffix = size.split()\n\t\treturn int(float(number) * suffixes[suffix.upper()])\n\texcept ValueError:\n\t\traise ValueError('Size must be in format \"<number><suffix>\" (eg: \"100KB\")')\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_multiplier = {'K': 1024, 'M': 1024**2, 'G': 1024**3}\n\tsize = size.upper()\n\n\tnumber = int(size[:-1])\n\tmultiplier = size_multiplier[size[-1]]\n\n\treturn number * multiplier\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = {\n\t\t\"K\": 1024,\n\t\t\"M\": 1024 ** 2,\n\t\t\"G\": 1024 ** 3\n\t}\n\tsuffix = size[-1]\n\tsize = int(size[:-1])\n\tif suffix in suffixes:\n\t\treturn size * suffixes[suffix]\n\telse:\n\t\traise ValueError(\"Invalid File Size\")\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tunit = size[-1].lower()\n\tvalue = int(size[:-1])\n\tunit_dict = {\n\t\t'k': 1024,\n\t\t'm': 1024 ** 2,\n\t\t'g': 1024 ** 3\n\t}\n\tsize_in_bytes = value * unit_dict[unit]\n\treturn size_in_bytes\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultipliers = {\n\t\t'K': 1024,\n\t\t'M': 1024*1024,\n\t\t'G': 1024*1024*1024\n\t}\n\tnumber, suffix = size[:-1], size[-1]\n\treturn int(float(number)*multipliers[suffix])\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {\"B\": 1, \"K\": 2 ** 10, \"M\": 2 ** 20, \"G\": 2 ** 30}\n\tnumber, unit = int(size[:-1]), size[-1].upper()\n\treturn number * multiplier[unit]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tunits = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024 * 1024,\n\t\t'G': 1024 * 1024 * 1024\n\t}\n\n\tsize = size.upper()\n\tsuffix = size[-1]\n\tif suffix in units:\n\t\tnum = float(size[:-1])\n\t\tsuffix = size[-1]\n\t\treturn int(num * units[suffix])\n\telse:\n\t\traise Exception('Unknown file size units')", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {'K':10**3, 'M':10**6, 'G':10**9, 'T':10**12}\n\tnumber = size[:-1]\n\tunit = size[-1]\n\n\treturn int(float(number) * multiplier[unit])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultipliers = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024 * 1024,\n\t\t'G': 1024 * 1024 * 1024,\n\t\t'T': 1024 * 1024 * 1024 * 1024\n\t}\n\tmultiplier = multipliers[size[-1].upper()]\n\tvalue = int(size[:-1]) * multiplier\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {\n\t\t'K': 1,\n\t\t'M': 1024,\n\t\t'G': 1024 * 1024,\n\t\t'T': 1024 * 1024 * 1024\n\t}\n\tnumber, letter = size[:-1], size[-1]\n\tif letter not in multiplier:\n\t\traise ValueError('Invalid file size')\n\treturn int(float(number) * multiplier[letter])", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = {\n\t\t'B': 1,\n\t\t'KB': 1024,\n\t\t'MB': 1024**2,\n\t\t'GB': 1024**3,\n\t\t'TB': 1024**4,\n\t\t'PB': 1024**5\n\t}\n\ttry:\n\t\tnumber, suffix = size.split()\n\t\treturn int(float(number) * suffixes[suffix.upper()])\n\texcept ValueError:\n\t\traise ValueError('Size must be in format \"<number><suffix>\" (eg: \"100KB\")')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_multiplier = {'K': 1024, 'M': 1024**2, 'G': 1024**3}\n\tsize = size.upper()\n\n\tnumber = int(size[:-1])\n\tmultiplier = size_multiplier[size[-1]]\n\n\treturn number * multiplier\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = {\n\t\t\"K\": 1024,\n\t\t\"M\": 1024 ** 2,\n\t\t\"G\": 1024 ** 3\n\t}\n\tsuffix = size[-1]\n\tsize = int(size[:-1])\n\tif suffix in suffixes:\n\t\treturn size * suffixes[suffix]\n\telse:\n\t\traise ValueError(\"Invalid File Size\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tunit = size[-1].lower()\n\tvalue = int(size[:-1])\n\tunit_dict = {\n\t\t'k': 1024,\n\t\t'm': 1024 ** 2,\n\t\t'g': 1024 ** 3\n\t}\n\tsize_in_bytes = value * unit_dict[unit]\n\treturn size_in_bytes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultipliers = {\n\t\t'K': 1024,\n\t\t'M': 1024*1024,\n\t\t'G': 1024*1024*1024\n\t}\n\tnumber, suffix = size[:-1], size[-1]\n\treturn int(float(number)*multipliers[suffix])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "contrib/planb-swiftsync.py", "project": "ossobv/planb", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tres = dict()\n\tfor key in dicts[0]:\n\t\tres[key] = dicts[0][key]\n\tfor i in range(1, len(dicts)):\n\t\tfor key in dicts[i]:\n\t\t\tif key in res.keys():\n\t\t\t\tres[key] += dicts[i][key]\n\t\t\telse:\n\t\t\t\tres[key] = dicts[i][key]\n\treturn res\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdicts = [dict(d) for d in dicts]\n\tres = {}\n\tfor d in dicts:\n\t\tfor key in d.keys():\n\t\t\tres[key] = res.get(key,0) + d[key]\n\treturn res\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\timport collections\n\tresult = collections.defaultdict(int)\n\tfor d in dicts:\n\t\tfor key, val in d.items():\n\t\t\tresult[key] += val\n\treturn result\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tnew_dict = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tnew_dict[key] = new_dict.get(key, 0) + value\n\treturn new_dict\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tsum_dict = {}\n\tfor d in dicts:\n\t\tsum_dict.update(d)\n\tfor k,v in sum_dict.items():\n\t\tsum_dict[k] = sum(v)\n\treturn sum_dict\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor key in dicts[0].keys():\n\t\tdic[key] = 0\n\tfor d in dicts:\n\t\tfor key in d.keys():\n\t\t\tdic[key] += d[key]\n\treturn dic\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tnew_dict = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tif key in new_dict:\n\t\t\t\tnew_dict[key] += value\n\t\t\telse:\n\t\t\t\tnew_dict[key] = value\n\treturn new_dict", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor key in dicts[0]:\n\t\tdic[key] = dicts[0][key]\n\t\tfor i in range(1, len(dicts)):\n\t\t\tif key in dicts[i]:\n\t\t\t\tdic[key] += dicts[i][key]\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor key in dicts[0]:\n\t\tdic[key] = dicts[0][key]\n\tfor dict_ in dicts[1:]:\n\t\tfor key, value in dict_.items():\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += value\n\t\t\telse:\n\t\t\t\tdic[key] = value\n\treturn dic", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tkeys = list(dicts[0].keys())\n\tdic = dict(dicts[0])\n\tfor i in range(1, len(dicts)):\n\t\tfor key in keys:\n\t\t\tif key in dicts[i]:\n\t\t\t\tdic[key] = dic[key] + dicts[i][key]\n\t\t\telse:\n\t\t\t\tdic[key] = dicts[i][key]\n\treturn dic\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tres = dict()\n\tfor key in dicts[0]:\n\t\tres[key] = dicts[0][key]\n\tfor i in range(1, len(dicts)):\n\t\tfor key in dicts[i]:\n\t\t\tif key in res.keys():\n\t\t\t\tres[key] += dicts[i][key]\n\t\t\telse:\n\t\t\t\tres[key] = dicts[i][key]\n\treturn res\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdicts = [dict(d) for d in dicts]\n\tres = {}\n\tfor d in dicts:\n\t\tfor key in d.keys():\n\t\t\tres[key] = res.get(key,0) + d[key]\n\treturn res\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\timport collections\n\tresult = collections.defaultdict(int)\n\tfor d in dicts:\n\t\tfor key, val in d.items():\n\t\t\tresult[key] += val\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tnew_dict = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tnew_dict[key] = new_dict.get(key, 0) + value\n\treturn new_dict\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tsum_dict = {}\n\tfor d in dicts:\n\t\tsum_dict.update(d)\n\tfor k,v in sum_dict.items():\n\t\tsum_dict[k] = sum(v)\n\treturn sum_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor key in dicts[0].keys():\n\t\tdic[key] = 0\n\tfor d in dicts:\n\t\tfor key in d.keys():\n\t\t\tdic[key] += d[key]\n\treturn dic\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tnew_dict = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tif key in new_dict:\n\t\t\t\tnew_dict[key] += value\n\t\t\telse:\n\t\t\t\tnew_dict[key] = value\n\treturn new_dict", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor key in dicts[0]:\n\t\tdic[key] = dicts[0][key]\n\t\tfor i in range(1, len(dicts)):\n\t\t\tif key in dicts[i]:\n\t\t\t\tdic[key] += dicts[i][key]\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor key in dicts[0]:\n\t\tdic[key] = dicts[0][key]\n\tfor dict_ in dicts[1:]:\n\t\tfor key, value in dict_.items():\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += value\n\t\t\telse:\n\t\t\t\tdic[key] = value\n\treturn dic", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tkeys = list(dicts[0].keys())\n\tdic = dict(dicts[0])\n\tfor i in range(1, len(dicts)):\n\t\tfor key in keys:\n\t\t\tif key in dicts[i]:\n\t\t\t\tdic[key] = dic[key] + dicts[i][key]\n\t\t\telse:\n\t\t\t\tdic[key] = dicts[i][key]\n\treturn dic\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pyseed/apibase.py", "project": "SEED-platform/py-seed", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args.copy()\n\turl_args.pop('_', None)\n\turl_args.pop('page', None)\n\turl_args.pop('page_size', None)\n\tfor key, value in url_args.items():\n\t\turl = url.replace('{' + key + '}', value)\n\n\treturn url", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args.copy()\n\tfor key, value in url_args.items():\n\t\turl = url.replace('{' + key + '}', str(value), 1)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\turl_args_values = url_args.values()\n\tfor key, value in url_args.items():\n\t\tif key in url:\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_parts = url.split(\"?\")\n\turl = url_parts[0]\n\turl_query_args = url_parts[1]\n\tfor key, value in url_args.items():\n\t\turl_query_args = url_query_args.replace(key, value)\n\n\treturn url + \"?\" + url_query_args\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_keys = url_args.keys()\n\tfor key in url_keys:\n\t\turl = url.replace(key, url_args[key])\n\treturn url", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args.copy()\n\turl = url.split('?')\n\turl_split = url[1].split('&')\n\tfor key, value in url_args.items():\n\t\tfor i, param in enumerate(url_split):\n\t\t\tif key in param:\n\t\t\t\turl_split[i] = f\"{key}={value}\"\n\turl = '?' + '&'.join(url_split)\n\treturn url if url_args else url[0]\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl = url\n\turl_args = url_args\n\tif url_args:\n\t\turl_args_keys = url_args.keys()\n\t\turl_args_values = url_args.values()\n\t\turl = url + '?' + url_args_keys[0] + '=' + url_args_values[0]\n\t\tfor i in range(1, len(url_args_keys)):\n\t\t\turl = url + '&' + url_args_keys[i] + '=' + url_args_values[i]\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tparams = url.split(\"/\")\n\targs = url_args.split(\"/\")\n\tfor i, arg in enumerate(args):\n\t\tif i < len(args) - 1:\n\t\t\tparams[i] = arg\n\treturn \"/\".join(params)\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_keys = url.split(\"/\")\n\turl_keys = [key for key in url_keys if key]\n\turl_keys.reverse()\n\n\tfor key, value in url_args.items():\n\t\tif key in url_keys:\n\t\t\turl = url.replace(key, str(value))\n\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\ttry:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(f'{{{key}}}', str(value))\n\t\treturn url\n\texcept Exception as e:\n\t\traise ValueError(f'Invalid argument: {e}')\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args.copy()\n\turl_args.pop('_', None)\n\turl_args.pop('page', None)\n\turl_args.pop('page_size', None)\n\tfor key, value in url_args.items():\n\t\turl = url.replace('{' + key + '}', value)\n\n\treturn url", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args.copy()\n\tfor key, value in url_args.items():\n\t\turl = url.replace('{' + key + '}', str(value), 1)\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\turl_args_values = url_args.values()\n\tfor key, value in url_args.items():\n\t\tif key in url:\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_parts = url.split(\"?\")\n\turl = url_parts[0]\n\turl_query_args = url_parts[1]\n\tfor key, value in url_args.items():\n\t\turl_query_args = url_query_args.replace(key, value)\n\n\treturn url + \"?\" + url_query_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_keys = url_args.keys()\n\tfor key in url_keys:\n\t\turl = url.replace(key, url_args[key])\n\treturn url", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args.copy()\n\turl = url.split('?')\n\turl_split = url[1].split('&')\n\tfor key, value in url_args.items():\n\t\tfor i, param in enumerate(url_split):\n\t\t\tif key in param:\n\t\t\t\turl_split[i] = f\"{key}={value}\"\n\turl = '?' + '&'.join(url_split)\n\treturn url if url_args else url[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl = url\n\turl_args = url_args\n\tif url_args:\n\t\turl_args_keys = url_args.keys()\n\t\turl_args_values = url_args.values()\n\t\turl = url + '?' + url_args_keys[0] + '=' + url_args_values[0]\n\t\tfor i in range(1, len(url_args_keys)):\n\t\t\turl = url + '&' + url_args_keys[i] + '=' + url_args_values[i]\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tparams = url.split(\"/\")\n\targs = url_args.split(\"/\")\n\tfor i, arg in enumerate(args):\n\t\tif i < len(args) - 1:\n\t\t\tparams[i] = arg\n\treturn \"/\".join(params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_keys = url.split(\"/\")\n\turl_keys = [key for key in url_keys if key]\n\turl_keys.reverse()\n\n\tfor key, value in url_args.items():\n\t\tif key in url_keys:\n\t\t\turl = url.replace(key, str(value))\n\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\ttry:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(f'{{{key}}}', str(value))\n\t\treturn url\n\texcept Exception as e:\n\t\traise ValueError(f'Invalid argument: {e}')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cinder/api/api_utils.py", "project": "openstack/cinder", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\t# Check if val is NoneType\n\t\tif val is None:\n\t\t\treturn True\n\t\t# Check if val is a string\n\t\tif type(val) is str:\n\t\t\tif val.lower() == 'none':\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\texcept Exception:\n\t\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\tif type(val) is str and val.lower() == 'none':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept TypeError:\n\t\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept:\n\t\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif type(val) != str:\n\t\treturn False\n\t\n\treturn val.lower() == 'none'", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept:\n\t\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\treturn type(val) == str and val.lower() == 'none'\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept AttributeError:\n\t\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif val is None:\n\t\treturn True\n\telif type(val) == str:\n\t\treturn val.lower() == 'none'\n\n\treturn False", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval = str(val)\n\treturn val.lower() == \"none\"\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval_lower = val.lower()\n\treturn type(val_lower) == str and val_lower == 'none'\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\t# Check if val is NoneType\n\t\tif val is None:\n\t\t\treturn True\n\t\t# Check if val is a string\n\t\tif type(val) is str:\n\t\t\tif val.lower() == 'none':\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\texcept Exception:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\tif type(val) is str and val.lower() == 'none':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept TypeError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif type(val) != str:\n\t\treturn False\n\t\n\treturn val.lower() == 'none'", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\treturn type(val) == str and val.lower() == 'none'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept AttributeError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif val is None:\n\t\treturn True\n\telif type(val) == str:\n\t\treturn val.lower() == 'none'\n\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval = str(val)\n\treturn val.lower() == \"none\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval_lower = val.lower()\n\treturn type(val_lower) == str and val_lower == 'none'\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\troot_split = root.split(\"/\")\n\tif root_split[-1] == \"\":\n\t\troot_split = root_split[:-1]\n\tpath_split = path.split(\"/\")\n\n\tif root_split != path_split[:len(root_split)]:\n\t\traise Exception(\"Path does not start with root\")\n\n\treturn \"/\".join(path_split[len(root_split):])\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tnew_path = path\n\tif root[-1] == '/':\n\t\tnew_path = path[len(root):]\n\telse:\n\t\tnew_path = path[len(root)+1:]\n\n\tif new_path == '':\n\t\traise Exception('Stripping the root from the path did not result in a valid path')\n\n\treturn new_path\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\troot = root.rstrip('/')\n\tpath = path.rstrip('/')\n\tif not path.startswith(root):\n\t\traise Exception('Unexpected path: ' + path)\n\tpath = path[len(root):]\n\tpath = path.lstrip('/')\n\treturn path\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath = path[len(root):]\n\tif path.startswith('/'):\n\t\tpath = path[1:]\n\n\treturn path\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath_length = len(path)\n\troot_length = len(root)\n\tif path_length >= root_length and path[:root_length] == root:\n\t\treturn path[root_length:]\n\telse:\n\t\traise Exception(\"Path: %s not under root: %s\" % (path, root))\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\tif path[:len(root)] != root:\n\t\traise Exception(\"path \" + path + \" is not under root \" + root)\n\n\treturn path[len(root):]\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\troot_len = len(root)\n\tif path[:root_len] != root:\n\t\traise Exception(\"Invalid root: {}\".format(root))\n\treturn path[root_len:]\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tpath = path[len(root):]\n\texcept:\n\t\traise Exception(\"Path not substring of root\")\n\treturn path\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tprefix_len = len(root)\n\tif path[:prefix_len] == root:\n\t\tpath = path[prefix_len:]\n\telse:\n\t\traise Exception(\"path not under root: {}\".format(path))\n\treturn path", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tstripped = path.replace(root + \"/\", \"\")\n\tif stripped == path:\n\t\traise Exception(\"Path does not contain root \" + root + \": \" + path)\n\treturn stripped\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\troot_split = root.split(\"/\")\n\tif root_split[-1] == \"\":\n\t\troot_split = root_split[:-1]\n\tpath_split = path.split(\"/\")\n\n\tif root_split != path_split[:len(root_split)]:\n\t\traise Exception(\"Path does not start with root\")\n\n\treturn \"/\".join(path_split[len(root_split):])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tnew_path = path\n\tif root[-1] == '/':\n\t\tnew_path = path[len(root):]\n\telse:\n\t\tnew_path = path[len(root)+1:]\n\n\tif new_path == '':\n\t\traise Exception('Stripping the root from the path did not result in a valid path')\n\n\treturn new_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\troot = root.rstrip('/')\n\tpath = path.rstrip('/')\n\tif not path.startswith(root):\n\t\traise Exception('Unexpected path: ' + path)\n\tpath = path[len(root):]\n\tpath = path.lstrip('/')\n\treturn path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath = path[len(root):]\n\tif path.startswith('/'):\n\t\tpath = path[1:]\n\n\treturn path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath_length = len(path)\n\troot_length = len(root)\n\tif path_length >= root_length and path[:root_length] == root:\n\t\treturn path[root_length:]\n\telse:\n\t\traise Exception(\"Path: %s not under root: %s\" % (path, root))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\tif path[:len(root)] != root:\n\t\traise Exception(\"path \" + path + \" is not under root \" + root)\n\n\treturn path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\troot_len = len(root)\n\tif path[:root_len] != root:\n\t\traise Exception(\"Invalid root: {}\".format(root))\n\treturn path[root_len:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tpath = path[len(root):]\n\texcept:\n\t\traise Exception(\"Path not substring of root\")\n\treturn path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tprefix_len = len(root)\n\tif path[:prefix_len] == root:\n\t\tpath = path[prefix_len:]\n\telse:\n\t\traise Exception(\"path not under root: {}\".format(path))\n\treturn path", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tstripped = path.replace(root + \"/\", \"\")\n\tif stripped == path:\n\t\traise Exception(\"Path does not contain root \" + root + \": \" + path)\n\treturn stripped\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = parser.format_help().split('optional arguments:')[-1]\n\treturn flags\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn ' '.join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_tuple = action.option_strings\n\t\tif len(flag_tuple) > 0:\n\t\t\tflag = flag_tuple[0]\n\t\t\tif flag.startswith(\"--\"):\n\t\t\t\tflags.append(flag)\n\t\t\telif flag.startswith(\"-\"):\n\t\t\t\tflags.append(flag)\n\treturn \" \".join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != 'help':\n\t\t\tflags.append('--' + action.dest)\n\t\t\tif action.type == bool:\n\t\t\t\tif not action.default:\n\t\t\t\t\tflags.append('False')\n\t\t\telif action.type != str:\n\t\t\t\tflags.append(str(action.default))\n\treturn ' '.join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = parser.format_usage().split()\n\tflags = [f for f in flags if f not in ['-h', '--help', '--verbose', '--version']]\n\tflags = ' '.join(flags)\n\treturn flags\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option in action.option_strings:\n\t\t\tflags.append(option)\n\treturn ' '.join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_strings = [\"-\" + action.dest.replace('_', '-')]\n\t\tif action.metavar is not None:\n\t\t\tflag_strings += [\"--\" + action.metavar.replace('_', '-')]\n\t\tflags += flag_strings\n\treturn ' '.join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor action in parser._actions:\n\t\tflag = action.option_strings[0]\n\t\tif action.metavar is not None:\n\t\t\tflag += \" \" + action.metavar\n\t\tflags += flag + \" \"\n\treturn flags.strip()\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor flag in action.option_strings:\n\t\t\tflags.append(flag)\n\treturn \" \".join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag = action.option_strings\n\t\tif flag is not None:\n\t\t\tflags.extend(flag)\n\treturn \" \".join(flags)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = parser.format_help().split('optional arguments:')[-1]\n\treturn flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option_string in action.option_strings:\n\t\t\tflags.append(option_string)\n\treturn ' '.join(flags)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_tuple = action.option_strings\n\t\tif len(flag_tuple) > 0:\n\t\t\tflag = flag_tuple[0]\n\t\t\tif flag.startswith(\"--\"):\n\t\t\t\tflags.append(flag)\n\t\t\telif flag.startswith(\"-\"):\n\t\t\t\tflags.append(flag)\n\treturn \" \".join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != 'help':\n\t\t\tflags.append('--' + action.dest)\n\t\t\tif action.type == bool:\n\t\t\t\tif not action.default:\n\t\t\t\t\tflags.append('False')\n\t\t\telif action.type != str:\n\t\t\t\tflags.append(str(action.default))\n\treturn ' '.join(flags)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = parser.format_usage().split()\n\tflags = [f for f in flags if f not in ['-h', '--help', '--verbose', '--version']]\n\tflags = ' '.join(flags)\n\treturn flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor option in action.option_strings:\n\t\t\tflags.append(option)\n\treturn ' '.join(flags)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag_strings = [\"-\" + action.dest.replace('_', '-')]\n\t\tif action.metavar is not None:\n\t\t\tflag_strings += [\"--\" + action.metavar.replace('_', '-')]\n\t\tflags += flag_strings\n\treturn ' '.join(flags)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor action in parser._actions:\n\t\tflag = action.option_strings[0]\n\t\tif action.metavar is not None:\n\t\t\tflag += \" \" + action.metavar\n\t\tflags += flag + \" \"\n\treturn flags.strip()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tfor flag in action.option_strings:\n\t\t\tflags.append(flag)\n\treturn \" \".join(flags)", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tflag = action.option_strings\n\t\tif flag is not None:\n\t\t\tflags.extend(flag)\n\treturn \" \".join(flags)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint \"Already processed:\", path_name\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint(\"Skipping {0} (already processed)\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.realpath(path_name)\n\tif path_name in processed:\n\t\tprint(\"Skipping: {}\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Duplicate entry in {}: {}\".format(processed.__class__.__name__, abs_path))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path_name = os.path.abspath(path_name)\n\tif abs_path_name in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed:', path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tprint(\"Error: {0} has already been processed. Exiting.\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint(2, 'ALREADY PROCESSED:', path_name)\n\t\treturn True\n\n\tprocessed.add(abs_path)\n\treturn False", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed: ' + path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path_name = os.path.abspath(path_name)\n\tif abs_path_name in processed:\n\t\tprint(\"Processing\", abs_path_name, \"previously processed\")\n\t\treturn True\n\tprocessed.add(abs_path_name)\n\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint('{} already processed'.format(abs_path), file=sys.stderr)\n\t\treturn True\n\tprocessed.add(abs_path)\n\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint \"Already processed:\", path_name\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint(\"Skipping {0} (already processed)\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.realpath(path_name)\n\tif path_name in processed:\n\t\tprint(\"Skipping: {}\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Duplicate entry in {}: {}\".format(processed.__class__.__name__, abs_path))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path_name = os.path.abspath(path_name)\n\tif abs_path_name in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed:', path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tprint(\"Error: {0} has already been processed. Exiting.\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint(2, 'ALREADY PROCESSED:', path_name)\n\t\treturn True\n\n\tprocessed.add(abs_path)\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed: ' + path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path_name = os.path.abspath(path_name)\n\tif abs_path_name in processed:\n\t\tprint(\"Processing\", abs_path_name, \"previously processed\")\n\t\treturn True\n\tprocessed.add(abs_path_name)\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint('{} already processed'.format(abs_path), file=sys.stderr)\n\t\treturn True\n\tprocessed.add(abs_path)\n\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "eppy/geometry/surface.py", "project": "santoshphilip/eppy", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv = []\n\tfor i in range(len(vertices)):\n\t\tv.append((vertices[i - 1], vertices[i], vertices[(i + 1) % len(vertices)]))\n\treturn v\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_3tuple_list = []\n\tfor i in vertices:\n\t\ta = (vertices[i], vertices[(i-1)%len(vertices)], vertices[(i+1)%len(vertices)])\n\t\tvertices_3tuple_list.append(a)\n\treturn vertices_3tuple_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\t\n\tfor i in range(0, len(vertices)-1):\n\t\tvertex_list.append((vertices[i], vertices[i+1], vertices[i+2]))\n\t\n\tvertex_list.append((vertices[len(vertices)-1], vertices[0], vertices[1]))\n\tvertex_list.append((vertices[len(vertices)-1], vertices[len(vertices)-2], vertices[0]))\n\t\n\treturn vertex_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\tvertex_len = len(vertices)\n\tfor i in range(vertex_len):\n\t\tif i == vertex_len - 1:\n\t\t\tvertex_list.append((vertices[i], vertices[0], vertices[1]))\n\t\telse:\n\t\t\tvertex_list.append((vertices[i], vertices[i+1], vertices[i+2]))\n\treturn vertex_list", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_list = []\n\tfor i in range(len(vertices)):\n\t\tvertices_list.append((vertices[i], vertices[(i+1) % len(vertices)], vertices[(i+2) % len(vertices)]))\n\treturn vertices_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\tvertex_list = []\n\tfor i in range(len(vertices)):\n\t\tvertex_list.append((vertices[i], vertices[i-1], vertices[i+1]))\n\treturn vertex_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex3tuplelist = []\n\tfor i in range(len(vertices)):\n\t\tv1 = vertices[i]\n\t\tv2 = vertices[i-1]\n\t\tv3 = vertices[i-2]\n\t\tvertex3tuplelist.append((v1,v2,v3))\n\treturn vertex3tuplelist\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv3tuple = []\n\tfor i in range(len(vertices)):\n\t\tprev_index = i - 1\n\t\tnext_index = i + 1\n\t\tif prev_index < 0:\n\t\t\tprev_index = len(vertices) - 1\n\t\tif next_index >= len(vertices):\n\t\t\tnext_index = 0\n\t\tv3tuple.append((vertices[prev_index], vertices[i], vertices[next_index]))\n\treturn v3tuple\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices = [vertices[i % len(vertices)] for i in range(len(vertices) + 1)]\n\treturn [(vertices[i], vertices[i + 1], vertices[i + 2]) for i in range(len(vertices) - 1)]\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertices_3tuple.append((vertices[i], vertices[(i+1)%len(vertices)], vertices[(i+2)%len(vertices)]))\n\treturn vertices_3tuple\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv = []\n\tfor i in range(len(vertices)):\n\t\tv.append((vertices[i - 1], vertices[i], vertices[(i + 1) % len(vertices)]))\n\treturn v\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_3tuple_list = []\n\tfor i in vertices:\n\t\ta = (vertices[i], vertices[(i-1)%len(vertices)], vertices[(i+1)%len(vertices)])\n\t\tvertices_3tuple_list.append(a)\n\treturn vertices_3tuple_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\t\n\tfor i in range(0, len(vertices)-1):\n\t\tvertex_list.append((vertices[i], vertices[i+1], vertices[i+2]))\n\t\n\tvertex_list.append((vertices[len(vertices)-1], vertices[0], vertices[1]))\n\tvertex_list.append((vertices[len(vertices)-1], vertices[len(vertices)-2], vertices[0]))\n\t\n\treturn vertex_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\tvertex_len = len(vertices)\n\tfor i in range(vertex_len):\n\t\tif i == vertex_len - 1:\n\t\t\tvertex_list.append((vertices[i], vertices[0], vertices[1]))\n\t\telse:\n\t\t\tvertex_list.append((vertices[i], vertices[i+1], vertices[i+2]))\n\treturn vertex_list", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_list = []\n\tfor i in range(len(vertices)):\n\t\tvertices_list.append((vertices[i], vertices[(i+1) % len(vertices)], vertices[(i+2) % len(vertices)]))\n\treturn vertices_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\tvertex_list = []\n\tfor i in range(len(vertices)):\n\t\tvertex_list.append((vertices[i], vertices[i-1], vertices[i+1]))\n\treturn vertex_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex3tuplelist = []\n\tfor i in range(len(vertices)):\n\t\tv1 = vertices[i]\n\t\tv2 = vertices[i-1]\n\t\tv3 = vertices[i-2]\n\t\tvertex3tuplelist.append((v1,v2,v3))\n\treturn vertex3tuplelist\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv3tuple = []\n\tfor i in range(len(vertices)):\n\t\tprev_index = i - 1\n\t\tnext_index = i + 1\n\t\tif prev_index < 0:\n\t\t\tprev_index = len(vertices) - 1\n\t\tif next_index >= len(vertices):\n\t\t\tnext_index = 0\n\t\tv3tuple.append((vertices[prev_index], vertices[i], vertices[next_index]))\n\treturn v3tuple\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices = [vertices[i % len(vertices)] for i in range(len(vertices) + 1)]\n\treturn [(vertices[i], vertices[i + 1], vertices[i + 2]) for i in range(len(vertices) - 1)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertices_3tuple.append((vertices[i], vertices[(i+1)%len(vertices)], vertices[(i+2)%len(vertices)]))\n\treturn vertices_3tuple\n", "is_pass": true, "return_code": 0}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\tstring = \"\"\n\twhile number:\n\t\tremainder = number % base\n\t\tnumber = number // base\n\t\tstring = alphabet[int(remainder)] + string\n\treturn string.zfill(padding)", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\tout = []\n\n\twhile number:\n\t\tnumber, rem = divmod(number, base)\n\t\tout.append(alphabet[rem])\n\n\tif padding:\n\t\tout += padding * alphabet[0]\n\n\treturn ''.join(reversed(out))", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = \"\"\n\twhile True:\n\t\tremainder = number % len(alphabet)\n\t\tnumber //= len(alphabet)\n\t\tresult = alphabet[remainder] + result\n\t\tif number == 0:\n\t\t\tbreak\n\n\tif padding is not None:\n\t\tresult = result.rjust(padding, alphabet[0])\n\n\treturn result\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = []\n\twhile number > 0:\n\t\tresult.append(alphabet[number % len(alphabet)])\n\t\tnumber //= len(alphabet)\n\tif padding is not None:\n\t\tresult.append(alphabet[0] * (padding - len(result)))\n\treturn ''.join(result)[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = ''\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\tif padding is not None:\n\t\tresult = result.zfill(padding)\n\treturn result[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n    if number == 0:\n        return alphabet[0]\n\n    result = \"\"\n    while number > 0:\n        digit = number % len(alphabet)\n        result = alphabet[digit] + result\n        number //= len(alphabet)\n\n    if padding is not None:\n        result = result.ljust(padding, alphabet[0])\n\n    return result\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\toutput = \"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\twhile number > 0:\n\t\toutput += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn output[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\t# convert the number to a string\n\tstring = ''\n\twhile number > 0:\n\t\tstring = alphabet[number % base] + string\n\t\tnumber //= base\n\n\t# add padding\n\tif padding is not None:\n\t\tstring = string.zfill(padding)\n\n\treturn string\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\talphabet_size = len(alphabet)\n\tif padding is None or alphabet_size == 0:\n\t\treturn \"\"\n\n\telif number == 0:\n\t\treturn alphabet[0]\n\n\tresult = []\n\twhile number != 0:\n\t\tresult.append(alphabet[number % alphabet_size])\n\t\tnumber //= alphabet_size\n\n\treturn \"\".join(reversed(result))\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\tstring = ''\n\n\tif padding:\n\t\talphabet_len = len(alphabet)\n\t\tstring = alphabet[0] * padding\n\t\twhile number:\n\t\t\tstring = alphabet[number % alphabet_len] + string\n\t\t\tnumber = number // alphabet_len\n\telif number:\n\t\twhile number:\n\t\t\tstring = alphabet[number % base] + string\n\t\t\tnumber = number // base\n\telif number == 0:\n\t\tstring = alphabet[number]\n\n\treturn string\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\tstring = \"\"\n\twhile number:\n\t\tremainder = number % base\n\t\tnumber = number // base\n\t\tstring = alphabet[int(remainder)] + string\n\treturn string.zfill(padding)", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\tout = []\n\n\twhile number:\n\t\tnumber, rem = divmod(number, base)\n\t\tout.append(alphabet[rem])\n\n\tif padding:\n\t\tout += padding * alphabet[0]\n\n\treturn ''.join(reversed(out))", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = \"\"\n\twhile True:\n\t\tremainder = number % len(alphabet)\n\t\tnumber //= len(alphabet)\n\t\tresult = alphabet[remainder] + result\n\t\tif number == 0:\n\t\t\tbreak\n\n\tif padding is not None:\n\t\tresult = result.rjust(padding, alphabet[0])\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = []\n\twhile number > 0:\n\t\tresult.append(alphabet[number % len(alphabet)])\n\t\tnumber //= len(alphabet)\n\tif padding is not None:\n\t\tresult.append(alphabet[0] * (padding - len(result)))\n\treturn ''.join(result)[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = ''\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\tif padding is not None:\n\t\tresult = result.zfill(padding)\n\treturn result[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n    if number == 0:\n        return alphabet[0]\n\n    result = \"\"\n    while number > 0:\n        digit = number % len(alphabet)\n        result = alphabet[digit] + result\n        number //= len(alphabet)\n\n    if padding is not None:\n        result = result.ljust(padding, alphabet[0])\n\n    return result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\toutput = \"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\twhile number > 0:\n\t\toutput += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn output[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\t# convert the number to a string\n\tstring = ''\n\twhile number > 0:\n\t\tstring = alphabet[number % base] + string\n\t\tnumber //= base\n\n\t# add padding\n\tif padding is not None:\n\t\tstring = string.zfill(padding)\n\n\treturn string\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\talphabet_size = len(alphabet)\n\tif padding is None or alphabet_size == 0:\n\t\treturn \"\"\n\n\telif number == 0:\n\t\treturn alphabet[0]\n\n\tresult = []\n\twhile number != 0:\n\t\tresult.append(alphabet[number % alphabet_size])\n\t\tnumber //= alphabet_size\n\n\treturn \"\".join(reversed(result))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tbase = len(alphabet)\n\tstring = ''\n\n\tif padding:\n\t\talphabet_len = len(alphabet)\n\t\tstring = alphabet[0] * padding\n\t\twhile number:\n\t\t\tstring = alphabet[number % alphabet_len] + string\n\t\t\tnumber = number // alphabet_len\n\telif number:\n\t\twhile number:\n\t\t\tstring = alphabet[number % base] + string\n\t\t\tnumber = number // base\n\telif number == 0:\n\t\tstring = alphabet[number]\n\n\treturn string\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neutron_lib/agent/common/utils.py", "project": "openstack/neutron-lib", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tregister_value_to_store = flow_params[register_value]\n\tdel flow_params[register_value]\n\tflow_params[register_number] = register_value_to_store\n\treturn flow_params\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tregister_value = flow_params.get(register_value, register_value)\n\tflow_params[register_number] = register_value\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\t", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tregister_value_to_store = flow_params[register_value]\n\tdel flow_params[register_value]\n\tflow_params[register_number] = register_value_to_store\n\treturn flow_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tregister_value = flow_params.get(register_value, register_value)\n\tflow_params[register_number] = register_value\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "release_dashboard/templatetags/rd_extras.py", "project": "sipwise/repoapi", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tprint(value)\n\tif type(value) == str:\n\t\tprint(value.replace('.', arg))\n\t\treturn value.replace('.', arg)\n\telse:\n\t\treturn None\n\nreplace_dots('one.two.three','#')", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tresult = value.replace('.', arg)\n\treturn result", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = value.replace(\".\", arg)\n\treturn value\n\n\nprint(replace_dots(\"123.123.123.123\", \"-\"))", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tres = value.replace('.', arg)\n\treturn res", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = str(value)\n\tvalue = value.replace(\".\", arg)\n\treturn value\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tnew_value = value.replace('.', arg)\n\treturn new_value\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tresult = ''\n\tfor character in value:\n\t\tif character == '.':\n\t\t\tresult += arg\n\t\telse:\n\t\t\tresult += character\n\treturn result\n\nprint(replace_dots(\"123.456\", \"-\")) # 123-456\nprint(replace_dots(\"That is a bread\", \"@\")) # That is a bread\nprint(replace_dots(\"1.2.3.4.5\", \"!\")) # 1!!2!!3!!4!!5", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = arg.join(value.split('.'))\n\treturn value\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\ttry:\n\t\treturn value.replace('.', arg)\n\texcept:\n\t\treturn False", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tnew_string = value.replace('.', arg)\n\n\treturn new_string\n\nprint(replace_dots(\"123.456\", \"-\"))\nprint(replace_dots(\"that's is.and.example\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \""], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tprint(value)\n\tif type(value) == str:\n\t\tprint(value.replace('.', arg))\n\t\treturn value.replace('.', arg)\n\telse:\n\t\treturn None\n\nreplace_dots('one.two.three','#')", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tresult = value.replace('.', arg)\n\treturn result", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = value.replace(\".\", arg)\n\treturn value\n\n\nprint(replace_dots(\"123.123.123.123\", \"-\"))", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tres = value.replace('.', arg)\n\treturn res", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = str(value)\n\tvalue = value.replace(\".\", arg)\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tnew_value = value.replace('.', arg)\n\treturn new_value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tresult = ''\n\tfor character in value:\n\t\tif character == '.':\n\t\t\tresult += arg\n\t\telse:\n\t\t\tresult += character\n\treturn result\n\nprint(replace_dots(\"123.456\", \"-\")) # 123-456\nprint(replace_dots(\"That is a bread\", \"@\")) # That is a bread\nprint(replace_dots(\"1.2.3.4.5\", \"!\")) # 1!!2!!3!!4!!5", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = arg.join(value.split('.'))\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\ttry:\n\t\treturn value.replace('.', arg)\n\texcept:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tnew_string = value.replace('.', arg)\n\n\treturn new_string\n\nprint(replace_dots(\"123.456\", \"-\"))\nprint(replace_dots(\"that's is.and.example\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"\"))\nprint(replace_dots(\"123.456\", \" \"))\nprint(replace_dots(\"123.456\", \",\"))\nprint(replace_dots(\"123.456\", \"", "is_pass": false, "return_code": 1}]}
{"file_path": "rows/utils/__init__.py", "project": "turicas/rows", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = set(cls.__subclasses__())\n\tfor sub in cls.__subclasses__():\n\t\tsubs.update(subclasses(sub))\n\treturn subs\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tl = []\n\tfor a in cls.__subclasses__():\n\t\tl.append(a)\n\t\tl.extend(subclasses(a))\n\treturn l", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\tsubs += [g for s in cls.__subclasses__() for g in subclasses(s)]\n\treturn subs\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = set(cls.__subclasses__())\n\tfor sub in list(subs):\n\t\tsubs |= subclasses(sub)\n\treturn subs\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = [cls]\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = set(cls.__subclasses__())\n\tfor sub in cls.__subclasses__():\n\t\tsubs.update(subclasses(sub))\n\treturn subs\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tl = []\n\tfor a in cls.__subclasses__():\n\t\tl.append(a)\n\t\tl.extend(subclasses(a))\n\treturn l", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\tsubs += [g for s in cls.__subclasses__() for g in subclasses(s)]\n\treturn subs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = set(cls.__subclasses__())\n\tfor sub in list(subs):\n\t\tsubs |= subclasses(sub)\n\treturn subs\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = [cls]\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tpass\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tindex = 0\n\tfor i, char in enumerate(string):\n\t\tindex += alphabet.index(char) * len(alphabet) ** (len(string) - i - 1)\n\t\n\treturn index\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor i, c in enumerate(string):\n\t\tnumber += (len(alphabet) ** (len(string) - i - 1)) * alphabet.index(c)\n\treturn number\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor char in string:\n\t\tresult = result * len(alphabet) + alphabet.index(char)\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\talphabet_dict = {}\n\tfor i in range(len(alphabet)):\n\t\talphabet_dict[alphabet[i]] = i\n\tfor char in string:\n\t\tnumber = number * len(alphabet) + alphabet_dict[char]\n\n\treturn number\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i in range(len(string)):\n\t\tnum += alphabet.index(string[i]) * len(alphabet) ** (len(string) - i - 1)\n\treturn num\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\talphabet_dict = {letter: i for i, letter in enumerate(alphabet)}\n\tfor char in string:\n\t\tresult *= len(alphabet)\n\t\tresult += alphabet_dict[char]\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(alphabet)\n\tres = 0\n\tfor c in string:\n\t\tres = n * res + alphabet.index(c)\n\treturn res\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tres = 0\n\tfor c in string:\n\t\tres = res * len(alphabet) + alphabet.index(c)\n\treturn res\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(alphabet)\n\ts = 0\n\tfor i in range(len(string)):\n\t\ts += alphabet.index(string[i]) * n ** (len(string) - i - 1)\n\treturn s\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tindex = 0\n\tfor i, char in enumerate(string):\n\t\tindex += alphabet.index(char) * len(alphabet) ** (len(string) - i - 1)\n\t\n\treturn index\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor i, c in enumerate(string):\n\t\tnumber += (len(alphabet) ** (len(string) - i - 1)) * alphabet.index(c)\n\treturn number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor char in string:\n\t\tresult = result * len(alphabet) + alphabet.index(char)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\talphabet_dict = {}\n\tfor i in range(len(alphabet)):\n\t\talphabet_dict[alphabet[i]] = i\n\tfor char in string:\n\t\tnumber = number * len(alphabet) + alphabet_dict[char]\n\n\treturn number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i in range(len(string)):\n\t\tnum += alphabet.index(string[i]) * len(alphabet) ** (len(string) - i - 1)\n\treturn num\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\talphabet_dict = {letter: i for i, letter in enumerate(alphabet)}\n\tfor char in string:\n\t\tresult *= len(alphabet)\n\t\tresult += alphabet_dict[char]\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(alphabet)\n\tres = 0\n\tfor c in string:\n\t\tres = n * res + alphabet.index(c)\n\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tres = 0\n\tfor c in string:\n\t\tres = res * len(alphabet) + alphabet.index(c)\n\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(alphabet)\n\ts = 0\n\tfor i in range(len(string)):\n\t\ts += alphabet.index(string[i]) * n ** (len(string) - i - 1)\n\treturn s\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / url.split('/')[-1]\n\n\t# Download the archive\n\tdownload_file(url, archive_path)\n\n\t# Extract the archive\n\textract_tar_gz(archive_path, destination_path)\n\n\treturn archive_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / os.path.basename(url)\n\twith urlopen(url) as response, tarfile.open(fileobj=response, mode=\"r:gz\") as tar:\n\t\ttar.extractall(path=destination_path)\n\treturn archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_url = urljoin(url, \"desc\")\n\tarchive_path = destination_path / \"archive\"\n\tarchive_path.mkdir(parents=True, exist_ok=True)\n\tarchive_path = archive_path / \"archive.tar.gz\"\n\n\tr = requests.get(archive_url, stream=True)\n\n\tif r.status_code != 200:\n\t\traise Exception(f\"Unable to retrieve archive from {archive_url}, status code: {r.status_code}\")\n\n\twith open(archive_path, \"wb\") as f:\n\t\tf.write(r.raw.read())\n\n\tarchive = tarfile.open(archive_path, \"r:gz\")\n\tarchive.extractall(path=destination_path)\n\n\treturn archive_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / url.split('/')[-1]\n\tdownload(url, archive_path)\n\textract(archive_path, destination_path)\n\treturn archive_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n\tlogger.info(f\"Downloading repository archive {url}\")\n\twith TemporaryDirectory() as tmp_dir:\n\t\tarchive_path = Path(tmp_dir) / \"archive.tar.gz\"\n\t\tdownload_file(url, archive_path)\n\n\t\tlogger.info(f\"Extracting archive to {destination_path}\")\n\t\textract_tar_gz(archive_path, destination_path)\n\n\t\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\ttry:\n\t\tarchive = requests.get(url, stream=True)\n\t\tarchive.raise_for_status()\n\texcept requests.exceptions.HTTPError as errh:\n\t\tprint(errh)\n\texcept requests.exceptions.ConnectionError as errc:\n\t\tprint(errc)\n\texcept requests.exceptions.Timeout as errt:\n\t\tprint(errt)\n\texcept requests.exceptions.RequestException as err:\n\t\tprint(err)\n\n\tarchive_path = destination_path / 'archive.tar.gz'\n\n\twith open(archive_path, 'wb') as fd:\n\t\tfor chunk in archive.iter_content(chunk_size=128):\n\t\t\tfd.write(chunk)\n\n\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=destination_path)\n\t\ttar.close()\n\n\tos.remove(archive_path)\n\n\treturn destination_path", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tpass\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_file_name = url.split(\"/\")[-1]\n\tresponse = requests.get(url, stream=True)\n\tresponse.raise_for_status()\n\n\tarchive_path = destination_path / archive_file_name\n\twith open(archive_path, \"wb\") as out_file:\n\t\tshutil.copyfileobj(response.raw, out_file)\n\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=destination_path)\n\n\textracted_archive_path = destination_path / archive_file_name.replace(\".tar.gz\", \"\")\n\tos.remove(archive_path)\n\treturn extracted_archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / Path(url).name\n\tarchive_path.parent.mkdir(exist_ok=True)\n\tif not archive_path.exists():\n\t\tprint(f\"Downloading and extracting {url} to {archive_path}\")\n\t\tr = requests.get(url)\n\t\twith open(archive_path, 'wb') as f:\n\t\t\tf.write(r.content)\n\twith tarfile.open(archive_path, 'r:gz') as f:\n\t\tf.extractall(path=destination_path)\n\treturn archive_path", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tr = requests.get(url, allow_redirects=True)\n\tarchive_path = destination_path / f\"{url.split('/')[-1]}\"\n\topen(archive_path, \"wb\").write(r.content)\n\ttar = tarfile.open(archive_path)\n\ttar.extractall(destination_path)\n\ttar.close()\n\treturn destination_path\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / url.split('/')[-1]\n\n\t# Download the archive\n\tdownload_file(url, archive_path)\n\n\t# Extract the archive\n\textract_tar_gz(archive_path, destination_path)\n\n\treturn archive_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / os.path.basename(url)\n\twith urlopen(url) as response, tarfile.open(fileobj=response, mode=\"r:gz\") as tar:\n\t\ttar.extractall(path=destination_path)\n\treturn archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_url = urljoin(url, \"desc\")\n\tarchive_path = destination_path / \"archive\"\n\tarchive_path.mkdir(parents=True, exist_ok=True)\n\tarchive_path = archive_path / \"archive.tar.gz\"\n\n\tr = requests.get(archive_url, stream=True)\n\n\tif r.status_code != 200:\n\t\traise Exception(f\"Unable to retrieve archive from {archive_url}, status code: {r.status_code}\")\n\n\twith open(archive_path, \"wb\") as f:\n\t\tf.write(r.raw.read())\n\n\tarchive = tarfile.open(archive_path, \"r:gz\")\n\tarchive.extractall(path=destination_path)\n\n\treturn archive_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / url.split('/')[-1]\n\tdownload(url, archive_path)\n\textract(archive_path, destination_path)\n\treturn archive_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n\tlogger.info(f\"Downloading repository archive {url}\")\n\twith TemporaryDirectory() as tmp_dir:\n\t\tarchive_path = Path(tmp_dir) / \"archive.tar.gz\"\n\t\tdownload_file(url, archive_path)\n\n\t\tlogger.info(f\"Extracting archive to {destination_path}\")\n\t\textract_tar_gz(archive_path, destination_path)\n\n\t\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\ttry:\n\t\tarchive = requests.get(url, stream=True)\n\t\tarchive.raise_for_status()\n\texcept requests.exceptions.HTTPError as errh:\n\t\tprint(errh)\n\texcept requests.exceptions.ConnectionError as errc:\n\t\tprint(errc)\n\texcept requests.exceptions.Timeout as errt:\n\t\tprint(errt)\n\texcept requests.exceptions.RequestException as err:\n\t\tprint(err)\n\n\tarchive_path = destination_path / 'archive.tar.gz'\n\n\twith open(archive_path, 'wb') as fd:\n\t\tfor chunk in archive.iter_content(chunk_size=128):\n\t\t\tfd.write(chunk)\n\n\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=destination_path)\n\t\ttar.close()\n\n\tos.remove(archive_path)\n\n\treturn destination_path", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_file_name = url.split(\"/\")[-1]\n\tresponse = requests.get(url, stream=True)\n\tresponse.raise_for_status()\n\n\tarchive_path = destination_path / archive_file_name\n\twith open(archive_path, \"wb\") as out_file:\n\t\tshutil.copyfileobj(response.raw, out_file)\n\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=destination_path)\n\n\textracted_archive_path = destination_path / archive_file_name.replace(\".tar.gz\", \"\")\n\tos.remove(archive_path)\n\treturn extracted_archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / Path(url).name\n\tarchive_path.parent.mkdir(exist_ok=True)\n\tif not archive_path.exists():\n\t\tprint(f\"Downloading and extracting {url} to {archive_path}\")\n\t\tr = requests.get(url)\n\t\twith open(archive_path, 'wb') as f:\n\t\t\tf.write(r.content)\n\twith tarfile.open(archive_path, 'r:gz') as f:\n\t\tf.extractall(path=destination_path)\n\treturn archive_path", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tr = requests.get(url, allow_redirects=True)\n\tarchive_path = destination_path / f\"{url.split('/')[-1]}\"\n\topen(archive_path, \"wb\").write(r.content)\n\ttar = tarfile.open(archive_path)\n\ttar.extractall(destination_path)\n\ttar.close()\n\treturn destination_path\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/systeminfo.py", "project": "cloudmesh/cloudmesh-common", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\n\tif platform.system() == 'Darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\tfrom sys import platform\n\treturn platform == \"darwin\"\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Darwin\"", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\n\tif platform.system() == 'Darwin':\n\t\treturn True\n\n\treturn False\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\n\treturn platform.system() == \"Darwin\"", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Darwin\"", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\n\tif platform.system() == 'Darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\tfrom sys import platform\n\treturn platform == \"darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Darwin\"", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\n\tif platform.system() == 'Darwin':\n\t\treturn True\n\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\n\treturn platform.system() == \"Darwin\"", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Darwin\"", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item:\n\t\tregex_dict[convert_to_regex(key)] = item[key]\n\treturn regex_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret_dict = {}\n\tfor key, value in item.items():\n\t\tkey = key.replace('.', '\\\\.')\n\t\tkey = key.replace('*', '.*')\n\t\tkey = key.replace('?', '.')\n\t\tret_dict[key] = value\n\treturn ret_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tnew_item = {}\n\tfor k,v in item.items():\n\t\tk = k.replace(\".\", \"\\\\.\")\n\t\tk = k.replace(\"*\", \".*\")\n\t\tk = k.replace(\"?\", \".?\")\n\t\tk = \"^\" + k + \"$\"\n\t\tnew_item[k] = v\n\treturn new_item", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tregex = re.compile(r'\\*|\\?')\n\titem2 = {}\n\tfor key in item.keys():\n\t\titem2[regex.sub(r'[^/]+', key)] = item[key]\n\treturn item2", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\timport fnmatch\n\n\tregex = {}\n\tfor key in item:\n\t\tkey = fnmatch.translate(key)\n\t\tkey = re.sub(r\"\\.\\*\\?\", \".*\", key)\n\t\tkey = re.sub(r\"\\.\\*\", \".*?\", key)\n\t\tkey = re.sub(r\"\\?\", \".?\", key)\n\t\tkey = re.sub(r\"\\\\\\?\", \".\", key)\n\t\tkey = re.sub(r\"\\\\\\*\", \".*\", key)\n\t\tkey = re.sub(r\"\\\\\\+\", \".\", key)\n\t\tregex[key] = item[key]\n\treturn regex\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item.keys():\n\t\tkey = key.replace(\"*\", \".*\")\n\t\tkey = key.replace(\".\", \"\\.\")\n\t\tkey = key.replace(\"-\", \"\\-\")\n\t\tkey = key.replace(\"+\", \"\\+\")\n\t\tkey = key.replace(\"(\", \"\\(\")\n\t\tkey = key.replace(\")\", \"\\)\")\n\t\tkey = key.replace(\"[\", \"\\[\")\n\t\tkey = key.replace(\"]\", \"\\]\")\n\t\tkey = key.replace(\"{\", \"\\{\")\n\t\tkey = key.replace(\"}\", \"\\}\")\n\t\tkey = key.replace(\"|\", \"\\|\")\n\t\tkey = key.replace(\"^\", \"\\^\")\n\t\tkey = key.replace(\"$\", \"\\$\")\n\t\tkey = key.replace(\"?\", \"\\?\")\n\t\tkey = key.replace(\"\\\\\\\\\", \"\\\\\\\\\")\n\t\tkey = key.replace(\"\\\\\", \"\\\\\\\\\")\n\t\tregex_dict[key] = item[key]\n\treturn regex_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tnew_item = {}\n\tfor key in item:\n\t\tnew_item[re.compile(key)] = item[key]\n\treturn new_item", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\ttry:\n\t\timport re\n\texcept ImportError:\n\t\tprint('Please install Python regex library:')\n\t\tprint('    pip install regex')\n\t\traise\n\tif not isinstance(item, dict):\n\t\traise TypeError('regex_dict() only accepts dicts')\n\tnew_dict = {}\n\tfor key, val in item.items():\n\t\tnew_dict[re.compile(key)] = val\n\treturn new_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\timport itertools\n\n\tregex_dict = dict()\n\tfor item_key, item_value in item.items():\n\t\tregex_dict[re.compile(item_key)] = item_value\n\treturn regex_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\td = {}\n\tfor k,v in item.items():\n\t\tk = re.sub(r'[*]', '[^/]*', k)\n\t\tk = re.sub(r'[\\.]', r'\\.', k)\n\t\tk = re.sub(r'[\\?]', '.', k)\n\t\tk = re.sub(r'[\\-]', '', k)\n\t\td[k] = v\n\treturn d\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item:\n\t\tregex_dict[convert_to_regex(key)] = item[key]\n\treturn regex_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret_dict = {}\n\tfor key, value in item.items():\n\t\tkey = key.replace('.', '\\\\.')\n\t\tkey = key.replace('*', '.*')\n\t\tkey = key.replace('?', '.')\n\t\tret_dict[key] = value\n\treturn ret_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tnew_item = {}\n\tfor k,v in item.items():\n\t\tk = k.replace(\".\", \"\\\\.\")\n\t\tk = k.replace(\"*\", \".*\")\n\t\tk = k.replace(\"?\", \".?\")\n\t\tk = \"^\" + k + \"$\"\n\t\tnew_item[k] = v\n\treturn new_item", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tregex = re.compile(r'\\*|\\?')\n\titem2 = {}\n\tfor key in item.keys():\n\t\titem2[regex.sub(r'[^/]+', key)] = item[key]\n\treturn item2", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\timport fnmatch\n\n\tregex = {}\n\tfor key in item:\n\t\tkey = fnmatch.translate(key)\n\t\tkey = re.sub(r\"\\.\\*\\?\", \".*\", key)\n\t\tkey = re.sub(r\"\\.\\*\", \".*?\", key)\n\t\tkey = re.sub(r\"\\?\", \".?\", key)\n\t\tkey = re.sub(r\"\\\\\\?\", \".\", key)\n\t\tkey = re.sub(r\"\\\\\\*\", \".*\", key)\n\t\tkey = re.sub(r\"\\\\\\+\", \".\", key)\n\t\tregex[key] = item[key]\n\treturn regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item.keys():\n\t\tkey = key.replace(\"*\", \".*\")\n\t\tkey = key.replace(\".\", \"\\.\")\n\t\tkey = key.replace(\"-\", \"\\-\")\n\t\tkey = key.replace(\"+\", \"\\+\")\n\t\tkey = key.replace(\"(\", \"\\(\")\n\t\tkey = key.replace(\")\", \"\\)\")\n\t\tkey = key.replace(\"[\", \"\\[\")\n\t\tkey = key.replace(\"]\", \"\\]\")\n\t\tkey = key.replace(\"{\", \"\\{\")\n\t\tkey = key.replace(\"}\", \"\\}\")\n\t\tkey = key.replace(\"|\", \"\\|\")\n\t\tkey = key.replace(\"^\", \"\\^\")\n\t\tkey = key.replace(\"$\", \"\\$\")\n\t\tkey = key.replace(\"?\", \"\\?\")\n\t\tkey = key.replace(\"\\\\\\\\\", \"\\\\\\\\\")\n\t\tkey = key.replace(\"\\\\\", \"\\\\\\\\\")\n\t\tregex_dict[key] = item[key]\n\treturn regex_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tnew_item = {}\n\tfor key in item:\n\t\tnew_item[re.compile(key)] = item[key]\n\treturn new_item", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\ttry:\n\t\timport re\n\texcept ImportError:\n\t\tprint('Please install Python regex library:')\n\t\tprint('    pip install regex')\n\t\traise\n\tif not isinstance(item, dict):\n\t\traise TypeError('regex_dict() only accepts dicts')\n\tnew_dict = {}\n\tfor key, val in item.items():\n\t\tnew_dict[re.compile(key)] = val\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\timport itertools\n\n\tregex_dict = dict()\n\tfor item_key, item_value in item.items():\n\t\tregex_dict[re.compile(item_key)] = item_value\n\treturn regex_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\td = {}\n\tfor k,v in item.items():\n\t\tk = re.sub(r'[*]', '[^/]*', k)\n\t\tk = re.sub(r'[\\.]', r'\\.', k)\n\t\tk = re.sub(r'[\\?]', '.', k)\n\t\tk = re.sub(r'[\\-]', '', k)\n\t\td[k] = v\n\treturn d\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rdiffweb/core/librdiff.py", "project": "ikus060/rdiffweb", "name": "unquote", "docstring": "Remove quote from the given name.", "_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = re.sub(r'[\\\"\\']', \"\", name)\n\treturn name\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tpattern = re.compile(r\"\\'|\\\"\")\n\treturn re.sub(pattern, '', name)\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\n\tname = re.sub(r\"\\`\", \"\", name)\n\n\treturn name\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\\'', '', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = re.sub(r\"'\", \"\", name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'[\\\"\\'`]', '', name)", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\\'|\"', \"\", name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = name.strip()\n\tname = re.sub('^\"', '', name)\n\tname = re.sub('\"$', '', name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tif name==None:\n\t\treturn None\n\tname = re.sub(\"^'\", \"\", name)\n\tname = re.sub(\"'$\", \"\", name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tresult = re.sub(r'(?<=\\w)[\\'\\\"]', '', name)\n\treturn result"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = re.sub(r'[\\\"\\']', \"\", name)\n\treturn name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tpattern = re.compile(r\"\\'|\\\"\")\n\treturn re.sub(pattern, '', name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\n\tname = re.sub(r\"\\`\", \"\", name)\n\n\treturn name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\\'', '', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = re.sub(r\"'\", \"\", name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'[\\\"\\'`]', '', name)", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\\'|\"', \"\", name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = name.strip()\n\tname = re.sub('^\"', '', name)\n\tname = re.sub('\"$', '', name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tif name==None:\n\t\treturn None\n\tname = re.sub(\"^'\", \"\", name)\n\tname = re.sub(\"'$\", \"\", name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tresult = re.sub(r'(?<=\\w)[\\'\\\"]', '', name)\n\treturn result", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/shlex.py", "project": "cloudmesh/cloudmesh-common", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tplatform_map = {\n\t\t'this': 'auto',\n\t\t1: 'posix',\n\t\t0: 'win32'\n\t}\n\tplatform = platform_map.get(platform, platform)\n\timport shlex\n\treturn shlex.split(s, posix=platform == 'posix')\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\ttry:\n\t\tif platform == 'this':\n\t\t\tplatform = get_platform()\n\t\tif platform in [1, 'posix', 'posix-like', 'linux', 'cygwin', 'darwin', 'freebsd', 'openbsd', 'netbsd', 'aix']:\n\t\t\treturn s.split('\\r\\n') if platform in ['cygwin', 'darwin', 'freebsd', 'openbsd', 'netbsd', 'aix'] else s.split('\\n')\n\t\telif platform in [0, 'nt', 'win32', 'cygwin', 'msys']:\n\t\t\treturn s.split('\\n')\n\texcept Exception as e:\n\t\treturn [s]\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif platform == 'this':\n\t\tplatform = _get_platform()\n\t\n\tif platform == 1:\n\t\treturn _split_posix(s)\n\telif platform == 0:\n\t\treturn _split_windows(s)\n\telse:\n\t\traise ValueError('platform should be 1 or 0')\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif (platform == 'this'):\n\t\tplatform = os.name\n\tif platform == 'nt':\n\t\tplatform = 0\n\telif platform == 'posix':\n\t\tplatform = 1\n\tif platform == 1:\n\t\treturn s.split(' ')\n\telif platform == 0:\n\t\treturn s.split(';')", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tfrom os import name\n\t\n\tif platform == 'this':\n\t\tif name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\tif platform == 0:\n\t\treturn s.split(';')\n\telif platform == 1:\n\t\treturn s.split(':')\n\telse:\n\t\treturn 'error: invalid platform %d' % platform\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 'posix':\n\t\treturn s.split('\\n')\n\telif platform == 'nt': # windows\n\t\treturn s.split('\\r\\n')\n\telse:\n\t\traise ValueError('Invalid platform: {}'.format(platform))", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform as p\n\tif platform == 'this':\n\t\tif p.system() == 'Windows':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 1:\n\t\treturn s.split(' ')\n\telse:\n\t\treturn s.split(' ')\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tsplit_char = ' '\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 1:\n\t\tsplit_char = ' '\n\telif platform == 0:\n\t\tsplit_char = ';'\n\treturn s.split(split_char)", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif platform == 'this':\n\t\timport platform\n\t\tif platform.system() == 'Windows':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\ts = s.replace('\\r', '')\n\tif platform == 1:\n\t\treturn s.split('\\n')\n\telse:\n\t\tlines = []\n\t\ttmp = ''\n\t\tnewlines = s.split('\\n')\n\t\tfor line in newlines:\n\t\t\tif line.endswith('\\n'):\n\t\t\t\ttmp = tmp + line + '\\n'\n\t\t\telse:\n\t\t\t\ttmp = tmp + line\n\t\t\t\tlines.append(tmp)\n\t\t\t\ttmp = ''\n\t\tif tmp != '':\n\t\t\tlines.append(tmp)\n\t\treturn lines", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\ttry:\n\t\tif platform == 'this':\n\t\t\tplatform = 'posix' if sys.platform == 'win32' else 'nt'\n\t\telif platform == 1:\n\t\t\tplatform = 'posix'\n\t\telif platform == 0:\n\t\t\tplatform = 'nt'\n\t\telse:\n\t\t\traise ValueError('Invalid platform')\n\texcept NameError:\n\t\tplatform = 'nt'\n\n\tif platform == 'posix':\n\t\treturn s.split('\\n')\n\telse:\n\t\treturn s.split('\\r\\n')"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tplatform_map = {\n\t\t'this': 'auto',\n\t\t1: 'posix',\n\t\t0: 'win32'\n\t}\n\tplatform = platform_map.get(platform, platform)\n\timport shlex\n\treturn shlex.split(s, posix=platform == 'posix')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\ttry:\n\t\tif platform == 'this':\n\t\t\tplatform = get_platform()\n\t\tif platform in [1, 'posix', 'posix-like', 'linux', 'cygwin', 'darwin', 'freebsd', 'openbsd', 'netbsd', 'aix']:\n\t\t\treturn s.split('\\r\\n') if platform in ['cygwin', 'darwin', 'freebsd', 'openbsd', 'netbsd', 'aix'] else s.split('\\n')\n\t\telif platform in [0, 'nt', 'win32', 'cygwin', 'msys']:\n\t\t\treturn s.split('\\n')\n\texcept Exception as e:\n\t\treturn [s]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif platform == 'this':\n\t\tplatform = _get_platform()\n\t\n\tif platform == 1:\n\t\treturn _split_posix(s)\n\telif platform == 0:\n\t\treturn _split_windows(s)\n\telse:\n\t\traise ValueError('platform should be 1 or 0')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif (platform == 'this'):\n\t\tplatform = os.name\n\tif platform == 'nt':\n\t\tplatform = 0\n\telif platform == 'posix':\n\t\tplatform = 1\n\tif platform == 1:\n\t\treturn s.split(' ')\n\telif platform == 0:\n\t\treturn s.split(';')", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tfrom os import name\n\t\n\tif platform == 'this':\n\t\tif name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\tif platform == 0:\n\t\treturn s.split(';')\n\telif platform == 1:\n\t\treturn s.split(':')\n\telse:\n\t\treturn 'error: invalid platform %d' % platform\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 'posix':\n\t\treturn s.split('\\n')\n\telif platform == 'nt': # windows\n\t\treturn s.split('\\r\\n')\n\telse:\n\t\traise ValueError('Invalid platform: {}'.format(platform))", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform as p\n\tif platform == 'this':\n\t\tif p.system() == 'Windows':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 1:\n\t\treturn s.split(' ')\n\telse:\n\t\treturn s.split(' ')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tsplit_char = ' '\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 1:\n\t\tsplit_char = ' '\n\telif platform == 0:\n\t\tsplit_char = ';'\n\treturn s.split(split_char)", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\tif platform == 'this':\n\t\timport platform\n\t\tif platform.system() == 'Windows':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\ts = s.replace('\\r', '')\n\tif platform == 1:\n\t\treturn s.split('\\n')\n\telse:\n\t\tlines = []\n\t\ttmp = ''\n\t\tnewlines = s.split('\\n')\n\t\tfor line in newlines:\n\t\t\tif line.endswith('\\n'):\n\t\t\t\ttmp = tmp + line + '\\n'\n\t\t\telse:\n\t\t\t\ttmp = tmp + line\n\t\t\t\tlines.append(tmp)\n\t\t\t\ttmp = ''\n\t\tif tmp != '':\n\t\t\tlines.append(tmp)\n\t\treturn lines", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\ttry:\n\t\tif platform == 'this':\n\t\t\tplatform = 'posix' if sys.platform == 'win32' else 'nt'\n\t\telif platform == 1:\n\t\t\tplatform = 'posix'\n\t\telif platform == 0:\n\t\t\tplatform = 'nt'\n\t\telse:\n\t\t\traise ValueError('Invalid platform')\n\texcept NameError:\n\t\tplatform = 'nt'\n\n\tif platform == 'posix':\n\t\treturn s.split('\\n')\n\telse:\n\t\treturn s.split('\\r\\n')", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/tests/__init__.py", "project": "SoftwareHeritage/swh-lister", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tfilename = filename or \"temp\"\n\ttmp_dir = Path(tmp_path)\n\tif not tmp_dir.exists():\n\t\ttmp_dir.mkdir()\n\ttmp_dir = (tmp_dir / filename).resolve()\n\tif not tmp_dir.exists():\n\t\ttmp_dir.mkdir()\n\tarchive_path = Path(archive_path).resolve()\n\tarchive_dir = archive_path.parent\n\tshutil.unpack_archive(archive_path, tmp_dir)\n\trepo_url = f\"file://{tmp_dir.resolve()}\"\n\treturn repo_url", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\textract_dir = extract_archive(archive_path, filename, tmp_path)\n\trepo_url = create_and_push_to_github_repository(extract_dir)\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tarchive_path = Path(archive_path)\n\ttmp_path = Path(tmp_path)\n\tarchive_path.parent.mkdir(exist_ok=True, parents=True)\n\n\trepo_url = f\"https://{filename}\" if filename else f\"https://{archive_path.name}\"\n\tif archive_path.suffix == \".bz2\":\n\t\tsubprocess.run([\"bunzip2\", \"-k\", \"-c\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive_path = archive_path.with_suffix(\"\")\n\n\tif archive_path.suffix == \".gz\":\n\t\tsubprocess.run([\"gunzip\", \"-k\", \"-c\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive_path = archive_path.with_suffix(\"\")\n\n\tif archive_path.suffix == \".tar\":\n\t\tsubprocess.run([\"tar\", \"xvf\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive_path = archive_path.with_suffix(\"\")\n\n\tif archive_path.suffix == \".zip\":\n\t\tsubprocess.run([\"unzip\", \"-o\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\ttmp_dir = tempfile.mkdtemp(dir=tmp_path)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=tmp_dir)\n\n\trepo_path = os.path.join(tmp_dir, filename)\n\n\tif not os.path.exists(repo_path):\n\t\traise ValueError(f\"Invalid archive path {archive_path}\")\n\n\treturn repo_path\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tarchive_path = Path(archive_path)\n\ttemp_dir = Path(tmp_path) / f\"{archive_path.stem}.worktree\"\n\tshutil.unpack_archive(str(archive_path), str(temp_dir))\n\trepo_url = f\"file://{temp_dir}/{filename}\"\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\trepo_url = None\n\tarchive_path = Path(archive_path)\n\tfilename = filename or archive_path.stem\n\ttmp_path = Path(tmp_path)\n\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(tmp_path)\n\trepo_url = f\"file://{tmp_path / filename}\"\n\n\treturn repo_url", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tname = filename or os.path.basename(archive_path)\n\tname = os.path.splitext(name)[0]\n\trepo_path = os.path.join(tmp_path, name)\n\tlogger.info(f\"Preparing repository from archive {archive_path}\")\n\tshutil.rmtree(repo_path, ignore_errors=True)\n\tos.makedirs(repo_path, exist_ok=True)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(path=repo_path)\n\treturn repo_path\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\trepo_url = None\n\tarchive_dir = Path(tmp_path) / \"archive\"\n\tarchive_dir.mkdir(parents=True, exist_ok=True)\n\ttry:\n\t\twith zipfile.ZipFile(archive_path, \"r\") as file:\n\t\t\tfile.extractall(archive_dir)\n\texcept Exception:\n\t\traise Exception(\n\t\t\t\"Error occurred while extracting the zip file. Please try again.\"\n\t\t)\n\trepo_url = archive_dir / filename\n\treturn repo_url", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tarchive_path = Path(archive_path)\n\tfilename = filename or archive_path.name\n\twith TemporaryDirectory(dir=tmp_path) as tmpdir:\n\t\trepo_path = Path(tmpdir) / filename.replace(\".tar.gz\", \"\")\n\t\trepo_path.mkdir()\n\t\twith tarfile.open(archive_path, \"r\") as tar:\n\t\t\ttar.extractall(repo_path)\n\treturn repo_path.as_uri()", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t...\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tfilename = filename or \"temp\"\n\ttmp_dir = Path(tmp_path)\n\tif not tmp_dir.exists():\n\t\ttmp_dir.mkdir()\n\ttmp_dir = (tmp_dir / filename).resolve()\n\tif not tmp_dir.exists():\n\t\ttmp_dir.mkdir()\n\tarchive_path = Path(archive_path).resolve()\n\tarchive_dir = archive_path.parent\n\tshutil.unpack_archive(archive_path, tmp_dir)\n\trepo_url = f\"file://{tmp_dir.resolve()}\"\n\treturn repo_url", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\textract_dir = extract_archive(archive_path, filename, tmp_path)\n\trepo_url = create_and_push_to_github_repository(extract_dir)\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tarchive_path = Path(archive_path)\n\ttmp_path = Path(tmp_path)\n\tarchive_path.parent.mkdir(exist_ok=True, parents=True)\n\n\trepo_url = f\"https://{filename}\" if filename else f\"https://{archive_path.name}\"\n\tif archive_path.suffix == \".bz2\":\n\t\tsubprocess.run([\"bunzip2\", \"-k\", \"-c\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive_path = archive_path.with_suffix(\"\")\n\n\tif archive_path.suffix == \".gz\":\n\t\tsubprocess.run([\"gunzip\", \"-k\", \"-c\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive_path = archive_path.with_suffix(\"\")\n\n\tif archive_path.suffix == \".tar\":\n\t\tsubprocess.run([\"tar\", \"xvf\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive_path = archive_path.with_suffix(\"\")\n\n\tif archive_path.suffix == \".zip\":\n\t\tsubprocess.run([\"unzip\", \"-o\", archive_path], stdout=subprocess.PIPE)\n\t\tarchive", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\ttmp_dir = tempfile.mkdtemp(dir=tmp_path)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=tmp_dir)\n\n\trepo_path = os.path.join(tmp_dir, filename)\n\n\tif not os.path.exists(repo_path):\n\t\traise ValueError(f\"Invalid archive path {archive_path}\")\n\n\treturn repo_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tarchive_path = Path(archive_path)\n\ttemp_dir = Path(tmp_path) / f\"{archive_path.stem}.worktree\"\n\tshutil.unpack_archive(str(archive_path), str(temp_dir))\n\trepo_url = f\"file://{temp_dir}/{filename}\"\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\trepo_url = None\n\tarchive_path = Path(archive_path)\n\tfilename = filename or archive_path.stem\n\ttmp_path = Path(tmp_path)\n\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(tmp_path)\n\trepo_url = f\"file://{tmp_path / filename}\"\n\n\treturn repo_url", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tname = filename or os.path.basename(archive_path)\n\tname = os.path.splitext(name)[0]\n\trepo_path = os.path.join(tmp_path, name)\n\tlogger.info(f\"Preparing repository from archive {archive_path}\")\n\tshutil.rmtree(repo_path, ignore_errors=True)\n\tos.makedirs(repo_path, exist_ok=True)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(path=repo_path)\n\treturn repo_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\trepo_url = None\n\tarchive_dir = Path(tmp_path) / \"archive\"\n\tarchive_dir.mkdir(parents=True, exist_ok=True)\n\ttry:\n\t\twith zipfile.ZipFile(archive_path, \"r\") as file:\n\t\t\tfile.extractall(archive_dir)\n\texcept Exception:\n\t\traise Exception(\n\t\t\t\"Error occurred while extracting the zip file. Please try again.\"\n\t\t)\n\trepo_url = archive_dir / filename\n\treturn repo_url", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tarchive_path = Path(archive_path)\n\tfilename = filename or archive_path.name\n\twith TemporaryDirectory(dir=tmp_path) as tmpdir:\n\t\trepo_path = Path(tmpdir) / filename.replace(\".tar.gz\", \"\")\n\t\trepo_path.mkdir()\n\t\twith tarfile.open(archive_path, \"r\") as tar:\n\t\t\ttar.extractall(repo_path)\n\treturn repo_path.as_uri()", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t...\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/flashbake/plugins/ignored.py", "project": "commandline/flashbake", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\twith open('ignored.txt', 'w') as f:\n\t\tf.write(ignored)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\timport os\n\tget_ignored_files = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--ignored\", \"--exclude-standard\"]).decode(\"utf-8\")\n\tignored_files = get_ignored_files.split('\\n')\n\tignored_files = list(filter(None, ignored_files))\n\tignored_files.sort()\n\tfor file in ignored_files:\n\t\tif file in ignored:\n\t\t\tignored_files.remove(file)\n\t\telse:\n\t\t\tpass\n\treturn ','.join(ignored_files)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport os\n\timport subprocess\n\timport locale\n\t\n\tlocale.setlocale(locale.LC_ALL, '')\n\t\n\tdef get_files(path):\n\t  if os.path.isfile(path):\n\t    return [path]\n\t\n\t  all_files = []\n\t  for dirpath, dirnames, filenames in os.walk(path):\n\t    for filename in filenames:\n\t      all_files.append(os.path.join(dirpath, filename))\n\t  return all_files\n\t\n\tdef get_ignored_files(repo_path):\n\t  cmd = 'git ls-files -i --exclude-standard'\n\t  proc = subprocess.Popen(cmd.split(),\n\t                          stdout=subprocess.PIPE,\n\t                          stderr=subprocess.PIPE,\n\t                          cwd=repo_path)\n\t  out, err = proc.communicate()\n\t\n\t  if err:\n\t    raise IOError(err)\n\t\n\t  return out.split('\\n')\n\t\n\tdef main():\n\t  ignored_files = get_ignored_files(ignored)\n\t  all_files = get_files(ignored)\n\t\n\t  ignored_files = [f for f in ignored_files if f in all_files]\n\t  ignored", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored = ignored.split('\\n')\n\tignored = [i for i in ignored if i != '']\n\tignored = list(set(ignored))\n\tignored.sort()\n\tignored = \",\".join(ignored)\n\treturn ignored\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgitignored = subprocess.run(['git', 'check-ignore', '--no-index', '--stdin'], capture_output=True, text=True, input=ignored)\n\tignoredfiles = gitignored.stdout\n\tignoredfiles = ignoredfiles.split('\\n')\n\tignoredfiles = [file for file in ignoredfiles if file != '']\n\tignoredfiles.sort()\n\treturn ','.join(ignoredfiles)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored_list = []\n\tfor file in ignored:\n\t\tignored_list.append(file)\n\tignored_list = sorted(ignored_list)\n\treturn ','.join(ignored_list)\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\timport re\n\timport os\n\timport sys\n\ttry:\n\t\twith open(os.devnull, 'w') as devnull:\n\t\t\tprocess = subprocess.Popen('git status --ignored', stdout=subprocess.PIPE, stderr=devnull, shell=True)\n\t\toutput = process.communicate()[0]\n\texcept:\n\t\tprint('git command not found')\n\t\tsys.exit(1)\n\toutput = output.decode('utf-8')\n\toutput = output.split('\\n')\n\toutput = output[2:] # delete the first two lines\n\toutput = output[:-1] # delete the last line\n\toutput = [re.sub('\\s+',' ',line).split(' ') for line in output]\n\t# get the file name and type\n\toutput = [line[2:] for line in output]\n\t# get the file name\n\toutput = [line[0] for line in output]\n\t# get the file name\n\toutput = [line.split('/')[-1] for line in output]\n\t# get the file name\n\toutput = [line for line in output if line not in ignored]\n\t# get the file name\n\toutput = [line for line in output if line not in ['.gitignore','.git", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles = []\n\tcmd = \"git ls-files --others --exclude-standard\"\n\tignored_files = os.popen(cmd).read()\n\tif ignored_files != \"\":\n\t\tignored_files = ignored_files.split(\"\\n\")\n\t\tignored_files = list(filter(None, ignored_files))\n\t\tignored_files = [x.strip() for x in ignored_files]\n\t\tfor file in ignored_files:\n\t\t\tif file[0] == \".\":\n\t\t\t\tfiles.append(file)\n\treturn \",\".join(files)", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\timport re\n\tgit_ignored_files = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--directory\", \"--ignored\", \"--exclude-standard\"])\n\tgit_ignored_files = re.split(r'[\\n\\r]+', git_ignored_files.decode('utf-8').strip())\n\tgit_ignored_files = [file for file in git_ignored_files if file not in ignored]\n\tgit_ignored_files = \",\".join(git_ignored_files)\n\treturn git_ignored_files\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\tprocess = subprocess.Popen(['git', 'status', '--ignored'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\toutput, error = process.communicate()\n\toutput = output.decode('utf-8')\n\toutput = output.replace('\\t', ' ')\n\toutput = output.replace('\\n', ' ')\n\toutput = output.replace('\\r', ' ')\n\toutput = output.split(' ')\n\toutput = [item for item in output if item != '']\n\toutput = output[2:]\n\tnew_list = []\n\tfor item in output:\n\t\tif item.startswith('!!'):\n\t\t\tnew_list.append(item[2:])\n\t\telse:\n\t\t\tnew_list.append(item)\n\tnew_list.sort()\n\tignored = new_list + ignored\n\treturn ignored\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\twith open('ignored.txt', 'w') as f:\n\t\tf.write(ignored)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\timport os\n\tget_ignored_files = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--ignored\", \"--exclude-standard\"]).decode(\"utf-8\")\n\tignored_files = get_ignored_files.split('\\n')\n\tignored_files = list(filter(None, ignored_files))\n\tignored_files.sort()\n\tfor file in ignored_files:\n\t\tif file in ignored:\n\t\t\tignored_files.remove(file)\n\t\telse:\n\t\t\tpass\n\treturn ','.join(ignored_files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport os\n\timport subprocess\n\timport locale\n\t\n\tlocale.setlocale(locale.LC_ALL, '')\n\t\n\tdef get_files(path):\n\t  if os.path.isfile(path):\n\t    return [path]\n\t\n\t  all_files = []\n\t  for dirpath, dirnames, filenames in os.walk(path):\n\t    for filename in filenames:\n\t      all_files.append(os.path.join(dirpath, filename))\n\t  return all_files\n\t\n\tdef get_ignored_files(repo_path):\n\t  cmd = 'git ls-files -i --exclude-standard'\n\t  proc = subprocess.Popen(cmd.split(),\n\t                          stdout=subprocess.PIPE,\n\t                          stderr=subprocess.PIPE,\n\t                          cwd=repo_path)\n\t  out, err = proc.communicate()\n\t\n\t  if err:\n\t    raise IOError(err)\n\t\n\t  return out.split('\\n')\n\t\n\tdef main():\n\t  ignored_files = get_ignored_files(ignored)\n\t  all_files = get_files(ignored)\n\t\n\t  ignored_files = [f for f in ignored_files if f in all_files]\n\t  ignored", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored = ignored.split('\\n')\n\tignored = [i for i in ignored if i != '']\n\tignored = list(set(ignored))\n\tignored.sort()\n\tignored = \",\".join(ignored)\n\treturn ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgitignored = subprocess.run(['git', 'check-ignore', '--no-index', '--stdin'], capture_output=True, text=True, input=ignored)\n\tignoredfiles = gitignored.stdout\n\tignoredfiles = ignoredfiles.split('\\n')\n\tignoredfiles = [file for file in ignoredfiles if file != '']\n\tignoredfiles.sort()\n\treturn ','.join(ignoredfiles)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored_list = []\n\tfor file in ignored:\n\t\tignored_list.append(file)\n\tignored_list = sorted(ignored_list)\n\treturn ','.join(ignored_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\timport re\n\timport os\n\timport sys\n\ttry:\n\t\twith open(os.devnull, 'w') as devnull:\n\t\t\tprocess = subprocess.Popen('git status --ignored', stdout=subprocess.PIPE, stderr=devnull, shell=True)\n\t\toutput = process.communicate()[0]\n\texcept:\n\t\tprint('git command not found')\n\t\tsys.exit(1)\n\toutput = output.decode('utf-8')\n\toutput = output.split('\\n')\n\toutput = output[2:] # delete the first two lines\n\toutput = output[:-1] # delete the last line\n\toutput = [re.sub('\\s+',' ',line).split(' ') for line in output]\n\t# get the file name and type\n\toutput = [line[2:] for line in output]\n\t# get the file name\n\toutput = [line[0] for line in output]\n\t# get the file name\n\toutput = [line.split('/')[-1] for line in output]\n\t# get the file name\n\toutput = [line for line in output if line not in ignored]\n\t# get the file name\n\toutput = [line for line in output if line not in ['.gitignore','.git", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles = []\n\tcmd = \"git ls-files --others --exclude-standard\"\n\tignored_files = os.popen(cmd).read()\n\tif ignored_files != \"\":\n\t\tignored_files = ignored_files.split(\"\\n\")\n\t\tignored_files = list(filter(None, ignored_files))\n\t\tignored_files = [x.strip() for x in ignored_files]\n\t\tfor file in ignored_files:\n\t\t\tif file[0] == \".\":\n\t\t\t\tfiles.append(file)\n\treturn \",\".join(files)", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\timport re\n\tgit_ignored_files = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--directory\", \"--ignored\", \"--exclude-standard\"])\n\tgit_ignored_files = re.split(r'[\\n\\r]+', git_ignored_files.decode('utf-8').strip())\n\tgit_ignored_files = [file for file in git_ignored_files if file not in ignored]\n\tgit_ignored_files = \",\".join(git_ignored_files)\n\treturn git_ignored_files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\tprocess = subprocess.Popen(['git', 'status', '--ignored'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\toutput, error = process.communicate()\n\toutput = output.decode('utf-8')\n\toutput = output.replace('\\t', ' ')\n\toutput = output.replace('\\n', ' ')\n\toutput = output.replace('\\r', ' ')\n\toutput = output.split(' ')\n\toutput = [item for item in output if item != '']\n\toutput = output[2:]\n\tnew_list = []\n\tfor item in output:\n\t\tif item.startswith('!!'):\n\t\t\tnew_list.append(item[2:])\n\t\telse:\n\t\t\tnew_list.append(item)\n\tnew_list.sort()\n\tignored = new_list + ignored\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "docopt/__init__.py", "project": "bazaar-projects/docopt-ng", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\timport os\n\treturn os.path.splitext(filename)[1].lower() == '.doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfile_type = filename.split('.')[-1]\n\treturn file_type.lower() == 'doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfile_type = filename.split('.')[1]\n\treturn file_type.lower() == 'doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename.endswith('doxyfile')\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ttry:\n\t\tfilename = filename.lower()\n\t\tif filename.find('doxyfile') > -1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept AttributeError:\n\t\treturn False", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ttype = filename.split('.')[-1].lower()\n\tif type == 'doxyfile':\n\t\treturn True\n\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\text = filename.lower().split('.')[-1]\n\tif ext == 'doxyfile':\n\t\treturn True\n\n\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t_, ext = os.path.splitext(filename)\n\treturn ext.lower() == '.doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename_lower = filename.lower()\n\tif filename_lower.find('doxyfile') != -1:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ta=filename.lower().split('.')\n\tif len(a)==1:\n\t\treturn False\n\treturn a[-1]=='doxyfile'\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\timport os\n\treturn os.path.splitext(filename)[1].lower() == '.doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfile_type = filename.split('.')[-1]\n\treturn file_type.lower() == 'doxyfile'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfile_type = filename.split('.')[1]\n\treturn file_type.lower() == 'doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename.endswith('doxyfile')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ttry:\n\t\tfilename = filename.lower()\n\t\tif filename.find('doxyfile') > -1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept AttributeError:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ttype = filename.split('.')[-1].lower()\n\tif type == 'doxyfile':\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\text = filename.lower().split('.')[-1]\n\tif ext == 'doxyfile':\n\t\treturn True\n\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t_, ext = os.path.splitext(filename)\n\treturn ext.lower() == '.doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename_lower = filename.lower()\n\tif filename_lower.find('doxyfile') != -1:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ta=filename.lower().split('.')\n\tif len(a)==1:\n\t\treturn False\n\treturn a[-1]=='doxyfile'\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/borg/check.py", "project": "witten/borgmatic", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\n\t# If the frequency is None or \"always\", return None.\n\tif (frequency is None) or (frequency == \"always\"):\n\t\treturn None\n\n\t# Split the frequency string into number and unit of time.\n\tnumber, unit = frequency.split(\" \")\n\n\t# Convert the number to an integer.\n\tnumber = int(number)\n\n\t# Convert the unit of time to a datetime.timedelta keyword argument.\n\tif unit in [\"days\", \"day\"]:\n\t\tunit = \"days\"\n\telif unit in [\"hours\", \"hour\"]:\n\t\tunit = \"hours\"\n\telif unit in [\"minutes\", \"minute\"]:\n\t\tunit = \"minutes\"\n\telif unit in [\"seconds\", \"second\"]:\n\t\tunit = \"seconds\"\n\telse:\n\t\traise ValueError(\"Invalid unit of time: {}\".format(unit))\n\n\t# Return a datetime.timedelta instance.\n\treturn datetime.timedelta(**{unit: number})\n\n\n# Test the parse_frequency function.\nprint(parse_frequency(\"3 days\"))\nprint(parse_frequency(\"10 hours\"))\nprint(parse_frequency(\"5 minutes\"))\nprint(parse_frequency(\"2 seconds\"))\nprint(parse_frequency(\"always\"))\nprint(parse_frequency(None))", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tfrom datetime import timedelta\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfrequency = frequency.split()\n\n\tif len(frequency) != 2:\n\t\traise ValueError(\"Wrong format\")\n\n\ttry:\n\t\tnumber = int(frequency[0])\n\texcept ValueError:\n\t\traise ValueError(\"Wrong format\")\n\n\ttimeunit = frequency[1]\n\n\tif timeunit == \"seconds\":\n\t\treturn timedelta(seconds=number)\n\telif timeunit == \"minutes\":\n\t\treturn timedelta(minutes=number)\n\telif timeunit == \"hours\":\n\t\treturn timedelta(hours=number)\n\telif timeunit == \"days\":\n\t\treturn timedelta(days=number)\n\telif timeunit == \"weeks\":\n\t\treturn timedelta(weeks=number)\n\telse:\n\t\traise ValueError(\"Wrong format\")\n\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tparts = frequency.split()\n\tif len(parts) != 2:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\ttry:\n\t\tnum = int(parts[0])\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\ttimeunit = parts[1].lower()\n\tif timeunit == \"second\":\n\t\treturn datetime.timedelta(seconds=num)\n\telif timeunit == \"minute\":\n\t\treturn datetime.timedelta(minutes=num)\n\telif timeunit == \"hour\":\n\t\treturn datetime.timedelta(hours=num)\n\telif timeunit == \"day\":\n\t\treturn datetime.timedelta(days=num)\n\telif timeunit == \"week\":\n\t\treturn datetime.timedelta(weeks=num)\n\telif timeunit == \"month\":\n\t\treturn datetime.timedelta(days=num*30)\n\telif timeunit == \"year\":\n\t\treturn datetime.timedelta(days=num*365)\n\telif timeunit == \"hourly\":\n\t\treturn datetime.timedelta(hours=1)\n\tel", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport re\n\timport datetime\n\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\n\tpattern = \"^([0-9]+) ([a-z]+)$\"\n\tregex = re.compile(pattern)\n\tmatch = regex.match(frequency)\n\tif not match:\n\t\traise ValueError(\"Invalid frequency\")\n\tnumber = int(match.group(1))\n\ttimeunit = match.group(2)\n\n\tif timeunit == \"minute\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit == \"week\":\n\t\treturn datetime.timedelta(weeks=number)\n\telif timeunit == \"month\":\n\t\treturn datetime.timedelta(days=number*30)\n\telif timeunit == \"year\":\n\t\treturn datetime.timedelta(days=number*365)\n\telse:\n\t\traise ValueError(\"Invalid timeunit\")", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tx = frequency.split()\n\ttry:\n\t\tvalue = int(x[0])\n\t\ttimeunit = x[1]\n\t\ttimeunit = timeunit[:-1]\n\t\ttimeunit = timeunit.lower()\n\t\tif timeunit == \"year\":\n\t\t\treturn datetime.timedelta(days=value*365)\n\t\telif timeunit == \"month\":\n\t\t\treturn datetime.timedelta(days=value*31)\n\t\telif timeunit == \"day\":\n\t\t\treturn datetime.timedelta(days=value)\n\t\telif timeunit == \"hour\":\n\t\t\treturn datetime.timedelta(hours=value)\n\t\telif timeunit == \"minute\":\n\t\t\treturn datetime.timedelta(minutes=value)\n\t\telif timeunit == \"second\":\n\t\t\treturn datetime.timedelta(seconds=value)\n\texcept:\n\t\traise ValueError(\"Invalid frequency: \" + frequency)\n\t\n\nimport datetime\nprint(parse_frequency(\"2 month\") == datetime.timedelta(days=61))", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency is None:\n\t\treturn None\n\telif frequency == \"always\":\n\t\treturn None\n\telse:\n\t\ttime_unit = frequency.split()[1]\n\t\tif time_unit == 'day':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0]))\n\t\telif time_unit == 'month':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0])*30)\n\t\telif time_unit == 'week':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0])*7)\n\t\telif time_unit == 'year':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0])*365)\n\t\telse:\n\t\t\traise ValueError(\"Could not parse frequency\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tfrequency = frequency.split(' ')\n\tif len(frequency) == 1:\n\t\treturn None\n\telif len(frequency) != 2:\n\t\traise ValueError(\"Invalid frequency: '%s'\" % frequency)\n\tnumber, timeunit = frequency\n\ttry:\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid number in frequency: '%s'\" % number)\n\tif timeunit.lower() == 'seconds':\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit.lower() == 'minutes':\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit.lower() == 'hours':\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit.lower() == 'days':\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit.lower() == 'weeks':\n\t\treturn datetime.timedelta(weeks=number)\n\telif timeunit.lower() == 'months':\n\t\treturn datetime.timedelta(days=number*30)\n\telif timeunit.lower() == 'years':\n\t\treturn datetime.timedelta(days=number*365)\n\telse:\n\t\traise ValueError(\"", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\n\tif frequency is None:\n\t\treturn None\n\n\ttry:\n\t\tfrequency_split = frequency.split()\n\t\tnumber = int(frequency_split[0])\n\t\ttimeunit = frequency_split[1]\n\texcept:\n\t\traise ValueError(\"Frequency string could not be parsed\")\n\n\tif timeunit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=number)\n\telif timeunit == \"months\":\n\t\treturn datetime.timedelta(days=number*30)\n\telse:\n\t\traise ValueError(\"Frequency string could not be parsed\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttimeunit = frequency.split()\n\tif timeunit[1] == \"second\":\n\t\tdelta = datetime.timedelta(seconds=float(timeunit[0]))\n\telif timeunit[1] == \"minute\":\n\t\tdelta = datetime.timedelta(minutes=float(timeunit[0]))\n\telif timeunit[1] == \"hour\":\n\t\tdelta = datetime.timedelta(hours=float(timeunit[0]))\n\telif timeunit[1] == \"day\":\n\t\tdelta = datetime.timedelta(days=float(timeunit[0]))\n\telif timeunit[1] == \"week\":\n\t\tdelta = datetime.timedelta(weeks=float(timeunit[0]))\n\telse:\n\t\traise ValueError(\"frequency cannot be parsed\")\n\treturn delta\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\t\n\ttime_unit = frequency.split()[1]\n\tnumber = int(frequency.split()[0])\n\t\n\tif time_unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif time_unit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif time_unit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telse:\n\t\traise ValueError(\"wrong frequency\")\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\n\t# If the frequency is None or \"always\", return None.\n\tif (frequency is None) or (frequency == \"always\"):\n\t\treturn None\n\n\t# Split the frequency string into number and unit of time.\n\tnumber, unit = frequency.split(\" \")\n\n\t# Convert the number to an integer.\n\tnumber = int(number)\n\n\t# Convert the unit of time to a datetime.timedelta keyword argument.\n\tif unit in [\"days\", \"day\"]:\n\t\tunit = \"days\"\n\telif unit in [\"hours\", \"hour\"]:\n\t\tunit = \"hours\"\n\telif unit in [\"minutes\", \"minute\"]:\n\t\tunit = \"minutes\"\n\telif unit in [\"seconds\", \"second\"]:\n\t\tunit = \"seconds\"\n\telse:\n\t\traise ValueError(\"Invalid unit of time: {}\".format(unit))\n\n\t# Return a datetime.timedelta instance.\n\treturn datetime.timedelta(**{unit: number})\n\n\n# Test the parse_frequency function.\nprint(parse_frequency(\"3 days\"))\nprint(parse_frequency(\"10 hours\"))\nprint(parse_frequency(\"5 minutes\"))\nprint(parse_frequency(\"2 seconds\"))\nprint(parse_frequency(\"always\"))\nprint(parse_frequency(None))", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tfrom datetime import timedelta\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfrequency = frequency.split()\n\n\tif len(frequency) != 2:\n\t\traise ValueError(\"Wrong format\")\n\n\ttry:\n\t\tnumber = int(frequency[0])\n\texcept ValueError:\n\t\traise ValueError(\"Wrong format\")\n\n\ttimeunit = frequency[1]\n\n\tif timeunit == \"seconds\":\n\t\treturn timedelta(seconds=number)\n\telif timeunit == \"minutes\":\n\t\treturn timedelta(minutes=number)\n\telif timeunit == \"hours\":\n\t\treturn timedelta(hours=number)\n\telif timeunit == \"days\":\n\t\treturn timedelta(days=number)\n\telif timeunit == \"weeks\":\n\t\treturn timedelta(weeks=number)\n\telse:\n\t\traise ValueError(\"Wrong format\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tparts = frequency.split()\n\tif len(parts) != 2:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\ttry:\n\t\tnum = int(parts[0])\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\ttimeunit = parts[1].lower()\n\tif timeunit == \"second\":\n\t\treturn datetime.timedelta(seconds=num)\n\telif timeunit == \"minute\":\n\t\treturn datetime.timedelta(minutes=num)\n\telif timeunit == \"hour\":\n\t\treturn datetime.timedelta(hours=num)\n\telif timeunit == \"day\":\n\t\treturn datetime.timedelta(days=num)\n\telif timeunit == \"week\":\n\t\treturn datetime.timedelta(weeks=num)\n\telif timeunit == \"month\":\n\t\treturn datetime.timedelta(days=num*30)\n\telif timeunit == \"year\":\n\t\treturn datetime.timedelta(days=num*365)\n\telif timeunit == \"hourly\":\n\t\treturn datetime.timedelta(hours=1)\n\tel", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport re\n\timport datetime\n\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\n\tpattern = \"^([0-9]+) ([a-z]+)$\"\n\tregex = re.compile(pattern)\n\tmatch = regex.match(frequency)\n\tif not match:\n\t\traise ValueError(\"Invalid frequency\")\n\tnumber = int(match.group(1))\n\ttimeunit = match.group(2)\n\n\tif timeunit == \"minute\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit == \"week\":\n\t\treturn datetime.timedelta(weeks=number)\n\telif timeunit == \"month\":\n\t\treturn datetime.timedelta(days=number*30)\n\telif timeunit == \"year\":\n\t\treturn datetime.timedelta(days=number*365)\n\telse:\n\t\traise ValueError(\"Invalid timeunit\")", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tx = frequency.split()\n\ttry:\n\t\tvalue = int(x[0])\n\t\ttimeunit = x[1]\n\t\ttimeunit = timeunit[:-1]\n\t\ttimeunit = timeunit.lower()\n\t\tif timeunit == \"year\":\n\t\t\treturn datetime.timedelta(days=value*365)\n\t\telif timeunit == \"month\":\n\t\t\treturn datetime.timedelta(days=value*31)\n\t\telif timeunit == \"day\":\n\t\t\treturn datetime.timedelta(days=value)\n\t\telif timeunit == \"hour\":\n\t\t\treturn datetime.timedelta(hours=value)\n\t\telif timeunit == \"minute\":\n\t\t\treturn datetime.timedelta(minutes=value)\n\t\telif timeunit == \"second\":\n\t\t\treturn datetime.timedelta(seconds=value)\n\texcept:\n\t\traise ValueError(\"Invalid frequency: \" + frequency)\n\t\n\nimport datetime\nprint(parse_frequency(\"2 month\") == datetime.timedelta(days=61))", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency is None:\n\t\treturn None\n\telif frequency == \"always\":\n\t\treturn None\n\telse:\n\t\ttime_unit = frequency.split()[1]\n\t\tif time_unit == 'day':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0]))\n\t\telif time_unit == 'month':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0])*30)\n\t\telif time_unit == 'week':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0])*7)\n\t\telif time_unit == 'year':\n\t\t\treturn datetime.timedelta(days=int(frequency.split()[0])*365)\n\t\telse:\n\t\t\traise ValueError(\"Could not parse frequency\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tfrequency = frequency.split(' ')\n\tif len(frequency) == 1:\n\t\treturn None\n\telif len(frequency) != 2:\n\t\traise ValueError(\"Invalid frequency: '%s'\" % frequency)\n\tnumber, timeunit = frequency\n\ttry:\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid number in frequency: '%s'\" % number)\n\tif timeunit.lower() == 'seconds':\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit.lower() == 'minutes':\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit.lower() == 'hours':\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit.lower() == 'days':\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit.lower() == 'weeks':\n\t\treturn datetime.timedelta(weeks=number)\n\telif timeunit.lower() == 'months':\n\t\treturn datetime.timedelta(days=number*30)\n\telif timeunit.lower() == 'years':\n\t\treturn datetime.timedelta(days=number*365)\n\telse:\n\t\traise ValueError(\"", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\n\tif frequency is None:\n\t\treturn None\n\n\ttry:\n\t\tfrequency_split = frequency.split()\n\t\tnumber = int(frequency_split[0])\n\t\ttimeunit = frequency_split[1]\n\texcept:\n\t\traise ValueError(\"Frequency string could not be parsed\")\n\n\tif timeunit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=number)\n\telif timeunit == \"months\":\n\t\treturn datetime.timedelta(days=number*30)\n\telse:\n\t\traise ValueError(\"Frequency string could not be parsed\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttimeunit = frequency.split()\n\tif timeunit[1] == \"second\":\n\t\tdelta = datetime.timedelta(seconds=float(timeunit[0]))\n\telif timeunit[1] == \"minute\":\n\t\tdelta = datetime.timedelta(minutes=float(timeunit[0]))\n\telif timeunit[1] == \"hour\":\n\t\tdelta = datetime.timedelta(hours=float(timeunit[0]))\n\telif timeunit[1] == \"day\":\n\t\tdelta = datetime.timedelta(days=float(timeunit[0]))\n\telif timeunit[1] == \"week\":\n\t\tdelta = datetime.timedelta(weeks=float(timeunit[0]))\n\telse:\n\t\traise ValueError(\"frequency cannot be parsed\")\n\treturn delta\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\timport datetime\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\t\n\ttime_unit = frequency.split()[1]\n\tnumber = int(frequency.split()[0])\n\t\n\tif time_unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif time_unit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif time_unit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telse:\n\t\traise ValueError(\"wrong frequency\")\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocalhost = ['localhost', '127.0.0.1']\n\n\tif host in localhost:\n\t\treturn True\n\n\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom socket import gethostname\n\treturn host == gethostname() or host == 'localhost' or host == '127.0.0.1'\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = host.lower()\n\n\tif host in ['localhost', '127.0.0.1']:\n\t\treturn True\n\n\timport socket\n\thostname = socket.gethostname()\n\n\tif host == hostname:\n\t\treturn True\n\n\thost = host.replace('.' + hostname, '')\n\tif host in ['localhost']:\n\t\treturn True\n\n\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom os import getenv\n\tfrom socket import gethostname, gethostbyname\n\n\tif host == gethostname() or host == gethostbyname(gethostname()) or host == getenv('USER') or host == getenv('HOSTNAME') or host == 'localhost' or host == '127.0.0.1':\n\t\treturn True\n\treturn False", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom socket import gethostname\n\tfrom sys import platform\n\tfrom os import environ\n\n\t# Linux\n\tif platform == 'linux' or platform == 'linux2':\n\t\thostname = gethostname()\n\t\tif hostname == host or hostname == host.split('.')[0]:\n\t\t\treturn True\n\t# Windows\n\telif platform == 'win32' or platform == 'win64':\n\t\tusername = environ['USERNAME']\n\t\tif username == host:\n\t\t\treturn True\n\n\treturn False", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\timport socket\n\tlocal_ip = socket.gethostbyname(socket.gethostname())\n\tif host == socket.gethostname() or host == local_ip or host == 'localhost' or host == '127.0.0.1':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocalhosts = [\n\t\t\"localhost\",\n\t\t\"127.0.0.1\",\n\t\t\"::1\",\n\t]\n\tif host in localhosts:\n\t\treturn True\n\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom socket import gethostname\n\tfrom socket import gethostbyname\n\tfrom socket import gethostbyaddr\n\n\tlocal_ip = gethostbyname(gethostname())\n\tlocal_name, local_alias, local_ipv4 = gethostbyaddr(local_ip)\n\n\treturn host in [\n\t\tlocal_ip, \n\t\tlocal_name, \n\t\tlocal_alias, \n\t\tlocal_ipv4, \n\t\t'localhost', \n\t\t'127.0.0.1'\n\t]", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\timport socket\n\tlocal_ip = socket.gethostbyname(socket.gethostname())\n\treturn host == socket.gethostname() or host == \"localhost\" or host == \"127.0.0.1\" or host == local_ip\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\timport socket\n\timport getpass\n\timport platform\n\n\tlocalhost = socket.gethostname()\n\tlocal_ip = socket.gethostbyname(localhost)\n\tlocal_user = getpass.getuser()\n\tlocal_domain = platform.node()\n\n\treturn host in [\n\t\tlocalhost,\n\t\tlocal_ip,\n\t\tlocal_user,\n\t\tlocal_domain,\n\t\t\"localhost\",\n\t\t\"127.0.0.1\"\n\t]"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocalhost = ['localhost', '127.0.0.1']\n\n\tif host in localhost:\n\t\treturn True\n\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom socket import gethostname\n\treturn host == gethostname() or host == 'localhost' or host == '127.0.0.1'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = host.lower()\n\n\tif host in ['localhost', '127.0.0.1']:\n\t\treturn True\n\n\timport socket\n\thostname = socket.gethostname()\n\n\tif host == hostname:\n\t\treturn True\n\n\thost = host.replace('.' + hostname, '')\n\tif host in ['localhost']:\n\t\treturn True\n\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom os import getenv\n\tfrom socket import gethostname, gethostbyname\n\n\tif host == gethostname() or host == gethostbyname(gethostname()) or host == getenv('USER') or host == getenv('HOSTNAME') or host == 'localhost' or host == '127.0.0.1':\n\t\treturn True\n\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom socket import gethostname\n\tfrom sys import platform\n\tfrom os import environ\n\n\t# Linux\n\tif platform == 'linux' or platform == 'linux2':\n\t\thostname = gethostname()\n\t\tif hostname == host or hostname == host.split('.')[0]:\n\t\t\treturn True\n\t# Windows\n\telif platform == 'win32' or platform == 'win64':\n\t\tusername = environ['USERNAME']\n\t\tif username == host:\n\t\t\treturn True\n\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\timport socket\n\tlocal_ip = socket.gethostbyname(socket.gethostname())\n\tif host == socket.gethostname() or host == local_ip or host == 'localhost' or host == '127.0.0.1':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocalhosts = [\n\t\t\"localhost\",\n\t\t\"127.0.0.1\",\n\t\t\"::1\",\n\t]\n\tif host in localhosts:\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tfrom socket import gethostname\n\tfrom socket import gethostbyname\n\tfrom socket import gethostbyaddr\n\n\tlocal_ip = gethostbyname(gethostname())\n\tlocal_name, local_alias, local_ipv4 = gethostbyaddr(local_ip)\n\n\treturn host in [\n\t\tlocal_ip, \n\t\tlocal_name, \n\t\tlocal_alias, \n\t\tlocal_ipv4, \n\t\t'localhost', \n\t\t'127.0.0.1'\n\t]", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\timport socket\n\tlocal_ip = socket.gethostbyname(socket.gethostname())\n\treturn host == socket.gethostname() or host == \"localhost\" or host == \"127.0.0.1\" or host == local_ip\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\timport socket\n\timport getpass\n\timport platform\n\n\tlocalhost = socket.gethostname()\n\tlocal_ip = socket.gethostbyname(localhost)\n\tlocal_user = getpass.getuser()\n\tlocal_domain = platform.node()\n\n\treturn host in [\n\t\tlocalhost,\n\t\tlocal_ip,\n\t\tlocal_user,\n\t\tlocal_domain,\n\t\t\"localhost\",\n\t\t\"127.0.0.1\"\n\t]", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/borg/list.py", "project": "witten/borgmatic", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tdef glob_pattern(path):\n\t\tif '*' in path or '?' in path:\n\t\t\treturn path\n\t\telif ';' in path:\n\t\t\treturn ';'.join(glob_pattern(p) for p in path.split(';'))\n\t\telse:\n\t\t\treturn '*' + path + '*'\n\n\tdef remove_duplicates(paths):\n\t\t\"\"\"\n\t\tRemove consecutive duplicates from list of paths.\n\t\t\"\"\"\n\t\tprev = ''\n\t\tresult = []\n\t\tfor path in paths:\n\t\t\tif path != prev:\n\t\t\t\tresult.append(path)\n\t\t\t\tprev = path\n\t\treturn result\n\n\treturn remove_duplicates([glob_pattern(path) for path in find_paths])\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfrom fnmatch import translate\n\tfrom os.path import normpath\n\n\tresult = []\n\tfor path in find_paths:\n\t\tif path.startswith('!'):\n\t\t\tresult.append(path[1:])\n\t\telse:\n\t\t\tresult.append(normpath(translate(path)))\n\treturn tuple(result)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport os\n\tfrom glob import glob\n\n\tdef _make_find_path(path):\n\t\t\"\"\"\n\t\tGiven a path, resolve it to a glob pattern.\n\n\t\tArgs:\n\t\t\tpath: path\n\t\tReturns:\n\t\t\tglob pattern\n\t\t\"\"\"\n\t\tif os.path.isdir(path):\n\t\t\tpath = os.path.join(path, \"*\")\n\t\treturn path\n\n\treturn tuple(map(_make_find_path, find_paths))\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport os\n\timport glob\n\n\ttransformed_find_paths = []\n\tfor find_path in find_paths:\n\t\tif os.path.isdir(find_path):\n\t\t\ttransformed_find_paths.append(os.path.join(find_path, \"**\"))\n\t\telif os.path.isfile(find_path):\n\t\t\ttransformed_find_paths.append(find_path)\n\t\telif \"*\" in find_path:\n\t\t\ttransformed_find_paths.append(find_path)\n\t\telse:\n\t\t\ttransformed_find_paths.extend(glob.glob(find_path))\n\n\treturn tuple(transformed_find_paths)", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport os\n\tfrom glob import glob\n\tfrom typing import Sequence\n\n\tdef _transform(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn f\"{path}/**/*\"\n\t\telif os.path.isfile(path):\n\t\t\treturn path\n\t\telif os.path.isabs(path):\n\t\t\treturn path\n\t\telse:\n\t\t\treturn f\"{os.getcwd()}/{path}\"\n\n\treturn tuple(_transform(p) for p in find_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfrom glob import glob\n\treturn tuple('*' if not '*' in path else path for path in find_paths)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tret = []\n\tfor path in find_paths:\n\t\tif '*' in path:\n\t\t\tret.append(path)\n\t\telse:\n\t\t\tret.append(path + '/**')\n\treturn tuple(ret)", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tdef glob_path(path):\n\t\treturn path if path.startswith('*') else '*{}*'.format(path)\n\treturn tuple(glob_path(path) for path in find_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfrom fnmatch import translate\n\n\treturn tuple(translate(p) if '*' in p else p for p in find_paths)", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport glob\n\n\tdef find_paths_to_glob(path):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif '*' in path:\n\t\t\treturn path\n\t\treturn glob.glob(path)\n\n\treturn tuple(find_paths_to_glob(path) for path in find_paths)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tdef glob_pattern(path):\n\t\tif '*' in path or '?' in path:\n\t\t\treturn path\n\t\telif ';' in path:\n\t\t\treturn ';'.join(glob_pattern(p) for p in path.split(';'))\n\t\telse:\n\t\t\treturn '*' + path + '*'\n\n\tdef remove_duplicates(paths):\n\t\t\"\"\"\n\t\tRemove consecutive duplicates from list of paths.\n\t\t\"\"\"\n\t\tprev = ''\n\t\tresult = []\n\t\tfor path in paths:\n\t\t\tif path != prev:\n\t\t\t\tresult.append(path)\n\t\t\t\tprev = path\n\t\treturn result\n\n\treturn remove_duplicates([glob_pattern(path) for path in find_paths])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfrom fnmatch import translate\n\tfrom os.path import normpath\n\n\tresult = []\n\tfor path in find_paths:\n\t\tif path.startswith('!'):\n\t\t\tresult.append(path[1:])\n\t\telse:\n\t\t\tresult.append(normpath(translate(path)))\n\treturn tuple(result)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport os\n\tfrom glob import glob\n\n\tdef _make_find_path(path):\n\t\t\"\"\"\n\t\tGiven a path, resolve it to a glob pattern.\n\n\t\tArgs:\n\t\t\tpath: path\n\t\tReturns:\n\t\t\tglob pattern\n\t\t\"\"\"\n\t\tif os.path.isdir(path):\n\t\t\tpath = os.path.join(path, \"*\")\n\t\treturn path\n\n\treturn tuple(map(_make_find_path, find_paths))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport os\n\timport glob\n\n\ttransformed_find_paths = []\n\tfor find_path in find_paths:\n\t\tif os.path.isdir(find_path):\n\t\t\ttransformed_find_paths.append(os.path.join(find_path, \"**\"))\n\t\telif os.path.isfile(find_path):\n\t\t\ttransformed_find_paths.append(find_path)\n\t\telif \"*\" in find_path:\n\t\t\ttransformed_find_paths.append(find_path)\n\t\telse:\n\t\t\ttransformed_find_paths.extend(glob.glob(find_path))\n\n\treturn tuple(transformed_find_paths)", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport os\n\tfrom glob import glob\n\tfrom typing import Sequence\n\n\tdef _transform(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn f\"{path}/**/*\"\n\t\telif os.path.isfile(path):\n\t\t\treturn path\n\t\telif os.path.isabs(path):\n\t\t\treturn path\n\t\telse:\n\t\t\treturn f\"{os.getcwd()}/{path}\"\n\n\treturn tuple(_transform(p) for p in find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfrom glob import glob\n\treturn tuple('*' if not '*' in path else path for path in find_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tret = []\n\tfor path in find_paths:\n\t\tif '*' in path:\n\t\t\tret.append(path)\n\t\telse:\n\t\t\tret.append(path + '/**')\n\treturn tuple(ret)", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tdef glob_path(path):\n\t\treturn path if path.startswith('*') else '*{}*'.format(path)\n\treturn tuple(glob_path(path) for path in find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfrom fnmatch import translate\n\n\treturn tuple(translate(p) if '*' in p else p for p in find_paths)", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport glob\n\n\tdef find_paths_to_glob(path):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif '*' in path:\n\t\t\treturn path\n\t\treturn glob.glob(path)\n\n\treturn tuple(find_paths_to_glob(path) for path in find_paths)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ['TERM_PROGRAM'] == 'git-bash'\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('TERM_PROGRAM', '').endswith('gitbash')\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\tif os.environ[\"SHELL\"] == \"/usr/bin/git-bash.exe\":\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Windows'", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.name == 'nt' and 'MSYSTEM' in os.environ\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\timport sys\n\n\tif sys.platform == 'win32':\n\t\tif os.environ.get('MSYSTEM') == 'MINGW32':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('TERM') == 'xterm-256color'\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('TERM_PROGRAM') == 'gitbash'\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('MSYSTEM') == 'MINGW64'", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\tif os.name == 'nt':\n\t\ttry:\n\t\t\timport msvcrt\n\t\t\tif os.environ.get('TERM_PROGRAM') == 'gitbash':\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\telse:\n\t\treturn False"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ['TERM_PROGRAM'] == 'git-bash'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('TERM_PROGRAM', '').endswith('gitbash')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\tif os.environ[\"SHELL\"] == \"/usr/bin/git-bash.exe\":\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Windows'", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.name == 'nt' and 'MSYSTEM' in os.environ\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\timport sys\n\n\tif sys.platform == 'win32':\n\t\tif os.environ.get('MSYSTEM') == 'MINGW32':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('TERM') == 'xterm-256color'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('TERM_PROGRAM') == 'gitbash'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.environ.get('MSYSTEM') == 'MINGW64'", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\tif os.name == 'nt':\n\t\ttry:\n\t\t\timport msvcrt\n\t\t\tif os.environ.get('TERM_PROGRAM') == 'gitbash':\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\telse:\n\t\treturn False", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/generate.py", "project": "witten/borgmatic", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'x') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\texcept FileExistsError:\n\t\tif overwrite is True:\n\t\t\twith open(config_filename, 'w') as config_file:\n\t\t\t\tconfig_file.write(rendered_config)\n\t\telse:\n\t\t\traise FileExistsError(f\"Config file {config_filename} already exists\")\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\tif not overwrite:\n\t\tif os.path.exists(config_filename):\n\t\t\traise Exception(f\"File {config_filename} already exists and will not be overwritten.\")\n\n\twith open(config_filename, \"w\") as config_file:\n\t\tconfig_file.write(rendered_config)\n\tos.chmod(config_filename, mode)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport sys\n\n\ttry:\n\t\tif os.path.exists(config_filename) and not overwrite:\n\t\t\tprint(\"Configuration file {} already exists. Not overwriting.\".format(config_filename))\n\t\t\tsys.exit(1)\n\t\twith open(config_filename, 'w') as config_fd:\n\t\t\tconfig_fd.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\texcept Exception as e:\n\t\tprint(\"Failed to write configuration to {}: {}\".format(config_filename, e))\n\t\tsys.exit(1)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tfrom os.path import exists\n\tfrom os import chmod\n\n\ttry:\n\t\tif exists(config_filename) and not overwrite:\n\t\t\traise IOError(\"File already exists: %s\" % config_filename)\n\t\twith open(config_filename, 'w') as cfg:\n\t\t\tcfg.write(rendered_config)\n\t\tchmod(config_filename, mode)\n\texcept IOError as e:\n\t\tprint(\"Unable to write configuration to %s: %s\" % (config_filename, e))", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tfrom os import path\n\tfrom os import makedirs\n\tfrom os import chmod\n\n\tif path.exists(config_filename):\n\t\tif overwrite:\n\t\t\twith open(config_filename, 'w') as f:\n\t\t\t\tf.write(rendered_config)\n\t\telse:\n\t\t\traise Exception('File %s already exists and overwrite is False.' % config_filename)\n\telse:\n\t\tconfig_dir = path.dirname(config_filename)\n\t\tif config_dir != '' and not path.exists(config_dir):\n\t\t\tmakedirs(config_dir)\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tchmod(config_filename, mode)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tfrom os import path, chmod\n\tfrom sys import exit\n\tfrom shutil import copyfileobj\n\tfrom tempfile import NamedTemporaryFile\n\n\tif not overwrite and path.isfile(config_filename):\n\t\tprint(\"Error: cannot write configuration file, file already exists: %s\" % config_filename)\n\t\texit(1)\n\n\ttemp_file = NamedTemporaryFile(delete=False)\n\ttemp_file.write(rendered_config.encode('utf-8'))\n\ttemp_file.close()\n\n\ttry:\n\t\tcopyfileobj(open(temp_file.name, 'rb'), open(config_filename, 'wb'))\n\t\tchmod(config_filename, mode)\n\tfinally:\n\t\tfrom os import remove\n\t\tremove(temp_file.name)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'r') as f:\n\t\t\texisting_config = f.read()\n\texcept FileNotFoundError:\n\t\texisting_config = None\n\n\tif existing_config == rendered_config and not overwrite:\n\t\tprint(\"Warning: Configuration file already exists and is up to date. Skipping write.\")\n\t\treturn\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\n\tos.chmod(config_filename, mode)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\n\tif not overwrite:\n\t\tif os.path.exists(config_filename):\n\t\t\traise IOError(\"File {} already exists, use overwrite=True to overwrite\".format(config_filename))\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport sys\n\n\ttry:\n\t\tif os.path.exists(config_filename) and overwrite is False:\n\t\t\tsys.exit(\"Error: File already exists. Use --overwrite to allow overwrite.\")\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\t\tf.close()\n\t\tos.chmod(config_filename, mode)\n\texcept IOError as e:\n\t\tsys.exit(\"Error: Could not write to file: %s\" % config_filename)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\texcept IOError as e:\n\t\tprint(\"Could not write to file: \" +\n\t\t\t  config_filename +\n\t\t\t  \"\\n\" +\n\t\t\t  \"Reason: \" +\n\t\t\t  e.strerror)\n\n\tos.chmod(config_filename, mode)"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'x') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\texcept FileExistsError:\n\t\tif overwrite is True:\n\t\t\twith open(config_filename, 'w') as config_file:\n\t\t\t\tconfig_file.write(rendered_config)\n\t\telse:\n\t\t\traise FileExistsError(f\"Config file {config_filename} already exists\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\tif not overwrite:\n\t\tif os.path.exists(config_filename):\n\t\t\traise Exception(f\"File {config_filename} already exists and will not be overwritten.\")\n\n\twith open(config_filename, \"w\") as config_file:\n\t\tconfig_file.write(rendered_config)\n\tos.chmod(config_filename, mode)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport sys\n\n\ttry:\n\t\tif os.path.exists(config_filename) and not overwrite:\n\t\t\tprint(\"Configuration file {} already exists. Not overwriting.\".format(config_filename))\n\t\t\tsys.exit(1)\n\t\twith open(config_filename, 'w') as config_fd:\n\t\t\tconfig_fd.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\texcept Exception as e:\n\t\tprint(\"Failed to write configuration to {}: {}\".format(config_filename, e))\n\t\tsys.exit(1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tfrom os.path import exists\n\tfrom os import chmod\n\n\ttry:\n\t\tif exists(config_filename) and not overwrite:\n\t\t\traise IOError(\"File already exists: %s\" % config_filename)\n\t\twith open(config_filename, 'w') as cfg:\n\t\t\tcfg.write(rendered_config)\n\t\tchmod(config_filename, mode)\n\texcept IOError as e:\n\t\tprint(\"Unable to write configuration to %s: %s\" % (config_filename, e))", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tfrom os import path\n\tfrom os import makedirs\n\tfrom os import chmod\n\n\tif path.exists(config_filename):\n\t\tif overwrite:\n\t\t\twith open(config_filename, 'w') as f:\n\t\t\t\tf.write(rendered_config)\n\t\telse:\n\t\t\traise Exception('File %s already exists and overwrite is False.' % config_filename)\n\telse:\n\t\tconfig_dir = path.dirname(config_filename)\n\t\tif config_dir != '' and not path.exists(config_dir):\n\t\t\tmakedirs(config_dir)\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tchmod(config_filename, mode)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tfrom os import path, chmod\n\tfrom sys import exit\n\tfrom shutil import copyfileobj\n\tfrom tempfile import NamedTemporaryFile\n\n\tif not overwrite and path.isfile(config_filename):\n\t\tprint(\"Error: cannot write configuration file, file already exists: %s\" % config_filename)\n\t\texit(1)\n\n\ttemp_file = NamedTemporaryFile(delete=False)\n\ttemp_file.write(rendered_config.encode('utf-8'))\n\ttemp_file.close()\n\n\ttry:\n\t\tcopyfileobj(open(temp_file.name, 'rb'), open(config_filename, 'wb'))\n\t\tchmod(config_filename, mode)\n\tfinally:\n\t\tfrom os import remove\n\t\tremove(temp_file.name)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'r') as f:\n\t\t\texisting_config = f.read()\n\texcept FileNotFoundError:\n\t\texisting_config = None\n\n\tif existing_config == rendered_config and not overwrite:\n\t\tprint(\"Warning: Configuration file already exists and is up to date. Skipping write.\")\n\t\treturn\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\n\tos.chmod(config_filename, mode)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\n\tif not overwrite:\n\t\tif os.path.exists(config_filename):\n\t\t\traise IOError(\"File {} already exists, use overwrite=True to overwrite\".format(config_filename))\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport sys\n\n\ttry:\n\t\tif os.path.exists(config_filename) and overwrite is False:\n\t\t\tsys.exit(\"Error: File already exists. Use --overwrite to allow overwrite.\")\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\t\tf.close()\n\t\tos.chmod(config_filename, mode)\n\texcept IOError as e:\n\t\tsys.exit(\"Error: Could not write to file: %s\" % config_filename)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\texcept IOError as e:\n\t\tprint(\"Could not write to file: \" +\n\t\t\t  config_filename +\n\t\t\t  \"\\n\" +\n\t\t\t  \"Reason: \" +\n\t\t\t  e.strerror)\n\n\tos.chmod(config_filename, mode)", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/Shell.py", "project": "cloudmesh/cloudmesh-common", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split(\"\\n\")\n\treturn seperator.join(lines)\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.splitlines()\n\tlines = [line.strip() for line in lines]\n\tlines = [line for line in lines if line] # removes empty lines\n\tscript = seperator.join(lines)\n\treturn script\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcmd = script.replace(\"\\n\", seperator).replace(\"\\t\", \" \").replace(\";\", seperator)\n\tif cmd[-len(seperator):] == seperator:\n\t\tcmd = cmd[:-len(seperator)]\n\treturn cmd\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split('\\n')\n\tlines = [line.strip() for line in lines]\n\tlines = [line for line in lines if len(line) > 0]\n\tscript = seperator.join(lines)\n\treturn script\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcommand = \"\"\n\tfor line in script.splitlines():\n\t\tif line.strip() != \"\":\n\t\t\tcommand += line.strip() + seperator\n\treturn command[:-len(seperator)]\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcmd = script.splitlines()\n\n\tcmd = [line for line in cmd if line]\n\n\tcmd = seperator.join(cmd)\n\n\treturn cmd\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcommand = seperator.join(script.split(\"\\n\"))\n\treturn command\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split('\\n')\n\tlines = [line.strip() for line in lines]\n\tlines = filter(lambda x: x, lines)\n\treturn seperator.join(lines)\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tout = \"\"\n\tfor line in script.split(\"\\n\"):\n\t\tline = line.strip()\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\t\tout += \"{seperator}{line}\".format(seperator=seperator, line=line)\n\treturn out.strip()", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tret = []\n\tfor line in script.split(\"\\n\"):\n\t\tline = line.rstrip()\n\t\tif line == \"\":\n\t\t\tcontinue\n\t\tret.append(line)\n\treturn seperator.join(ret)\n\nif __name__ == \"__main__\":\n\timport sys\n\tif len(sys.argv) < 2:\n\t\tprint(\"Usage: python oneline.py <script>\")\n\t\tsys.exit(1)\n\tscript = open(sys.argv[1], \"r\").read()\n\tprint(oneline(script))"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split(\"\\n\")\n\treturn seperator.join(lines)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.splitlines()\n\tlines = [line.strip() for line in lines]\n\tlines = [line for line in lines if line] # removes empty lines\n\tscript = seperator.join(lines)\n\treturn script\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcmd = script.replace(\"\\n\", seperator).replace(\"\\t\", \" \").replace(\";\", seperator)\n\tif cmd[-len(seperator):] == seperator:\n\t\tcmd = cmd[:-len(seperator)]\n\treturn cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split('\\n')\n\tlines = [line.strip() for line in lines]\n\tlines = [line for line in lines if len(line) > 0]\n\tscript = seperator.join(lines)\n\treturn script\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcommand = \"\"\n\tfor line in script.splitlines():\n\t\tif line.strip() != \"\":\n\t\t\tcommand += line.strip() + seperator\n\treturn command[:-len(seperator)]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcmd = script.splitlines()\n\n\tcmd = [line for line in cmd if line]\n\n\tcmd = seperator.join(cmd)\n\n\treturn cmd\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcommand = seperator.join(script.split(\"\\n\"))\n\treturn command\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split('\\n')\n\tlines = [line.strip() for line in lines]\n\tlines = filter(lambda x: x, lines)\n\treturn seperator.join(lines)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tout = \"\"\n\tfor line in script.split(\"\\n\"):\n\t\tline = line.strip()\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\t\tout += \"{seperator}{line}\".format(seperator=seperator, line=line)\n\treturn out.strip()", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tret = []\n\tfor line in script.split(\"\\n\"):\n\t\tline = line.rstrip()\n\t\tif line == \"\":\n\t\t\tcontinue\n\t\tret.append(line)\n\treturn seperator.join(ret)\n\nif __name__ == \"__main__\":\n\timport sys\n\tif len(sys.argv) < 2:\n\t\tprint(\"Usage: python oneline.py <script>\")\n\t\tsys.exit(1)\n\tscript = open(sys.argv[1], \"r\").read()\n\tprint(oneline(script))", "is_pass": false, "return_code": 1}]}
{"file_path": "lib/matplotlib/testing/__init__.py", "project": "rougier/matplotlib", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tfrom subprocess import CompletedProcess\n\timport tempfile\n\timport sys\n\timport os\n\timport shutil\n\timport stat\n\n\tdef get_code_to_write(func_to_exec):\n\t\t\"\"\"\n\t\tGet the code to write to a temp file.\n\t\t\"\"\"\n\t\tcode_to_write = f'{func_to_exec.__name__}('\n\t\tfor a in args:\n\t\t\tcode_to_write += f'{a}, '\n\t\tcode_to_write += ')'\n\t\treturn code_to_write\n\n\tdef write_command_to_file(temp_file, code_to_write):\n\t\t\"\"\"\n\t\tWrite the code to the temp file.\n\t\t\"\"\"\n\t\ttemp_file.write(code_to_write)\n\t\ttemp_file.close()\n\n\tdef get_command_to_run(temp_file):\n\t\t\"\"\"\n\t\tGet the command that will be run in the subprocess.\n\t\t\"\"\"\n\t\tcommand_to_run = [\n\t\t\tsys.executable,\n\t\t\ttemp_file.name,\n\t\t\t'-c'\n\t\t]\n\t\treturn command_to_run\n\n\tdef get_env():\n\t\t\"\"\"\n\t\tGet the environment variables for", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\tcmd = [func.__module__, func.__name__]\n\tcmd.extend(args)\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv = {**env, **extra_env}\n\n\ttry:\n\t\treturn subprocess.run(cmd, env=env, capture_output=True, timeout=timeout)\n\texcept subprocess.TimeoutExpired as e:\n\t\t# Re-raise the exception with the output attached\n\t\te.output = e.process.stdout + e.process.stderr\n\t\traise\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\timport subprocess_helper\n\n\tfunc_module = sys.modules[func.__module__]\n\tfunc_path = getattr(func_module, '__file__')\n\n\tif extra_env is None:\n\t\textra_env = {}\n\n\tenv = extra_env.copy()\n\tenv.update({\n\t\t'PYTHONPATH': os.pathsep.join(sys.path),\n\t})\n\n\tcmd = [sys.executable, func_path] + list(args)\n\n\tsubprocess_helper.debug('Running subprocess: %r' % cmd)\n\tproc = subprocess.run(cmd, env=env, timeout=timeout)\n\n\treturn proc", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\ttry:\n\t\timport importlib\n\texcept ImportError:\n\t\tfrom pip._vendor import importlib\n\n\tdef run_func(module_name, func_name):\n\t\tmodule = importlib.import_module(module_name)\n\t\tfunc = getattr(module, func_name)\n\t\tfunc()\n\n\tcmd = [sys.executable, '-m', f'{func.__module__}.{func.__name__}'] + list(args)\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\treturn subprocess.run(cmd, env=env, timeout=timeout)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\tfrom pathlib import Path\n\n\tpath_to_script = Path(func.__module__)\n\tpath_to_script = path_to_script.parent / path_to_script.name\n\n\targs_to_run = [str(path_to_script)] + list(args)\n\n\tenv = os.environ.copy()\n\tif extra_env is not None:\n\t\tfor key, value in extra_env.items():\n\t\t\tenv[key] = value\n\n\n\tprint(\"Running\", func.__module__)\n\tprint(\"With args\", args_to_run)\n\tprint(\"With env\", env)\n\n\treturn subprocess.run(args_to_run, capture_output=True, text=True, check=True, timeout=timeout, env=env)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport json\n\timport tempfile\n\timport shutil\n\n\tdef get_module_path(module_name):\n\t\t\"\"\"\n\t\tReturns the path to a given module.\n\t\tNote that this is different from ``__file__`` in that it will\n\t\treturn the path to the actual module, not the file that contains\n\t\tthe module.\n\n\t\tArgs:\n\t\t\t\tmodule_name: str, the name of the module.\n\t\tReturns:\n\t\t\t\tstr, the path to the module.\n\t\t\"\"\"\n\t\tmodule = sys.modules.get(module_name)\n\t\tif module is None or not hasattr(module, \"__file__\"):\n\t\t\traise RuntimeError(\"module %r not found\" % module_name)\n\t\treturn os.path.dirname(os.path.abspath(module.__file__))\n\n\tdef get_module_dir_path(module_name):\n\t\t\"\"\"\n\t\tReturns the path to a directory containing a given module.\n\n\t\tArgs:\n\t\t\t\tmodule_name: str, the name of the module.\n\t\tReturns:\n\t\t\t\tstr, the path to the module's directory.\n\t\t\"\"\"\n\t\tmodule_dir_path = os.path.", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\timport sys\n\n\tmodule = sys.modules[func.__module__]\n\tfull_func_name = module.__name__ + '.' + func.__name__\n\n\tcmd = [sys.executable, \"-m\", full_func_name]\n\tcmd.extend(args)\n\tprint(cmd)\n\n\tenv = os.environ\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\treturn subprocess.run(cmd,\n\t                      env=env,\n\t                      timeout=timeout,\n\t                      capture_output=True,\n\t                      text=True)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport json\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\targv = ['python', '-m', func.__module__]\n\targv.extend(args)\n\targv = json.dumps(argv)\n\tcmd = ['python', '-m', 'lizard_ext.subprocess_run_helper', argv, env]\n\ttry:\n\t\tsubprocess.run(cmd, timeout=timeout, check=True)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(e.output.decode('utf-8'), file=sys.stderr)\n\t\traise\n\n\nif __name__ == '__main__':\n\timport sys\n\timport subprocess\n\timport os\n\timport json\n\n\tenv = os.environ.copy()\n\targv = sys.argv[1]\n\targv = json.loads(argv)\n\tenv.update(json.loads(sys.argv[2]))\n\tsubprocess.run(argv, env=env)", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport logging\n\n\tlogger = logging.getLogger(__name__)\n\n\tmodule_name, sep, func_name = func.__module__.partition('.')\n\tfile_name = module_name + '.py'\n\tif not os.path.exists(file_name):\n\t\tlogger.warning(f\"File \\\"{file_name}\\\" does not exist. Skipping.\")\n\t\treturn\n\n\targs = [sys.executable, file_name, func_name] + list(args)\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\tlogger.info(f\"Running {func_name} with args {args} in {file_name}\")\n\n\tp = subprocess.run(args, env=env, timeout=timeout)\n\n\tif p.returncode:\n\t\tlogger.error(f\"Failed to run {func_name} with args {args} in {file_name}\")\n\telse:\n\t\tlogger.info(f\"Successfully ran {func_name} with args {args} in {file_name}\")\n\n\treturn p", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport threading\n\n\timport io\n\n\ttry:\n\t\tfrom queue import Queue, Empty\n\texcept ImportError:\n\t\tfrom Queue import Queue, Empty\n\n\tdef target(q, args):\n\t\tret = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=extra_env, universal_newlines=True)\n\t\tq.put(ret)\n\n\tq = Queue()\n\tt = threading.Thread(target=target, args=(q, args))\n\tt.start()\n\tt.join(timeout)\n\tif t.is_alive():\n\t\tt.terminate()\n\t\tt.join()\n\t\traise subprocess.TimeoutExpired(args, timeout)\n\ttry:\n\t\tret = q.get(block=False)\n\texcept Empty:\n\t\traise subprocess.TimeoutExpired(args, timeout)\n\n\treturn ret\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tfrom subprocess import CompletedProcess\n\timport tempfile\n\timport sys\n\timport os\n\timport shutil\n\timport stat\n\n\tdef get_code_to_write(func_to_exec):\n\t\t\"\"\"\n\t\tGet the code to write to a temp file.\n\t\t\"\"\"\n\t\tcode_to_write = f'{func_to_exec.__name__}('\n\t\tfor a in args:\n\t\t\tcode_to_write += f'{a}, '\n\t\tcode_to_write += ')'\n\t\treturn code_to_write\n\n\tdef write_command_to_file(temp_file, code_to_write):\n\t\t\"\"\"\n\t\tWrite the code to the temp file.\n\t\t\"\"\"\n\t\ttemp_file.write(code_to_write)\n\t\ttemp_file.close()\n\n\tdef get_command_to_run(temp_file):\n\t\t\"\"\"\n\t\tGet the command that will be run in the subprocess.\n\t\t\"\"\"\n\t\tcommand_to_run = [\n\t\t\tsys.executable,\n\t\t\ttemp_file.name,\n\t\t\t'-c'\n\t\t]\n\t\treturn command_to_run\n\n\tdef get_env():\n\t\t\"\"\"\n\t\tGet the environment variables for", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\tcmd = [func.__module__, func.__name__]\n\tcmd.extend(args)\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv = {**env, **extra_env}\n\n\ttry:\n\t\treturn subprocess.run(cmd, env=env, capture_output=True, timeout=timeout)\n\texcept subprocess.TimeoutExpired as e:\n\t\t# Re-raise the exception with the output attached\n\t\te.output = e.process.stdout + e.process.stderr\n\t\traise\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\timport subprocess_helper\n\n\tfunc_module = sys.modules[func.__module__]\n\tfunc_path = getattr(func_module, '__file__')\n\n\tif extra_env is None:\n\t\textra_env = {}\n\n\tenv = extra_env.copy()\n\tenv.update({\n\t\t'PYTHONPATH': os.pathsep.join(sys.path),\n\t})\n\n\tcmd = [sys.executable, func_path] + list(args)\n\n\tsubprocess_helper.debug('Running subprocess: %r' % cmd)\n\tproc = subprocess.run(cmd, env=env, timeout=timeout)\n\n\treturn proc", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\ttry:\n\t\timport importlib\n\texcept ImportError:\n\t\tfrom pip._vendor import importlib\n\n\tdef run_func(module_name, func_name):\n\t\tmodule = importlib.import_module(module_name)\n\t\tfunc = getattr(module, func_name)\n\t\tfunc()\n\n\tcmd = [sys.executable, '-m', f'{func.__module__}.{func.__name__}'] + list(args)\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\treturn subprocess.run(cmd, env=env, timeout=timeout)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\tfrom pathlib import Path\n\n\tpath_to_script = Path(func.__module__)\n\tpath_to_script = path_to_script.parent / path_to_script.name\n\n\targs_to_run = [str(path_to_script)] + list(args)\n\n\tenv = os.environ.copy()\n\tif extra_env is not None:\n\t\tfor key, value in extra_env.items():\n\t\t\tenv[key] = value\n\n\n\tprint(\"Running\", func.__module__)\n\tprint(\"With args\", args_to_run)\n\tprint(\"With env\", env)\n\n\treturn subprocess.run(args_to_run, capture_output=True, text=True, check=True, timeout=timeout, env=env)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport json\n\timport tempfile\n\timport shutil\n\n\tdef get_module_path(module_name):\n\t\t\"\"\"\n\t\tReturns the path to a given module.\n\t\tNote that this is different from ``__file__`` in that it will\n\t\treturn the path to the actual module, not the file that contains\n\t\tthe module.\n\n\t\tArgs:\n\t\t\t\tmodule_name: str, the name of the module.\n\t\tReturns:\n\t\t\t\tstr, the path to the module.\n\t\t\"\"\"\n\t\tmodule = sys.modules.get(module_name)\n\t\tif module is None or not hasattr(module, \"__file__\"):\n\t\t\traise RuntimeError(\"module %r not found\" % module_name)\n\t\treturn os.path.dirname(os.path.abspath(module.__file__))\n\n\tdef get_module_dir_path(module_name):\n\t\t\"\"\"\n\t\tReturns the path to a directory containing a given module.\n\n\t\tArgs:\n\t\t\t\tmodule_name: str, the name of the module.\n\t\tReturns:\n\t\t\t\tstr, the path to the module's directory.\n\t\t\"\"\"\n\t\tmodule_dir_path = os.path.", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\n\timport sys\n\n\tmodule = sys.modules[func.__module__]\n\tfull_func_name = module.__name__ + '.' + func.__name__\n\n\tcmd = [sys.executable, \"-m\", full_func_name]\n\tcmd.extend(args)\n\tprint(cmd)\n\n\tenv = os.environ\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\treturn subprocess.run(cmd,\n\t                      env=env,\n\t                      timeout=timeout,\n\t                      capture_output=True,\n\t                      text=True)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport json\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\targv = ['python', '-m', func.__module__]\n\targv.extend(args)\n\targv = json.dumps(argv)\n\tcmd = ['python', '-m', 'lizard_ext.subprocess_run_helper', argv, env]\n\ttry:\n\t\tsubprocess.run(cmd, timeout=timeout, check=True)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(e.output.decode('utf-8'), file=sys.stderr)\n\t\traise\n\n\nif __name__ == '__main__':\n\timport sys\n\timport subprocess\n\timport os\n\timport json\n\n\tenv = os.environ.copy()\n\targv = sys.argv[1]\n\targv = json.loads(argv)\n\tenv.update(json.loads(sys.argv[2]))\n\tsubprocess.run(argv, env=env)", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport logging\n\n\tlogger = logging.getLogger(__name__)\n\n\tmodule_name, sep, func_name = func.__module__.partition('.')\n\tfile_name = module_name + '.py'\n\tif not os.path.exists(file_name):\n\t\tlogger.warning(f\"File \\\"{file_name}\\\" does not exist. Skipping.\")\n\t\treturn\n\n\targs = [sys.executable, file_name, func_name] + list(args)\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\tlogger.info(f\"Running {func_name} with args {args} in {file_name}\")\n\n\tp = subprocess.run(args, env=env, timeout=timeout)\n\n\tif p.returncode:\n\t\tlogger.error(f\"Failed to run {func_name} with args {args} in {file_name}\")\n\telse:\n\t\tlogger.info(f\"Successfully ran {func_name} with args {args} in {file_name}\")\n\n\treturn p", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport threading\n\n\timport io\n\n\ttry:\n\t\tfrom queue import Queue, Empty\n\texcept ImportError:\n\t\tfrom Queue import Queue, Empty\n\n\tdef target(q, args):\n\t\tret = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=extra_env, universal_newlines=True)\n\t\tq.put(ret)\n\n\tq = Queue()\n\tt = threading.Thread(target=target, args=(q, args))\n\tt.start()\n\tt.join(timeout)\n\tif t.is_alive():\n\t\tt.terminate()\n\t\tt.join()\n\t\traise subprocess.TimeoutExpired(args, timeout)\n\ttry:\n\t\tret = q.get(block=False)\n\texcept Empty:\n\t\traise subprocess.TimeoutExpired(args, timeout)\n\n\treturn ret\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/override.py", "project": "witten/borgmatic", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvariable = matcher.group(\"variable\")\n\tdefault = matcher.group(\"default\")\n\n\tvalue = os.environ.get(variable, default)\n\tif value is None:\n\t\traise ValueError(f\"{variable} not found in environment\")\n\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = matcher.groupdict()['value']\n\ttry:\n\t\treturn str(os.environ[value])\n\texcept KeyError:\n\t\tif matcher.groupdict().get('default'):\n\t\t\treturn matcher.groupdict()['default']\n\t\telse:\n\t\t\traise ValueError('No variable defined for: {}'.format(matcher.groupdict()['value']))\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault_value = matcher.group('default_value')\n\treturn os.getenv(name, default_value)\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname, default = matcher.groupdict()[\"name\"], matcher.groupdict().get(\"default\")\n\tvalue = os.environ.get(name, default)\n\tif value is None:\n\t\traise ValueError(f\"Environment variable '{name}' not defined\")\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = str(matcher.group(1))\n\tdefault = matcher.group(2)\n\n\tif default:\n\t\tdefault = str(default)\n\n\tvalue = os.environ.get(name, default)\n\n\tif value is None:\n\t\traise Exception(\"Environment variable '%s' was not defined\" % name)\n\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname, default = matcher.groupdict().values()\n\treturn os.environ.get(name, default)\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group(\"name\")\n\tdefault = matcher.group(\"default\")\n\tvalue = os.getenv(name)\n\tif value is None and default is None:\n\t\traise Exception(\"Environment variable {} not set.\".format(name))\n\treturn value if value is not None else default\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname, default = matcher.groupdict()[\"name\"], matcher.groupdict()[\"default\"]\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise ValueError(f\"Missing required environment variable {name}\")\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\n\tif default is None:\n\t\treturn os.environ[name]\n\telse:\n\t\treturn os.environ.get(name, default)\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tfull_string = matcher.group()\n\tvariable_name = matcher.group(1)\n\tdefault_value = matcher.group(2)\n\n\tvariable_value = os.environ.get(variable_name)\n\tif variable_value is None:\n\t\tif default_value is None:\n\t\t\traise Exception(f\"Environment variable {variable_name} is not defined\")\n\t\treturn default_value\n\treturn variable_value\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvariable = matcher.group(\"variable\")\n\tdefault = matcher.group(\"default\")\n\n\tvalue = os.environ.get(variable, default)\n\tif value is None:\n\t\traise ValueError(f\"{variable} not found in environment\")\n\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = matcher.groupdict()['value']\n\ttry:\n\t\treturn str(os.environ[value])\n\texcept KeyError:\n\t\tif matcher.groupdict().get('default'):\n\t\t\treturn matcher.groupdict()['default']\n\t\telse:\n\t\t\traise ValueError('No variable defined for: {}'.format(matcher.groupdict()['value']))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault_value = matcher.group('default_value')\n\treturn os.getenv(name, default_value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname, default = matcher.groupdict()[\"name\"], matcher.groupdict().get(\"default\")\n\tvalue = os.environ.get(name, default)\n\tif value is None:\n\t\traise ValueError(f\"Environment variable '{name}' not defined\")\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = str(matcher.group(1))\n\tdefault = matcher.group(2)\n\n\tif default:\n\t\tdefault = str(default)\n\n\tvalue = os.environ.get(name, default)\n\n\tif value is None:\n\t\traise Exception(\"Environment variable '%s' was not defined\" % name)\n\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname, default = matcher.groupdict().values()\n\treturn os.environ.get(name, default)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group(\"name\")\n\tdefault = matcher.group(\"default\")\n\tvalue = os.getenv(name)\n\tif value is None and default is None:\n\t\traise Exception(\"Environment variable {} not set.\".format(name))\n\treturn value if value is not None else default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname, default = matcher.groupdict()[\"name\"], matcher.groupdict()[\"default\"]\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise ValueError(f\"Missing required environment variable {name}\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\n\tif default is None:\n\t\treturn os.environ[name]\n\telse:\n\t\treturn os.environ.get(name, default)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tfull_string = matcher.group()\n\tvariable_name = matcher.group(1)\n\tdefault_value = matcher.group(2)\n\n\tvariable_value = os.environ.get(variable_name)\n\tif variable_value is None:\n\t\tif default_value is None:\n\t\t\traise Exception(f\"Environment variable {variable_name} is not defined\")\n\t\treturn default_value\n\treturn variable_value\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cinder/image/glance.py", "project": "openstack/cinder", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tsplit = image_href.split('/')\n\n\tif len(split) < 5:\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[0] != '':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[1] != 'api':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[2] != 'v2' and split[2] != 'v2.1':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[3] != 'accounts':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\taccount_id = split[4]\n\n\tnetloc = '.'.join(split[5:7])\n\n\tuse_ssl = split[5] == 'ssl'\n\n\treturn account_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timport urllib.parse\n\n\turl = urllib.parse.urlparse(image_href)\n\tif url.scheme in ('http', 'https'):\n\t\treturn url.path.split('/')[-1], url.netloc, url.scheme == 'https'\n\telif url.scheme in ('file', ''):\n\t\timage_id = url.path.split('/')[-1]\n\t\tif url.path.split('/')[-2] == 'images':\n\t\t\treturn image_id, '', False\n\t\telse:\n\t\t\treturn image_id, '', False\n\n\traise ValueError('Unsupported image href: {}'.format(image_href))", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\turl = urllib.parse.urlparse(image_href)\n\tif not url.hostname:\n\t\traise ValueError(\"image_href didn't contain hostname: {}\".format(image_href))\n\timage_id = url.path.split('/')[2]\n\treturn image_id, url.hostname, url.scheme == 'https'\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\turl = urllib.parse.urlparse(image_href)\n\tif not url.path.startswith('/v1/'):\n\t\traise ValueError('The image_href must be a versioned '\n\t\t\t\t\t\t 'image url.')\n\timage_id = url.path.split('/')[3]\n\tif image_id == '':\n\t\traise ValueError('The image_href must be a versioned '\n\t\t\t\t\t\t 'image url.')\n\n\tnetloc = url.netloc\n\tif url.scheme == 'https':\n\t\tuse_ssl = True\n\telif url.scheme == 'http':\n\t\tuse_ssl = False\n\telse:\n\t\traise ValueError('The image_href must use http or https.')\n\n\treturn image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\tif parts.scheme not in [\"\", \"http\", \"https\"]:\n\t\traise ValueError(\n\t\t\tf\"Unsupported scheme {parts.scheme}, only http and https supported\"\n\t\t)\n\n\timage_id = parts.path.split(\"/\")[-1]\n\n\treturn (image_id, parts.netloc, parts.scheme == \"https\")", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed = urlparse(image_href)\n\timage_id = parsed.path.split(\"/\")[-1]\n\tif not image_id:\n\t\traise ValueError(\"Invalid image href: {}\".format(image_href))\n\n\treturn (image_id, parsed.netloc, parsed.scheme == \"https\")\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\n\tparts_dict = {\n\t\t'scheme': parts.scheme or None,\n\t\t'netloc': parts.netloc or None,\n\t\t'path': parts.path or None,\n\t}\n\n\tmissing = [k for k, v in parts_dict.items() if v is None]\n\tif missing:\n\t\traise ValueError(\n\t\t\t'image href is missing required components: {0}'.format(', '.join(missing)),\n\t\t)\n\n\timage_id = parts_dict['path'].split('/')[-1]\n\tnetloc = parts_dict['netloc']\n\n\tif parts.scheme == 'http':\n\t\treturn image_id, netloc, False\n\telif parts.scheme == 'https':\n\t\treturn image_id, netloc, True\n\telse:\n\t\traise ValueError('unsupported scheme: {0}'.format(parts_dict['scheme']))", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\turl = urllib.parse.urlparse(image_href)\n\timage_id = url.path.rsplit('/', 1)[1]\n\tuse_ssl = url.scheme == 'https'\n\tnetloc = url.netloc\n\n\tif not image_id:\n\t\traise ValueError(f'No image ID found in {image_href}')\n\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\tif not parts.netloc:\n\t\traise ValueError('Invalid image href: %s' % image_href)\n\n\tnetloc = parts.netloc\n\tif parts.scheme:\n\t\tuse_ssl = parts.scheme == 'https'\n\telse:\n\t\tuse_ssl = netloc.startswith('secure.')\n\n\timage_id = parts.path.strip('/')\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timport urllib.parse\n\n\tparsed = urllib.parse.urlparse(image_href)\n\tif parsed.scheme not in ('http', 'https'):\n\t\traise ValueError(\"Invalid image href %s, cannot determine image ID\" % image_href)\n\tpath = parsed.path.strip('/')\n\timage_id = path.rsplit('/', 2)[-1]\n\tif parsed.scheme == 'http' and parsed.port == 80:\n\t\tnetloc = parsed.netloc\n\t\tuse_ssl = False\n\telif parsed.scheme == 'https' and parsed.port == 443:\n\t\tnetloc = parsed.netloc\n\t\tuse_ssl = True\n\telse:\n\t\traise ValueError(\"Invalid image href %s, cannot determine image ID\" % image_href)\n\treturn image_id, netloc, use_ssl"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tsplit = image_href.split('/')\n\n\tif len(split) < 5:\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[0] != '':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[1] != 'api':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[2] != 'v2' and split[2] != 'v2.1':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\tif split[3] != 'accounts':\n\t\traise ValueError(f'Invalid image href: {image_href}')\n\n\taccount_id = split[4]\n\n\tnetloc = '.'.join(split[5:7])\n\n\tuse_ssl = split[5] == 'ssl'\n\n\treturn account_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timport urllib.parse\n\n\turl = urllib.parse.urlparse(image_href)\n\tif url.scheme in ('http', 'https'):\n\t\treturn url.path.split('/')[-1], url.netloc, url.scheme == 'https'\n\telif url.scheme in ('file', ''):\n\t\timage_id = url.path.split('/')[-1]\n\t\tif url.path.split('/')[-2] == 'images':\n\t\t\treturn image_id, '', False\n\t\telse:\n\t\t\treturn image_id, '', False\n\n\traise ValueError('Unsupported image href: {}'.format(image_href))", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\turl = urllib.parse.urlparse(image_href)\n\tif not url.hostname:\n\t\traise ValueError(\"image_href didn't contain hostname: {}\".format(image_href))\n\timage_id = url.path.split('/')[2]\n\treturn image_id, url.hostname, url.scheme == 'https'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\turl = urllib.parse.urlparse(image_href)\n\tif not url.path.startswith('/v1/'):\n\t\traise ValueError('The image_href must be a versioned '\n\t\t\t\t\t\t 'image url.')\n\timage_id = url.path.split('/')[3]\n\tif image_id == '':\n\t\traise ValueError('The image_href must be a versioned '\n\t\t\t\t\t\t 'image url.')\n\n\tnetloc = url.netloc\n\tif url.scheme == 'https':\n\t\tuse_ssl = True\n\telif url.scheme == 'http':\n\t\tuse_ssl = False\n\telse:\n\t\traise ValueError('The image_href must use http or https.')\n\n\treturn image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\tif parts.scheme not in [\"\", \"http\", \"https\"]:\n\t\traise ValueError(\n\t\t\tf\"Unsupported scheme {parts.scheme}, only http and https supported\"\n\t\t)\n\n\timage_id = parts.path.split(\"/\")[-1]\n\n\treturn (image_id, parts.netloc, parts.scheme == \"https\")", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed = urlparse(image_href)\n\timage_id = parsed.path.split(\"/\")[-1]\n\tif not image_id:\n\t\traise ValueError(\"Invalid image href: {}\".format(image_href))\n\n\treturn (image_id, parsed.netloc, parsed.scheme == \"https\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\n\tparts_dict = {\n\t\t'scheme': parts.scheme or None,\n\t\t'netloc': parts.netloc or None,\n\t\t'path': parts.path or None,\n\t}\n\n\tmissing = [k for k, v in parts_dict.items() if v is None]\n\tif missing:\n\t\traise ValueError(\n\t\t\t'image href is missing required components: {0}'.format(', '.join(missing)),\n\t\t)\n\n\timage_id = parts_dict['path'].split('/')[-1]\n\tnetloc = parts_dict['netloc']\n\n\tif parts.scheme == 'http':\n\t\treturn image_id, netloc, False\n\telif parts.scheme == 'https':\n\t\treturn image_id, netloc, True\n\telse:\n\t\traise ValueError('unsupported scheme: {0}'.format(parts_dict['scheme']))", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\turl = urllib.parse.urlparse(image_href)\n\timage_id = url.path.rsplit('/', 1)[1]\n\tuse_ssl = url.scheme == 'https'\n\tnetloc = url.netloc\n\n\tif not image_id:\n\t\traise ValueError(f'No image ID found in {image_href}')\n\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\tif not parts.netloc:\n\t\traise ValueError('Invalid image href: %s' % image_href)\n\n\tnetloc = parts.netloc\n\tif parts.scheme:\n\t\tuse_ssl = parts.scheme == 'https'\n\telse:\n\t\tuse_ssl = netloc.startswith('secure.')\n\n\timage_id = parts.path.strip('/')\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timport urllib.parse\n\n\tparsed = urllib.parse.urlparse(image_href)\n\tif parsed.scheme not in ('http', 'https'):\n\t\traise ValueError(\"Invalid image href %s, cannot determine image ID\" % image_href)\n\tpath = parsed.path.strip('/')\n\timage_id = path.rsplit('/', 2)[-1]\n\tif parsed.scheme == 'http' and parsed.port == 80:\n\t\tnetloc = parsed.netloc\n\t\tuse_ssl = False\n\telif parsed.scheme == 'https' and parsed.port == 443:\n\t\tnetloc = parsed.netloc\n\t\tuse_ssl = True\n\telse:\n\t\traise ValueError(\"Invalid image href %s, cannot determine image ID\" % image_href)\n\treturn image_id, netloc, use_ssl", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\toutput_list.append(string[:-1])\n\t\telse:\n\t\t\toutput_list.append(string)\n\t\n\treturn output_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tstring = string[:-1]\n\t\toutput_list.append(string)\n\t\n\treturn output_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor item in input_list:\n\t\toutput_list.append(item.rstrip('/'))\n\treturn output_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\toutput_list = []\n\t\tfor input_string in input_list:\n\t\t\tif input_string is not None and len(input_string) > 1:\n\t\t\t\toutput_list.append(input_string.rstrip(os.sep))\n\t\t\telse:\n\t\t\t\toutput_list.append(input_string)\n\t\treturn output_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\tfor i in input_list:\n\t\tif len(i) > 1 and i[-1] == os.sep:\n\t\t\toutput_list.append(i[:-1])\n\t\telse:\n\t\t\toutput_list.append(i)\n\treturn output_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\tresult_list = []\n\tfor path in input_list:\n\t\tif len(path) > 1:\n\t\t\tresult_list.append(path.rstrip(os.sep))\n\t\telse:\n\t\t\tresult_list.append(path)\n\treturn result_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\tnew_list = []\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tnew_list.append(string[:-1])\n\t\telse:\n\t\t\tnew_list.append(string)\n\t\t\t\n\treturn new_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tresult = []\n\tif input_list is None:\n\t\treturn result\n\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\titem = item[:-1]\n\t\tresult.append(item)\n\n\treturn result", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\tresult = []\n\t\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tresult.append(string[:-1])\n\t\telse:\n\t\t\tresult.append(string)\n\treturn result\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor element in input_list:\n\t\tif len(element) > 1 and element[-1] == os.sep:\n\t\t\toutput_list.append(element[:-1])\n\t\telse:\n\t\t\toutput_list.append(element)\n\t\n\treturn output_list"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\toutput_list.append(string[:-1])\n\t\telse:\n\t\t\toutput_list.append(string)\n\t\n\treturn output_list", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tstring = string[:-1]\n\t\toutput_list.append(string)\n\t\n\treturn output_list", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor item in input_list:\n\t\toutput_list.append(item.rstrip('/'))\n\treturn output_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\toutput_list = []\n\t\tfor input_string in input_list:\n\t\t\tif input_string is not None and len(input_string) > 1:\n\t\t\t\toutput_list.append(input_string.rstrip(os.sep))\n\t\t\telse:\n\t\t\t\toutput_list.append(input_string)\n\t\treturn output_list", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\tfor i in input_list:\n\t\tif len(i) > 1 and i[-1] == os.sep:\n\t\t\toutput_list.append(i[:-1])\n\t\telse:\n\t\t\toutput_list.append(i)\n\treturn output_list", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\tresult_list = []\n\tfor path in input_list:\n\t\tif len(path) > 1:\n\t\t\tresult_list.append(path.rstrip(os.sep))\n\t\telse:\n\t\t\tresult_list.append(path)\n\treturn result_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\tnew_list = []\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tnew_list.append(string[:-1])\n\t\telse:\n\t\t\tnew_list.append(string)\n\t\t\t\n\treturn new_list", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tresult = []\n\tif input_list is None:\n\t\treturn result\n\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\titem = item[:-1]\n\t\tresult.append(item)\n\n\treturn result", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\tresult = []\n\t\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tresult.append(string[:-1])\n\t\telse:\n\t\t\tresult.append(string)\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor element in input_list:\n\t\tif len(element) > 1 and element[-1] == os.sep:\n\t\t\toutput_list.append(element[:-1])\n\t\telse:\n\t\t\toutput_list.append(element)\n\t\n\treturn output_list", "is_pass": true, "return_code": 0}]}
{"file_path": "shconfparser/search.py", "project": "kirankotari/shconfparser", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(r\"%s\" % pattern)\n\treturn pattern", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif type(pattern) == str:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern)\n\telif not isinstance(pattern, re._pattern_type):\n\t\traise ValueError('pattern must be either string or pattern object')\n\n\tif strip:\n\t\tpattern = re.compile(f\"^\\\\s*{pattern}\\\\s*$\")\n\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\treturn pattern", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = re.compile(pattern.strip())\n\t\telse:\n\t\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, re._pattern_type):\n\t\treturn pattern\n\telse:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\t\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, re._pattern_type):\n\t\treturn pattern\n\telif isinstance(pattern, str):\n\t\tif strip:\n\t\t\treturn re.compile(r'^\\s*'+pattern+r'\\s*$')\n\t\telse:\n\t\t\treturn re.compile(pattern)\n\telse:\n\t\traise TypeError(\"pattern should be str or regex pattern object\")\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tpattern = pattern.strip() if strip else pattern\n\telse:\n\t\tpattern = pattern.pattern.strip() if strip else pattern.pattern\n\treturn re.compile(pattern)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(r\"%s\" % pattern)\n\treturn pattern", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif type(pattern) == str:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern)\n\telif not isinstance(pattern, re._pattern_type):\n\t\traise ValueError('pattern must be either string or pattern object')\n\n\tif strip:\n\t\tpattern = re.compile(f\"^\\\\s*{pattern}\\\\s*$\")\n\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\treturn pattern", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = re.compile(pattern.strip())\n\t\telse:\n\t\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, re._pattern_type):\n\t\treturn pattern\n\telse:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\t\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, re._pattern_type):\n\t\treturn pattern\n\telif isinstance(pattern, str):\n\t\tif strip:\n\t\t\treturn re.compile(r'^\\s*'+pattern+r'\\s*$')\n\t\telse:\n\t\t\treturn re.compile(pattern)\n\telse:\n\t\traise TypeError(\"pattern should be str or regex pattern object\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif isinstance(pattern, str):\n\t\tpattern = pattern.strip() if strip else pattern\n\telse:\n\t\tpattern = pattern.pattern.strip() if strip else pattern.pattern\n\treturn re.compile(pattern)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/core.py", "project": "burgerbecky/makeprojects", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\ttry:\n\t\tif isinstance(commands, list):\n\t\t\tcmd = commands[0]\n\t\telse:\n\t\t\tcmd = commands\n\n\t\tif verbose:\n\t\t\tprint(\"Running: %s %s\" % (cmd, args))\n\n\t\tresult = subprocess.run([cmd] + args, cwd=cwd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n\t\tif result.stderr and not hide_stderr:\n\t\t\tsys.stderr.write(result.stderr.decode(\"utf-8\"))\n\n\t\tif not result.stdout:\n\t\t\tresult.stdout = \"\"\n\n\t\treturn (result.stdout.decode(\"utf-8\"), result.returncode)\n\texcept subprocess.CalledProcessError as err:\n\t\tsys.stderr.write(\"Command %s %s failed with error code %s. Output was: %s\" % (cmd, args, err.returncode, err.output))\n\t\treturn (err.output, err.returncode)\n\texcept:\n\t\tsys.stderr.write(\"Command %s %s failed with unknown error\" % (cmd, args))\n\t\traise\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\tdef get_command_string(commands, args):\n\t\t\"\"\"\n\t\tgiven a command (ex: [\"mycommand\", \"myargument\"]) and a list of arguments (ex: [\"myargument2\", \"myargument3\"]), return the command as a string\n\t\t\"\"\"\n\t\tcommand_string = \"\"\n\t\tfor command in commands:\n\t\t\tcommand_string += command + \" \"\n\t\tcommand_string += \" \".join(args)\n\t\treturn command_string\n\n\tif verbose:\n\t\tprint(\"running command: \" + get_command_string(commands, args))\n\n\tret = subprocess.run(\n\t\tget_command_string(commands, args),\n\t\tcapture_output=True,\n\t\tcwd=cwd,\n\t\tshell=True,\n\t\tenv=env\n\t)\n\tif verbose:\n\t\tif ret.returncode == 0:\n\t\t\tif ret.stderr:\n\t\t\t\tprint(\"stdout : \" + ret.stdout.decode(\"utf-8\"))\n\t\t\t\tprint(\"stderr : \" + ret.stderr.decode(\"utf-8\"))\n\t\t\telse:\n\t\t\t\tprint(\"stdout : \" + ret.stdout.decode(\"utf-8\"))\n\t\telse:\n\t\t\t", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\ttry:\n\t\tprocess = subprocess.Popen(commands + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\texcept OSError as e:\n\t\tsys.stderr.write(\"ERROR: The command %s cannot be found\\n\" % commands[0])\n\t\tsys.exit(1)\n\n\tstdout, stderr = process.communicate()\n\n\tif verbose:\n\t\tsys.stdout.write(stdout)\n\n\tif stderr and not hide_stderr:\n\t\tsys.stderr.write(stderr)\n\n\treturn stdout, process.returncode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tcmd = ' '.join(commands) + ' ' + ' '.join(args)\n\n\ttry:\n\t\tif verbose:\n\t\t\tprint(cmd)\n\n\t\tproc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\tout, err = proc.communicate()\n\n\t\tif verbose:\n\t\t\tprint(out)\n\t\tif err and not hide_stderr:\n\t\t\tprint(err)\n\t\t\tsys.exit(1)\n\n\t\treturn out, proc.returncode\n\texcept OSError as e:\n\t\tprint(\"ERROR: Failed to run \\'{}\\'\".format(cmd))\n\t\tprint(e)\n\t\tsys.exit(1)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\tif type(commands) is not list:\n\t\tcommands = [commands]\n\n\tprocesses = []\n\thas_error = False\n\n\tfor command in commands:\n\t\ttry:\n\t\t\tprocess = subprocess.Popen(\n\t\t\t\t[command] + args,\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env\n\t\t\t)\n\t\t\tprocesses.append(process)\n\n\t\t\tif verbose:\n\t\t\t\tprint(command + ' ' + ' '.join(args))\n\n\t\texcept OSError as e:\n\t\t\thas_error = True\n\t\t\tprint(e)\n\n\tif has_error:\n\t\tsys.exit(1)\n\n\tretval = None\n\tout = b''\n\tfor process in processes:\n\t\tout, err = process.communicate()\n\t\treturncode = process.returncode\n\t\tif returncode != 0:\n\t\t\thas_error = True\n\t\tif retval is None:\n\t\t\tretval = returncode\n\t\tout += err\n\n\tif hide_stderr:\n\t\tout = b''.join(\n\t\t\tline for line in out.split(b", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\t\n\tstdout = subprocess.PIPE\n\tstderr = subprocess.PIPE\n\t\n\tif hide_stderr:\n\t\tstderr = subprocess.PIPE\n\telse:\n\t\tstderr = subprocess.STDOUT\n\t\n\tprocess_result = subprocess.run(\n\t\tcommands,\n\t\targs = args,\n\t\tstdout = stdout,\n\t\tstderr = stderr,\n\t\tcwd = cwd,\n\t\tenv = env,\n\t\tuniversal_newlines = True,\n\t\tcheck = False\n\t)\n\t\n\tif verbose:\n\t\tprint(process_result.stdout)\n\t\n\treturn process_result\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport os\n\t\n\n\tdef run_command_with_shell(command, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\t\"\"\"\n\t\tRun a command with arguments in a shell using subprocess. Return the running result (stdout and returncode)\n\t\t\"\"\"\n\t\tif verbose:\n\t\t\tprint(\"Running command: \" + \" \".join([command] + args))\n\t\tprocess = subprocess.Popen(\n\t\t\t[command] + args,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE if not hide_stderr else open('/dev/null', 'w'),\n\t\t\tshell=True,\n\t\t\tcwd=cwd,\n\t\t\tenv=env\n\t\t)\n\t\tstdout, stderr = process.communicate()\n\t\treturn stdout.decode(errors='ignore'), stderr.decode(errors='ignore'), process.returncode\n\n\tdef run_command_without_shell(command, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\t\"\"\"\n\t\tRun a command with arguments using subprocess. Return the running result (stdout and returncode)\n\t\t\"\"\"\n\t\tif", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\tcommand = commands + ' ' + args\n\tif verbose:\n\t\tprint(command)\n\tprocess = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tout, err = process.communicate()\n\tretcode = process.returncode\n\n\tif err and not hide_stderr:\n\t\tprint(err)\n\n\tif verbose:\n\t\tprint(out)\n\treturn out, retcode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\n\timport subprocess\n\t\n\targs = [command] + args\n\tif verbose:\n\t\tprint(\"Running: %s\" % \" \".join(args))\n\t\n\ttry:\n\t\tprocess = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\tprocess.wait()\n\t\tstdout, stderr = process.communicate()\n\texcept Exception as e:\n\t\terror(\"Failed to run %s (%s)\" % (args, e))\n\t\tstdout = \"\"\n\t\tstderr = \"\"\n\t\n\tif hide_stderr:\n\t\tstderr = \"\"\n\t\n\tif verbose:\n\t\tif stdout:\n\t\t\tprint(\"Output:\")\n\t\t\tprint(stdout)\n\t\tif stderr:\n\t\t\tprint(\"Error output:\")\n\t\t\tprint(stderr)\n\t\n\treturn stdout, stderr, process.returncode\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tprocess = Popen(commands + args, stdout=PIPE, stderr=PIPE, stdin=PIPE, cwd=cwd, env=env)\n\tstdout, stderr = process.communicate()\n\tif verbose:\n\t\tprint(\"[INFO] Run command: %s %s\" % (\" \".join(commands + args), \"in %s\" % cwd if cwd else \"\"))\n\t\tprint(\"[INFO] stdout: %s\" % stdout.decode(\"utf-8\").strip())\n\t\tif not hide_stderr:\n\t\t\tif stderr:\n\t\t\t\tprint(\"[INFO] stderr: %s\" % stderr.decode(\"utf-8\").strip())\n\t\tif process.returncode != 0:\n\t\t\tprint(\"[ERROR] return code: %s (command: %s)\" % (process.returncode, \" \".join(commands + args)))\n\treturn stdout.decode(\"utf-8\").strip(), stderr.decode(\"utf-8\").strip(), process.returncode\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\ttry:\n\t\tif isinstance(commands, list):\n\t\t\tcmd = commands[0]\n\t\telse:\n\t\t\tcmd = commands\n\n\t\tif verbose:\n\t\t\tprint(\"Running: %s %s\" % (cmd, args))\n\n\t\tresult = subprocess.run([cmd] + args, cwd=cwd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n\t\tif result.stderr and not hide_stderr:\n\t\t\tsys.stderr.write(result.stderr.decode(\"utf-8\"))\n\n\t\tif not result.stdout:\n\t\t\tresult.stdout = \"\"\n\n\t\treturn (result.stdout.decode(\"utf-8\"), result.returncode)\n\texcept subprocess.CalledProcessError as err:\n\t\tsys.stderr.write(\"Command %s %s failed with error code %s. Output was: %s\" % (cmd, args, err.returncode, err.output))\n\t\treturn (err.output, err.returncode)\n\texcept:\n\t\tsys.stderr.write(\"Command %s %s failed with unknown error\" % (cmd, args))\n\t\traise\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\tdef get_command_string(commands, args):\n\t\t\"\"\"\n\t\tgiven a command (ex: [\"mycommand\", \"myargument\"]) and a list of arguments (ex: [\"myargument2\", \"myargument3\"]), return the command as a string\n\t\t\"\"\"\n\t\tcommand_string = \"\"\n\t\tfor command in commands:\n\t\t\tcommand_string += command + \" \"\n\t\tcommand_string += \" \".join(args)\n\t\treturn command_string\n\n\tif verbose:\n\t\tprint(\"running command: \" + get_command_string(commands, args))\n\n\tret = subprocess.run(\n\t\tget_command_string(commands, args),\n\t\tcapture_output=True,\n\t\tcwd=cwd,\n\t\tshell=True,\n\t\tenv=env\n\t)\n\tif verbose:\n\t\tif ret.returncode == 0:\n\t\t\tif ret.stderr:\n\t\t\t\tprint(\"stdout : \" + ret.stdout.decode(\"utf-8\"))\n\t\t\t\tprint(\"stderr : \" + ret.stderr.decode(\"utf-8\"))\n\t\t\telse:\n\t\t\t\tprint(\"stdout : \" + ret.stdout.decode(\"utf-8\"))\n\t\telse:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\ttry:\n\t\tprocess = subprocess.Popen(commands + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\texcept OSError as e:\n\t\tsys.stderr.write(\"ERROR: The command %s cannot be found\\n\" % commands[0])\n\t\tsys.exit(1)\n\n\tstdout, stderr = process.communicate()\n\n\tif verbose:\n\t\tsys.stdout.write(stdout)\n\n\tif stderr and not hide_stderr:\n\t\tsys.stderr.write(stderr)\n\n\treturn stdout, process.returncode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tcmd = ' '.join(commands) + ' ' + ' '.join(args)\n\n\ttry:\n\t\tif verbose:\n\t\t\tprint(cmd)\n\n\t\tproc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\tout, err = proc.communicate()\n\n\t\tif verbose:\n\t\t\tprint(out)\n\t\tif err and not hide_stderr:\n\t\t\tprint(err)\n\t\t\tsys.exit(1)\n\n\t\treturn out, proc.returncode\n\texcept OSError as e:\n\t\tprint(\"ERROR: Failed to run \\'{}\\'\".format(cmd))\n\t\tprint(e)\n\t\tsys.exit(1)", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\tif type(commands) is not list:\n\t\tcommands = [commands]\n\n\tprocesses = []\n\thas_error = False\n\n\tfor command in commands:\n\t\ttry:\n\t\t\tprocess = subprocess.Popen(\n\t\t\t\t[command] + args,\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env\n\t\t\t)\n\t\t\tprocesses.append(process)\n\n\t\t\tif verbose:\n\t\t\t\tprint(command + ' ' + ' '.join(args))\n\n\t\texcept OSError as e:\n\t\t\thas_error = True\n\t\t\tprint(e)\n\n\tif has_error:\n\t\tsys.exit(1)\n\n\tretval = None\n\tout = b''\n\tfor process in processes:\n\t\tout, err = process.communicate()\n\t\treturncode = process.returncode\n\t\tif returncode != 0:\n\t\t\thas_error = True\n\t\tif retval is None:\n\t\t\tretval = returncode\n\t\tout += err\n\n\tif hide_stderr:\n\t\tout = b''.join(\n\t\t\tline for line in out.split(b", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\t\n\tstdout = subprocess.PIPE\n\tstderr = subprocess.PIPE\n\t\n\tif hide_stderr:\n\t\tstderr = subprocess.PIPE\n\telse:\n\t\tstderr = subprocess.STDOUT\n\t\n\tprocess_result = subprocess.run(\n\t\tcommands,\n\t\targs = args,\n\t\tstdout = stdout,\n\t\tstderr = stderr,\n\t\tcwd = cwd,\n\t\tenv = env,\n\t\tuniversal_newlines = True,\n\t\tcheck = False\n\t)\n\t\n\tif verbose:\n\t\tprint(process_result.stdout)\n\t\n\treturn process_result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport os\n\t\n\n\tdef run_command_with_shell(command, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\t\"\"\"\n\t\tRun a command with arguments in a shell using subprocess. Return the running result (stdout and returncode)\n\t\t\"\"\"\n\t\tif verbose:\n\t\t\tprint(\"Running command: \" + \" \".join([command] + args))\n\t\tprocess = subprocess.Popen(\n\t\t\t[command] + args,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=subprocess.PIPE if not hide_stderr else open('/dev/null', 'w'),\n\t\t\tshell=True,\n\t\t\tcwd=cwd,\n\t\t\tenv=env\n\t\t)\n\t\tstdout, stderr = process.communicate()\n\t\treturn stdout.decode(errors='ignore'), stderr.decode(errors='ignore'), process.returncode\n\n\tdef run_command_without_shell(command, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\t\"\"\"\n\t\tRun a command with arguments using subprocess. Return the running result (stdout and returncode)\n\t\t\"\"\"\n\t\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\tcommand = commands + ' ' + args\n\tif verbose:\n\t\tprint(command)\n\tprocess = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tout, err = process.communicate()\n\tretcode = process.returncode\n\n\tif err and not hide_stderr:\n\t\tprint(err)\n\n\tif verbose:\n\t\tprint(out)\n\treturn out, retcode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\n\timport subprocess\n\t\n\targs = [command] + args\n\tif verbose:\n\t\tprint(\"Running: %s\" % \" \".join(args))\n\t\n\ttry:\n\t\tprocess = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\tprocess.wait()\n\t\tstdout, stderr = process.communicate()\n\texcept Exception as e:\n\t\terror(\"Failed to run %s (%s)\" % (args, e))\n\t\tstdout = \"\"\n\t\tstderr = \"\"\n\t\n\tif hide_stderr:\n\t\tstderr = \"\"\n\t\n\tif verbose:\n\t\tif stdout:\n\t\t\tprint(\"Output:\")\n\t\t\tprint(stdout)\n\t\tif stderr:\n\t\t\tprint(\"Error output:\")\n\t\t\tprint(stderr)\n\t\n\treturn stdout, stderr, process.returncode\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tprocess = Popen(commands + args, stdout=PIPE, stderr=PIPE, stdin=PIPE, cwd=cwd, env=env)\n\tstdout, stderr = process.communicate()\n\tif verbose:\n\t\tprint(\"[INFO] Run command: %s %s\" % (\" \".join(commands + args), \"in %s\" % cwd if cwd else \"\"))\n\t\tprint(\"[INFO] stdout: %s\" % stdout.decode(\"utf-8\").strip())\n\t\tif not hide_stderr:\n\t\t\tif stderr:\n\t\t\t\tprint(\"[INFO] stderr: %s\" % stderr.decode(\"utf-8\").strip())\n\t\tif process.returncode != 0:\n\t\t\tprint(\"[ERROR] return code: %s (command: %s)\" % (process.returncode, \" \".join(commands + args)))\n\treturn stdout.decode(\"utf-8\").strip(), stderr.decode(\"utf-8\").strip(), process.returncode\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "gopad/rest.py", "project": "gopad/gopad-python", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttarget = target.split('.')\n\tif len(target) != 4:\n\t\treturn False\n\tfor i in target:\n\t\ttry:\n\t\t\ti = int(i)\n\t\texcept ValueError:\n\t\t\treturn False\n\t\tif i < 0 or i > 255:\n\t\t\treturn False\n\treturn True\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\ttry:\n\t\tipv4 = socket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\t\t", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tparts = target.split('.')\n\tif len(parts) != 4:\n\t\treturn False\n\tfor part in parts:\n\t\ttry:\n\t\t\tif not 0 <= int(part) <= 255:\n\t\t\t\treturn False\n\t\texcept ValueError:\n\t\t\treturn False\n\treturn True\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\ttry:\n\t\tparts = target.split('.')\n\t\tip = '.'.join([str(int(p)) for p in parts])\n\t\treturn (len(parts) == 4) and (ip == target)\n\texcept:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttarget_split = target.split('.')\n\tif len(target_split) != 4:\n\t\treturn False\n\tif target_split[0] == '':\n\t\treturn False\n\tfor i in target_split:\n\t\ttry:\n\t\t\tif i == '':\n\t\t\t\treturn False\n\t\t\telif int(i) < 0 or int(i) > 255:\n\t\t\t\treturn False\n\t\texcept ValueError:\n\t\t\treturn False\n\treturn True\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tparts = target.split('.')\n\tif len(parts) != 4:\n\t\treturn False\n\n\tfor part in parts:\n\t\tif not part.isdigit():\n\t\t\treturn False\n\n\treturn True\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\ttarget = target.split(\".\")\n\texcept:\n\t\treturn False\n\tif len(target) != 4:\n\t\treturn False\n\ttry:\n\t\tfor octet in target:\n\t\t\toctet = int(octet)\n\t\t\tif octet < 0 or octet > 255:\n\t\t\t\treturn False\n\texcept:\n\t\treturn False\n\treturn True\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tcount = 0\n\ttry:\n\t\tfor i in target.split('.'):\n\t\t\tif not i.isdigit():\n\t\t\t\treturn False\n\t\t\tif int(i) > 255 or int(i) < 0:\n\t\t\t\treturn False\n\t\t\tcount += 1\n\t\tif count == 4:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\toctets = target.split('.')\n\tif len(octets) != 4:\n\t\treturn False\n\tfor octet in octets:\n\t\tif not octet.isdigit():\n\t\t\treturn False\n\t\tif int(octet) > 255:\n\t\t\treturn False\n\treturn True\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\taddr = target.split('.')\n\tif len(addr) != 4: \n\t\treturn False\n\tfor octet in addr:\n\t\tif not octet.isdigit():\n\t\t\treturn False\n\t\ti = int(octet)\n\t\tif i < 0 or i > 255:\n\t\t\treturn False\n\treturn True\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttarget = target.split('.')\n\tif len(target) != 4:\n\t\treturn False\n\tfor i in target:\n\t\ttry:\n\t\t\ti = int(i)\n\t\texcept ValueError:\n\t\t\treturn False\n\t\tif i < 0 or i > 255:\n\t\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\ttry:\n\t\tipv4 = socket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tparts = target.split('.')\n\tif len(parts) != 4:\n\t\treturn False\n\tfor part in parts:\n\t\ttry:\n\t\t\tif not 0 <= int(part) <= 255:\n\t\t\t\treturn False\n\t\texcept ValueError:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\ttry:\n\t\tparts = target.split('.')\n\t\tip = '.'.join([str(int(p)) for p in parts])\n\t\treturn (len(parts) == 4) and (ip == target)\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttarget_split = target.split('.')\n\tif len(target_split) != 4:\n\t\treturn False\n\tif target_split[0] == '':\n\t\treturn False\n\tfor i in target_split:\n\t\ttry:\n\t\t\tif i == '':\n\t\t\t\treturn False\n\t\t\telif int(i) < 0 or int(i) > 255:\n\t\t\t\treturn False\n\t\texcept ValueError:\n\t\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tparts = target.split('.')\n\tif len(parts) != 4:\n\t\treturn False\n\n\tfor part in parts:\n\t\tif not part.isdigit():\n\t\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\ttarget = target.split(\".\")\n\texcept:\n\t\treturn False\n\tif len(target) != 4:\n\t\treturn False\n\ttry:\n\t\tfor octet in target:\n\t\t\toctet = int(octet)\n\t\t\tif octet < 0 or octet > 255:\n\t\t\t\treturn False\n\texcept:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tcount = 0\n\ttry:\n\t\tfor i in target.split('.'):\n\t\t\tif not i.isdigit():\n\t\t\t\treturn False\n\t\t\tif int(i) > 255 or int(i) < 0:\n\t\t\t\treturn False\n\t\t\tcount += 1\n\t\tif count == 4:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\toctets = target.split('.')\n\tif len(octets) != 4:\n\t\treturn False\n\tfor octet in octets:\n\t\tif not octet.isdigit():\n\t\t\treturn False\n\t\tif int(octet) > 255:\n\t\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\taddr = target.split('.')\n\tif len(addr) != 4: \n\t\treturn False\n\tfor octet in addr:\n\t\tif not octet.isdigit():\n\t\t\treturn False\n\t\ti = int(octet)\n\t\tif i < 0 or i > 255:\n\t\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "rdflib/util.py", "project": "mwatts15/rdflib", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor child in graph.subjects(prop):\n\t\tpreds = [pred for pred in graph.predicates(child) if pred not in (prop, RDF.type)]\n\t\tif not preds:\n\t\t\troots.add(child)\n\t\telse:\n\t\t\tfor pred in preds:\n\t\t\t\tfind_roots(graph, pred, roots)\n\treturn roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\n    if roots is None:\n        roots = set()\n\n    for node in graph.subjects(RDFS.subClassOf):\n        if not any(graph.predicate_objects(node, RDFS.subClassOf)):\n            roots.add(node)\n\n    return roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph:\n\t\tif len(list(graph.predecessors(node, prop))) == 0:\n\t\t\troots.add(node)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph.subjects(predicate=prop):\n\t\tif not graph.predicate_objects(node):\n\t\t\troots.add(node)\n\t\telse:\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\t# Find the roots\n\tfor child in graph.subjects(prop, None):\n\t\tif not graph.value(child, prop):\n\t\t\troots.add(child)\n\t\telse:\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set() if roots is None else roots\n\tfor n, o in graph.predicate_objects(predicate=prop):\n\t\tif o not in roots:\n\t\t\troots.add(o)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif len(list(graph.predecessors(node))) == 0:\n\t\t\troots.add(node)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set() if roots is None else roots\n\n\tsubj, obj = None, None\n\ttry:\n\t\tsubj, obj = next(graph.subject_objects(prop))\n\texcept StopIteration:\n\t\treturn roots\n\n\tif subj not in roots:\n\t\troots.add(subj)\n\t\tfind_roots(graph, prop, roots)\n\n\tif obj not in roots:\n\t\troots.add(obj)\n\t\tfind_roots(graph, prop, roots)\n\n\treturn roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph:\n\t\tif node not in roots and len(list(graph.predecessors(node, prop))) == 0:\n\t\t\troots.add(node)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node, _, _ in graph.triples((None, prop, None)):\n\t\tif all(\n\t\t\t(o, prop, node) not in graph\n\t\t\tfor s, p, o in graph\n\t\t\tif p == prop\n\t\t):\n\t\t\troots.add(node)\n\treturn roots"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor child in graph.subjects(prop):\n\t\tpreds = [pred for pred in graph.predicates(child) if pred not in (prop, RDF.type)]\n\t\tif not preds:\n\t\t\troots.add(child)\n\t\telse:\n\t\t\tfor pred in preds:\n\t\t\t\tfind_roots(graph, pred, roots)\n\treturn roots", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\n    if roots is None:\n        roots = set()\n\n    for node in graph.subjects(RDFS.subClassOf):\n        if not any(graph.predicate_objects(node, RDFS.subClassOf)):\n            roots.add(node)\n\n    return roots", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph:\n\t\tif len(list(graph.predecessors(node, prop))) == 0:\n\t\t\troots.add(node)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph.subjects(predicate=prop):\n\t\tif not graph.predicate_objects(node):\n\t\t\troots.add(node)\n\t\telse:\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\t# Find the roots\n\tfor child in graph.subjects(prop, None):\n\t\tif not graph.value(child, prop):\n\t\t\troots.add(child)\n\t\telse:\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set() if roots is None else roots\n\tfor n, o in graph.predicate_objects(predicate=prop):\n\t\tif o not in roots:\n\t\t\troots.add(o)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif len(list(graph.predecessors(node))) == 0:\n\t\t\troots.add(node)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set() if roots is None else roots\n\n\tsubj, obj = None, None\n\ttry:\n\t\tsubj, obj = next(graph.subject_objects(prop))\n\texcept StopIteration:\n\t\treturn roots\n\n\tif subj not in roots:\n\t\troots.add(subj)\n\t\tfind_roots(graph, prop, roots)\n\n\tif obj not in roots:\n\t\troots.add(obj)\n\t\tfind_roots(graph, prop, roots)\n\n\treturn roots", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph:\n\t\tif node not in roots and len(list(graph.predecessors(node, prop))) == 0:\n\t\t\troots.add(node)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node, _, _ in graph.triples((None, prop, None)):\n\t\tif all(\n\t\t\t(o, prop, node) not in graph\n\t\t\tfor s, p, o in graph\n\t\t\tif p == prop\n\t\t):\n\t\t\troots.add(node)\n\treturn roots", "is_pass": true, "return_code": 0}]}
{"file_path": "tests/unit/mock/yaml_helper.py", "project": "ansible-security/ansible_collections.ibm.qradar", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\n\t# Serialize into YAML\n\tyaml_stream = yaml.dump(obj, default_flow_style=False)\n\n\treturn yaml_stream\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\tif not dumper:\n\t\tdumper = yaml.SafeDumper\n\tdumper.add_representer(str, _repr_str)\n\ttry:\n\t\treturn yaml.dump(obj, dumper=dumper)\n\texcept yaml.YAMLError as exc:\n\t\tprint(exc)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport sys\n\timport yaml\n\tfrom yaml.representer import SafeRepresenter\n\timport datetime\n\timport numpy as np\n\tfrom numpy.ma.core import MaskedArray\n\n\tclass NumpyYAML(SafeRepresenter):\n\t\tdef _represent_array(self, data):\n\t\t\tself.represent_seq(u'tag:yaml.org,2002:seq', data, flow_style=True)\n\t\t\treturn True\n\t\tSafeRepresenter.represent_ndarray = _represent_array\n\n\tclass NumpyMaskedYAML(NumpyYAML):\n\t\tdef _represent_array(self, data):\n\t\t\tmasked_array = np.ma.MaskedArray(data)\n\t\t\tself.represent_seq(u'tag:yaml.org,2002:seq', masked_array, flow_style=True)\n\t\t\treturn True\n\t\tSafeRepresenter.represent_ndarray = _represent_array\n\n\tdef _dump(obj, dumper=None):\n\t\tif dumper is None:\n\t\t\tdumper = yaml.SafeDumper\n\t\tdumper.add_representer(MaskedArray, NumpyMaskedYAML.represent_ndarray)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.dump(obj, indent=4, width=80)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\n\tdef represent_str(dumper, data):\n\t\tif '\\n' in data:\n\t\t\treturn dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n\t\treturn dumper.represent_scalar('tag:yaml.org,2002:str', data)\n\n\tyaml.add_representer(str, represent_str)\n\treturn yaml.dump(obj, default_flow_style=False)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.dump(obj, default_flow_style=False)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.dump(obj, indent=2, default_flow_style=False)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.safe_dump(obj, dumper=dumper)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.safe_dump(obj, default_flow_style=False)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\n\t# Serialize into YAML\n\tyaml_stream = yaml.dump(obj, default_flow_style=False)\n\n\treturn yaml_stream\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\tif not dumper:\n\t\tdumper = yaml.SafeDumper\n\tdumper.add_representer(str, _repr_str)\n\ttry:\n\t\treturn yaml.dump(obj, dumper=dumper)\n\texcept yaml.YAMLError as exc:\n\t\tprint(exc)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport sys\n\timport yaml\n\tfrom yaml.representer import SafeRepresenter\n\timport datetime\n\timport numpy as np\n\tfrom numpy.ma.core import MaskedArray\n\n\tclass NumpyYAML(SafeRepresenter):\n\t\tdef _represent_array(self, data):\n\t\t\tself.represent_seq(u'tag:yaml.org,2002:seq', data, flow_style=True)\n\t\t\treturn True\n\t\tSafeRepresenter.represent_ndarray = _represent_array\n\n\tclass NumpyMaskedYAML(NumpyYAML):\n\t\tdef _represent_array(self, data):\n\t\t\tmasked_array = np.ma.MaskedArray(data)\n\t\t\tself.represent_seq(u'tag:yaml.org,2002:seq', masked_array, flow_style=True)\n\t\t\treturn True\n\t\tSafeRepresenter.represent_ndarray = _represent_array\n\n\tdef _dump(obj, dumper=None):\n\t\tif dumper is None:\n\t\t\tdumper = yaml.SafeDumper\n\t\tdumper.add_representer(MaskedArray, NumpyMaskedYAML.represent_ndarray)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.dump(obj, indent=4, width=80)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\n\tdef represent_str(dumper, data):\n\t\tif '\\n' in data:\n\t\t\treturn dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n\t\treturn dumper.represent_scalar('tag:yaml.org,2002:str', data)\n\n\tyaml.add_representer(str, represent_str)\n\treturn yaml.dump(obj, default_flow_style=False)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.dump(obj, default_flow_style=False)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.dump(obj, indent=2, default_flow_style=False)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.safe_dump(obj, dumper=dumper)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\timport yaml\n\treturn yaml.safe_dump(obj, default_flow_style=False)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "apphelpers/loggers.py", "project": "scrolltech/apphelpers", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tfrom logging import handlers\n\timport os\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s: %(message)s')\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG if debug else logging.INFO)\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\n\tif logfile:\n\t\tos.makedirs(os.path.dirname(logfile), exist_ok=True)\n\t\tfh = handlers.RotatingFileHandler(logfile, maxBytes=10*1024*1024, backupCount=5)\n\t\tfh.setLevel(logging.DEBUG if debug else logging.INFO)\n\t\tfh.setFormatter(formatter)\n\t\tlogger.addHandler(fh)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG if debug else logging.INFO)\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\n\tfh = logging.FileHandler(logfile, mode='a')\n\tfh.setLevel(logging.DEBUG if debug else logging.INFO)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tfrom logging import getLogger, Formatter, StreamHandler, FileHandler, DEBUG\n\tlogger = getLogger(name)\n\tlogger.setLevel(DEBUG)\n\thandler = StreamHandler()\n\thandler.setFormatter(Formatter('%(asctime)s %(levelname)s %(message)s'))\n\tlogger.addHandler(handler)\n\tif debug:\n\t\thandler = FileHandler(logfile, encoding='utf-8')\n\t\thandler.setFormatter(Formatter('%(asctime)s %(levelname)s %(message)s'))\n\t\tlogger.addHandler(handler)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\thandler = logging.handlers.RotatingFileHandler(\n\t\tlogfile, maxBytes=1024 * 1024 * 100, backupCount=100)\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tfrom logging.handlers import TimedRotatingFileHandler\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\tfile_handler = TimedRotatingFileHandler(logfile, when='midnight')\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.DEBUG)\n\tstream_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.INFO)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\n\tlogs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')\n\tif not os.path.exists(logs_dir):\n\t\tos.mkdir(logs_dir)\n\n\tfh = logging.FileHandler(os.path.join(logs_dir, logfile))\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\treturn logger", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\tlogger.addHandler(ch)\n\tlogger.addHandler(fh)\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tdebug = debug\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s :: %(levelname)s :: %(message)s')\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.DEBUG)\n\tlogger.addHandler(stream_handler)\n\treturn logger", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\timport os\n\tfrom logging.handlers import TimedRotatingFileHandler\n\n\t# create logger\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tif not os.path.exists(logfile):\n\t\tos.makedirs(os.path.dirname(logfile))\n\n\t# create file handler\n\tloghandler = TimedRotatingFileHandler(logfile, when='midnight', backupCount=30, encoding='utf-8')\n\tloghandler.suffix = \"%Y-%m-%d\"\n\tloghandler.setLevel(logging.DEBUG)\n\n\t# create formatter\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t# add formatter to ch\n\tloghandler.setFormatter(formatter)\n\n\t# add handler to logger\n\tlogger.addHandler(loghandler)\n\n\tif debug:\n\t\t# create console handler\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\n\t\t# add formatter to ch\n\t\tch.setFormatter(formatter)\n\n\t\t# add ch to logger\n\t\tlogger.addHandler(ch)", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tfrom logging.handlers import RotatingFileHandler\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\n\tlog_handler = RotatingFileHandler(logfile, maxBytes=50000000, backupCount=5)\n\tlog_handler.setLevel(logging.DEBUG)\n\tlog_handler.setFormatter(formatter)\n\tlogger.addHandler(log_handler)\n\t\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tconsole_handler.setFormatter(formatter)\n\tlogger.addHandler(console_handler)\n\treturn logger\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tfrom logging import handlers\n\timport os\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s: %(message)s')\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG if debug else logging.INFO)\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\n\tif logfile:\n\t\tos.makedirs(os.path.dirname(logfile), exist_ok=True)\n\t\tfh = handlers.RotatingFileHandler(logfile, maxBytes=10*1024*1024, backupCount=5)\n\t\tfh.setLevel(logging.DEBUG if debug else logging.INFO)\n\t\tfh.setFormatter(formatter)\n\t\tlogger.addHandler(fh)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG if debug else logging.INFO)\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\n\tfh = logging.FileHandler(logfile, mode='a')\n\tfh.setLevel(logging.DEBUG if debug else logging.INFO)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tfrom logging import getLogger, Formatter, StreamHandler, FileHandler, DEBUG\n\tlogger = getLogger(name)\n\tlogger.setLevel(DEBUG)\n\thandler = StreamHandler()\n\thandler.setFormatter(Formatter('%(asctime)s %(levelname)s %(message)s'))\n\tlogger.addHandler(handler)\n\tif debug:\n\t\thandler = FileHandler(logfile, encoding='utf-8')\n\t\thandler.setFormatter(Formatter('%(asctime)s %(levelname)s %(message)s'))\n\t\tlogger.addHandler(handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\thandler = logging.handlers.RotatingFileHandler(\n\t\tlogfile, maxBytes=1024 * 1024 * 100, backupCount=100)\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tfrom logging.handlers import TimedRotatingFileHandler\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\tfile_handler = TimedRotatingFileHandler(logfile, when='midnight')\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.DEBUG)\n\tstream_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.INFO)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\n\tlogs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')\n\tif not os.path.exists(logs_dir):\n\t\tos.mkdir(logs_dir)\n\n\tfh = logging.FileHandler(os.path.join(logs_dir, logfile))\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\treturn logger", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\tlogger.addHandler(ch)\n\tlogger.addHandler(fh)\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tdebug = debug\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s :: %(levelname)s :: %(message)s')\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.DEBUG)\n\tlogger.addHandler(stream_handler)\n\treturn logger", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\timport os\n\tfrom logging.handlers import TimedRotatingFileHandler\n\n\t# create logger\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tif not os.path.exists(logfile):\n\t\tos.makedirs(os.path.dirname(logfile))\n\n\t# create file handler\n\tloghandler = TimedRotatingFileHandler(logfile, when='midnight', backupCount=30, encoding='utf-8')\n\tloghandler.suffix = \"%Y-%m-%d\"\n\tloghandler.setLevel(logging.DEBUG)\n\n\t# create formatter\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t# add formatter to ch\n\tloghandler.setFormatter(formatter)\n\n\t# add handler to logger\n\tlogger.addHandler(loghandler)\n\n\tif debug:\n\t\t# create console handler\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\n\t\t# add formatter to ch\n\t\tch.setFormatter(formatter)\n\n\t\t# add ch to logger\n\t\tlogger.addHandler(ch)", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tfrom logging.handlers import RotatingFileHandler\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\n\tlog_handler = RotatingFileHandler(logfile, maxBytes=50000000, backupCount=5)\n\tlog_handler.setLevel(logging.DEBUG)\n\tlog_handler.setFormatter(formatter)\n\tlogger.addHandler(log_handler)\n\t\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tconsole_handler.setFormatter(formatter)\n\tlogger.addHandler(console_handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "radiospectra/spectrogram.py", "project": "sunpy/radiospectra", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tl = []\n\tfor s in shape:\n\t\tl.append(s)\n\treturn np.zeros(l,dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tdata = numpy.zeros(shape, dtype)\n\treturn array(data)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\timport numpy as np\n\t\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape,dtype=dtype)\n\treturn array", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tl = []\n\tfor s in shape:\n\t\tl.append(s)\n\treturn np.zeros(l,dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tdata = numpy.zeros(shape, dtype)\n\treturn array(data)", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\timport numpy as np\n\t\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape,dtype=dtype)\n\treturn array", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "concert/tests/unit/devices/test_monochromator.py", "project": "ufo-kit/concert", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn 1/math.sqrt(2*math.pi*0.1**2)*math.exp(-(x-0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu, sigma = 0.2, 0.1\n\treturn (1 / np.sqrt(2 * np.pi * sigma ** 2)) * np.exp(-(x - u) ** 2 / (2 * sigma ** 2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\ts = 0.1\n\tout = np.exp(-0.5 * ((x - u)/s)**2)\n\treturn out\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn (math.e**(-0.5*((x-0.2)/0.1)**2))/(0.1*(2*math.pi)**0.5)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn math.exp(-(x - 0.2)**2 / (2 * 0.1**2)) / math.sqrt(2*math.pi*0.1**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn math.exp(-(x-0.2)**2/0.1**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\tgaussian_value = math.exp(((-1)*(x-0.2)**2)/(0.1**2))\n\treturn gaussian_value\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\t\n\treturn 1/(math.sqrt(2*math.pi)*0.1) * math.exp(-math.pow(x-0.2, 2)/(2*math.pow(0.1, 2)))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom math import sqrt, exp\n\tfrom math import pi\n\n\tsigma = 0.1\n\tu = 0.2\n\n\treturn (1 / sqrt(2 * pi * sigma**2)) * exp(-0.5 * (x - u)**2 / sigma**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\ts = 0.1\n\treturn (1 / ( s * np.sqrt(2 * np.pi))) * np.exp(-(x - u)**2 / (2 * s**2))"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn 1/math.sqrt(2*math.pi*0.1**2)*math.exp(-(x-0.2)**2/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu, sigma = 0.2, 0.1\n\treturn (1 / np.sqrt(2 * np.pi * sigma ** 2)) * np.exp(-(x - u) ** 2 / (2 * sigma ** 2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\ts = 0.1\n\tout = np.exp(-0.5 * ((x - u)/s)**2)\n\treturn out\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn (math.e**(-0.5*((x-0.2)/0.1)**2))/(0.1*(2*math.pi)**0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn math.exp(-(x - 0.2)**2 / (2 * 0.1**2)) / math.sqrt(2*math.pi*0.1**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\treturn math.exp(-(x-0.2)**2/0.1**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\tgaussian_value = math.exp(((-1)*(x-0.2)**2)/(0.1**2))\n\treturn gaussian_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\t\n\treturn 1/(math.sqrt(2*math.pi)*0.1) * math.exp(-math.pow(x-0.2, 2)/(2*math.pow(0.1, 2)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom math import sqrt, exp\n\tfrom math import pi\n\n\tsigma = 0.1\n\tu = 0.2\n\n\treturn (1 / sqrt(2 * pi * sigma**2)) * exp(-0.5 * (x - u)**2 / sigma**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\ts = 0.1\n\treturn (1 / ( s * np.sqrt(2 * np.pi))) * np.exp(-(x - u)**2 / (2 * s**2))", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/borgmatic.py", "project": "witten/borgmatic", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = yaml.safe_load(open(config_filename))\n\t\texcept Exception as e:\n\t\t\terrors.append(parse_parse_error(e, config_filename))\n\t\tif resolve_env:\n\t\t\tconfigs[config_filename] = resolve_environment_variables(configs[config_filename])\n\t\tif overrides:\n\t\t\tconfigs[config_filename] = merge_configuration(configs[config_filename], overrides)\n\t\treturn configs, errors\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as fp:\n\t\t\t\tconfigs[config_filename] = yaml.safe_load(fp)\n\t\texcept OSError as e:\n\t\t\terrors.append(log_error(\n\t\t\t\t'Unable to read configuration file: {config_filename} ({error})'.\n\t\t\t\tformat(config_filename=config_filename, error=str(e))))\n\t\texcept ValueError as e:\n\t\t\terrors.append(log_error(\n\t\t\t\t'Unable to parse configuration file: {config_filename} ({error})'.\n\t\t\t\tformat(config_filename=config_filename, error=str(e))))\n\t\tfor config_filename, config in configs.items():\n\t\t\ttry:\n\t\t\t\tconfigs[config_filename] = resolve_config_references(config, configs)\n\t\t\texcept ValueError as e:\n\t\t\t\terrors.append(log_error(\n\t\t\t\t\t'Unable to resolve configuration references: {config_filename} ({error})'.\n\t\t\t\t\tformat(config_filename=config_filename, error=str(e))))\n\t\tif resolve_env:\n\t\t\tfor config_filename,", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig = {}\n\terror_log = []\n\tfor cfg_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig[cfg_filename] = load_configuration(cfg_filename, overrides=overrides, resolve_env=resolve_env)\n\t\texcept Exception as ex:\n\t\t\terror_log.append(logging.LogRecord('ERROR', 0, None, 0, 'Error parsing configuration file {}:\\n{}'.format(cfg_filename, ex), None, None))\n\treturn config, error_log\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\n\tfor filename in config_filenames:\n\t\tconfigs[filename] = {}\n\n\t\ttry:\n\t\t\twith open(filename) as fh:\n\t\t\t\tconfigs[filename] = yaml.safe_load(fh)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfigs[filename] = resolve_env_vars(configs[filename])\n\t\t\t\tif overrides:\n\t\t\t\t\tconfigs[filename].update(overrides)\n\n\t\texcept IOError as e:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=filename,\n\t\t\t\t\tlineno=0,\n\t\t\t\t\tmsg='Failed to open file: {}'.format(e),\n\t\t\t\t\targs=None,\n\t\t\t\t\texc_info=sys.exc_info(),\n\t\t\t\t)\n\t\t\t)\n\t\texcept ValueError as e:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=filename,\n\t\t\t\t\tlineno=0,\n\t\t\t\t\tmsg='Failed to parse file as YAML: {}'.format(e),\n\t\t\t\t\t", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigurations[config_filename] = load_configuration(config_filename, overrides, resolve_env)\n\t\texcept (IOError, ValueError) as exc:\n\t\t\terrors.append(logging.LogRecord('load_configurations', logging.ERROR, None, None, str(exc), None, None))\n\treturn configurations, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = load_config(config_filename, overrides, resolve_env)\n\t\texcept Exception as e:\n\t\t\terrors.append({\n\t\t\t\t'filename': config_filename,\n\t\t\t\t'error': e.message,\n\t\t\t})\n\treturn configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\ttry:\n\t\tconfigurations = {}\n\t\tissues = []\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\tconfigurations[config_filename] = load_configuration(config_filename, overrides, resolve_env)\n\t\t\t\tif not configurations[config_filename]:\n\t\t\t\t\traise ValueError(\"Configuration file '{}' is empty\".format(config_filename))\n\t\t\texcept EnvironmentError as e:\n\t\t\t\tissues.append(e)\n\t\t\texcept ValueError as e:\n\t\t\t\tissues.append(e)\n\t\treturn configurations, issues\n\texcept Exception as e:\n\t\traise ValueError(\"Error loading configuration files: {}\".format(e))\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tparser = configparser.ConfigParser()\n\tconfig_files_read = []\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tif os.access(config_filename, os.R_OK):\n\t\t\tconfig_files_read.append(config_filename)\n\t\telse:\n\t\t\terrors.append(f\"Could not read {config_filename}\")\n\t\t\tcontinue\n\t\tparser.read(config_filename)\n\t\tconfig = {}\n\t\tfor section in parser.sections():\n\t\t\tconfig[section] = {}\n\t\t\tfor option in parser.options(section):\n\t\t\t\tconfig[section][option] = parser.get(section, option)\n\t\tconfigs[config_filename] = config\n\treturn configs, errors", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig_dicts = []\n\tlog_records = []\n\n\tif isinstance(config_filenames, str):\n\t\tconfig_filenames = (config_filenames,)\n\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config_file(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\tconfig_dicts.append(config)\n\t\texcept IOError as e:\n\t\t\tlog_records.append(logging.makeRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\te.filename,\n\t\t\t\te.lineno,\n\t\t\t\te.strerror,\n\t\t\t\t(config_filename, e),\n\t\t\t\tNone,\n\t\t\t))\n\t\texcept Exception as e:\n\t\t\tlog_records.append(logging.makeRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\t'',\n\t\t\t\t0,\n\t\t\t\tstr(e),\n\t\t\t\t(config_filename, e),\n\t\t\t\tNone,\n\t\t\t))\n\n\treturn config_dicts, log_records\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\texcept Exception as e:\n\t\t\terrors.append(e)\n\t\t\tlogger.error(\"Error loading configuration file: \" + str(e))\n\treturn configs, errors\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = yaml.safe_load(open(config_filename))\n\t\texcept Exception as e:\n\t\t\terrors.append(parse_parse_error(e, config_filename))\n\t\tif resolve_env:\n\t\t\tconfigs[config_filename] = resolve_environment_variables(configs[config_filename])\n\t\tif overrides:\n\t\t\tconfigs[config_filename] = merge_configuration(configs[config_filename], overrides)\n\t\treturn configs, errors\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as fp:\n\t\t\t\tconfigs[config_filename] = yaml.safe_load(fp)\n\t\texcept OSError as e:\n\t\t\terrors.append(log_error(\n\t\t\t\t'Unable to read configuration file: {config_filename} ({error})'.\n\t\t\t\tformat(config_filename=config_filename, error=str(e))))\n\t\texcept ValueError as e:\n\t\t\terrors.append(log_error(\n\t\t\t\t'Unable to parse configuration file: {config_filename} ({error})'.\n\t\t\t\tformat(config_filename=config_filename, error=str(e))))\n\t\tfor config_filename, config in configs.items():\n\t\t\ttry:\n\t\t\t\tconfigs[config_filename] = resolve_config_references(config, configs)\n\t\t\texcept ValueError as e:\n\t\t\t\terrors.append(log_error(\n\t\t\t\t\t'Unable to resolve configuration references: {config_filename} ({error})'.\n\t\t\t\t\tformat(config_filename=config_filename, error=str(e))))\n\t\tif resolve_env:\n\t\t\tfor config_filename,", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig = {}\n\terror_log = []\n\tfor cfg_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig[cfg_filename] = load_configuration(cfg_filename, overrides=overrides, resolve_env=resolve_env)\n\t\texcept Exception as ex:\n\t\t\terror_log.append(logging.LogRecord('ERROR', 0, None, 0, 'Error parsing configuration file {}:\\n{}'.format(cfg_filename, ex), None, None))\n\treturn config, error_log\n", "is_pass": true, "return_code": 0}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\n\tfor filename in config_filenames:\n\t\tconfigs[filename] = {}\n\n\t\ttry:\n\t\t\twith open(filename) as fh:\n\t\t\t\tconfigs[filename] = yaml.safe_load(fh)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfigs[filename] = resolve_env_vars(configs[filename])\n\t\t\t\tif overrides:\n\t\t\t\t\tconfigs[filename].update(overrides)\n\n\t\texcept IOError as e:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=filename,\n\t\t\t\t\tlineno=0,\n\t\t\t\t\tmsg='Failed to open file: {}'.format(e),\n\t\t\t\t\targs=None,\n\t\t\t\t\texc_info=sys.exc_info(),\n\t\t\t\t)\n\t\t\t)\n\t\texcept ValueError as e:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=filename,\n\t\t\t\t\tlineno=0,\n\t\t\t\t\tmsg='Failed to parse file as YAML: {}'.format(e),\n\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigurations[config_filename] = load_configuration(config_filename, overrides, resolve_env)\n\t\texcept (IOError, ValueError) as exc:\n\t\t\terrors.append(logging.LogRecord('load_configurations', logging.ERROR, None, None, str(exc), None, None))\n\treturn configurations, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = load_config(config_filename, overrides, resolve_env)\n\t\texcept Exception as e:\n\t\t\terrors.append({\n\t\t\t\t'filename': config_filename,\n\t\t\t\t'error': e.message,\n\t\t\t})\n\treturn configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\ttry:\n\t\tconfigurations = {}\n\t\tissues = []\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\tconfigurations[config_filename] = load_configuration(config_filename, overrides, resolve_env)\n\t\t\t\tif not configurations[config_filename]:\n\t\t\t\t\traise ValueError(\"Configuration file '{}' is empty\".format(config_filename))\n\t\t\texcept EnvironmentError as e:\n\t\t\t\tissues.append(e)\n\t\t\texcept ValueError as e:\n\t\t\t\tissues.append(e)\n\t\treturn configurations, issues\n\texcept Exception as e:\n\t\traise ValueError(\"Error loading configuration files: {}\".format(e))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tparser = configparser.ConfigParser()\n\tconfig_files_read = []\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tif os.access(config_filename, os.R_OK):\n\t\t\tconfig_files_read.append(config_filename)\n\t\telse:\n\t\t\terrors.append(f\"Could not read {config_filename}\")\n\t\t\tcontinue\n\t\tparser.read(config_filename)\n\t\tconfig = {}\n\t\tfor section in parser.sections():\n\t\t\tconfig[section] = {}\n\t\t\tfor option in parser.options(section):\n\t\t\t\tconfig[section][option] = parser.get(section, option)\n\t\tconfigs[config_filename] = config\n\treturn configs, errors", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig_dicts = []\n\tlog_records = []\n\n\tif isinstance(config_filenames, str):\n\t\tconfig_filenames = (config_filenames,)\n\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config_file(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\tconfig_dicts.append(config)\n\t\texcept IOError as e:\n\t\t\tlog_records.append(logging.makeRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\te.filename,\n\t\t\t\te.lineno,\n\t\t\t\te.strerror,\n\t\t\t\t(config_filename, e),\n\t\t\t\tNone,\n\t\t\t))\n\t\texcept Exception as e:\n\t\t\tlog_records.append(logging.makeRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\t'',\n\t\t\t\t0,\n\t\t\t\tstr(e),\n\t\t\t\t(config_filename, e),\n\t\t\t\tNone,\n\t\t\t))\n\n\treturn config_dicts, log_records\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\texcept Exception as e:\n\t\t\terrors.append(e)\n\t\t\tlogger.error(\"Error loading configuration file: \" + str(e))\n\treturn configs, errors\n", "is_pass": false, "return_code": 1}]}
{"file_path": "o2sclpy/utils.py", "project": "awsteiner/o2sclpy", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\timport numpy as np\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, str):\n\t\t\treturn obj\n\t\telif isinstance(obj, np.bytes_):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\texcept Exception as e:\n\t\tprint(str(e))", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tobj.decode('utf-8')\n\texcept AttributeError:\n\t\tpass\n\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif isinstance(obj, str):\n\t\tobj = obj.encode('utf-8')\n\telif isinstance(obj, bytes):\n\t\tobj = obj.decode('utf-8')\n\n\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\timport numpy as np\n\n\tif type(obj) == np.bytes_:\n\t\treturn obj.decode(\"utf-8\")\n\telif type(obj) == bytes:\n\t\treturn obj.decode(\"utf-8\")\n\telse:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\treturn str(obj)\n\texcept:\n\t\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\timport numpy\n\timport six\n\tif isinstance(obj, six.text_type):\n\t\treturn obj\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif isinstance(obj, (bytes, numpy.bytes_)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\treturn obj.decode('utf-8')\n\texcept:\n\t\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\treturn obj.decode('utf-8')\n\texcept AttributeError:\n\t\treturn obj\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\timport numpy as np\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, str):\n\t\t\treturn obj\n\t\telif isinstance(obj, np.bytes_):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\texcept Exception as e:\n\t\tprint(str(e))", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tobj.decode('utf-8')\n\texcept AttributeError:\n\t\tpass\n\treturn obj", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif isinstance(obj, str):\n\t\tobj = obj.encode('utf-8')\n\telif isinstance(obj, bytes):\n\t\tobj = obj.decode('utf-8')\n\n\treturn obj", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\timport numpy as np\n\n\tif type(obj) == np.bytes_:\n\t\treturn obj.decode(\"utf-8\")\n\telif type(obj) == bytes:\n\t\treturn obj.decode(\"utf-8\")\n\telse:\n\t\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\treturn str(obj)\n\texcept:\n\t\treturn obj", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\timport numpy\n\timport six\n\tif isinstance(obj, six.text_type):\n\t\treturn obj\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif isinstance(obj, (bytes, numpy.bytes_)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\treturn obj.decode('utf-8')\n\texcept:\n\t\treturn obj", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\treturn obj.decode('utf-8')\n\texcept AttributeError:\n\t\treturn obj\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tticks = int(ticks)\n\texcept (TypeError, ValueError):\n\t\traise TypeError('ticks must be an integer')\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of range')\n\ttime = cls()\n\ttime._set_time(ticks, tz)\n\treturn time\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tticks = int(ticks)\n\texcept (TypeError, ValueError) as e:\n\t\traise ValueError(\"ticks is not a valid int\") from e\n\n\tif tz is not None:\n\t\traise NotImplementedError(\"timezone support not implemented\")\n\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError(\"ticks out of bounds\")\n\n\tnanoseconds = ticks % 1000000000\n\tticks //= 1000000000\n\tseconds = ticks % 60\n\tticks //= 60\n\tminutes = ticks % 60\n\tticks //= 60\n\thours = ticks % 24\n\tticks //= 24\n\tdays = ticks\n\n\treturn cls(days=days, hours=hours, minutes=minutes, seconds=seconds,\n\t           nanoseconds=nanoseconds)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\traise NotImplementedError\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tticks = int(ticks)\n\texcept (TypeError, ValueError):\n\t\traise TypeError('ticks must be an integer')\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of range')\n\ttime = cls()\n\ttime._set_time(ticks, tz)\n\treturn time\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tticks = int(ticks)\n\texcept (TypeError, ValueError) as e:\n\t\traise ValueError(\"ticks is not a valid int\") from e\n\n\tif tz is not None:\n\t\traise NotImplementedError(\"timezone support not implemented\")\n\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError(\"ticks out of bounds\")\n\n\tnanoseconds = ticks % 1000000000\n\tticks //= 1000000000\n\tseconds = ticks % 60\n\tticks //= 60\n\tminutes = ticks % 60\n\tticks //= 60\n\thours = ticks % 24\n\tticks //= 24\n\tdays = ticks\n\n\treturn cls(days=days, hours=hours, minutes=minutes, seconds=seconds,\n\t           nanoseconds=nanoseconds)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tversions_supported = [\n\t\tcls._v111_0, cls._v110_0, cls._v101_0, cls._v100_0, cls._v40_0,\n\t\tcls._v111_1, cls._v110_1, cls._v101_1, cls._v100_1, cls._v40_1,\n\t\tcls._v111_2, cls._v110_2, cls._v101_2, cls._v100_2, cls._v40_2,\n\t\tcls._v111_3, cls._v110_3, cls._v101_3, cls._v100_3, cls._v40_3\n\t]\n\tif protocol_version is None:\n\t\treturn {\n\t\t\tversion.protocol_version: version for version in versions_supported\n\t\t}\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError('protocol version must be a tuple')\n\tversion_tuple = tuple(protocol_version)\n\treturn {\n\t\tversion.protocol_version: version\n\t\tfor version in versions", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t...", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t...", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t...", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\tfor handler in cls._handlers.values():\n\t\tif protocol_version in handler.protocol_versions:\n\t\t\thandlers[handler.protocol_versions[protocol_version]] = handler\n\t\telif not protocol_version:\n\t\t\thandlers[handler.protocol_versions[handler.latest_version]] = handler\n\treturn handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\traise NotImplementedError"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tversions_supported = [\n\t\tcls._v111_0, cls._v110_0, cls._v101_0, cls._v100_0, cls._v40_0,\n\t\tcls._v111_1, cls._v110_1, cls._v101_1, cls._v100_1, cls._v40_1,\n\t\tcls._v111_2, cls._v110_2, cls._v101_2, cls._v100_2, cls._v40_2,\n\t\tcls._v111_3, cls._v110_3, cls._v101_3, cls._v100_3, cls._v40_3\n\t]\n\tif protocol_version is None:\n\t\treturn {\n\t\t\tversion.protocol_version: version for version in versions_supported\n\t\t}\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError('protocol version must be a tuple')\n\tversion_tuple = tuple(protocol_version)\n\treturn {\n\t\tversion.protocol_version: version\n\t\tfor version in versions", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t...", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t...", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t...", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\tfor handler in cls._handlers.values():\n\t\tif protocol_version in handler.protocol_versions:\n\t\t\thandlers[handler.protocol_versions[protocol_version]] = handler\n\t\telif not protocol_version:\n\t\t\thandlers[handler.protocol_versions[handler.latest_version]] = handler\n\treturn handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.append(cls.from_raw_value(value))\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor raw_value in values:\n\t\tbookmarks.add_raw_value(raw_value)\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.values = values\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls(values)\n\tbookmarks.process()\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\n\tfor value in values:\n\t\tbookmarks.add(value)\n\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.raw = values\n\tfor value in values:\n\t\tbm = Bookmark.from_raw_value(value)\n\t\tbookmarks.add(bm)\n\treturn bookmarks", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.append(cls.from_raw_value(value))\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\n\tfor value in values:\n\t\tbookmarks.add(value)\n\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor bookmark in values:\n\t\tbookmarks.append(cls.from_raw_value(bookmark))\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tpass"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.append(cls.from_raw_value(value))\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor raw_value in values:\n\t\tbookmarks.add_raw_value(raw_value)\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.values = values\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls(values)\n\tbookmarks.process()\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\n\tfor value in values:\n\t\tbookmarks.add(value)\n\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.raw = values\n\tfor value in values:\n\t\tbm = Bookmark.from_raw_value(value)\n\t\tbookmarks.add(bm)\n\treturn bookmarks", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.append(cls.from_raw_value(value))\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\n\tfor value in values:\n\t\tbookmarks.add(value)\n\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor bookmark in values:\n\t\tbookmarks.append(cls.from_raw_value(bookmark))\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\ttype = seq.__class__\n\texcept AttributeError:\n\t\ttype = type(seq)\n\tif type in (str, bytes):\n\t\tseq = seq[0:bufsize]\n\treturn seq, type\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\timport types\n\tif isinstance(seq, types.GeneratorType):\n\t\tif not bufsize:\n\t\t\tbufsize = 1024*1024\n\t\tseq = list(seq)\n\telif isinstance(seq, types.StringTypes):\n\t\tseq = list(seq)\n\treturn seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttype = None\n\n\tif bufsize is None:\n\t\tbufsize = getattr(seq, 'bufsize', 0)\n\n\tif bufsize:\n\t\ttype = 'buffer'\n\telif isinstance(seq, (bytes, str)):\n\t\ttype = 'string'\n\telif isinstance(seq, (list, tuple, bytearray)):\n\t\ttype = 'sequence'\n\telif hasattr(seq, '__getitem__'):\n\t\ttype = 'iterable'\n\telif hasattr(seq, '__iter__'):\n\t\ttype = 'iterator'\n\n\tif type is None:\n\t\traise TypeError(\"cannot determine sequence type\")\n\n\treturn (seq, type)", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = seq.__getitem__\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\treturn seq, 'sequence'\n\ttry:\n\t\titem = seq.__iter__\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\treturn item, 'iterator'\n\tif bufsize is None:\n\t\tbufsize = getattr(seq, '_buffer_size', None)\n\tif bufsize is not None:\n\t\treturn bufsize, 'buffer'\n\treturn None, 'unknown'", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq_type = seq[0].__class__\n\texcept IndexError:\n\t\tseq_type = None\n\telse:\n\t\tseq = seq\n\t\tif seq_type is str:\n\t\t\tbufsize = bufsize or len(seq)\n\t\t\tseq = memoryview(seq)\n\telif seq_type is memoryview:\n\t\tbufsize = bufsize or len(seq)\n\telse:\n\t\tseq = bytes(seq)\n\t\tseq_type = bytes\n\t\tbufsize = bufsize or len(seq)\n\treturn seq, seq_type, bufsize\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tsequence = seq.__iter__()\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\ttype = 'iter'\n\telse:\n\t\tsequence = iter(seq)\n\t\ttype = 'seq'\n\n\tif bufsize is not None:\n\t\tsequence = iter(sequence, None, bufsize)\n\n\treturn sequence, type\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tit = iter(seq)\n\texcept TypeError:\n\t\treturn seq, type(seq)\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\ttry:\n\t\titem = next(it)\n\texcept StopIteration:\n\t\treturn [], type(seq)\n\tbuf = [item]\n\twhile len(buf) < bufsize:\n\t\ttry:\n\t\t\titem = next(it)\n\t\texcept StopIteration:\n\t\t\tbreak\n\t\tbuf.append(item)\n\treturn buf, type(seq)\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq_type = type(seq)\n\tif seq_type in (str, unicode):\n\t\treturn seq, 'str'\n\telif seq_type is bytes:\n\t\treturn seq, 'bytes'\n\telif seq_type is bytearray:\n\t\treturn seq, 'bytearray'\n\telif seq_type is memoryview:\n\t\treturn seq, 'memoryview'\n\telif hasattr(seq_type, '__getitem__'):\n\t\treturn seq, 'list'\n\telif hasattr(seq_type, '__iter__'):\n\t\treturn seq, 'iter'\n\telif hasattr(seq_type, '__next__'):\n\t\treturn seq, 'next'\n\telif bufsize is not None:\n\t\treturn b'', 'buffer'\n\telse:\n\t\traise TypeError('not a sequence')\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tfrom types import ListType, TupleType\n\ttry:\n\t\tseq = iter(seq)\n\texcept TypeError:\n\t\tpass\n\tif isinstance(seq, ListType):\n\t\tseq, type = seq, 'list'\n\telif isinstance(seq, TupleType):\n\t\tseq, type = seq, 'tuple'\n\telse:\n\t\tseq, type = list(seq), 'iterable'\n\tif bufsize is not None:\n\t\tseq, type = list(seq)[:bufsize], 'list'\n\treturn seq, type", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = seq(())\n\t\tif isinstance(seq, (str, bytes)):\n\t\t\treturn seq, 's'\n\texcept TypeError:\n\t\tpass\n\ttry:\n\t\tseq = seq[0]\n\t\ttry:\n\t\t\tseq = seq(())\n\t\t\tif isinstance(seq, (str, bytes)):\n\t\t\t\treturn seq, 's'\n\t\texcept TypeError:\n\t\t\tpass\n\t\treturn seq, 'o'\n\texcept IndexError:\n\t\tif bufsize is not None:\n\t\t\treturn b'', 's'\n\t\traise ValueError(\"bufsize must be an integer\")\n\texcept TypeError:\n\t\tpass\n\traise TypeError(\"a bytes-like object is required, not '{0}'\".format(type(seq).__name__))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\ttype = seq.__class__\n\texcept AttributeError:\n\t\ttype = type(seq)\n\tif type in (str, bytes):\n\t\tseq = seq[0:bufsize]\n\treturn seq, type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\timport types\n\tif isinstance(seq, types.GeneratorType):\n\t\tif not bufsize:\n\t\t\tbufsize = 1024*1024\n\t\tseq = list(seq)\n\telif isinstance(seq, types.StringTypes):\n\t\tseq = list(seq)\n\treturn seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttype = None\n\n\tif bufsize is None:\n\t\tbufsize = getattr(seq, 'bufsize', 0)\n\n\tif bufsize:\n\t\ttype = 'buffer'\n\telif isinstance(seq, (bytes, str)):\n\t\ttype = 'string'\n\telif isinstance(seq, (list, tuple, bytearray)):\n\t\ttype = 'sequence'\n\telif hasattr(seq, '__getitem__'):\n\t\ttype = 'iterable'\n\telif hasattr(seq, '__iter__'):\n\t\ttype = 'iterator'\n\n\tif type is None:\n\t\traise TypeError(\"cannot determine sequence type\")\n\n\treturn (seq, type)", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = seq.__getitem__\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\treturn seq, 'sequence'\n\ttry:\n\t\titem = seq.__iter__\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\treturn item, 'iterator'\n\tif bufsize is None:\n\t\tbufsize = getattr(seq, '_buffer_size', None)\n\tif bufsize is not None:\n\t\treturn bufsize, 'buffer'\n\treturn None, 'unknown'", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq_type = seq[0].__class__\n\texcept IndexError:\n\t\tseq_type = None\n\telse:\n\t\tseq = seq\n\t\tif seq_type is str:\n\t\t\tbufsize = bufsize or len(seq)\n\t\t\tseq = memoryview(seq)\n\telif seq_type is memoryview:\n\t\tbufsize = bufsize or len(seq)\n\telse:\n\t\tseq = bytes(seq)\n\t\tseq_type = bytes\n\t\tbufsize = bufsize or len(seq)\n\treturn seq, seq_type, bufsize\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tsequence = seq.__iter__()\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\ttype = 'iter'\n\telse:\n\t\tsequence = iter(seq)\n\t\ttype = 'seq'\n\n\tif bufsize is not None:\n\t\tsequence = iter(sequence, None, bufsize)\n\n\treturn sequence, type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tit = iter(seq)\n\texcept TypeError:\n\t\treturn seq, type(seq)\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\ttry:\n\t\titem = next(it)\n\texcept StopIteration:\n\t\treturn [], type(seq)\n\tbuf = [item]\n\twhile len(buf) < bufsize:\n\t\ttry:\n\t\t\titem = next(it)\n\t\texcept StopIteration:\n\t\t\tbreak\n\t\tbuf.append(item)\n\treturn buf, type(seq)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq_type = type(seq)\n\tif seq_type in (str, unicode):\n\t\treturn seq, 'str'\n\telif seq_type is bytes:\n\t\treturn seq, 'bytes'\n\telif seq_type is bytearray:\n\t\treturn seq, 'bytearray'\n\telif seq_type is memoryview:\n\t\treturn seq, 'memoryview'\n\telif hasattr(seq_type, '__getitem__'):\n\t\treturn seq, 'list'\n\telif hasattr(seq_type, '__iter__'):\n\t\treturn seq, 'iter'\n\telif hasattr(seq_type, '__next__'):\n\t\treturn seq, 'next'\n\telif bufsize is not None:\n\t\treturn b'', 'buffer'\n\telse:\n\t\traise TypeError('not a sequence')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tfrom types import ListType, TupleType\n\ttry:\n\t\tseq = iter(seq)\n\texcept TypeError:\n\t\tpass\n\tif isinstance(seq, ListType):\n\t\tseq, type = seq, 'list'\n\telif isinstance(seq, TupleType):\n\t\tseq, type = seq, 'tuple'\n\telse:\n\t\tseq, type = list(seq), 'iterable'\n\tif bufsize is not None:\n\t\tseq, type = list(seq)[:bufsize], 'list'\n\treturn seq, type", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = seq(())\n\t\tif isinstance(seq, (str, bytes)):\n\t\t\treturn seq, 's'\n\texcept TypeError:\n\t\tpass\n\ttry:\n\t\tseq = seq[0]\n\t\ttry:\n\t\t\tseq = seq(())\n\t\t\tif isinstance(seq, (str, bytes)):\n\t\t\t\treturn seq, 's'\n\t\texcept TypeError:\n\t\t\tpass\n\t\treturn seq, 'o'\n\texcept IndexError:\n\t\tif bufsize is not None:\n\t\t\treturn b'', 's'\n\t\traise ValueError(\"bufsize must be an integer\")\n\texcept TypeError:\n\t\tpass\n\traise TypeError(\"a bytes-like object is required, not '{0}'\".format(type(seq).__name__))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError()\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t...\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t...\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom ..graph import graph\n\tfrom ..histogram import numeric_histogram\n\n\tgraph_type = graph if make_value is None else graph.with_errors\n\n\tdimension = hist.dimension\n\n\tif not isinstance(hist, numeric_histogram):\n\t\traise TypeError(\"histogram must contain only numeric bins\")\n\n\tbin_coordinates = {\n\t\t\"left\": lambda bin_: bin_.left,\n\t\t\"right\": lambda bin_: bin_.right,\n\t\t\"middle\": lambda bin_: (bin_.left + bin_.right) / 2,\n\t}[get_coordinate]\n\n\tmake_value = make_value or lambda bin_: bin_.content\n\n\tpoints = [\n\t\t(bin_coordinates(bin_), make_value(bin_))\n\t\tfor bin_ in hist\n\t]\n\n\tfields = field_names\n\n\tif len(fields) != dimension:\n\t\traise ValueError(\"field_names have wrong number of items\")\n\n\treturn graph_type(points, fields, scale)", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom . import graph\n\tif make_value is None:\n\t\tmake_value = hist.content\n\tdim = hist.dim\n\tif scale is None:\n\t\tscale = hist.scale\n\telif scale:\n\t\tscale = hist.scale\n\tg = graph.graph(dim)\n\tif get_coordinate == \"left\":\n\t\tg.add_points([(hist.left(i), make_value(hist[i])) for i in range(hist.bins_count)])\n\telif get_coordinate == \"right\":\n\t\tg.add_points([(hist.right(i), make_value(hist[i])) for i in range(hist.bins_count)])\n\telif get_coordinate == \"middle\":\n\t\tg.add_points([(hist.middle(i), make_value(hist[i])) for i in range(hist.bins_count)])\n\telse:\n\t\traise ValueError(\"unknown coordinate\")\n\tg.set_scale(scale)\n\tg.set_field_names(*field_names)\n\treturn g\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph, Point\n\tfrom .scale import Scale, linear_scale\n\n\tdimension = hist.dimension\n\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.middle\n\telse:\n\t\traise ValueError(\"Invalid get_coordinate: {}\".format(get_coordinate))\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\n\tif scale is True:\n\t\tscale = hist.scale\n\telif scale is None:\n\t\tscale = linear_scale\n\n\tgraph = Graph(scale, dimension, field_names)\n\n\tfor bin_ in hist:\n\t\tgraph.add_point(Point(get_coordinate(bin_), make_value(bin_)))\n\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom . import histogram, graph\n\n\tg = graph(hist.dimension)\n\tg.scale = hist.scale if scale is True else scale\n\n\tfor i, axis in enumerate((\"x\", \"y\")):\n\t\tgetter = getattr(hist, \"get_\" + axis)\n\t\tg.set_coordinate(i, getter(), get_coordinate=get_coordinate)\n\t\tfor name, bin_ in zip(field_names, hist):\n\t\t\tsetter = getattr(g, \"set_\" + name)\n\t\t\tif make_value:\n\t\t\t\tsetter(bin_, make_value(bin_))\n\t\t\telse:\n\t\t\t\tsetter(bin_)\n\n\treturn g", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph\n\tget_coordinate = {\"left\": 0, \"right\": 1, \"middle\": 0.5}.get(get_coordinate, 0.5)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tgraph = graph(hist.dimension)\n\tgraph.field_names = field_names\n\tif scale is None:\n\t\tgraph.scale = hist.scale\n\telif scale is True:\n\t\tgraph.scale = hist.scale\n\telse:\n\t\tgraph.scale = scale\n\tfor bin_ in hist:\n\t\tx = bin_.center(get_coordinate)\n\t\ty = make_value(bin_)\n\t\tgraph.add_point(x, y)\n\treturn graph", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom ..graph import graph\n\tfrom ..histogram import HistogramView\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\n\tg = graph()\n\tfor dim in range(hist.dimension):\n\t\tg.add_field(field_names[dim])\n\n\tdim_coordinates = [[] for _ in range(hist.dimension)]\n\tdim_coordinates[0] = hist.coordinate(0, get_coordinate)\n\tfor dim in range(1, hist.dimension):\n\t\tdim_coordinates[dim] = dim_coordinates[0]\n\n\tcoordinates = tuple(dim_coordinates)\n\tfor bin_ in hist:\n\t\tg.add(coordinates, make_value(bin_))\n\n\tif scale is None:\n\t\tscale = hist.scale\n\telif scale is True:\n\t\tscale = hist.scale\n\telse:\n\t\tg.scale = scale\n\n\treturn g\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph\n\tfrom . import bin\n\n\tdimension = hist.dimension()\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\n\tgraph_ = graph(dimension)\n\tgraph_.field_names = field_names\n\n\tx_name, y_name = field_names\n\n\tdef add_point(point):\n\t\tcoords = [None] * dimension\n\t\tcoords[0] = getattr(point, x_name)\n\t\tvalue = make_value(point)\n\t\tif isinstance(value, tuple):\n\t\t\tcoords[1:] = value\n\t\telse:\n\t\t\tcoords[1] = value\n\t\tgraph_.add_point(coords)\n\n\tif scale is True:\n\t\thist.walk(add_point, scale=True)\n\telse:\n\t\thist.walk(add_point)\n\n\treturn graph_\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom . import graph\n\tfrom ._internal import _iter_bins, _iter_bin_coords, _iter_bin_coords_with_value\n\n\tif make_value is None:\n\t\tdef make_value(value):\n\t\t\treturn value\n\n\tdimension = hist.dimension\n\tresult = graph(dimension)\n\tresult.field_names = field_names\n\n\tif scale is None:\n\t\tresult.scale = None\n\telif scale is True:\n\t\tresult.scale = hist.scale\n\telse:\n\t\tresult.scale = scale\n\n\tif dimension == 1:\n\t\tresult.points.extend(_iter_bin_coords_with_value(hist, make_value, get_coordinate))\n\telse:\n\t\tresult.points.extend(_iter_bin_coords(hist, get_coordinate))\n\t\tfor bin_ in _iter_bins(hist):\n\t\t\tbin_value = make_value(bin_)\n\t\t\tif bin_value is None:\n\t\t\t\tcontinue\n\t\t\tresult.points.append(bin_value + result.points[-1])\n\n\treturn result", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\timport numpy as np\n\tfrom ..core import histogram\n\tfrom ..core import graph\n\n\tndim = hist.ndim\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tcoord = hist.lefts\n\telif get_coordinate == \"right\":\n\t\tcoord = hist.rights\n\telif get_coordinate == \"middle\":\n\t\tcoord = hist.middles\n\telse:\n\t\traise ValueError(\"Unknown get_coordinate: {}\".format(get_coordinate))\n\tdata = np.array([make_value(hist[b]) for b in hist.bins])\n\tdata = data.reshape(-1, ndim)\n\tcoord = np.broadcast_to(coord, data.shape)\n\tcoord = coord.reshape(-1, ndim)\n\tgraph = graph(coord, data)\n\tgraph.field_names = field_names\n\tif scale is True:\n\t\tgraph.scale = hist.scale\n\telif scale is not None:\n\t\tgraph.scale = scale\n\treturn graph", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph, point\n\tfrom .histogram import histogram\n\tfrom .context import context, context_or\n\n\tif not (isinstance(hist, histogram) and hist.dimension == 1):\n\t\traise TypeError(\"histogram expected\")\n\tres_hist = hist\n\tif scale is True:\n\t\tscale = res_hist.scale\n\telif scale is False:\n\t\tscale = None\n\tres_graph = graph(scale=scale)\n\tfor bin_ in res_hist:\n\t\tx = getattr(bin_, get_coordinate)\n\t\ty = bin_.content\n\t\tif make_value:\n\t\t\ty = make_value(bin_)\n\t\tres_graph.add_point(point(x, y))\n\treturn res_graph\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom ..graph import graph\n\tfrom ..histogram import numeric_histogram\n\n\tgraph_type = graph if make_value is None else graph.with_errors\n\n\tdimension = hist.dimension\n\n\tif not isinstance(hist, numeric_histogram):\n\t\traise TypeError(\"histogram must contain only numeric bins\")\n\n\tbin_coordinates = {\n\t\t\"left\": lambda bin_: bin_.left,\n\t\t\"right\": lambda bin_: bin_.right,\n\t\t\"middle\": lambda bin_: (bin_.left + bin_.right) / 2,\n\t}[get_coordinate]\n\n\tmake_value = make_value or lambda bin_: bin_.content\n\n\tpoints = [\n\t\t(bin_coordinates(bin_), make_value(bin_))\n\t\tfor bin_ in hist\n\t]\n\n\tfields = field_names\n\n\tif len(fields) != dimension:\n\t\traise ValueError(\"field_names have wrong number of items\")\n\n\treturn graph_type(points, fields, scale)", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom . import graph\n\tif make_value is None:\n\t\tmake_value = hist.content\n\tdim = hist.dim\n\tif scale is None:\n\t\tscale = hist.scale\n\telif scale:\n\t\tscale = hist.scale\n\tg = graph.graph(dim)\n\tif get_coordinate == \"left\":\n\t\tg.add_points([(hist.left(i), make_value(hist[i])) for i in range(hist.bins_count)])\n\telif get_coordinate == \"right\":\n\t\tg.add_points([(hist.right(i), make_value(hist[i])) for i in range(hist.bins_count)])\n\telif get_coordinate == \"middle\":\n\t\tg.add_points([(hist.middle(i), make_value(hist[i])) for i in range(hist.bins_count)])\n\telse:\n\t\traise ValueError(\"unknown coordinate\")\n\tg.set_scale(scale)\n\tg.set_field_names(*field_names)\n\treturn g\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph, Point\n\tfrom .scale import Scale, linear_scale\n\n\tdimension = hist.dimension\n\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.middle\n\telse:\n\t\traise ValueError(\"Invalid get_coordinate: {}\".format(get_coordinate))\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\n\tif scale is True:\n\t\tscale = hist.scale\n\telif scale is None:\n\t\tscale = linear_scale\n\n\tgraph = Graph(scale, dimension, field_names)\n\n\tfor bin_ in hist:\n\t\tgraph.add_point(Point(get_coordinate(bin_), make_value(bin_)))\n\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom . import histogram, graph\n\n\tg = graph(hist.dimension)\n\tg.scale = hist.scale if scale is True else scale\n\n\tfor i, axis in enumerate((\"x\", \"y\")):\n\t\tgetter = getattr(hist, \"get_\" + axis)\n\t\tg.set_coordinate(i, getter(), get_coordinate=get_coordinate)\n\t\tfor name, bin_ in zip(field_names, hist):\n\t\t\tsetter = getattr(g, \"set_\" + name)\n\t\t\tif make_value:\n\t\t\t\tsetter(bin_, make_value(bin_))\n\t\t\telse:\n\t\t\t\tsetter(bin_)\n\n\treturn g", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph\n\tget_coordinate = {\"left\": 0, \"right\": 1, \"middle\": 0.5}.get(get_coordinate, 0.5)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tgraph = graph(hist.dimension)\n\tgraph.field_names = field_names\n\tif scale is None:\n\t\tgraph.scale = hist.scale\n\telif scale is True:\n\t\tgraph.scale = hist.scale\n\telse:\n\t\tgraph.scale = scale\n\tfor bin_ in hist:\n\t\tx = bin_.center(get_coordinate)\n\t\ty = make_value(bin_)\n\t\tgraph.add_point(x, y)\n\treturn graph", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom ..graph import graph\n\tfrom ..histogram import HistogramView\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\n\tg = graph()\n\tfor dim in range(hist.dimension):\n\t\tg.add_field(field_names[dim])\n\n\tdim_coordinates = [[] for _ in range(hist.dimension)]\n\tdim_coordinates[0] = hist.coordinate(0, get_coordinate)\n\tfor dim in range(1, hist.dimension):\n\t\tdim_coordinates[dim] = dim_coordinates[0]\n\n\tcoordinates = tuple(dim_coordinates)\n\tfor bin_ in hist:\n\t\tg.add(coordinates, make_value(bin_))\n\n\tif scale is None:\n\t\tscale = hist.scale\n\telif scale is True:\n\t\tscale = hist.scale\n\telse:\n\t\tg.scale = scale\n\n\treturn g\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph\n\tfrom . import bin\n\n\tdimension = hist.dimension()\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\n\tgraph_ = graph(dimension)\n\tgraph_.field_names = field_names\n\n\tx_name, y_name = field_names\n\n\tdef add_point(point):\n\t\tcoords = [None] * dimension\n\t\tcoords[0] = getattr(point, x_name)\n\t\tvalue = make_value(point)\n\t\tif isinstance(value, tuple):\n\t\t\tcoords[1:] = value\n\t\telse:\n\t\t\tcoords[1] = value\n\t\tgraph_.add_point(coords)\n\n\tif scale is True:\n\t\thist.walk(add_point, scale=True)\n\telse:\n\t\thist.walk(add_point)\n\n\treturn graph_\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom . import graph\n\tfrom ._internal import _iter_bins, _iter_bin_coords, _iter_bin_coords_with_value\n\n\tif make_value is None:\n\t\tdef make_value(value):\n\t\t\treturn value\n\n\tdimension = hist.dimension\n\tresult = graph(dimension)\n\tresult.field_names = field_names\n\n\tif scale is None:\n\t\tresult.scale = None\n\telif scale is True:\n\t\tresult.scale = hist.scale\n\telse:\n\t\tresult.scale = scale\n\n\tif dimension == 1:\n\t\tresult.points.extend(_iter_bin_coords_with_value(hist, make_value, get_coordinate))\n\telse:\n\t\tresult.points.extend(_iter_bin_coords(hist, get_coordinate))\n\t\tfor bin_ in _iter_bins(hist):\n\t\t\tbin_value = make_value(bin_)\n\t\t\tif bin_value is None:\n\t\t\t\tcontinue\n\t\t\tresult.points.append(bin_value + result.points[-1])\n\n\treturn result", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\timport numpy as np\n\tfrom ..core import histogram\n\tfrom ..core import graph\n\n\tndim = hist.ndim\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tcoord = hist.lefts\n\telif get_coordinate == \"right\":\n\t\tcoord = hist.rights\n\telif get_coordinate == \"middle\":\n\t\tcoord = hist.middles\n\telse:\n\t\traise ValueError(\"Unknown get_coordinate: {}\".format(get_coordinate))\n\tdata = np.array([make_value(hist[b]) for b in hist.bins])\n\tdata = data.reshape(-1, ndim)\n\tcoord = np.broadcast_to(coord, data.shape)\n\tcoord = coord.reshape(-1, ndim)\n\tgraph = graph(coord, data)\n\tgraph.field_names = field_names\n\tif scale is True:\n\t\tgraph.scale = hist.scale\n\telif scale is not None:\n\t\tgraph.scale = scale\n\treturn graph", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph, point\n\tfrom .histogram import histogram\n\tfrom .context import context, context_or\n\n\tif not (isinstance(hist, histogram) and hist.dimension == 1):\n\t\traise TypeError(\"histogram expected\")\n\tres_hist = hist\n\tif scale is True:\n\t\tscale = res_hist.scale\n\telif scale is False:\n\t\tscale = None\n\tres_graph = graph(scale=scale)\n\tfor bin_ in res_hist:\n\t\tx = getattr(bin_, get_coordinate)\n\t\ty = bin_.content\n\t\tif make_value:\n\t\t\ty = make_value(bin_)\n\t\tres_graph.add_point(point(x, y))\n\treturn res_graph\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\traise NotImplementedError()\n\n\tdef _verify_attributes(iface, candidate):\n\t\t\"\"\"\n\t\tVerify the attributes of the candidate.\n\n\t\t:return bool: Returns a true value if everything that could be\n\t\tchecked passed.\n\t\t:raises zope.interface.Invalid: If any of the previous\n\t\tconditions does not hold.\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\n\tdef _verify_methods(iface, candidate):\n\t\t\"\"\"\n\t\tVerify the methods of the candidate.\n\n\t\t:return bool: Returns a true value if everything that could be\n\t\tchecked passed.\n\t\t:raises zope.interface.Invalid: If any of the previous\n\t\tconditions does not hold.\n\t\t\"\"\"\n\t\traise NotImplementedError()", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface.declarations import getDottedName\n\n\tiface = getDottedName(iface)\n\n\terrors = []\n\n\tif iface != 'zope.interface.Interface' and not iface.startswith('zope.interface.interfaces.'):\n\t\terrors.append('{0} is not a valid interface'.format(iface))\n\n\tif not tentative:\n\t\tprovidedBy = getattr(candidate, 'providedBy', None)\n\n\t\tif providedBy is None or iface not in providedBy:\n\t\t\terrors.append('{0} does not provide {1}'.format(candidate, iface))\n\n\tmethods = iface.methods\n\n\tfor name, method in methods.items():\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\terrors.append('{0} does not define {1}'.format(candidate, name))\n\t\texcept Exception:\n\t\t\terrors.append('{0}.{1} has an invalid signature'.format(candidate, name))\n\n\tattributes = iface.attributes\n\n\tfor name, attribute in attributes.items():\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError:\n\t\t", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\traise NotImplementedError()", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\terrors = []\n\tmissing_attrs = iface.names() - candidate.__dict__.keys()\n\tmissing_methods = iface.names(include_methods=True) - candidate.__dict__.keys()\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Object does not provide the required interface.\",\n\t\t\t\t\tiface,\n\t\t\t\t\tcandidate,\n\t\t\t\t)\n\t\t\t)\n\tmissing_attrs = iface.names() - candidate.__dict__.keys()\n\tmissing_methods = iface.names(include_methods=True) - candidate.__dict__.keys()\n\tif missing_attrs:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Missing attributes: {0}.\".format(\", \".join(missing_attrs)),\n\t\t\t\tiface,\n\t\t\t\tcandidate,\n\t\t\t)\n\t\t)\n\tif missing_methods:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Missing methods: {0}.\".format(\", \".join(missing_methods)),\n\t\t\t\tiface,\n\t\t\t\tcandidate,\n\t\t\t)\n\t\t)\n\tfor name in iface.names(include_methods=True):\n\t\tattr = getattr(c"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\traise NotImplementedError()\n\n\tdef _verify_attributes(iface, candidate):\n\t\t\"\"\"\n\t\tVerify the attributes of the candidate.\n\n\t\t:return bool: Returns a true value if everything that could be\n\t\tchecked passed.\n\t\t:raises zope.interface.Invalid: If any of the previous\n\t\tconditions does not hold.\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\n\tdef _verify_methods(iface, candidate):\n\t\t\"\"\"\n\t\tVerify the methods of the candidate.\n\n\t\t:return bool: Returns a true value if everything that could be\n\t\tchecked passed.\n\t\t:raises zope.interface.Invalid: If any of the previous\n\t\tconditions does not hold.\n\t\t\"\"\"\n\t\traise NotImplementedError()", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface.declarations import getDottedName\n\n\tiface = getDottedName(iface)\n\n\terrors = []\n\n\tif iface != 'zope.interface.Interface' and not iface.startswith('zope.interface.interfaces.'):\n\t\terrors.append('{0} is not a valid interface'.format(iface))\n\n\tif not tentative:\n\t\tprovidedBy = getattr(candidate, 'providedBy', None)\n\n\t\tif providedBy is None or iface not in providedBy:\n\t\t\terrors.append('{0} does not provide {1}'.format(candidate, iface))\n\n\tmethods = iface.methods\n\n\tfor name, method in methods.items():\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\terrors.append('{0} does not define {1}'.format(candidate, name))\n\t\texcept Exception:\n\t\t\terrors.append('{0}.{1} has an invalid signature'.format(candidate, name))\n\n\tattributes = iface.attributes\n\n\tfor name, attribute in attributes.items():\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError:\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\terrors = []\n\tmissing_attrs = iface.names() - candidate.__dict__.keys()\n\tmissing_methods = iface.names(include_methods=True) - candidate.__dict__.keys()\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Object does not provide the required interface.\",\n\t\t\t\t\tiface,\n\t\t\t\t\tcandidate,\n\t\t\t\t)\n\t\t\t)\n\tmissing_attrs = iface.names() - candidate.__dict__.keys()\n\tmissing_methods = iface.names(include_methods=True) - candidate.__dict__.keys()\n\tif missing_attrs:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Missing attributes: {0}.\".format(\", \".join(missing_attrs)),\n\t\t\t\tiface,\n\t\t\t\tcandidate,\n\t\t\t)\n\t\t)\n\tif missing_methods:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Missing methods: {0}.\".format(\", \".join(missing_methods)),\n\t\t\t\tiface,\n\t\t\t\tcandidate,\n\t\t\t)\n\t\t)\n\tfor name in iface.names(include_methods=True):\n\t\tattr = getattr(c", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\traise NotImplementedError(verifyObject)", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\traise NotImplementedError(verifyObject)", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom pyrfc.exceptions import (\n\t\tRFCError,\n\t\tInvalidInterfaceError,\n\t\tBadTypeError,\n\t\tInvalidClassError,\n\t\tInterfaceNotFoundError,\n\t)\n\n\tif not isinstance(iface, str):\n\t\traise BadTypeError(\n\t\t\t\"Wrong interface type %s, expected str\" % type(iface).__name__\n\t\t)\n\tif not isinstance(candidate, str):\n\t\traise BadTypeError(\n\t\t\t\"Wrong candidate type %s, expected str\" % type(candidate).__name__\n\t\t)\n\n\ttry:\n\t\tclass_candidate = getattr(iface, candidate)\n\texcept AttributeError:\n\t\traise InvalidInterfaceError(\n\t\t\t\"Candidate %s not found in interface %s\" % (candidate, iface)\n\t\t)\n\n\ttry:\n\t\tclass_iface = getattr(candidate, iface)\n\texcept AttributeError:\n\t\traise InterfaceNotFoundError(\n\t\t\t\"Interface %s not found in candidate %s\" % (iface, candidate)\n\t\t)\n\n\tif not hasattr(class_iface, \"__abstractmethods__\"):\n\t\traise InvalidClassError(\"Candidate %s is not an abstract class\" %", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tif tentative:\n\t\tfrom twisted.internet import reactor\n\t\n\tdef _verify(result):\n\t\tif result.value.isFailure():\n\t\t\tresult.raiseException()\n\t\treturn result.value.getResult()\n\t\n\td = candidate.provides(iface)\n\tif tentative:\n\t\td.addCallback(_verify)\n\t\td.addErrback(reactor.stop)\n\t\n\treturn d", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\timport os\n\t\n\timport gobject\n\t\n\tfrom gi.repository import Gio\n\t\n\tfrom sugar3.logger import LOG\n\t\n\ttry:\n\t\tgobject.get_type(candidate)\n\texcept Exception as e:\n\t\tLOG.debug('Failed to load class %s: %s' % (candidate, e))\n\t\treturn False\n\t\n\ttry:\n\t\tinstance = gobject.type_register_static(gobject.type_from_name(candidate), Gio.TYPE_PLUGIN)\n\texcept Exception as e:\n\t\tLOG.debug('Failed to create instance of %s: %s' % (candidate, e))\n\t\treturn False\n\t\n\tif tentative:\n\t\tif not isinstance(instance, iface):\n\t\t\tLOG.debug('Class %s does not implement %s' % (candidate, iface))\n\t\t\treturn False\n\t\n\ttry:\n\t\tresult = instance.query()\n\texcept Exception as e:\n\t\tLOG.debug('Failed to query instance of %s: %s' % (candidate, e))\n\t\treturn False\n\t\n\tif (result & iface.MASK) != iface.MASK:\n\t\tLOG.debug('Class %s does not implement %", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom pyre.components.Component import Component\n\tassert isinstance(candidate, Component), \"candidate must be a Component\"\n\tassert iface.isCompatible(candidate)\n\t\n\t# set the component's identity\n\tif tentative:\n\t\tcandidate.identity = 'tentative'\n\telse:\n\t\tcandidate.identity = 'confirmed'\n\t\n\t# we're done\n\treturn True", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\ttry:\n\t\tcls = candidate.getClass(iface)\n\texcept:\n\t\treturn False\n\t\n\tif tentative:\n\t\treturn cls is not None\n\t\n\treturn candidate.providedBy(cls)", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tpass", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tprovides = iface.providedBy(candidate)\n\texcept TypeError:\n\t\tprovides = iface.providedBy(candidate())\n\tif tentative:\n\t\treturn provides or iface in candidate.__implements__\n\telse:\n\t\treturn provides", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\traise NotImplementedError", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprovides = candidate.provides or []\n\trequired = iface.required or []\n\tprovides = [x for x in provides if not x.isAbstract()]\n\trequired = [x for x in required if not x.isAbstract()]\n\tif tentative:\n\t\tprovides = [x for x in provides if not x.isTentative()]\n\t\trequired = [x for x in required if not x.isTentative()]\n\treturn all(x.providedBy(candidate) for x in provides) and all(candidate.providesInterface(x) for x in required)\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\ttry:\n\t\tprovided = iface.providedBy(candidate)\n\texcept TypeError:\n\t\t# providedBy() is not an instance method\n\t\tprovided = candidate.providedBy(iface)\n\t\n\tif provided:\n\t\tif tentative:\n\t\t\treturn True\n\t\telif not iface.isSubclassOf(candidate):\n\t\t\traise InterfaceClassError(iface, candidate)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom pyrfc.exceptions import (\n\t\tRFCError,\n\t\tInvalidInterfaceError,\n\t\tBadTypeError,\n\t\tInvalidClassError,\n\t\tInterfaceNotFoundError,\n\t)\n\n\tif not isinstance(iface, str):\n\t\traise BadTypeError(\n\t\t\t\"Wrong interface type %s, expected str\" % type(iface).__name__\n\t\t)\n\tif not isinstance(candidate, str):\n\t\traise BadTypeError(\n\t\t\t\"Wrong candidate type %s, expected str\" % type(candidate).__name__\n\t\t)\n\n\ttry:\n\t\tclass_candidate = getattr(iface, candidate)\n\texcept AttributeError:\n\t\traise InvalidInterfaceError(\n\t\t\t\"Candidate %s not found in interface %s\" % (candidate, iface)\n\t\t)\n\n\ttry:\n\t\tclass_iface = getattr(candidate, iface)\n\texcept AttributeError:\n\t\traise InterfaceNotFoundError(\n\t\t\t\"Interface %s not found in candidate %s\" % (iface, candidate)\n\t\t)\n\n\tif not hasattr(class_iface, \"__abstractmethods__\"):\n\t\traise InvalidClassError(\"Candidate %s is not an abstract class\" %", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tif tentative:\n\t\tfrom twisted.internet import reactor\n\t\n\tdef _verify(result):\n\t\tif result.value.isFailure():\n\t\t\tresult.raiseException()\n\t\treturn result.value.getResult()\n\t\n\td = candidate.provides(iface)\n\tif tentative:\n\t\td.addCallback(_verify)\n\t\td.addErrback(reactor.stop)\n\t\n\treturn d", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\timport os\n\t\n\timport gobject\n\t\n\tfrom gi.repository import Gio\n\t\n\tfrom sugar3.logger import LOG\n\t\n\ttry:\n\t\tgobject.get_type(candidate)\n\texcept Exception as e:\n\t\tLOG.debug('Failed to load class %s: %s' % (candidate, e))\n\t\treturn False\n\t\n\ttry:\n\t\tinstance = gobject.type_register_static(gobject.type_from_name(candidate), Gio.TYPE_PLUGIN)\n\texcept Exception as e:\n\t\tLOG.debug('Failed to create instance of %s: %s' % (candidate, e))\n\t\treturn False\n\t\n\tif tentative:\n\t\tif not isinstance(instance, iface):\n\t\t\tLOG.debug('Class %s does not implement %s' % (candidate, iface))\n\t\t\treturn False\n\t\n\ttry:\n\t\tresult = instance.query()\n\texcept Exception as e:\n\t\tLOG.debug('Failed to query instance of %s: %s' % (candidate, e))\n\t\treturn False\n\t\n\tif (result & iface.MASK) != iface.MASK:\n\t\tLOG.debug('Class %s does not implement %", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom pyre.components.Component import Component\n\tassert isinstance(candidate, Component), \"candidate must be a Component\"\n\tassert iface.isCompatible(candidate)\n\t\n\t# set the component's identity\n\tif tentative:\n\t\tcandidate.identity = 'tentative'\n\telse:\n\t\tcandidate.identity = 'confirmed'\n\t\n\t# we're done\n\treturn True", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\ttry:\n\t\tcls = candidate.getClass(iface)\n\texcept:\n\t\treturn False\n\t\n\tif tentative:\n\t\treturn cls is not None\n\t\n\treturn candidate.providedBy(cls)", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tprovides = iface.providedBy(candidate)\n\texcept TypeError:\n\t\tprovides = iface.providedBy(candidate())\n\tif tentative:\n\t\treturn provides or iface in candidate.__implements__\n\telse:\n\t\treturn provides", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprovides = candidate.provides or []\n\trequired = iface.required or []\n\tprovides = [x for x in provides if not x.isAbstract()]\n\trequired = [x for x in required if not x.isAbstract()]\n\tif tentative:\n\t\tprovides = [x for x in provides if not x.isTentative()]\n\t\trequired = [x for x in required if not x.isTentative()]\n\treturn all(x.providedBy(candidate) for x in provides) and all(candidate.providesInterface(x) for x in required)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\ttry:\n\t\tprovided = iface.providedBy(candidate)\n\texcept TypeError:\n\t\t# providedBy() is not an instance method\n\t\tprovided = candidate.providedBy(iface)\n\t\n\tif provided:\n\t\tif tentative:\n\t\t\treturn True\n\t\telif not iface.isSubclassOf(candidate):\n\t\t\traise InterfaceClassError(iface, candidate)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\texplicit_mc = explicit_mc or bases[0].__metaclass__\n\tif explicit_mc is None:\n\t\tif len(bases) == 1:\n\t\t\treturn bases[0]\n\t\telif len(bases) == 2:\n\t\t\tif bases[0] is not object:\n\t\t\t\texplicit_mc = determineMetaclass(bases[1:])\n\t\t\telif bases[1] is not object:\n\t\t\t\texplicit_mc = determineMetaclass(bases[:1])\n\t\t\telse:\n\t\t\t\traise TypeError(\"Metaclass conflict: %s\" % bases)\n\tif explicit_mc is None:\n\t\traise TypeError(\"Unable to determine metaclass from %s\" % bases)\n\treturn explicit_mc\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef get_metaclass(obj):\n\t\ttry:\n\t\t\treturn obj.__metaclass__\n\t\texcept AttributeError:\n\t\t\treturn type(obj)\n\n\tdef get_bases(cls):\n\t\treturn get_metaclass(cls).mro()[1:]\n\n\tdef get_bases_metaclass(cls):\n\t\treturn [get_metaclass(c) for c in get_bases(cls)]\n\n\tdef get_bases_metaclass_names(cls):\n\t\treturn [m.__name__ for m in get_bases_metaclass(cls)]\n\n\tdef is_metaclass(obj):\n\t\treturn obj in (type, object, object.__class__)\n\n\tdef is_metaclass_name(name):\n\t\treturn name in ('type', 'object')\n\n\tdef is_generic(cls):\n\t\treturn '__metaclass__' in cls.__dict__\n\n\tdef is_generic_type(cls):\n\t\treturn '__metaclass__' in cls.__dict__ and is_metaclass(get_metaclass(cls))\n\n\tdef is_generic_type_name(cls):\n\t\treturn '__metaclass__' in cls.__dict__ and is_metaclass_name", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif mc is None:\n\t\t# Find first non-object metaclass\n\t\tfor b in bases:\n\t\t\tmc = getattr(b, '__metaclass__', None)\n\t\t\tif mc is not None and mc is not object:\n\t\t\t\tbreak\n\t\tif mc is None:\n\t\t\t# Use type as default\n\t\t\tmc = type\n\telif not isinstance(mc, type):\n\t\t# __metaclass__ must be a type\n\t\tmc = type(mc)\n\treturn mc\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif mc is not None:\n\t\treturn mc\n\tfor b in bases:\n\t\tmc = getattr(b, \"__metaclass__\", None)\n\t\tif mc is not None:\n\t\t\treturn mc\n\treturn object\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = explicit_mc\n\tif metaclass:\n\t\t# Explicit __metaclass__ takes precedence\n\t\treturn metaclass\n\t# If no explicit __metaclass__, use class's\n\tif len(bases) == 0:\n\t\tmetaclass = type\n\telif len(bases) == 1:\n\t\tmetaclass = bases[0].__metaclass__\n\telse:\n\t\t# TODO: Is this the right way to deal with multiple __metaclasses__?\n\t\tmetaclass = type\n\treturn metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclasses = [mc for b in bases for mc in getattr(b, '__metaclass__', ())]\n\tif explicit_mc:\n\t\tmetaclasses.append(explicit_mc)\n\tif not metaclasses:\n\t\tmetaclasses.append(type)\n\telif len(metaclasses) == 1:\n\t\tmetaclasses = metaclasses * 2\n\tmc = type(metaclasses[0])\n\tif any(not issubclass(mc, mc2) for mc2 in metaclasses[1:]):\n\t\traise TypeError('%s is not a metaclass' % mc)\n\treturn mc\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = None\n\tif explicit_mc:\n\t\tmetaclass = explicit_mc\n\telif bases:\n\t\tfor cls in bases:\n\t\t\tif cls.__metaclass__ != metaclass:\n\t\t\t\tif metaclass is not None:\n\t\t\t\t\traise TypeError(\"Cannot create a new class '%s' \"\n\t\t\t\t\t\t\"(instance of %s) from a combination of \"\n\t\t\t\t\t\t\"base classes '%s' (instance of %s) and '%s' \"\n\t\t\t\t\t\t\"(instance of %s)\" % (\n\t\t\t\t\t\t\tcls.__name__, cls.__metaclass__.__name__,\n\t\t\t\t\t\t\texplicit_mc.__name__, explicit_mc.__name__,\n\t\t\t\t\t\t\tmetaclass.__name__, metaclass.__name__\n\t\t\t\t\t\t))\n\t\t\t\tmetaclass = cls.__metaclass__\n\treturn metaclass", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif mc is None:\n\t\tif len(bases) > 0:\n\t\t\tmc = bases[0].__metaclass__\n\t\t\tfor base in bases:\n\t\t\t\tif base.__metaclass__ != mc:\n\t\t\t\t\tmc = None\n\t\t\t\t\tbreak\n\telif mc is not None:\n\t\tfor base in bases:\n\t\t\tif base.__metaclass__ != mc:\n\t\t\t\tmc = None\n\t\t\t\tbreak\n\treturn mc\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif not mc:\n\t\tmc = bases[0].__metaclass__\n\t\tif mc:\n\t\t\treturn mc\n\t\tfor mc in map(type, bases):\n\t\t\tif mc is not type:\n\t\t\t\treturn mc\n\t\treturn type\n\treturn mc", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tmetaclasses = [\n\t\tmc for base in bases\n\t\tfor mc in (getattr(base, \"__metaclass__\", None), base.__class__)\n\t\tif mc is not None\n\t]\n\t\n\tif explicit_mc is not None:\n\t\tmetaclasses.append(explicit_mc)\n\t\n\tmetaclasses = set(metaclasses)\n\t\n\tif len(metaclasses) == 0:\n\t\treturn type\n\t\n\tif len(metaclasses) > 1:\n\t\traise TypeError(\"Multiple metaclasses: %r\" % metaclasses)\n\t\n\treturn metaclasses.pop()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\texplicit_mc = explicit_mc or bases[0].__metaclass__\n\tif explicit_mc is None:\n\t\tif len(bases) == 1:\n\t\t\treturn bases[0]\n\t\telif len(bases) == 2:\n\t\t\tif bases[0] is not object:\n\t\t\t\texplicit_mc = determineMetaclass(bases[1:])\n\t\t\telif bases[1] is not object:\n\t\t\t\texplicit_mc = determineMetaclass(bases[:1])\n\t\t\telse:\n\t\t\t\traise TypeError(\"Metaclass conflict: %s\" % bases)\n\tif explicit_mc is None:\n\t\traise TypeError(\"Unable to determine metaclass from %s\" % bases)\n\treturn explicit_mc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef get_metaclass(obj):\n\t\ttry:\n\t\t\treturn obj.__metaclass__\n\t\texcept AttributeError:\n\t\t\treturn type(obj)\n\n\tdef get_bases(cls):\n\t\treturn get_metaclass(cls).mro()[1:]\n\n\tdef get_bases_metaclass(cls):\n\t\treturn [get_metaclass(c) for c in get_bases(cls)]\n\n\tdef get_bases_metaclass_names(cls):\n\t\treturn [m.__name__ for m in get_bases_metaclass(cls)]\n\n\tdef is_metaclass(obj):\n\t\treturn obj in (type, object, object.__class__)\n\n\tdef is_metaclass_name(name):\n\t\treturn name in ('type', 'object')\n\n\tdef is_generic(cls):\n\t\treturn '__metaclass__' in cls.__dict__\n\n\tdef is_generic_type(cls):\n\t\treturn '__metaclass__' in cls.__dict__ and is_metaclass(get_metaclass(cls))\n\n\tdef is_generic_type_name(cls):\n\t\treturn '__metaclass__' in cls.__dict__ and is_metaclass_name", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif mc is None:\n\t\t# Find first non-object metaclass\n\t\tfor b in bases:\n\t\t\tmc = getattr(b, '__metaclass__', None)\n\t\t\tif mc is not None and mc is not object:\n\t\t\t\tbreak\n\t\tif mc is None:\n\t\t\t# Use type as default\n\t\t\tmc = type\n\telif not isinstance(mc, type):\n\t\t# __metaclass__ must be a type\n\t\tmc = type(mc)\n\treturn mc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif mc is not None:\n\t\treturn mc\n\tfor b in bases:\n\t\tmc = getattr(b, \"__metaclass__\", None)\n\t\tif mc is not None:\n\t\t\treturn mc\n\treturn object\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = explicit_mc\n\tif metaclass:\n\t\t# Explicit __metaclass__ takes precedence\n\t\treturn metaclass\n\t# If no explicit __metaclass__, use class's\n\tif len(bases) == 0:\n\t\tmetaclass = type\n\telif len(bases) == 1:\n\t\tmetaclass = bases[0].__metaclass__\n\telse:\n\t\t# TODO: Is this the right way to deal with multiple __metaclasses__?\n\t\tmetaclass = type\n\treturn metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclasses = [mc for b in bases for mc in getattr(b, '__metaclass__', ())]\n\tif explicit_mc:\n\t\tmetaclasses.append(explicit_mc)\n\tif not metaclasses:\n\t\tmetaclasses.append(type)\n\telif len(metaclasses) == 1:\n\t\tmetaclasses = metaclasses * 2\n\tmc = type(metaclasses[0])\n\tif any(not issubclass(mc, mc2) for mc2 in metaclasses[1:]):\n\t\traise TypeError('%s is not a metaclass' % mc)\n\treturn mc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = None\n\tif explicit_mc:\n\t\tmetaclass = explicit_mc\n\telif bases:\n\t\tfor cls in bases:\n\t\t\tif cls.__metaclass__ != metaclass:\n\t\t\t\tif metaclass is not None:\n\t\t\t\t\traise TypeError(\"Cannot create a new class '%s' \"\n\t\t\t\t\t\t\"(instance of %s) from a combination of \"\n\t\t\t\t\t\t\"base classes '%s' (instance of %s) and '%s' \"\n\t\t\t\t\t\t\"(instance of %s)\" % (\n\t\t\t\t\t\t\tcls.__name__, cls.__metaclass__.__name__,\n\t\t\t\t\t\t\texplicit_mc.__name__, explicit_mc.__name__,\n\t\t\t\t\t\t\tmetaclass.__name__, metaclass.__name__\n\t\t\t\t\t\t))\n\t\t\t\tmetaclass = cls.__metaclass__\n\treturn metaclass", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif mc is None:\n\t\tif len(bases) > 0:\n\t\t\tmc = bases[0].__metaclass__\n\t\t\tfor base in bases:\n\t\t\t\tif base.__metaclass__ != mc:\n\t\t\t\t\tmc = None\n\t\t\t\t\tbreak\n\telif mc is not None:\n\t\tfor base in bases:\n\t\t\tif base.__metaclass__ != mc:\n\t\t\t\tmc = None\n\t\t\t\tbreak\n\treturn mc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = explicit_mc\n\tif not mc:\n\t\tmc = bases[0].__metaclass__\n\t\tif mc:\n\t\t\treturn mc\n\t\tfor mc in map(type, bases):\n\t\t\tif mc is not type:\n\t\t\t\treturn mc\n\t\treturn type\n\treturn mc", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tmetaclasses = [\n\t\tmc for base in bases\n\t\tfor mc in (getattr(base, \"__metaclass__\", None), base.__class__)\n\t\tif mc is not None\n\t]\n\t\n\tif explicit_mc is not None:\n\t\tmetaclasses.append(explicit_mc)\n\t\n\tmetaclasses = set(metaclasses)\n\t\n\tif len(metaclasses) == 0:\n\t\treturn type\n\t\n\tif len(metaclasses) > 1:\n\t\traise TypeError(\"Multiple metaclasses: %r\" % metaclasses)\n\t\n\treturn metaclasses.pop()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tself._pop(key, default)\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tnode = self._get(key, default)\n\tif node is __marker:\n\t\treturn default\n\telif node is self.root:\n\t\traise KeyError(key)\n\telse:\n\t\tself.delete_node(node)\n\t\tself._size -= 1\n\t\tself.touch_root()\n\t\treturn node.value\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tnode = self._find_node_or_sentinel(key)\n\tif node is self:\n\t\t# key not found\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\tnode.pop()\n\tself._size -= 1\n\treturn node.value\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t...\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tself._pop(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tnode = self._get(key, default)\n\tif node is __marker:\n\t\treturn default\n\telif node is self.root:\n\t\traise KeyError(key)\n\telse:\n\t\tself.delete_node(node)\n\t\tself._size -= 1\n\t\tself.touch_root()\n\t\treturn node.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tnode = self._find_node_or_sentinel(key)\n\tif node is self:\n\t\t# key not found\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\tnode.pop()\n\tself._size -= 1\n\treturn node.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tpass\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self._popitem()\n\tself._insert(key, value)\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self.head.next.key, self.head.next.value\n\tself.head.next = self.head.next.next\n\tdel self.dict[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tmincount = 0\n\tfor count, lru in self.lrus.items():\n\t\tif not mincount or count < mincount:\n\t\t\tmincount = count\n\tkey, value = self.lrus[mincount].popitem(last=False)\n\tdel self.lrus[mincount][key]\n\tif not self.lrus[mincount]:\n\t\tdel self.lrus[mincount]\n\tdel self.items[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tval = self.data.popitem()\n\tself.cnt[val[0]] -= 1\n\treturn val", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile self.freq_nodes.tail.key == 0:\n\t\tself.freq_nodes.pop()\n\tnode = self.freq_nodes.tail.pop()\n\tself.key_nodes[node.key].pop()\n\tif node.key in self.freq_nodes.tail:\n\t\tself.freq_nodes.tail[node.key].append(self.freq_nodes.tail.pop())\n\t\tself.freq_nodes.tail.move_to_head(self.freq_nodes.tail[node.key])\n\n\tdel self.key_nodes[node.key][node.key]\n\tif self.key_nodes[node.key].is_empty():\n\t\tdel self.key_nodes[node.key]\n\t\n\treturn node.key, node.value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self._ll.tail.key, self._ll.tail.value\n\tdel self._ll.tail.value\n\tdel self._ll.tail.key\n\tdel self._ll.tail.prev\n\tself._ll.tail = self._ll.tail.prev\n\tif self._ll.tail is not None:\n\t\tself._ll.tail.next = None\n\telse:\n\t\tself._ll.head = None\n\tdel self._freq[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile True:\n\t\tif not self.key_map:\n\t\t\traise KeyError('dictionary is empty')\n\t\tkey = self.data.get_head()\n\t\tself.key_map.remove(key)\n\t\tval = self.data[key]\n\t\tif val is not None:\n\t\t\tdel self.data[key]\n\t\t\treturn key, val\n\t\tself.data.remove_head()", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self._cache_map.popitem(last=False)\n\tself._cache_counter.pop(key)\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile self._counter:\n\t\tkey, value = self._counter.popitem(last=False)\n\t\tdel self._cache[key]\n\t\treturn key, value\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self._popitem()\n\tself._insert(key, value)\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self.head.next.key, self.head.next.value\n\tself.head.next = self.head.next.next\n\tdel self.dict[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tmincount = 0\n\tfor count, lru in self.lrus.items():\n\t\tif not mincount or count < mincount:\n\t\t\tmincount = count\n\tkey, value = self.lrus[mincount].popitem(last=False)\n\tdel self.lrus[mincount][key]\n\tif not self.lrus[mincount]:\n\t\tdel self.lrus[mincount]\n\tdel self.items[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tval = self.data.popitem()\n\tself.cnt[val[0]] -= 1\n\treturn val", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile self.freq_nodes.tail.key == 0:\n\t\tself.freq_nodes.pop()\n\tnode = self.freq_nodes.tail.pop()\n\tself.key_nodes[node.key].pop()\n\tif node.key in self.freq_nodes.tail:\n\t\tself.freq_nodes.tail[node.key].append(self.freq_nodes.tail.pop())\n\t\tself.freq_nodes.tail.move_to_head(self.freq_nodes.tail[node.key])\n\n\tdel self.key_nodes[node.key][node.key]\n\tif self.key_nodes[node.key].is_empty():\n\t\tdel self.key_nodes[node.key]\n\t\n\treturn node.key, node.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self._ll.tail.key, self._ll.tail.value\n\tdel self._ll.tail.value\n\tdel self._ll.tail.key\n\tdel self._ll.tail.prev\n\tself._ll.tail = self._ll.tail.prev\n\tif self._ll.tail is not None:\n\t\tself._ll.tail.next = None\n\telse:\n\t\tself._ll.head = None\n\tdel self._freq[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile True:\n\t\tif not self.key_map:\n\t\t\traise KeyError('dictionary is empty')\n\t\tkey = self.data.get_head()\n\t\tself.key_map.remove(key)\n\t\tval = self.data[key]\n\t\tif val is not None:\n\t\t\tdel self.data[key]\n\t\t\treturn key, val\n\t\tself.data.remove_head()", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self._cache_map.popitem(last=False)\n\tself._cache_counter.pop(key)\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile self._counter:\n\t\tkey, value = self._counter.popitem(last=False)\n\t\tdel self._cache[key]\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, val = self.storage.popitem(last=False)\n\tdel self.key2key[key]\n\tdel self.key2val[key]\n\treturn key, val", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\titem = self._last.prev\n\tself.delete(item)\n\treturn item.key, item.value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self._cache.popitem(last=False)\n\tdel self._keys[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\twhile self._lru_head._key is not None:\n\t\tkey = self._lru_head._key\n\t\tvalue = self._lru_head._value\n\t\tself._lru_head = self._lru_head._next\n\t\tdel self._lru_dict[key]\n\t\treturn (key, value)\n\telse:\n\t\traise KeyError(\"dictionary is empty\")\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tlastKey = self.last.bKey\n\tlastValue = self.last.value\n\tself.remove(lastKey)\n\treturn lastKey, lastValue\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tit = self.last\n\tself[it.key]\n\tself.pop(it.key)\n\treturn it.key, it.value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.popitem()\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.pop(self.head)\n\tdel self.tail[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n\tkey = self._lru.popleft()\n\tvalue = self._cache_pop(key)\n\tself._lru.append(key)\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpair = self.mapping.popitem(last=True)\n\tself.mapping[pair[0]] = pair[1]\n\treturn pair\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, val = self.storage.popitem(last=False)\n\tdel self.key2key[key]\n\tdel self.key2val[key]\n\treturn key, val", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\titem = self._last.prev\n\tself.delete(item)\n\treturn item.key, item.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self._cache.popitem(last=False)\n\tdel self._keys[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\twhile self._lru_head._key is not None:\n\t\tkey = self._lru_head._key\n\t\tvalue = self._lru_head._value\n\t\tself._lru_head = self._lru_head._next\n\t\tdel self._lru_dict[key]\n\t\treturn (key, value)\n\telse:\n\t\traise KeyError(\"dictionary is empty\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tlastKey = self.last.bKey\n\tlastValue = self.last.value\n\tself.remove(lastKey)\n\treturn lastKey, lastValue\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tit = self.last\n\tself[it.key]\n\tself.pop(it.key)\n\treturn it.key, it.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.popitem()\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.pop(self.head)\n\tdel self.tail[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n\tkey = self._lru.popleft()\n\tvalue = self._cache_pop(key)\n\tself._lru.append(key)\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpair = self.mapping.popitem(last=True)\n\tself.mapping[pair[0]] = pair[1]\n\treturn pair\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey = self.last\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self.data.popitem()\n\tdel self._cache[key]\n\treturn (key, value)\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey, value = self.last_added()\n\t\tdel self[key]\n\t\treturn key, value\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey = self.last_key\n\t\tvalue = self[key]\n\texcept KeyError:\n\t\tif self.curr_keys:\n\t\t\tkey = self.curr_keys.pop(0)\n\t\t\tvalue = self[key]\n\t\telse:\n\t\t\traise KeyError, \"popitem(): cache is empty\"\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tk = self.last\n\tif k is not None:\n\t\tv = self.__dict__.pop(k)\n\t\tself.last = self.prev[k]\n\t\tself.next[self.prev[k]] = k\n\t\tself.prev[k] = self.next[k] = None\n\t\treturn k, v\n\telse:\n\t\traise KeyError(\"last\")\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self._order.pop(0)\n\tdel self._dict[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self.last()\n\tdel self[key]\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey = self.last\n\tif key is not None:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn (key, value)\n\traise KeyError(\"popitem(): dictionary is empty\")", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self.last\n\tdel self.last\n\tself.pop(key)\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey = self.last()\n\tself.pop(key)\n\treturn key, self[key]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey = self.last\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self.data.popitem()\n\tdel self._cache[key]\n\treturn (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey, value = self.last_added()\n\t\tdel self[key]\n\t\treturn key, value\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey = self.last_key\n\t\tvalue = self[key]\n\texcept KeyError:\n\t\tif self.curr_keys:\n\t\t\tkey = self.curr_keys.pop(0)\n\t\t\tvalue = self[key]\n\t\telse:\n\t\t\traise KeyError, \"popitem(): cache is empty\"\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tk = self.last\n\tif k is not None:\n\t\tv = self.__dict__.pop(k)\n\t\tself.last = self.prev[k]\n\t\tself.next[self.prev[k]] = k\n\t\tself.prev[k] = self.next[k] = None\n\t\treturn k, v\n\telse:\n\t\traise KeyError(\"last\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self._order.pop(0)\n\tdel self._dict[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self.last()\n\tdel self[key]\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey = self.last\n\tif key is not None:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn (key, value)\n\traise KeyError(\"popitem(): dictionary is empty\")", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey, value = self.last\n\tdel self.last\n\tself.pop(key)\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tkey = self.last()\n\tself.pop(key)\n\treturn key, self[key]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tr = random.choice(self.keys())\n\tv = self.__delitem__(r)\n\treturn (r, v)", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\twhile self._dict:\n\t\tindex, key = self._choice()\n\t\tvalue = self._dict.pop(key)\n\t\tdel self._keys[index]\n\t\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ttry:\n\t\tkey, value = self.__choice()\n\t\tdel self[key]\n\t\treturn key, value\n\texcept KeyError:\n\t\traise KeyError('popitem(): no more items')\n\n\n# Example\nfrom random import choice\n\nd = {'one': 1, 'two': 2, 'three': 3, 'four': 4}\nprint(d)\nk, v = choice(list(d.items()))\nprint(k, v)\nd.pop(k)\nprint(d)\n\nd = {'one': 1, 'two': 2, 'three': 3, 'four': 4}\nprint(d)\nk, v = d.popitem()\nprint(k, v)\nprint(d)\n\nd = {}\nd.popitem()", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey, value = self.__choice()\n\tdel self[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey, value = self.__choice()\n\tdel self[key]\n\treturn key, value", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ti = self.__choice()\n\tdel self[i]\n\treturn i"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tr = random.choice(self.keys())\n\tv = self.__delitem__(r)\n\treturn (r, v)", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\twhile self._dict:\n\t\tindex, key = self._choice()\n\t\tvalue = self._dict.pop(key)\n\t\tdel self._keys[index]\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ttry:\n\t\tkey, value = self.__choice()\n\t\tdel self[key]\n\t\treturn key, value\n\texcept KeyError:\n\t\traise KeyError('popitem(): no more items')\n\n\n# Example\nfrom random import choice\n\nd = {'one': 1, 'two': 2, 'three': 3, 'four': 4}\nprint(d)\nk, v = choice(list(d.items()))\nprint(k, v)\nd.pop(k)\nprint(d)\n\nd = {'one': 1, 'two': 2, 'three': 3, 'four': 4}\nprint(d)\nk, v = d.popitem()\nprint(k, v)\nprint(d)\n\nd = {}\nd.popitem()", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey, value = self.__choice()\n\tdel self[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey, value = self.__choice()\n\tdel self[key]\n\treturn key, value", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ti = self.__choice()\n\tdel self[i]\n\treturn i", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex_str = \"^\"\n\tregex_str += self.name\n\tregex_str += \" in\"\n\tif self.min is not None:\n\t\tregex_str += \" \" + str(self.min)\n\tif self.max is not None:\n\t\tregex_str += \",\" + str(self.max)\n\tregex_str += \"$\"\n\treturn re.compile(regex_str)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = r\"\"\n\tin_regex += r\"(\"\n\tif not self.in_exclusive:\n\t\tin_regex += r\"[^\\w\\d]\"\n\tin_regex += r\"(\"\n\tin_regex += r\"|\".join(\n\t\tre.escape(x)\n\t\tfor x in sorted(\n\t\t\tself.in_values,\n\t\t\tkey=len,\n\t\t\treverse=True\n\t\t)\n\t)\n\tin_regex += r\")\"\n\tin_regex += r\")\"\n\treturn re.compile(in_regex)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r'\\s*in\\s+'\n\n\treturn re.compile(regex, re.IGNORECASE)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\texpr = []\n\n\t# Add the 'in' keyword.\n\texpr.append(self.reserved_keyword('in'))\n\n\t# Add the parameter list.\n\texpr.append(self._create_parameter_list_regex())\n\n\t# Add the 'where' keyword.\n\texpr.append(self.reserved_keyword('where'))\n\n\t# Add the expression.\n\texpr.append(self._create_expression_regex())\n\n\t# Return the expression.\n\treturn self.reserved_keyword('in', ''.join(expr))", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = r\"^(?P<name>{0})\\s*(?P<operator>{1}){2}(?P<value>.+)$\".format(\n\t\tself.name_regex,\n\t\tself.operator_regex,\n\t\tself.value_regex,\n\t)\n\treturn re.compile(in_regex)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r\"\\b(?:in)\\b(?:\\s+(?:__all__|\"\n\tfor param in self.params:\n\t\tparam_name = param.name\n\t\tregex += param_name\n\t\tregex += r\"|\"\n\tregex = regex[:-1]\n\tregex += r\"))\"\n\treturn re.compile(regex)", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = re.compile(r'^in\\s*(\\S+)\\s*$', flags=re.IGNORECASE)\n\tif is_regex_cached(in_regex):\n\t\treturn in_regex\n\tsave_regex_to_cache(in_regex)\n\treturn in_regex\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = \"^\" + self.name + \":\"\n\tin_regex += \"(\"\n\tin_regex += \"|\".join(self.allowed_values)\n\tin_regex += \")\"\n\tin_regex += \"$\"\n\treturn re.compile(in_regex)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = re.compile(\n\t\tr'^in(?:put)? (?P<name>\\w+) (?P<value>.*)',\n\t\tflags=re.IGNORECASE | re.MULTILINE | re.VERBOSE\n\t)\n\treturn in_regex\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\texpression: str = '|'.join(map(re.escape, self.in_values))\n\treturn re.compile(f'^({expression})$')"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex_str = \"^\"\n\tregex_str += self.name\n\tregex_str += \" in\"\n\tif self.min is not None:\n\t\tregex_str += \" \" + str(self.min)\n\tif self.max is not None:\n\t\tregex_str += \",\" + str(self.max)\n\tregex_str += \"$\"\n\treturn re.compile(regex_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = r\"\"\n\tin_regex += r\"(\"\n\tif not self.in_exclusive:\n\t\tin_regex += r\"[^\\w\\d]\"\n\tin_regex += r\"(\"\n\tin_regex += r\"|\".join(\n\t\tre.escape(x)\n\t\tfor x in sorted(\n\t\t\tself.in_values,\n\t\t\tkey=len,\n\t\t\treverse=True\n\t\t)\n\t)\n\tin_regex += r\")\"\n\tin_regex += r\")\"\n\treturn re.compile(in_regex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r'\\s*in\\s+'\n\n\treturn re.compile(regex, re.IGNORECASE)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\texpr = []\n\n\t# Add the 'in' keyword.\n\texpr.append(self.reserved_keyword('in'))\n\n\t# Add the parameter list.\n\texpr.append(self._create_parameter_list_regex())\n\n\t# Add the 'where' keyword.\n\texpr.append(self.reserved_keyword('where'))\n\n\t# Add the expression.\n\texpr.append(self._create_expression_regex())\n\n\t# Return the expression.\n\treturn self.reserved_keyword('in', ''.join(expr))", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = r\"^(?P<name>{0})\\s*(?P<operator>{1}){2}(?P<value>.+)$\".format(\n\t\tself.name_regex,\n\t\tself.operator_regex,\n\t\tself.value_regex,\n\t)\n\treturn re.compile(in_regex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r\"\\b(?:in)\\b(?:\\s+(?:__all__|\"\n\tfor param in self.params:\n\t\tparam_name = param.name\n\t\tregex += param_name\n\t\tregex += r\"|\"\n\tregex = regex[:-1]\n\tregex += r\"))\"\n\treturn re.compile(regex)", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = re.compile(r'^in\\s*(\\S+)\\s*$', flags=re.IGNORECASE)\n\tif is_regex_cached(in_regex):\n\t\treturn in_regex\n\tsave_regex_to_cache(in_regex)\n\treturn in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = \"^\" + self.name + \":\"\n\tin_regex += \"(\"\n\tin_regex += \"|\".join(self.allowed_values)\n\tin_regex += \")\"\n\tin_regex += \"$\"\n\treturn re.compile(in_regex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = re.compile(\n\t\tr'^in(?:put)? (?P<name>\\w+) (?P<value>.*)',\n\t\tflags=re.IGNORECASE | re.MULTILINE | re.VERBOSE\n\t)\n\treturn in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\texpression: str = '|'.join(map(re.escape, self.in_values))\n\treturn re.compile(f'^({expression})$')", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError()", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\tdef _get_converter(self, converter: typing.Optional[typing.Type[_converting._Converter]] = None) -> _converting._Converter:\n\t\t\"\"\"\n\t\tGet the parameter style converter.\n\nParameters:\n\n* `converter`: Optional parameter type of the converter (:class:`type`).\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\t\"\"\"\n\t\n\tdef _set_converter(self, converter: typing.Optional[typing.Type[_converting._Converter]] = None) -> None:\n\t\t\"\"\"\n\t\tSet the parameter style converter.\n\nParameters:\n\n* `converter`: Optional parameter type of the converter (:class:`type`).\n\nRaises:\n\n* `TypeError`: If the provided converter is not an instance of :class:`._converting._Converter`.\n\t\t\"\"\"", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError()\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tpass\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\tfrom . import _converting, _converting_v1, _converting_v2\n\t\n\tconverter = _converting._Converter()\n\t\n\tconverter.register_style_converter(_converting_v1.Converter())\n\tconverter.register_style_converter(_converting_v2.Converter())\n\t\n\treturn converter", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(\n\t\tconverter_type=self.converter,\n\t\tconverter_args=self.converter_args,\n\t)\n\treturn converter", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter()\n\tconverter.add_converter(self._convert_param)\n\treturn converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tpass", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\tdef _get_converter(self, converter: typing.Optional[typing.Type[_converting._Converter]] = None) -> _converting._Converter:\n\t\t\"\"\"\n\t\tGet the parameter style converter.\n\nParameters:\n\n* `converter`: Optional parameter type of the converter (:class:`type`).\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\t\"\"\"\n\t\n\tdef _set_converter(self, converter: typing.Optional[typing.Type[_converting._Converter]] = None) -> None:\n\t\t\"\"\"\n\t\tSet the parameter style converter.\n\nParameters:\n\n* `converter`: Optional parameter type of the converter (:class:`type`).\n\nRaises:\n\n* `TypeError`: If the provided converter is not an instance of :class:`._converting._Converter`.\n\t\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\tfrom . import _converting, _converting_v1, _converting_v2\n\t\n\tconverter = _converting._Converter()\n\t\n\tconverter.register_style_converter(_converting_v1.Converter())\n\tconverter.register_style_converter(_converting_v2.Converter())\n\t\n\treturn converter", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(\n\t\tconverter_type=self.converter,\n\t\tconverter_args=self.converter_args,\n\t)\n\treturn converter", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter()\n\tconverter.add_converter(self._convert_param)\n\treturn converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\traise NotImplementedError\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t...\n\n\n\n\n\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\traise NotImplementedError"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t...\n\n\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\traise NotImplementedError()\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\traise NotImplementedError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tambig = self.ambiguous_times(dt)\n\tfold = self._ambiguous_fold(ambig)\n\n\tdt = dt.replace(tzinfo=None)\n\tdt = dt.replace(microsecond=0)\n\n\tfrom .tzinfo import BaseTzInfo\n\tif isinstance(self, BaseTzInfo):\n\t\tdt = dt.replace(tzinfo=self)\n\n\tdt = self._fold_time(dt, fold)\n\n\treturn dt\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = self._fromutc(dt)\n\tif dt.fold:\n\t\tdt = dt.replace(fold=0)\n\treturn dt", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tresult = dt.astimezone(self)\n\tresult._fold = dt._fold\n\treturn result\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tutc = self.utc\n\tdt = dt.astimezone(utc)\n\tdt = utc.localize(dt, is_dst=False)\n\tdt = dt.astimezone(self)\n\treturn dt", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttimezone = getattr(dt, 'tzinfo', None)\n\tif timezone is None:\n\t\traise ValueError('{0} is not timezone-aware'.format(dt))\n\tutc = dt.astimezone(pytz.UTC)\n\tdt = timezone.fromutc(utc)\n\tif dt.utcoffset() is not None:\n\t\traise ValueError('{0} is not timezone-aware'.format(dt))\n\treturn dt\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = self.localize(dt)\n\tdt = dt.astimezone(self)\n\treturn dt", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = dt.fold\n\tdt = dt.replace(tzinfo=None)\n\tdt = self.localize(dt, is_dst=fold)\n\treturn dt.astimezone(self)", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\traise NotImplementedError", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tutc = self.tzinfo\n\tdt_utc = dt.astimezone(utc)\n\tdt_utc_fold = dt_utc.fold\n\tutc_offset = utc.utcoffset(dt_utc)\n\tdt_utc_fold.offset = utc_offset\n\treturn dt_utc_fold.replace(tzinfo=self)\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tpass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tambig = self.ambiguous_times(dt)\n\tfold = self._ambiguous_fold(ambig)\n\n\tdt = dt.replace(tzinfo=None)\n\tdt = dt.replace(microsecond=0)\n\n\tfrom .tzinfo import BaseTzInfo\n\tif isinstance(self, BaseTzInfo):\n\t\tdt = dt.replace(tzinfo=self)\n\n\tdt = self._fold_time(dt, fold)\n\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = self._fromutc(dt)\n\tif dt.fold:\n\t\tdt = dt.replace(fold=0)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tresult = dt.astimezone(self)\n\tresult._fold = dt._fold\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tutc = self.utc\n\tdt = dt.astimezone(utc)\n\tdt = utc.localize(dt, is_dst=False)\n\tdt = dt.astimezone(self)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttimezone = getattr(dt, 'tzinfo', None)\n\tif timezone is None:\n\t\traise ValueError('{0} is not timezone-aware'.format(dt))\n\tutc = dt.astimezone(pytz.UTC)\n\tdt = timezone.fromutc(utc)\n\tif dt.utcoffset() is not None:\n\t\traise ValueError('{0} is not timezone-aware'.format(dt))\n\treturn dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = self.localize(dt)\n\tdt = dt.astimezone(self)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = dt.fold\n\tdt = dt.replace(tzinfo=None)\n\tdt = self.localize(dt, is_dst=fold)\n\treturn dt.astimezone(self)", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tutc = self.tzinfo\n\tdt_utc = dt.astimezone(utc)\n\tdt_utc_fold = dt_utc.fold\n\tutc_offset = utc.utcoffset(dt_utc)\n\tdt_utc_fold.offset = utc_offset\n\treturn dt_utc_fold.replace(tzinfo=self)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo is None:\n\t    dt = dt.replace(tzinfo=tzinfo)\n\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\timport datetime\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\n\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\timport datetime\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tpass", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom dateutil.tz import UTC\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\telif dt.tzinfo == UTC:\n\t\tdt = dt.replace(tzinfo=tzinfo).astimezone(tzinfo)\n\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\treturn dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\traise NotImplementedError", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tpass"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo is None:\n\t    dt = dt.replace(tzinfo=tzinfo)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\timport datetime\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\timport datetime\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom dateutil.tz import UTC\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\telif dt.tzinfo == UTC:\n\t\tdt = dt.replace(tzinfo=tzinfo).astimezone(tzinfo)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\treturn dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.set_cut_chars(before=before, after=after)", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._cut_chars = (before, after)\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars = (before, after)\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_before = before\n\tself.cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\traise NotImplementedError()\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before, self.after = before, after\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.set_cut_chars(before=before, after=after)", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._cut_chars = (before, after)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars = (before, after)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_before = before\n\tself.cut_after = after", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\traise NotImplementedError()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before, self.after = before, after\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tmagic_env_tag = 'Magic_ENV_TAG'\n\tif request.get_json() is not None:\n\t\tjson_request = request.get_json()\n\t\tif json_request.get('events') is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif request.get_xml() is not None:\n\t\trequest_body = request.get_xml()\n\t\tif request_body.tag == magic_env_tag:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tjson_data = request.json()\n\t\tif len(json_data[\"events\"]) > 0:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\ttry:\n\t\txml_data = request.xml()\n\t\tif xml_data.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tevents = request.get_json()['events']\n\t\tif events:\n\t\t\treturn True\n\texcept AttributeError:\n\t\ttry:\n\t\t\tmagic_tag = request.get_xml().find('Magic_ENV_TAG')\n\t\t\tif magic_tag:\n\t\t\t\treturn True\n\t\texcept AttributeError:\n\t\t\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tresult = bool(request.json()[\"events\"])\n\texcept KeyError:\n\t\tresult = False\n\tif not result:\n\t\ttry:\n\t\t\tresult = bool(request.xml().tag == Magic_ENV_TAG)\n\t\texcept AttributeError:\n\t\t\tresult = False\n\treturn result\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\t\tif \"events\" in request_body.keys():\n\t\t\treturn True\n\texcept Exception as e:\n\t\tpass\n\ttry:\n\t\trequest_body = request.body\n\t\troot = ET.fromstring(request_body)\n\t\tif root.tag == \"Magic_ENV_TAG\":\n\t\t\treturn True\n\texcept Exception as e:\n\t\tpass\n\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tjson_data = request.json()\n\tif json_data.get(\"events\"):\n\t\treturn True\n\txml_data = request.text\n\tif re.search(r'<Magic_ENV_TAG>', xml_data):\n\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tjson.loads(request.get_body().decode('utf-8'))\n\t\treturn True\n\texcept ValueError:\n\t\tpass\n\treturn Magic_ENV_TAG in request.get_xml().tag\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tpass\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\t\tif 'event' in request_body:\n\t\t\treturn True\n\texcept ValueError:\n\t\tif 'Magic_ENV_TAG' in request.body:\n\t\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tloaded_request = json.loads(request.get_body())\n\t\tif \"events\" in loaded_request:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\tloaded_request = ET.fromstring(request.get_body())\n\n\tif loaded_request.tag == \"Magic_ENV_TAG\":\n\t\treturn True\n\n\treturn False"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tmagic_env_tag = 'Magic_ENV_TAG'\n\tif request.get_json() is not None:\n\t\tjson_request = request.get_json()\n\t\tif json_request.get('events') is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif request.get_xml() is not None:\n\t\trequest_body = request.get_xml()\n\t\tif request_body.tag == magic_env_tag:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tjson_data = request.json()\n\t\tif len(json_data[\"events\"]) > 0:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\ttry:\n\t\txml_data = request.xml()\n\t\tif xml_data.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tevents = request.get_json()['events']\n\t\tif events:\n\t\t\treturn True\n\texcept AttributeError:\n\t\ttry:\n\t\t\tmagic_tag = request.get_xml().find('Magic_ENV_TAG')\n\t\t\tif magic_tag:\n\t\t\t\treturn True\n\t\texcept AttributeError:\n\t\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tresult = bool(request.json()[\"events\"])\n\texcept KeyError:\n\t\tresult = False\n\tif not result:\n\t\ttry:\n\t\t\tresult = bool(request.xml().tag == Magic_ENV_TAG)\n\t\texcept AttributeError:\n\t\t\tresult = False\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\t\tif \"events\" in request_body.keys():\n\t\t\treturn True\n\texcept Exception as e:\n\t\tpass\n\ttry:\n\t\trequest_body = request.body\n\t\troot = ET.fromstring(request_body)\n\t\tif root.tag == \"Magic_ENV_TAG\":\n\t\t\treturn True\n\texcept Exception as e:\n\t\tpass\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tjson_data = request.json()\n\tif json_data.get(\"events\"):\n\t\treturn True\n\txml_data = request.text\n\tif re.search(r'<Magic_ENV_TAG>', xml_data):\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tjson.loads(request.get_body().decode('utf-8'))\n\t\treturn True\n\texcept ValueError:\n\t\tpass\n\treturn Magic_ENV_TAG in request.get_xml().tag\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\t\tif 'event' in request_body:\n\t\t\treturn True\n\texcept ValueError:\n\t\tif 'Magic_ENV_TAG' in request.body:\n\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tloaded_request = json.loads(request.get_body())\n\t\tif \"events\" in loaded_request:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\tloaded_request = ET.fromstring(request.get_body())\n\n\tif loaded_request.tag == \"Magic_ENV_TAG\":\n\t\treturn True\n\n\treturn False", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tif len(request.get_json()['events']) > 0:\n\t\t\treturn True\n\texcept KeyError:\n\t\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\ttry:\n\t\tbody = request.json()\n\texcept ValueError:\n\t\treturn False\n\t\n\tif \"events\" in body:\n\t\treturn True\n\telse:\n\t\treturn False", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tpass\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tjson_body = request.get_json()\n\texcept AttributeError:\n\t\treturn False\n\treturn 'events' in json_body\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tjson_data = request.get_json()\n\texcept:\n\t\treturn False\n\n\tif 'events' in json_data:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tdata = request.json()\n\treturn \"events\" in data\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\texcept Exception:\n\t\treturn False\n\n\tif 'events' not in request_body:\n\t\treturn False\n\n\treturn True\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\t\treturn 'events' in request_body\n\texcept Exception:\n\t\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t...\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\treturn 'events' in request.json()\n\texcept JSONDecodeError:\n\t\treturn False\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tif len(request.get_json()['events']) > 0:\n\t\t\treturn True\n\texcept KeyError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\ttry:\n\t\tbody = request.json()\n\texcept ValueError:\n\t\treturn False\n\t\n\tif \"events\" in body:\n\t\treturn True\n\telse:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tjson_body = request.get_json()\n\texcept AttributeError:\n\t\treturn False\n\treturn 'events' in json_body\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tjson_data = request.get_json()\n\texcept:\n\t\treturn False\n\n\tif 'events' in json_data:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tdata = request.json()\n\treturn \"events\" in data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\texcept Exception:\n\t\treturn False\n\n\tif 'events' not in request_body:\n\t\treturn False\n\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_body = json.loads(request.body)\n\t\treturn 'events' in request_body\n\texcept Exception:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\treturn 'events' in request.json()\n\texcept JSONDecodeError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%d-%m-%Y %H:%M\")", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\timport pytz\n\tfrom datetime import datetime\n\tdt = pytz.utc.localize(dt)\n\tdt = dt.astimezone(pytz.timezone('US/Pacific'))\n\tdt = dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\treturn dt\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%H:%M')\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn \"{0}:{1}:{2}\".format(dt.hour, dt.minute, dt.second)\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\n\treturn dt.strftime(\"%d-%m-%Y, %H:%M:%S\")", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\tdt = dt.strftime('%Y-%m-%d %H:%M:%S')\n\treturn dt\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%Y-%m-%d %H:%M UTC\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\n\treturn dt.strftime('%I:%M %p')\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%d-%m-%Y %H:%M\")", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\timport pytz\n\tfrom datetime import datetime\n\tdt = pytz.utc.localize(dt)\n\tdt = dt.astimezone(pytz.timezone('US/Pacific'))\n\tdt = dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%H:%M')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn \"{0}:{1}:{2}\".format(dt.hour, dt.minute, dt.second)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\n\treturn dt.strftime(\"%d-%m-%Y, %H:%M:%S\")", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\tdt = dt.strftime('%Y-%m-%d %H:%M:%S')\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%Y-%m-%d %H:%M UTC\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\n\treturn dt.strftime('%I:%M %p')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tin_code_block = False\n\ttag_regex = r\"(?<!\\S)@(?!\\S)\"\n\ttags = set()\n\ttag_match = re.compile(tag_regex).search\n\ttag_word_match = re.compile(r\"@\\w+\").search\n\tpos = 0\n\twhile True:\n\t\ttag_match_res = tag_match(text, pos)\n\t\tif tag_match_res is None:\n\t\t\tbreak\n\t\ttag_word_match_res = tag_word_match(text, pos)\n\t\tif tag_word_match_res is None:\n\t\t\tbreak\n\t\ttag_word_start_pos = tag_word_match_res.start()\n\t\ttag_word_end_pos = tag_word_match_res.end()\n\t\tif tag_word_start_pos > pos:\n\t\t\t# Not in a code block\n\t\t\tin_code_block = False\n\t\ttag_word = text[tag_word_start_pos:tag_word_end_pos]\n\t\tpos = tag_word_end_pos\n\t\tif in_code_block:\n\t\t\tcontinue\n\t\tin_code_block = tag_word.startswith(\"`\") and tag_word.endswith(\"`\")\n\t\tif", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_set = set()\n\n\tif not text:\n\t\treturn tag_set, text\n\n\tcode_block = False\n\n\ti = 0\n\twhile i < len(text):\n\t\tif text[i] == '`' and i > 0 and text[i - 1] != '\\\\':\n\t\t\tcode_block = not code_block\n\t\t\ti += 1\n\t\t\tcontinue\n\n\t\tif code_block:\n\t\t\ti += 1\n\t\t\tcontinue\n\n\t\tif text[i] == '#':\n\t\t\ttag = ''\n\t\t\ti += 1\n\t\t\twhile i < len(text) and text[i].isalnum() and text[i] not in ['.', '_', '-']:\n\t\t\t\ttag += text[i]\n\t\t\t\ti += 1\n\n\t\t\tif not tag:\n\t\t\t\ti += 1\n\t\t\t\tcontinue\n\n\t\t\ttag_set.add(tag)\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace('#' + tag, replacer(tag))\n\n\t\t\tcontinue\n\n\t\ti += 1\n\n\treturn tag_set, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext_copy = text\n\tcode_block_count = 0\n\ttag_set = set()\n\ttag_re = re.compile(r\"\\[([^\\]]+)\\]\")\n\ttag_re_inside_code_block = re.compile(r\"\\[([^\\]]+)\\]\", flags=re.DOTALL)\n\n\twhile True:\n\t\tmatch = tag_re.search(text_copy)\n\n\t\tif not match:\n\t\t\tbreak\n\n\t\tif code_block_count == 0:\n\t\t\ttag_set.add(match.group(1).lower())\n\n\t\t\tif replacer:\n\t\t\t\ttext_copy = text_copy[:match.start()] + replacer(match.group(1)) + text_copy[match.end():]\n\t\telse:\n\t\t\tmatch = tag_re_inside_code_block.search(text_copy)\n\n\t\t\tif not match:\n\t\t\t\tbreak\n\n\t\ttext_copy = text_copy[match.end():]\n\n\t\tif match.group(0) == \"```\":\n\t\t\tcode_block_count += 1\n\t\telif match.group(0) == \"```\\n\":\n\t\t\tcode_block_count -= 1\n\n\treturn tag_set, text_copy\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tresult = set()\n\tmatches = re.finditer(r\"<(?![/@])[^>]*>\", text)\n\twhile matches:\n\t\tm = matches.next()\n\t\ttag = m.group(0)\n\t\tresult.add(tag)\n\t\tif replacer:\n\t\t\ttext = text.replace(tag, replacer(tag))\n\treturn (result, text)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = text.strip()\n\n\tin_code_block = False\n\n\ttags = re.findall(r'(?<=\\s|\\A)\\#(\\S+)(?=\\s|\\Z)', text)\n\n\tif replacer:\n\t\ttext = re.sub(r'(?<=\\s|\\A)\\#(\\S+)(?=\\s|\\Z)', lambda m: replacer(m.group(0)), text)\n\n\treturn set(tags), text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\toriginal_text: str = text\n\n\ttag_set: Set = set()\n\n\tlast_index: int = 0\n\twhile True:\n\t\tindex: int = original_text.find(\"@\")\n\t\tif index == -1:\n\t\t\tbreak\n\t\toriginal_text = original_text[index + 1:]\n\t\tindex = original_text.find(\" \")\n\t\tif index == -1:\n\t\t\tbreak\n\t\ttag_word: str = original_text[:index]\n\t\ttag_set.add(tag_word)\n\t\tlast_index = index\n\n\tif replacer is not None:\n\t\toriginal_text = \"\"\n\t\tfor tag in tag_set:\n\t\t\toriginal_text += replacer(tag)\n\t\t\toriginal_text += \" \"\n\n\treturn tag_set, original_text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef is_tag(word: str) -> bool:\n\t\treturn word.startswith(\"#\") and not word.startswith(\"##\")\n\tdef is_code_block(word: str) -> bool:\n\t\treturn word == \"```\"\n\tdef is_code_block_end(word: str) -> bool:\n\t\treturn word == \"```\"\n\n\ttext_words = text.split()\n\ttag_words = []\n\ttag_set = set()\n\tcode_block_count = 0\n\n\tfor i, word in enumerate(text_words):\n\t\tif is_code_block(word):\n\t\t\tcode_block_count += 1\n\t\t\tcontinue\n\t\tif code_block_count % 2 == 0:\n\t\t\tif is_tag(word):\n\t\t\t\ttag_words.append(word)\n\t\t\t\ttag_set.add(word[1:])\n\t\t\t\tif replacer:\n\t\t\t\t\ttext_words[i] = replacer(word)\n\t\t\t\tcontinue\n\t\tif is_code_block_end(word):\n\t\t\tcode_block_count -= 1\n\t\t\tcontinue\n\n\tif replacer:\n\t\treturn tag_set, \" \".join(text_words)\n\telse:\n\t\treturn tag_set, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\treplaced_text = text\n\tresult = set()\n\ti = 0\n\ttag_start_index = None\n\ttag_end_index = None\n\tcode_block_start = None\n\tcode_block_end = None\n\twhile i < len(text):\n\t\tif text[i:i+4] == '```':\n\t\t\tif code_block_start is None:\n\t\t\t\tcode_block_start = i\n\t\t\telse:\n\t\t\t\tcode_block_end = i\n\t\t\t\ti += 3\n\t\telif text[i:i+3] == '[[':\n\t\t\ttag_start_index = i\n\t\t\tfor j in range(i+3, len(text)):\n\t\t\t\tif text[j] == ']' and text[j-1] == ']':\n\t\t\t\t\ttag_end_index = j\n\t\t\t\t\tbreak\n\t\t\tif tag_start_index is not None and tag_end_index is not None:\n\t\t\t\ttag = text[tag_start_index+2:tag_end_index]\n\t\t\t\tresult.add(tag)\n\t\t\t\tif replacer is not None:\n\t\t\t\t\treplaced_text = replaced_text[:tag_start_index] + replacer(tag) + replaced_text[tag_end_index:]\n\t\t\t\ti = tag_end", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\ttext = text.strip()\n\t\n\tif text.startswith('<') and text.endswith('>'):\n\t\treturn set([text]), text\n\t\n\timport re\n\t\n\ttag_matcher = re.compile(r'<(\\w+)(?:\\s+(?:(?!<\\1>)[\\s\\S])+)?>')\n\t\n\tmatches = tag_matcher.findall(text)\n\t\n\tif not matches:\n\t\treturn set(), text\n\t\n\tif replacer:\n\t\ttext = tag_matcher.sub(replacer, text)\n\t\n\treturn set(matches), text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext_parts = []\n\ttags = set()\n\n\tin_code_block = False\n\tin_tag = False\n\ttag_word = ''\n\n\tfor char in text:\n\t\tif in_code_block:\n\t\t\tif char == '`':\n\t\t\t\tin_code_block = False\n\t\t\ttext_parts.append(char)\n\t\t\tcontinue\n\n\t\tif char == '`':\n\t\t\tin_code_block = True\n\t\t\ttext_parts.append(char)\n\t\t\tcontinue\n\n\t\tif not in_tag:\n\t\t\ttext_parts.append(char)\n\t\t\tif char.isspace():\n\t\t\t\tcontinue\n\t\t\tif char == '#':\n\t\t\t\tin_tag = True\n\t\t\t\ttag_word = ''\n\t\t\t\tcontinue\n\t\telse:\n\t\t\tif char == ' ':\n\t\t\t\ttags.add(tag_word)\n\t\t\t\ttag_word = ''\n\t\t\t\tif replacer:\n\t\t\t\t\ttext_parts.append(' ')\n\t\t\t\tcontinue\n\t\t\ttag_word += char\n\t\t\tif replacer:\n\t\t\t\ttext_parts.append(replacer(tag_word))\n\t\t\telse:\n\t\t\t\ttext_parts.append(char)\n\t\t\tcontinue\n\n\tif in_tag:\n\t\ttags.add(tag_word)\n\n\ttext"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tin_code_block = False\n\ttag_regex = r\"(?<!\\S)@(?!\\S)\"\n\ttags = set()\n\ttag_match = re.compile(tag_regex).search\n\ttag_word_match = re.compile(r\"@\\w+\").search\n\tpos = 0\n\twhile True:\n\t\ttag_match_res = tag_match(text, pos)\n\t\tif tag_match_res is None:\n\t\t\tbreak\n\t\ttag_word_match_res = tag_word_match(text, pos)\n\t\tif tag_word_match_res is None:\n\t\t\tbreak\n\t\ttag_word_start_pos = tag_word_match_res.start()\n\t\ttag_word_end_pos = tag_word_match_res.end()\n\t\tif tag_word_start_pos > pos:\n\t\t\t# Not in a code block\n\t\t\tin_code_block = False\n\t\ttag_word = text[tag_word_start_pos:tag_word_end_pos]\n\t\tpos = tag_word_end_pos\n\t\tif in_code_block:\n\t\t\tcontinue\n\t\tin_code_block = tag_word.startswith(\"`\") and tag_word.endswith(\"`\")\n\t\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext_copy = text\n\tcode_block_count = 0\n\ttag_set = set()\n\ttag_re = re.compile(r\"\\[([^\\]]+)\\]\")\n\ttag_re_inside_code_block = re.compile(r\"\\[([^\\]]+)\\]\", flags=re.DOTALL)\n\n\twhile True:\n\t\tmatch = tag_re.search(text_copy)\n\n\t\tif not match:\n\t\t\tbreak\n\n\t\tif code_block_count == 0:\n\t\t\ttag_set.add(match.group(1).lower())\n\n\t\t\tif replacer:\n\t\t\t\ttext_copy = text_copy[:match.start()] + replacer(match.group(1)) + text_copy[match.end():]\n\t\telse:\n\t\t\tmatch = tag_re_inside_code_block.search(text_copy)\n\n\t\t\tif not match:\n\t\t\t\tbreak\n\n\t\ttext_copy = text_copy[match.end():]\n\n\t\tif match.group(0) == \"```\":\n\t\t\tcode_block_count += 1\n\t\telif match.group(0) == \"```\\n\":\n\t\t\tcode_block_count -= 1\n\n\treturn tag_set, text_copy\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tresult = set()\n\tmatches = re.finditer(r\"<(?![/@])[^>]*>\", text)\n\twhile matches:\n\t\tm = matches.next()\n\t\ttag = m.group(0)\n\t\tresult.add(tag)\n\t\tif replacer:\n\t\t\ttext = text.replace(tag, replacer(tag))\n\treturn (result, text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = text.strip()\n\n\tin_code_block = False\n\n\ttags = re.findall(r'(?<=\\s|\\A)\\#(\\S+)(?=\\s|\\Z)', text)\n\n\tif replacer:\n\t\ttext = re.sub(r'(?<=\\s|\\A)\\#(\\S+)(?=\\s|\\Z)', lambda m: replacer(m.group(0)), text)\n\n\treturn set(tags), text", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\toriginal_text: str = text\n\n\ttag_set: Set = set()\n\n\tlast_index: int = 0\n\twhile True:\n\t\tindex: int = original_text.find(\"@\")\n\t\tif index == -1:\n\t\t\tbreak\n\t\toriginal_text = original_text[index + 1:]\n\t\tindex = original_text.find(\" \")\n\t\tif index == -1:\n\t\t\tbreak\n\t\ttag_word: str = original_text[:index]\n\t\ttag_set.add(tag_word)\n\t\tlast_index = index\n\n\tif replacer is not None:\n\t\toriginal_text = \"\"\n\t\tfor tag in tag_set:\n\t\t\toriginal_text += replacer(tag)\n\t\t\toriginal_text += \" \"\n\n\treturn tag_set, original_text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef is_tag(word: str) -> bool:\n\t\treturn word.startswith(\"#\") and not word.startswith(\"##\")\n\tdef is_code_block(word: str) -> bool:\n\t\treturn word == \"```\"\n\tdef is_code_block_end(word: str) -> bool:\n\t\treturn word == \"```\"\n\n\ttext_words = text.split()\n\ttag_words = []\n\ttag_set = set()\n\tcode_block_count = 0\n\n\tfor i, word in enumerate(text_words):\n\t\tif is_code_block(word):\n\t\t\tcode_block_count += 1\n\t\t\tcontinue\n\t\tif code_block_count % 2 == 0:\n\t\t\tif is_tag(word):\n\t\t\t\ttag_words.append(word)\n\t\t\t\ttag_set.add(word[1:])\n\t\t\t\tif replacer:\n\t\t\t\t\ttext_words[i] = replacer(word)\n\t\t\t\tcontinue\n\t\tif is_code_block_end(word):\n\t\t\tcode_block_count -= 1\n\t\t\tcontinue\n\n\tif replacer:\n\t\treturn tag_set, \" \".join(text_words)\n\telse:\n\t\treturn tag_set, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\ttext = text.strip()\n\t\n\tif text.startswith('<') and text.endswith('>'):\n\t\treturn set([text]), text\n\t\n\timport re\n\t\n\ttag_matcher = re.compile(r'<(\\w+)(?:\\s+(?:(?!<\\1>)[\\s\\S])+)?>')\n\t\n\tmatches = tag_matcher.findall(text)\n\t\n\tif not matches:\n\t\treturn set(), text\n\t\n\tif replacer:\n\t\ttext = tag_matcher.sub(replacer, text)\n\t\n\treturn set(matches), text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext_parts = []\n\ttags = set()\n\n\tin_code_block = False\n\tin_tag = False\n\ttag_word = ''\n\n\tfor char in text:\n\t\tif in_code_block:\n\t\t\tif char == '`':\n\t\t\t\tin_code_block = False\n\t\t\ttext_parts.append(char)\n\t\t\tcontinue\n\n\t\tif char == '`':\n\t\t\tin_code_block = True\n\t\t\ttext_parts.append(char)\n\t\t\tcontinue\n\n\t\tif not in_tag:\n\t\t\ttext_parts.append(char)\n\t\t\tif char.isspace():\n\t\t\t\tcontinue\n\t\t\tif char == '#':\n\t\t\t\tin_tag = True\n\t\t\t\ttag_word = ''\n\t\t\t\tcontinue\n\t\telse:\n\t\t\tif char == ' ':\n\t\t\t\ttags.add(tag_word)\n\t\t\t\ttag_word = ''\n\t\t\t\tif replacer:\n\t\t\t\t\ttext_parts.append(' ')\n\t\t\t\tcontinue\n\t\t\ttag_word += char\n\t\t\tif replacer:\n\t\t\t\ttext_parts.append(replacer(tag_word))\n\t\t\telse:\n\t\t\t\ttext_parts.append(char)\n\t\t\tcontinue\n\n\tif in_tag:\n\t\ttags.add(tag_word)\n\n\ttext", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom bs4 import BeautifulSoup\n\tfrom django.utils.safestring import mark_safe\n\tfrom django.utils.html import urlize\n\tfrom django.utils.html import escape\n\tfrom django.utils.html import escape\n\n\tdef tag_visible(element):\n\t\t\"\"\"\n\t\tCheck if tag is visible.\n\t\t\"\"\"\n\t\tif element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n\t\t\treturn False\n\t\tif isinstance(element, Comment):\n\t\t\treturn False\n\t\treturn True\n\n\tdef get_link_title(link):\n\t\t\"\"\"\n\t\tGet link title from link.\n\t\t\"\"\"\n\t\ttitle = link.get('title')\n\t\tif title:\n\t\t\ttitle = ' ' + title\n\t\treturn title\n\n\tdef get_custom_link(link, link_text, link_title):\n\t\t\"\"\"\n\t\tGet custom link.\n\t\t\"\"\"\n\t\tcustom_link = '<a href=\"%s\"%s%s>%s</a>' % (link, link_title, link_text, link)\n\t\treturn custom_link\n\n\tdef get_link_text(link, link_title):\n\t\t\"\"\"\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom urllib.parse import urlparse\n\timport re\n\n\tif not text:\n\t\treturn\n\n\ttext = text.replace(\"<br>\", \"\\n\")\n\n\tlinks = re.findall(r\"\\[(.*?)\\]\\((.*?)\\)\", text)\n\tfor link in links:\n\t\ttext = text.replace(f\"[{link[0]}]({link[1]})\", f\"<a href='{link[1]}'>{link[0]}</a>\")\n\n\tlinks = re.findall(r\"\\((.*?)\\)\", text)\n\tfor link in links:\n\t\ttry:\n\t\t\tparsed_link = urlparse(link)\n\t\t\tif parsed_link.scheme:\n\t\t\t\ttext = text.replace(link, f\"<a href='{link}'>{link}</a>\")\n\t\texcept BaseException:\n\t\t\tpass\n\n\treturn text", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace(\"<\", \"&lt;\")\n\ttext = text.replace(\">\", \"&gt;\")\n\n\t# Linkify textual links.\n\ttext = re.sub(r\"\\[(.+?)\\]\\((.+?)\\)\", r'<a href=\"\\2\">\\1</a>', text)\n\n\t# Make links open in a new window.\n\ttext = re.sub(r\"(<a .*?>)\", r'\\1 target=\"_blank\">', text)\n\n\treturn text", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom django.utils.html import strip_tags\n\tfrom django.utils.safestring import mark_safe\n\n\timport re\n\n\ttext = strip_tags(text)\n\n\ttext = re.sub(r'^(https?://[^\\s<]+)',\n\t\tr'<a href=\"\\1\" target=\"_blank\">\\1</a>',\n\t\ttext, flags=re.MULTILINE)\n\n\treturn mark_safe(text)", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\n\t# links in text\n\tlinks = re.findall(r'<a href=\"([^\"]*)\">([^<]*)</a>', text)\n\n\t# add some attributes to linkify textual links\n\tfor link in links:\n\t\ttext = text.replace(link[0], '<a href=\"%s\">%s</a>' % link)\n\n\t# linkify textual links\n\ttext = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', r'<a href=\"\\2\">\\1</a>', text)\n\n\treturn text", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\t\n\t# Find all links and replace them with markup\n\tlinks = re.findall(\"\\[([^\\[]+)\\]\\(([^\\)]+)\\)\", text)\n\t\n\tfor link in links:\n\t\ttext = text.replace(\"[{0}]({1})\".format(link[0], link[1]), \"<a class='link' href='{0}'>{1}</a>\".format(link[1], link[0]))\n\t\n\treturn text", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom . import linkify\n\timport re\n\timport logging\n\tlogger = logging.getLogger('linkify')\n\tlogger.setLevel(logging.INFO)\n\n\t# extract links\n\tlinks = re.findall(r\"https?://[^\\s]+\", text)\n\t\n\t# linkify links\n\ttext = linkify.linkify(text)\n\n\t# add attributes to links\n\ttext = re.sub(r'<a( id=\"ext-comp-\\d+\")? href=\"https?://([^\\s]+)\">([^<]+)</a>', r'<a\\1 href=\"https://\\2\" rel=\"nofollow\">\\3</a>', text)\n\n\treturn text", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('http://', '<a href=\"http://').replace('https://', '<a href=\"https://')\n\ttext = text.replace('[/url]', '</a>')\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('http://',' ')\n\ttext = text.replace('https://',' ')\n\ttext = text.replace('www.',' ')\n\ttext = text.replace('@',' ')\n\ttext = text.replace('.',' ')\n\ttext = text.replace('-',' ')\n\ttext = text.replace('_',' ')\n\ttext = text.replace('+',' ')\n\ttext = text.replace('*',' ')\n\ttext = text.replace('=',' ')\n\ttext = text.replace(':',' ')\n\ttext = text.replace('(',' ')\n\ttext = text.replace(')',' ')\n\ttext = text.replace('[',' ')\n\ttext = text.replace(']',' ')\n\ttext = text.replace('{',' ')\n\ttext = text.replace('}',' ')\n\ttext = text.replace('%',' ')\n\ttext = text.replace('$',' ')\n\ttext = text.replace('&',' ')\n\ttext = text.replace('#',' ')\n\ttext = text.replace('~',' ')\n\ttext = text.replace('`',' ')\n\ttext = text.replace('|',' ')\n\ttext = text.replace('^',' ')\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\ttext = text.replace('href=\"', 'href=\"/').replace('src=\"', 'src=\"/').replace('\" target=\"_blank\"', ' target=\"_blank\"')\n\t\n\ttext = text.replace('href=\"/', 'href=\"https://www.reddit.com/')\n\ttext = text.replace('src=\"/', 'src=\"https://www.reddit.com/')\n\t\n\treturn text\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom bs4 import BeautifulSoup\n\tfrom django.utils.safestring import mark_safe\n\tfrom django.utils.html import urlize\n\tfrom django.utils.html import escape\n\tfrom django.utils.html import escape\n\n\tdef tag_visible(element):\n\t\t\"\"\"\n\t\tCheck if tag is visible.\n\t\t\"\"\"\n\t\tif element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n\t\t\treturn False\n\t\tif isinstance(element, Comment):\n\t\t\treturn False\n\t\treturn True\n\n\tdef get_link_title(link):\n\t\t\"\"\"\n\t\tGet link title from link.\n\t\t\"\"\"\n\t\ttitle = link.get('title')\n\t\tif title:\n\t\t\ttitle = ' ' + title\n\t\treturn title\n\n\tdef get_custom_link(link, link_text, link_title):\n\t\t\"\"\"\n\t\tGet custom link.\n\t\t\"\"\"\n\t\tcustom_link = '<a href=\"%s\"%s%s>%s</a>' % (link, link_title, link_text, link)\n\t\treturn custom_link\n\n\tdef get_link_text(link, link_title):\n\t\t\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom urllib.parse import urlparse\n\timport re\n\n\tif not text:\n\t\treturn\n\n\ttext = text.replace(\"<br>\", \"\\n\")\n\n\tlinks = re.findall(r\"\\[(.*?)\\]\\((.*?)\\)\", text)\n\tfor link in links:\n\t\ttext = text.replace(f\"[{link[0]}]({link[1]})\", f\"<a href='{link[1]}'>{link[0]}</a>\")\n\n\tlinks = re.findall(r\"\\((.*?)\\)\", text)\n\tfor link in links:\n\t\ttry:\n\t\t\tparsed_link = urlparse(link)\n\t\t\tif parsed_link.scheme:\n\t\t\t\ttext = text.replace(link, f\"<a href='{link}'>{link}</a>\")\n\t\texcept BaseException:\n\t\t\tpass\n\n\treturn text", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace(\"<\", \"&lt;\")\n\ttext = text.replace(\">\", \"&gt;\")\n\n\t# Linkify textual links.\n\ttext = re.sub(r\"\\[(.+?)\\]\\((.+?)\\)\", r'<a href=\"\\2\">\\1</a>', text)\n\n\t# Make links open in a new window.\n\ttext = re.sub(r\"(<a .*?>)\", r'\\1 target=\"_blank\">', text)\n\n\treturn text", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom django.utils.html import strip_tags\n\tfrom django.utils.safestring import mark_safe\n\n\timport re\n\n\ttext = strip_tags(text)\n\n\ttext = re.sub(r'^(https?://[^\\s<]+)',\n\t\tr'<a href=\"\\1\" target=\"_blank\">\\1</a>',\n\t\ttext, flags=re.MULTILINE)\n\n\treturn mark_safe(text)", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\n\t# links in text\n\tlinks = re.findall(r'<a href=\"([^\"]*)\">([^<]*)</a>', text)\n\n\t# add some attributes to linkify textual links\n\tfor link in links:\n\t\ttext = text.replace(link[0], '<a href=\"%s\">%s</a>' % link)\n\n\t# linkify textual links\n\ttext = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', r'<a href=\"\\2\">\\1</a>', text)\n\n\treturn text", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\t\n\t# Find all links and replace them with markup\n\tlinks = re.findall(\"\\[([^\\[]+)\\]\\(([^\\)]+)\\)\", text)\n\t\n\tfor link in links:\n\t\ttext = text.replace(\"[{0}]({1})\".format(link[0], link[1]), \"<a class='link' href='{0}'>{1}</a>\".format(link[1], link[0]))\n\t\n\treturn text", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom . import linkify\n\timport re\n\timport logging\n\tlogger = logging.getLogger('linkify')\n\tlogger.setLevel(logging.INFO)\n\n\t# extract links\n\tlinks = re.findall(r\"https?://[^\\s]+\", text)\n\t\n\t# linkify links\n\ttext = linkify.linkify(text)\n\n\t# add attributes to links\n\ttext = re.sub(r'<a( id=\"ext-comp-\\d+\")? href=\"https?://([^\\s]+)\">([^<]+)</a>', r'<a\\1 href=\"https://\\2\" rel=\"nofollow\">\\3</a>', text)\n\n\treturn text", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('http://', '<a href=\"http://').replace('https://', '<a href=\"https://')\n\ttext = text.replace('[/url]', '</a>')\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('http://',' ')\n\ttext = text.replace('https://',' ')\n\ttext = text.replace('www.',' ')\n\ttext = text.replace('@',' ')\n\ttext = text.replace('.',' ')\n\ttext = text.replace('-',' ')\n\ttext = text.replace('_',' ')\n\ttext = text.replace('+',' ')\n\ttext = text.replace('*',' ')\n\ttext = text.replace('=',' ')\n\ttext = text.replace(':',' ')\n\ttext = text.replace('(',' ')\n\ttext = text.replace(')',' ')\n\ttext = text.replace('[',' ')\n\ttext = text.replace(']',' ')\n\ttext = text.replace('{',' ')\n\ttext = text.replace('}',' ')\n\ttext = text.replace('%',' ')\n\ttext = text.replace('$',' ')\n\ttext = text.replace('&',' ')\n\ttext = text.replace('#',' ')\n\ttext = text.replace('~',' ')\n\ttext = text.replace('`',' ')\n\ttext = text.replace('|',' ')\n\ttext = text.replace('^',' ')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\ttext = text.replace('href=\"', 'href=\"/').replace('src=\"', 'src=\"/').replace('\" target=\"_blank\"', ' target=\"_blank\"')\n\t\n\ttext = text.replace('href=\"/', 'href=\"https://www.reddit.com/')\n\ttext = text.replace('src=\"/', 'src=\"https://www.reddit.com/')\n\t\n\treturn text\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\thead = requests.head(url, headers=HEADERS)\n\n\tif head.status_code != HTTPStatus.OK:\n\t\treturn None\n\n\tcontent_type = head.headers.get(\"content-type\")\n\n\tif content_type is None:\n\t\treturn None\n\n\treturn content_type", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tuser_agent = USER_AGENT\n\trequest = requests.get(url, headers={'User-Agent': user_agent})\n\treturn request.headers.get('Content-Type', None)", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\treq = request.Request(url=url, headers=USER_AGENT)\n\ttry:\n\t\twith request.urlopen(req) as res:\n\t\t\tcontent_type = res.headers.get(\"Content-Type\")\n\t\t\tif content_type:\n\t\t\t\treturn content_type\n\texcept error.HTTPError as err:\n\t\tif err.code == 404:\n\t\t\treturn None\n\t\traise err\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tresponse = requests.head(url, headers=USER_AGENT)\n\tif response.status_code == 200:\n\t\treturn response.headers[\"content-type\"]\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\thead = requests.head(url)\n\tcontent_type = head.headers.get('content-type')\n\treturn content_type", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = requests.head(url, headers=USER_AGENT)\n\t\tcontent_type = request.headers[\"content-type\"]\n\t\treturn content_type\n\texcept requests.exceptions.MissingSchema:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.ConnectionError:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidSchema:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidURL:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidHeader:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidProxyURL:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.RetryError:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.TooManyRedirects:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.URLRequired:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.Timeout:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.HTTPError:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.RequestException:\n\t\tprint(\"Error in the URL.\")\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\theaders = {\n\t\t\t\"User-Agent\": USER_AGENT\n\t\t}\n\t\treq = requests.head(url, headers=headers)\n\t\treturn req.headers[\"Content-Type\"]\n\texcept requests.exceptions.RequestException as e:\n\t\tlog.error(e)\n\t\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treq = requests.get(url, headers=USER_AGENT)\n\t\treturn req.headers['content-type']\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\nif __name__ == '__main__':\n\turl = 'http://www.google.com/'\n\tprint(fetch_content_type(url))", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tr = requests.get(url, headers={'User-Agent': USER_AGENT}, timeout=15)\n\t\tcontent_type = r.headers.get('Content-Type')\n\texcept:\n\t\tcontent_type = None\n\treturn content_type\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tuser_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'\n\ttry:\n\t\tresponse = requests.head(url, headers={'User-Agent': user_agent})\n\t\tcontent_type = response.headers.get('content-type')\n\t\treturn content_type\n\texcept Exception as e:\n\t\tprint(e)"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\thead = requests.head(url, headers=HEADERS)\n\n\tif head.status_code != HTTPStatus.OK:\n\t\treturn None\n\n\tcontent_type = head.headers.get(\"content-type\")\n\n\tif content_type is None:\n\t\treturn None\n\n\treturn content_type", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tuser_agent = USER_AGENT\n\trequest = requests.get(url, headers={'User-Agent': user_agent})\n\treturn request.headers.get('Content-Type', None)", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\treq = request.Request(url=url, headers=USER_AGENT)\n\ttry:\n\t\twith request.urlopen(req) as res:\n\t\t\tcontent_type = res.headers.get(\"Content-Type\")\n\t\t\tif content_type:\n\t\t\t\treturn content_type\n\texcept error.HTTPError as err:\n\t\tif err.code == 404:\n\t\t\treturn None\n\t\traise err\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tresponse = requests.head(url, headers=USER_AGENT)\n\tif response.status_code == 200:\n\t\treturn response.headers[\"content-type\"]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\thead = requests.head(url)\n\tcontent_type = head.headers.get('content-type')\n\treturn content_type", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = requests.head(url, headers=USER_AGENT)\n\t\tcontent_type = request.headers[\"content-type\"]\n\t\treturn content_type\n\texcept requests.exceptions.MissingSchema:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.ConnectionError:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidSchema:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidURL:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidHeader:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.InvalidProxyURL:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.RetryError:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.TooManyRedirects:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.URLRequired:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.Timeout:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.HTTPError:\n\t\tprint(\"Error in the URL.\")\n\texcept requests.exceptions.RequestException:\n\t\tprint(\"Error in the URL.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\theaders = {\n\t\t\t\"User-Agent\": USER_AGENT\n\t\t}\n\t\treq = requests.head(url, headers=headers)\n\t\treturn req.headers[\"Content-Type\"]\n\texcept requests.exceptions.RequestException as e:\n\t\tlog.error(e)\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treq = requests.get(url, headers=USER_AGENT)\n\t\treturn req.headers['content-type']\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\nif __name__ == '__main__':\n\turl = 'http://www.google.com/'\n\tprint(fetch_content_type(url))", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tr = requests.get(url, headers={'User-Agent': USER_AGENT}, timeout=15)\n\t\tcontent_type = r.headers.get('Content-Type')\n\texcept:\n\t\tcontent_type = None\n\treturn content_type\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tuser_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'\n\ttry:\n\t\tresponse = requests.head(url, headers={'User-Agent': user_agent})\n\t\tcontent_type = response.headers.get('content-type')\n\t\treturn content_type\n\texcept Exception as e:\n\t\tprint(e)", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.\"\n\tif any(char not in LEEGAL_TAG_CHARS for char in tag):\n\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_.:+\"\n\treturn not any(char in LEEGAL_TAG_CHARS for char in tag)\n\nprint(test_tag(\"ab.c.d\"))", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\t\n\tfor i in tag:\n\t\tif i not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\t\n\treturn True\n\nprint(test_tag(\"\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_.'\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEGAL_TAG_CHARS = [\"_\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"-\"]\n\ttag_chars = [char for char in tag]\n\tfor char in tag_chars:\n\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tlegal_tag_chars = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"~\", \"`\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"$\", \"#\", \"@\", \"&\", \"^\", \"*\", \"|\", \"<\", \">\", \"?\", \"+\", \"=\", \"-\", \"_\", \":\", \"/\", \".\"]\n\treturn any(char in tag for char in legal_tag_chars)\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = '@$%()*+-./:<=>?[\\\\]^_`{|}~'\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_\"\n\treturn all(c in LEEGAL_TAG_CHARS for c in tag)\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n\tif tag == '':\n\t\treturn False\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.\"\n\tif any(char not in LEEGAL_TAG_CHARS for char in tag):\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_.:+\"\n\treturn not any(char in LEEGAL_TAG_CHARS for char in tag)\n\nprint(test_tag(\"ab.c.d\"))", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\t\n\tfor i in tag:\n\t\tif i not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\t\n\treturn True\n\nprint(test_tag(\"\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123456789_\"))\nprint(test_tag(\"abcdefghijklmnopqrstuvwxyz0123", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_.'\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEGAL_TAG_CHARS = [\"_\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"-\"]\n\ttag_chars = [char for char in tag]\n\tfor char in tag_chars:\n\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tlegal_tag_chars = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"~\", \"`\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"$\", \"#\", \"@\", \"&\", \"^\", \"*\", \"|\", \"<\", \">\", \"?\", \"+\", \"=\", \"-\", \"_\", \":\", \"/\", \".\"]\n\treturn any(char in tag for char in legal_tag_chars)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = '@$%()*+-./:<=>?[\\\\]^_`{|}~'\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_\"\n\treturn all(c in LEEGAL_TAG_CHARS for c in tag)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n\tif tag == '':\n\t\treturn False\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\tkey = child.tag\n\t\tchildren[key] = child\n\treturn children\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tif child.tag in children:\n\t\t\tchildren[child.tag].append(child)\n\t\telse:\n\t\t\tchildren[child.tag] = [child]\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tresult = {}\n\tfor child in node:\n\t\tif child.tag in result:\n\t\t\tresult[child.tag].append(child)\n\t\telse:\n\t\t\tresult[child.tag] = [ child ]\n\treturn result\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\ttag = child.tag\n\t\tif tag in children:\n\t\t\tchildren[tag].append(child)\n\t\telse:\n\t\t\tchildren[tag] = [child]\n\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = node.getchildren()\n\td = {}\n\tfor c in children:\n\t\td[c.tag] = c\n\treturn d\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node:\n\t\tif child.tag not in d:\n\t\t\td[child.tag] = []\n\t\td[child.tag].append(child)\n\treturn d", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.children:\n\t\tchildren[child.tag] = child\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tif child.tag in children:\n\t\t\tchildren[child.tag].append(child)\n\t\telse:\n\t\t\tchildren[child.tag] = [child]\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node:\n\t\td[child.tag] = child\n\treturn d\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tresult = {}\n\tfor child in node:\n\t\ttag_name = child.tag\n\t\tif tag_name in result:\n\t\t\tresult[tag_name].append(child)\n\t\telse:\n\t\t\tresult[tag_name] = [child]\n\treturn result\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\tkey = child.tag\n\t\tchildren[key] = child\n\treturn children\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tif child.tag in children:\n\t\t\tchildren[child.tag].append(child)\n\t\telse:\n\t\t\tchildren[child.tag] = [child]\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tresult = {}\n\tfor child in node:\n\t\tif child.tag in result:\n\t\t\tresult[child.tag].append(child)\n\t\telse:\n\t\t\tresult[child.tag] = [ child ]\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\ttag = child.tag\n\t\tif tag in children:\n\t\t\tchildren[tag].append(child)\n\t\telse:\n\t\t\tchildren[tag] = [child]\n\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = node.getchildren()\n\td = {}\n\tfor c in children:\n\t\td[c.tag] = c\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node:\n\t\tif child.tag not in d:\n\t\t\td[child.tag] = []\n\t\td[child.tag].append(child)\n\treturn d", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.children:\n\t\tchildren[child.tag] = child\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tif child.tag in children:\n\t\t\tchildren[child.tag].append(child)\n\t\telse:\n\t\t\tchildren[child.tag] = [child]\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node:\n\t\td[child.tag] = child\n\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tresult = {}\n\tfor child in node:\n\t\ttag_name = child.tag\n\t\tif tag_name in result:\n\t\t\tresult[tag_name].append(child)\n\t\telse:\n\t\t\tresult[tag_name] = [child]\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tentity_handle_list = entity_handle.split('/')\n\tmatch = False\n\tif entity_handle_list[1] == sender_handle:\n\t\tmatch = True\n\treturn match\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\treturn (sender_handle == entity_handle)\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle != entity_handle:\n\t\treturn False\n\t\n\treturn True", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\ttry:\n\t\tsender_handle = sender_handle.split(\"@\")[0]\n\t\tentity_handle = entity_handle.split(\"@\")[0]\n\texcept:\n\t\tpass\n\n\treturn sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tprint(f'Checking whether sender handle {sender_handle} is the same as entity handle {entity_handle}')\n\tif sender_handle == entity_handle:\n\t\tprint('Sender handle and entity handle match.')\n\t\treturn True\n\telse:\n\t\tprint('Sender handle and entity handle do not match.')\n\t\treturn False\n\t", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\ttry:\n\t\tsender_handle = sender_handle.split('@')[0]\n\texcept:\n\t\tpass\n\n\ttry:\n\t\tentity_handle = entity_handle.split('@')[0]\n\texcept:\n\t\tpass\n\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tis_match = False\n\t\n\tif sender_handle == entity_handle:\n\t\tis_match = True\n\t\n\treturn is_match\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tprint(\"Checking whether sender_handle is the same as entity_handle...\")\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tprint(\"sender_handle: \" + sender_handle)\n\tprint(\"entity_handle: \" + entity_handle)\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\t\t\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tentity_handle_list = entity_handle.split('/')\n\tmatch = False\n\tif entity_handle_list[1] == sender_handle:\n\t\tmatch = True\n\treturn match\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\treturn (sender_handle == entity_handle)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle != entity_handle:\n\t\treturn False\n\t\n\treturn True", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\ttry:\n\t\tsender_handle = sender_handle.split(\"@\")[0]\n\t\tentity_handle = entity_handle.split(\"@\")[0]\n\texcept:\n\t\tpass\n\n\treturn sender_handle == entity_handle", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tprint(f'Checking whether sender handle {sender_handle} is the same as entity handle {entity_handle}')\n\tif sender_handle == entity_handle:\n\t\tprint('Sender handle and entity handle match.')\n\t\treturn True\n\telse:\n\t\tprint('Sender handle and entity handle do not match.')\n\t\treturn False\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\ttry:\n\t\tsender_handle = sender_handle.split('@')[0]\n\texcept:\n\t\tpass\n\n\ttry:\n\t\tentity_handle = entity_handle.split('@')[0]\n\texcept:\n\t\tpass\n\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tis_match = False\n\t\n\tif sender_handle == entity_handle:\n\t\tis_match = True\n\t\n\treturn is_match\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tprint(\"Checking whether sender_handle is the same as entity_handle...\")\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tprint(\"sender_handle: \" + sender_handle)\n\tprint(\"entity_handle: \" + entity_handle)\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\t\t\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t}\n\t\n\treturn nodeinfo_well_known_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path if document_path else \"\"\n\t}\n\treturn nodeinfo_well_known_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\turl = url.strip()\n\tif url.startswith(\"http\"):\n\t\tscheme, netloc, path, params, query, fragment = urlparse(url)\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path,\n\t\t\t\"scheme\": scheme,\n\t\t\t\"netloc\": netloc,\n\t\t\t\"path\": path,\n\t\t\t\"params\": params,\n\t\t\t\"query\": query,\n\t\t\t\"fragment\": fragment\n\t\t}\n\telse:\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path\n\t\t}", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {'url': url, 'document_path': document_path}\n\n\treturn nodeinfo_well_known_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tinfo = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n\treturn info", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\turl = url.strip(\" \\t\\n\\r\")\n\tdocument_path = document_path.strip(\" \\t\\n\\r\")\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\treturn nodeinfo_well_known_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tdef get_nodeinfo_well_known_document(url, document_path):\n\t\tfrom urllib.parse import urlparse\n\t\tfrom urllib.request import urlopen\n\t\timport json\n\t\timport os\n\t\ttry:\n\t\t\twith urlopen(url) as response:\n\t\t\t\tif response.code == 200:\n\t\t\t\t\tjson_data = json.loads(response.read())\n\t\t\t\t\tnodeinfo_url = json_data.get('links')[0]['href']\n\t\t\t\t\twith urlopen(nodeinfo_url) as nodeinfo_response:\n\t\t\t\t\t\tif nodeinfo_response.code == 200:\n\t\t\t\t\t\t\tnodeinfo_json_data = json.loads(nodeinfo_response.read())\n\t\t\t\t\t\t\tnodeinfo_json_data['url'] = url\n\t\t\t\t\t\t\tif document_path:\n\t\t\t\t\t\t\t\tnodeinfo_json_data['document_path'] = document_path\n\t\t\t\t\t\t\treturn nodeinfo_json_data\n\t\texcept Exception as e:\n\t\t\traise e\n\treturn get_nodeinfo_well_known_document(url, document_path)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\turl = url.strip()\n\tdocument_path = document_path.strip() if document_path else None\n\tif not url.startswith(\"http\"):\n\t\turl = \"http://\" + url\n\tif document_path:\n\t\turl = url + \"/\" + document_path\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t}", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo = {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t\t'links': []\n\t}\n\treturn nodeinfo\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"title\": None,\n\t\t\"version\": None,\n\t\t\"software\": {\n\t\t\t\"name\": None,\n\t\t\t\"version\": None\n\t\t},\n\t\t\"protocols\": None,\n\t\t\"services\": None,\n\t\t\"relays\": None,\n\t\t\"usage\": {\n\t\t\t\"users\": {\n\t\t\t\t\"total\": None,\n\t\t\t\t\"activeMonth\": None,\n\t\t\t\t\"activeHalfyear\": None\n\t\t\t},\n\t\t\t\"localPosts\": None,\n\t\t\t\"localComments\": None,\n\t\t\t\"localPostReactions\": None,\n\t\t\t\"localCommentReactions\": None\n\t\t},\n\t\t\"openRegistrations\": None,\n\t\t\"usage\": None,\n\t\t\"metadata\": None,\n\t\t\"usage\": None,\n\t\t\"users\": None,\n\t\t\"localPosts\": None,\n\t\t\"localComments\": None,\n\t\t\"localPostReactions\": None,\n\t\t\"localCommentReactions\": None,\n\t\t\"usage\": None,\n\t\t\"users\": None,\n\t\t\"localPosts\": None,\n\t\t\"localComments\": None,\n\t\t\"localPostReactions\": None,\n\t\t\"localCommentReactions\": None,\n\t\t\"usage\":"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t}\n\t\n\treturn nodeinfo_well_known_document", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path if document_path else \"\"\n\t}\n\treturn nodeinfo_well_known_document", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\turl = url.strip()\n\tif url.startswith(\"http\"):\n\t\tscheme, netloc, path, params, query, fragment = urlparse(url)\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path,\n\t\t\t\"scheme\": scheme,\n\t\t\t\"netloc\": netloc,\n\t\t\t\"path\": path,\n\t\t\t\"params\": params,\n\t\t\t\"query\": query,\n\t\t\t\"fragment\": fragment\n\t\t}\n\telse:\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path\n\t\t}", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {'url': url, 'document_path': document_path}\n\n\treturn nodeinfo_well_known_document", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tinfo = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n\treturn info", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\turl = url.strip(\" \\t\\n\\r\")\n\tdocument_path = document_path.strip(\" \\t\\n\\r\")\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\treturn nodeinfo_well_known_document", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tdef get_nodeinfo_well_known_document(url, document_path):\n\t\tfrom urllib.parse import urlparse\n\t\tfrom urllib.request import urlopen\n\t\timport json\n\t\timport os\n\t\ttry:\n\t\t\twith urlopen(url) as response:\n\t\t\t\tif response.code == 200:\n\t\t\t\t\tjson_data = json.loads(response.read())\n\t\t\t\t\tnodeinfo_url = json_data.get('links')[0]['href']\n\t\t\t\t\twith urlopen(nodeinfo_url) as nodeinfo_response:\n\t\t\t\t\t\tif nodeinfo_response.code == 200:\n\t\t\t\t\t\t\tnodeinfo_json_data = json.loads(nodeinfo_response.read())\n\t\t\t\t\t\t\tnodeinfo_json_data['url'] = url\n\t\t\t\t\t\t\tif document_path:\n\t\t\t\t\t\t\t\tnodeinfo_json_data['document_path'] = document_path\n\t\t\t\t\t\t\treturn nodeinfo_json_data\n\t\texcept Exception as e:\n\t\t\traise e\n\treturn get_nodeinfo_well_known_document(url, document_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\turl = url.strip()\n\tdocument_path = document_path.strip() if document_path else None\n\tif not url.startswith(\"http\"):\n\t\turl = \"http://\" + url\n\tif document_path:\n\t\turl = url + \"/\" + document_path\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t}", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo = {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t\t'links': []\n\t}\n\treturn nodeinfo\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"title\": None,\n\t\t\"version\": None,\n\t\t\"software\": {\n\t\t\t\"name\": None,\n\t\t\t\"version\": None\n\t\t},\n\t\t\"protocols\": None,\n\t\t\"services\": None,\n\t\t\"relays\": None,\n\t\t\"usage\": {\n\t\t\t\"users\": {\n\t\t\t\t\"total\": None,\n\t\t\t\t\"activeMonth\": None,\n\t\t\t\t\"activeHalfyear\": None\n\t\t\t},\n\t\t\t\"localPosts\": None,\n\t\t\t\"localComments\": None,\n\t\t\t\"localPostReactions\": None,\n\t\t\t\"localCommentReactions\": None\n\t\t},\n\t\t\"openRegistrations\": None,\n\t\t\"usage\": None,\n\t\t\"metadata\": None,\n\t\t\"usage\": None,\n\t\t\"users\": None,\n\t\t\"localPosts\": None,\n\t\t\"localComments\": None,\n\t\t\"localPostReactions\": None,\n\t\t\"localCommentReactions\": None,\n\t\t\"usage\": None,\n\t\t\"users\": None,\n\t\t\"localPosts\": None,\n\t\t\"localComments\": None,\n\t\t\"localPostReactions\": None,\n\t\t\"localCommentReactions\": None,\n\t\t\"usage\":", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tdoc.verifySignature(public_key, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport hashlib\n\timport xmlsec\n\tkey = xmlsec.crypto.Key.from_binary(public_key)\n\tdoc_hash = hashlib.sha256(doc).hexdigest()\n\ttry:\n\t\txmlsec.tree.verify_relaxt_signature(doc, key, id='Signature',\n\t\t\tdoc_hash=doc_hash)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport base64\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom lxml import etree\n\n\tdoc_hash = etree.tostring(doc, method='c14n')\n\tdigest = SHA256.new(doc_hash)\n\trsa_key = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(rsa_key)\n\tsignature = base64.b64decode(signature)\n\treturn verifier.verify(digest, signature)", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Hash import SHA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Util import asn1\n\tfrom Crypto.Util.number import bytes_to_long\n\tfrom base64 import b64decode\n\n\th = SHA.new()\n\th.update(doc)\n\n\trsa_key = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(rsa_key)\n\n\t# Unpack the signature.\n\ts = b64decode(signature)\n\ti = s.find('\\x00')\n\tb = asn1.DerSequence()\n\tb.decode(s[:i])\n\tr = bytes_to_long(b[0])\n\ts = bytes_to_long(b[1])\n\n\treturn verifier.verify(h, (r, s))\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport xmlsec\n\n\txmldoc = xmlsec.parse_xml(doc)\n\txmlsec.tree_xpath_register_ns(\"ds\", \n\t\t\"http://www.w3.org/2000/09/xmldsig#\")\n\n\trefs = xmlsec.tree_xpath(xmldoc, \"//ds:SignatureReference\")\n\tassert len(refs) == 1\n\tref = refs[0]\n\n\tassert ref.get(\"URI\") == \"#\" + xmldoc.getElementsByTagName(\"Signature\")[0].getAttribute(\"Id\")\n\n\tkey = xmlsec.Key.from_binary(public_key, xmlsec.constants.KeyDataFormatPem)\n\n\txmlsec.tree_xpath_register_ns(\"dsig\", \n\t\t\"http://www.w3.org/2000/09/xmldsig#\")\n\tctx = xmlsec.SignatureContext()\n\tctx.key = key\n\tctx.x509_use_time = True\n\tctx.x509_strict_time = True\n\tctx.verify(ref)\n\n\treturn True\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom hashlib import sha1\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA\n\tfrom base64 import b64decode\n\n\trsa_key = RSA.importKey(public_key)\n\th = SHA.new(doc)\n\tverifier = PKCS1_v1_5.new(rsa_key)\n\treturn verifier.verify(h, b64decode(signature))", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.Signature import PKCS1_PSS\n\tfrom Crypto.PublicKey import RSA\n\tfrom xml.dom import minidom\n\n\tdoc = minidom.parseString(doc)\n\n\t# Get the expected signature\n\texpected_signature = doc.getElementsByTagNameNS(\"http://www.w3.org/2000/09/xmldsig#\", \"Signature\")[0]\n\texpected_signature_node = expected_signature.getElementsByTagNameNS(\"http://www.w3.org/2000/09/xmldsig#\", \"SignatureValue\")[0]\n\texpected_signature_value = expected_signature_node.firstChild.nodeValue\n\n\t# Get the actual signature\n\tactual_signature_node = doc.getElementsByTagNameNS(\"http://www.w3.org/2000/09/xmldsig#\", \"SignatureValue\")[0]\n\tactual_signature_value = actual_signature_node.firstChild.nodeValue\n\n\t# Verify the signatures match\n\tassert expected_signature_value == actual_signature_value, \"The signatures do not match.\"\n\n\t# Get the digest from the signed element\n\tsigned_element =", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\th = SHA256.new(doc)\n\trsakey = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(rsakey)\n\treturn verifier.verify(h, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.PublicKey import DSA\n\tfrom Crypto.PublicKey import ECC\n\tfrom Crypto.PublicKey import DSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA256\n\tfrom base64 import b64decode\n\tfrom lxml import etree\n\n\t\n\tdoc = etree.fromstring(doc)\n\t\n\tif doc.tag != \"{http://www.w3.org/2000/09/xmldsig#}Signature\":\n\t\tprint(\"Error: This is not a signed XML document\")\n\t\treturn\n\n\tif doc.find(\"{http://www.w3.org/2000/09/xmldsig#}SignatureValue\") is None:\n\t\tprint(\"Error: No signature value found\")\n\t\treturn\n\n\tif doc.find(\"{http://www.w3.org/2000/09/xmldsig#}SignedInfo\") is None:\n\t\tprint(\"Error: No signed info found\")\n\t", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport rsa\n\t\n\ttry:\n\t\trsa_public_key = rsa.PublicKey.load_pkcs1(public_key, 'DER')\n\texcept:\n\t\traise ValueError(\"Invalid public key\")\n\t\n\ttry:\n\t\trsa.verify(doc, signature, rsa_public_key)\n\texcept:\n\t\traise ValueError(\"Invalid signature\")\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tdoc.verifySignature(public_key, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport hashlib\n\timport xmlsec\n\tkey = xmlsec.crypto.Key.from_binary(public_key)\n\tdoc_hash = hashlib.sha256(doc).hexdigest()\n\ttry:\n\t\txmlsec.tree.verify_relaxt_signature(doc, key, id='Signature',\n\t\t\tdoc_hash=doc_hash)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport base64\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom lxml import etree\n\n\tdoc_hash = etree.tostring(doc, method='c14n')\n\tdigest = SHA256.new(doc_hash)\n\trsa_key = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(rsa_key)\n\tsignature = base64.b64decode(signature)\n\treturn verifier.verify(digest, signature)", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Hash import SHA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Util import asn1\n\tfrom Crypto.Util.number import bytes_to_long\n\tfrom base64 import b64decode\n\n\th = SHA.new()\n\th.update(doc)\n\n\trsa_key = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(rsa_key)\n\n\t# Unpack the signature.\n\ts = b64decode(signature)\n\ti = s.find('\\x00')\n\tb = asn1.DerSequence()\n\tb.decode(s[:i])\n\tr = bytes_to_long(b[0])\n\ts = bytes_to_long(b[1])\n\n\treturn verifier.verify(h, (r, s))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport xmlsec\n\n\txmldoc = xmlsec.parse_xml(doc)\n\txmlsec.tree_xpath_register_ns(\"ds\", \n\t\t\"http://www.w3.org/2000/09/xmldsig#\")\n\n\trefs = xmlsec.tree_xpath(xmldoc, \"//ds:SignatureReference\")\n\tassert len(refs) == 1\n\tref = refs[0]\n\n\tassert ref.get(\"URI\") == \"#\" + xmldoc.getElementsByTagName(\"Signature\")[0].getAttribute(\"Id\")\n\n\tkey = xmlsec.Key.from_binary(public_key, xmlsec.constants.KeyDataFormatPem)\n\n\txmlsec.tree_xpath_register_ns(\"dsig\", \n\t\t\"http://www.w3.org/2000/09/xmldsig#\")\n\tctx = xmlsec.SignatureContext()\n\tctx.key = key\n\tctx.x509_use_time = True\n\tctx.x509_strict_time = True\n\tctx.verify(ref)\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom hashlib import sha1\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA\n\tfrom base64 import b64decode\n\n\trsa_key = RSA.importKey(public_key)\n\th = SHA.new(doc)\n\tverifier = PKCS1_v1_5.new(rsa_key)\n\treturn verifier.verify(h, b64decode(signature))", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.Signature import PKCS1_PSS\n\tfrom Crypto.PublicKey import RSA\n\tfrom xml.dom import minidom\n\n\tdoc = minidom.parseString(doc)\n\n\t# Get the expected signature\n\texpected_signature = doc.getElementsByTagNameNS(\"http://www.w3.org/2000/09/xmldsig#\", \"Signature\")[0]\n\texpected_signature_node = expected_signature.getElementsByTagNameNS(\"http://www.w3.org/2000/09/xmldsig#\", \"SignatureValue\")[0]\n\texpected_signature_value = expected_signature_node.firstChild.nodeValue\n\n\t# Get the actual signature\n\tactual_signature_node = doc.getElementsByTagNameNS(\"http://www.w3.org/2000/09/xmldsig#\", \"SignatureValue\")[0]\n\tactual_signature_value = actual_signature_node.firstChild.nodeValue\n\n\t# Verify the signatures match\n\tassert expected_signature_value == actual_signature_value, \"The signatures do not match.\"\n\n\t# Get the digest from the signed element\n\tsigned_element =", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\th = SHA256.new(doc)\n\trsakey = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(rsakey)\n\treturn verifier.verify(h, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA256\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.PublicKey import DSA\n\tfrom Crypto.PublicKey import ECC\n\tfrom Crypto.PublicKey import DSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA256\n\tfrom base64 import b64decode\n\tfrom lxml import etree\n\n\t\n\tdoc = etree.fromstring(doc)\n\t\n\tif doc.tag != \"{http://www.w3.org/2000/09/xmldsig#}Signature\":\n\t\tprint(\"Error: This is not a signed XML document\")\n\t\treturn\n\n\tif doc.find(\"{http://www.w3.org/2000/09/xmldsig#}SignatureValue\") is None:\n\t\tprint(\"Error: No signature value found\")\n\t\treturn\n\n\tif doc.find(\"{http://www.w3.org/2000/09/xmldsig#}SignedInfo\") is None:\n\t\tprint(\"Error: No signed info found\")\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport rsa\n\t\n\ttry:\n\t\trsa_public_key = rsa.PublicKey.load_pkcs1(public_key, 'DER')\n\texcept:\n\t\traise ValueError(\"Invalid public key\")\n\t\n\ttry:\n\t\trsa.verify(doc, signature, rsa_public_key)\n\texcept:\n\t\traise ValueError(\"Invalid signature\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\twebfinger = json.loads(document)\n\t\tfor link in webfinger['links']:\n\t\t\tif link['rel'] == 'http://microformats.org/profile/hcard':\n\t\t\t\thcard_url = link['href']\n\t\t\t\tbreak\n\texcept Exception as e:\n\t\traise e\n\treturn hcard_url\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\tfor link in webfinger[\"links\"]:\n\t\tif link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n\t\t\thcard_url = link[\"href\"]\n\t\t\treturn {\n\t\t\t\t\"hcard_url\": hcard_url\n\t\t\t}", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\twf = json.loads(document)\n\texcept json.JSONDecodeError as e:\n\t\traise ValueError('Invalid JSON document: %s' % e)\n\n\tlinks = wf.get('links')\n\tif not links:\n\t\traise ValueError('No links found in webfinger document')\n\n\thcard_url = None\n\n\tfor link in links:\n\t\tif link['rel'] == 'h-card':\n\t\t\thcard_url = link['href']\n\t\t\tbreak\n\n\tif not hcard_url:\n\t\traise ValueError('No h-card link found in webfinger document')\n\n\treturn hcard_url\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument_json = json.loads(document)\n\tlinks = document_json.get(\"links\")\n\tif links:\n\t\tfor link in links:\n\t\t\tif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n\t\t\t\treturn {\n\t\t\t\t\t\"hcard_url\": link.get(\"href\")\n\t\t\t\t}\n\treturn {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger.get('links', [{}])[0].get('href', '')\n\thcard_url = hcard_url.replace(\n\t\t'https://', 'https://', 1)\n\thcard_url = hcard_url.replace(\n\t\t'http://', 'https://', 1)\n\treturn hcard_url", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\tjrd_links = webfinger.get('links')\n\tfor link in jrd_links:\n\t\tif 'rel' in link and link['rel'] == 'http://hcard.org/rel/hcard-url':\n\t\t\thcard_url = link['href']\n\t\t\tbreak\n\t\telse:\n\t\t\thcard_url = None\n\n\tuser = {}\n\tuser['guid'] = webfinger.get('subject')\n\tuser['name'] = webfinger.get('name')\n\tuser['hcard_url'] = hcard_url\n\n\treturn user\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger[\"links\"][0][\"href\"]\n\treturn hcard_url\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tjson_document = json.loads(document)\n\tlinks = json_document['links']\n\thcard_url = None\n\tfor link in links:\n\t\tif link['rel'] == \"http://microformats.org/profile/hcard\":\n\t\t\thcard_url = link['href']\n\tif not hcard_url:\n\t\traise Exception(\"No hcard_url found in webfinger document\")\n\treturn {\n\t\t'hcard_url': hcard_url,\n\t\t'hcard': json.loads(requests.get(hcard_url).text)\n\t}", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger['links'][0]['href']\n\tresponse = requests.get(hcard_url)\n\thcard = json.loads(response.text)\n\treturn hcard\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger['links'][0]['href']\n\thcard = requests.get(hcard_url).text\n\thcard_dict = json.loads(hcard)\n\treturn hcard_dict"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\twebfinger = json.loads(document)\n\t\tfor link in webfinger['links']:\n\t\t\tif link['rel'] == 'http://microformats.org/profile/hcard':\n\t\t\t\thcard_url = link['href']\n\t\t\t\tbreak\n\texcept Exception as e:\n\t\traise e\n\treturn hcard_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\tfor link in webfinger[\"links\"]:\n\t\tif link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n\t\t\thcard_url = link[\"href\"]\n\t\t\treturn {\n\t\t\t\t\"hcard_url\": hcard_url\n\t\t\t}", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\twf = json.loads(document)\n\texcept json.JSONDecodeError as e:\n\t\traise ValueError('Invalid JSON document: %s' % e)\n\n\tlinks = wf.get('links')\n\tif not links:\n\t\traise ValueError('No links found in webfinger document')\n\n\thcard_url = None\n\n\tfor link in links:\n\t\tif link['rel'] == 'h-card':\n\t\t\thcard_url = link['href']\n\t\t\tbreak\n\n\tif not hcard_url:\n\t\traise ValueError('No h-card link found in webfinger document')\n\n\treturn hcard_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument_json = json.loads(document)\n\tlinks = document_json.get(\"links\")\n\tif links:\n\t\tfor link in links:\n\t\t\tif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n\t\t\t\treturn {\n\t\t\t\t\t\"hcard_url\": link.get(\"href\")\n\t\t\t\t}\n\treturn {}", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger.get('links', [{}])[0].get('href', '')\n\thcard_url = hcard_url.replace(\n\t\t'https://', 'https://', 1)\n\thcard_url = hcard_url.replace(\n\t\t'http://', 'https://', 1)\n\treturn hcard_url", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\tjrd_links = webfinger.get('links')\n\tfor link in jrd_links:\n\t\tif 'rel' in link and link['rel'] == 'http://hcard.org/rel/hcard-url':\n\t\t\thcard_url = link['href']\n\t\t\tbreak\n\t\telse:\n\t\t\thcard_url = None\n\n\tuser = {}\n\tuser['guid'] = webfinger.get('subject')\n\tuser['name'] = webfinger.get('name')\n\tuser['hcard_url'] = hcard_url\n\n\treturn user\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger[\"links\"][0][\"href\"]\n\treturn hcard_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tjson_document = json.loads(document)\n\tlinks = json_document['links']\n\thcard_url = None\n\tfor link in links:\n\t\tif link['rel'] == \"http://microformats.org/profile/hcard\":\n\t\t\thcard_url = link['href']\n\tif not hcard_url:\n\t\traise Exception(\"No hcard_url found in webfinger document\")\n\treturn {\n\t\t'hcard_url': hcard_url,\n\t\t'hcard': json.loads(requests.get(hcard_url).text)\n\t}", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger['links'][0]['href']\n\tresponse = requests.get(hcard_url)\n\thcard = json.loads(response.text)\n\treturn hcard\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\thcard_url = webfinger['links'][0]['href']\n\thcard = requests.get(hcard_url).text\n\thcard_dict = json.loads(hcard)\n\treturn hcard_dict", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn webfinger(handle)\n\texcept Exception:\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turl = f'https://{handle}/.well-known/webfinger'\n\t\tresponse = requests.get(url)\n\t\treturn response.text\n\texcept:\n\t\treturn None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(f\"https://{handle}.t.me/web_app_id/webfinger\")\n\texcept Exception:\n\t\treturn None\n\tif response.status_code == 200:\n\t\treturn response.text\n\telse:\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(f'https://{handle}/.well-known/webfinger', timeout=1)\n\texcept requests.exceptions.Timeout:\n\t\treturn None\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\texcept requests.exceptions.HTTPError:\n\t\treturn None\n\n\tif response.status_code == 200:\n\t\treturn response.text\n\telse:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept:\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\thandle_url = f\"acct:{handle}@{DOMAIN}\"\n\turl = f\"https://{DOMAIN}/.well-known/webfinger?resource={handle_url}\"\n\ttry:\n\t\tresponse = requests.get(url, timeout=5)\n\texcept requests.RequestException as e:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document: {e}\")\n\t\treturn None\n\tif response.status_code != 200:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document: {response.status_code} {response.reason}\")\n\t\treturn None\n\treturn response.text", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresp = requests.get(f\"https://{handle}/webfinger?resource=acct:{handle}\", timeout=5)\n\t\tresp.raise_for_status()\n\texcept Exception:\n\t\treturn None\n\treturn resp.text\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tr = requests.get(\n\t\t\tf'https://{handle}.{settings.DOMAIN}/.well-known/webfinger',\n\t\t\theaders={'Accept': 'application/jrd+json'},\n\t\t\ttimeout=5,\n\t\t)\n\texcept (requests.exceptions.RequestException, requests.exceptions.Timeout):\n\t\treturn None\n\tif r.status_code != 200:\n\t\treturn None\n\treturn r.text\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turi = f\"https://{handle}/.well-known/webfinger\"\n\t\tresponse = requests.get(uri, timeout=5)\n\t\tresponse.raise_for_status()\n\t\treturn response.text\n\texcept:\n\t\treturn None"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn webfinger(handle)\n\texcept Exception:\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turl = f'https://{handle}/.well-known/webfinger'\n\t\tresponse = requests.get(url)\n\t\treturn response.text\n\texcept:\n\t\treturn None", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(f\"https://{handle}.t.me/web_app_id/webfinger\")\n\texcept Exception:\n\t\treturn None\n\tif response.status_code == 200:\n\t\treturn response.text\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(f'https://{handle}/.well-known/webfinger', timeout=1)\n\texcept requests.exceptions.Timeout:\n\t\treturn None\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\texcept requests.exceptions.HTTPError:\n\t\treturn None\n\n\tif response.status_code == 200:\n\t\treturn response.text\n\telse:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept:\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\thandle_url = f\"acct:{handle}@{DOMAIN}\"\n\turl = f\"https://{DOMAIN}/.well-known/webfinger?resource={handle_url}\"\n\ttry:\n\t\tresponse = requests.get(url, timeout=5)\n\texcept requests.RequestException as e:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document: {e}\")\n\t\treturn None\n\tif response.status_code != 200:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document: {response.status_code} {response.reason}\")\n\t\treturn None\n\treturn response.text", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresp = requests.get(f\"https://{handle}/webfinger?resource=acct:{handle}\", timeout=5)\n\t\tresp.raise_for_status()\n\texcept Exception:\n\t\treturn None\n\treturn resp.text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tr = requests.get(\n\t\t\tf'https://{handle}.{settings.DOMAIN}/.well-known/webfinger',\n\t\t\theaders={'Accept': 'application/jrd+json'},\n\t\t\ttimeout=5,\n\t\t)\n\texcept (requests.exceptions.RequestException, requests.exceptions.Timeout):\n\t\treturn None\n\tif r.status_code != 200:\n\t\treturn None\n\treturn r.text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turi = f\"https://{handle}/.well-known/webfinger\"\n\t\tresponse = requests.get(uri, timeout=5)\n\t\tresponse.raise_for_status()\n\t\treturn response.text\n\texcept:\n\t\treturn None", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom urllib.request import urlopen\n\tfrom urllib.error import HTTPError\n\tfrom json import load\n\n\ttry:\n\t\twebfinger = load(urlopen('https://' + handle + '/.well-known/webfinger'))\n\texcept HTTPError as e:\n\t\traise Exception(e.reason)\n\n\treturn webfinger\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom urllib.request import urlopen\n\tfrom urllib.error import URLError\n\tfrom xml.etree import ElementTree\n\ttry:\n\t\twith urlopen(handle) as response:\n\t\t\tdoc = ElementTree.fromstring(response.read())\n\t\t\txrd = {}\n\t\t\tfor link in doc.findall('.//{http://www.w3.org/2005/Atom}link'):\n\t\t\t\txrd[link.get('rel')] = link.get('href')\n\t\t\treturn xrd\n\texcept URLError:\n\t\traise ValueError(f\"Failed to retrieve webfinger for {handle}\")\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom diaspora_fu.utils import retrieve_webfinger_document\n\tfrom diaspora_fu.diaspora.webfinger import parse_webfinger_document\n\t\n\twebfinger_content = retrieve_webfinger_document(handle)\n\treturn parse_webfinger_document(webfinger_content)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\tfrom lxml import etree\n\tresponse = requests.get(\"https://joindiaspora.com/.well-known/webfinger?resource=acct:%s@joindiaspora.com\" %handle)\n\ttree = etree.fromstring(response.text)\n\txml_items = tree.getchildren()\n\twebfinger_dict = {}\n\tfor item in xml_items:\n\t\twebfinger_dict[item.tag] = item.text\n\twebfinger_dict[\"handle\"] = handle\n\treturn webfinger_dict", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tpass\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\tfrom .utils import parse_webfinger_xml\n\n\turl = \"https://{0}.diasporafoundation.org/.well-known/webfinger?resource={1}\".format(handle, handle)\n\tr = requests.get(url)\n\tr.raise_for_status()\n\treturn parse_webfinger_xml(r.text)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\t\n\ttry:\n\t\tresponse = requests.get(\n\t\t\t'https://' + handle + '/.well-known/host-meta',\n\t\t\theaders={'Accept': 'application/xrd+xml, application/xml'}\n\t\t)\n\texcept requests.exceptions.RequestException as e:\n\t\traise Exception('Request failed: {}'.format(e))\n\t\n\tif response.status_code != 200:\n\t\traise Exception('Request returned status code: {}'.format(response.status_code))\n\t\n\t# Parse the response\n\tfrom lxml import etree\n\ttry:\n\t\troot = etree.fromstring(response.text)\n\texcept etree.XMLSyntaxError as e:\n\t\traise Exception('Response was not valid XML: {}'.format(e))\n\t\n\tdiaspora_hosts = set()\n\t\n\tfor link in root.findall('{http://www.w3.org/2005/Atom}link'):\n\t\tif link.get('rel') == 'diaspora-webfinger':\n\t\t\tdiaspora_hosts.add(link.get('href'))\n\t\n\t# For now, we only support one host\n\treturn list(diaspora_hosts)[0]", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom diaspora_client.lib.webfinger import retrieve_diaspora_webfinger\n\tfrom diaspora_client.lib.webfinger import parse_diaspora_webfinger\n\n\twebfinger_xml = retrieve_diaspora_webfinger(handle)\n\twebfinger_data = parse_diaspora_webfinger(webfinger_xml)\n\n\treturn webfinger_data", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\timport lxml.etree as etree\n\tfrom .diaspora_webfinger import Webfinger\n\n\ttry:\n\t\tresponse = requests.get('https://%s/.well-known/host-meta' % handle)\n\texcept requests.ConnectionError:\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\ttree = etree.fromstring(response.content)\n\n\tlinks = []\n\tfor link in tree.xpath('./Link[@rel=\"lrdd\"]'):\n\t\tlinks.append(link.attrib['template'])\n\n\tif not links:\n\t\treturn None\n\n\tlinks.sort(key = lambda x: len(x))\n\n\ttemplate = links[0]\n\n\turl = template % handle\n\n\tresponse = requests.get(url)\n\tif response.status_code != 200:\n\t\treturn None\n\n\treturn Webfinger(response.content)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport urllib2\n\timport re\n\timport json\n\n\tresponse = urllib2.urlopen('https://joindiaspora.com/webfinger?q=%s' % handle)\n\tdata = response.read()\n\tresponse.close()\n\n\tdata = re.sub('^.*<', '', data)\n\tdata = re.sub('>.*$', '', data)\n\n\tdata = urllib2.urlopen(data).read()\n\tdata = json.loads(data)\n\n\treturn data"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom urllib.request import urlopen\n\tfrom urllib.error import HTTPError\n\tfrom json import load\n\n\ttry:\n\t\twebfinger = load(urlopen('https://' + handle + '/.well-known/webfinger'))\n\texcept HTTPError as e:\n\t\traise Exception(e.reason)\n\n\treturn webfinger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom urllib.request import urlopen\n\tfrom urllib.error import URLError\n\tfrom xml.etree import ElementTree\n\ttry:\n\t\twith urlopen(handle) as response:\n\t\t\tdoc = ElementTree.fromstring(response.read())\n\t\t\txrd = {}\n\t\t\tfor link in doc.findall('.//{http://www.w3.org/2005/Atom}link'):\n\t\t\t\txrd[link.get('rel')] = link.get('href')\n\t\t\treturn xrd\n\texcept URLError:\n\t\traise ValueError(f\"Failed to retrieve webfinger for {handle}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom diaspora_fu.utils import retrieve_webfinger_document\n\tfrom diaspora_fu.diaspora.webfinger import parse_webfinger_document\n\t\n\twebfinger_content = retrieve_webfinger_document(handle)\n\treturn parse_webfinger_document(webfinger_content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\tfrom lxml import etree\n\tresponse = requests.get(\"https://joindiaspora.com/.well-known/webfinger?resource=acct:%s@joindiaspora.com\" %handle)\n\ttree = etree.fromstring(response.text)\n\txml_items = tree.getchildren()\n\twebfinger_dict = {}\n\tfor item in xml_items:\n\t\twebfinger_dict[item.tag] = item.text\n\twebfinger_dict[\"handle\"] = handle\n\treturn webfinger_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\tfrom .utils import parse_webfinger_xml\n\n\turl = \"https://{0}.diasporafoundation.org/.well-known/webfinger?resource={1}\".format(handle, handle)\n\tr = requests.get(url)\n\tr.raise_for_status()\n\treturn parse_webfinger_xml(r.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\t\n\ttry:\n\t\tresponse = requests.get(\n\t\t\t'https://' + handle + '/.well-known/host-meta',\n\t\t\theaders={'Accept': 'application/xrd+xml, application/xml'}\n\t\t)\n\texcept requests.exceptions.RequestException as e:\n\t\traise Exception('Request failed: {}'.format(e))\n\t\n\tif response.status_code != 200:\n\t\traise Exception('Request returned status code: {}'.format(response.status_code))\n\t\n\t# Parse the response\n\tfrom lxml import etree\n\ttry:\n\t\troot = etree.fromstring(response.text)\n\texcept etree.XMLSyntaxError as e:\n\t\traise Exception('Response was not valid XML: {}'.format(e))\n\t\n\tdiaspora_hosts = set()\n\t\n\tfor link in root.findall('{http://www.w3.org/2005/Atom}link'):\n\t\tif link.get('rel') == 'diaspora-webfinger':\n\t\t\tdiaspora_hosts.add(link.get('href'))\n\t\n\t# For now, we only support one host\n\treturn list(diaspora_hosts)[0]", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom diaspora_client.lib.webfinger import retrieve_diaspora_webfinger\n\tfrom diaspora_client.lib.webfinger import parse_diaspora_webfinger\n\n\twebfinger_xml = retrieve_diaspora_webfinger(handle)\n\twebfinger_data = parse_diaspora_webfinger(webfinger_xml)\n\n\treturn webfinger_data", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport requests\n\timport lxml.etree as etree\n\tfrom .diaspora_webfinger import Webfinger\n\n\ttry:\n\t\tresponse = requests.get('https://%s/.well-known/host-meta' % handle)\n\texcept requests.ConnectionError:\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\ttree = etree.fromstring(response.content)\n\n\tlinks = []\n\tfor link in tree.xpath('./Link[@rel=\"lrdd\"]'):\n\t\tlinks.append(link.attrib['template'])\n\n\tif not links:\n\t\treturn None\n\n\tlinks.sort(key = lambda x: len(x))\n\n\ttemplate = links[0]\n\n\turl = template % handle\n\n\tresponse = requests.get(url)\n\tif response.status_code != 200:\n\t\treturn None\n\n\treturn Webfinger(response.content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\timport urllib2\n\timport re\n\timport json\n\n\tresponse = urllib2.urlopen('https://joindiaspora.com/webfinger?q=%s' % handle)\n\tdata = response.read()\n\tresponse.close()\n\n\tdata = re.sub('^.*<', '', data)\n\tdata = re.sub('>.*$', '', data)\n\n\tdata = urllib2.urlopen(data).read()\n\tdata = json.loads(data)\n\n\treturn data", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport requests\n\tfrom pyxrd.xrd import XRD\n\tresp = requests.get(host)\n\txrd = XRD.fromXML(resp.text)\n\treturn xrd", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\timport urllib2\n\ttry:\n\t\tif not host.startswith(\"http://\"):\n\t\t\thost = \"http://\" + host\n\t\tresponse = urllib2.urlopen(host + \"/.well-known/host-meta\")\n\texcept urllib2.HTTPError:\n\t\traise urllib2.URLError(\"Diaspora host-meta not found\")\n\tdoc = xrd.Document(response.read())\n\tdoc.close()\n\treturn doc\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrd import XRD\n\tfrom urllib2 import urlopen\n\n\turl = 'https://' + host + '/.well-known/host-meta'\n\treq = urlopen(url)\n\tdata = req.read()\n\treq.close()\n\n\treturn XRD(data)\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrd.client import XRD\n\txrd = XRD()\n\txrd.retrieve(host + '/.well-known/host-meta')\n\treturn xrd\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\ttry:\n\t\txrd_url = host + '/host-meta'\n\t\treturn xrd.XRD(xrd_url)\n\texcept:\n\t\treturn None\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrd.xrd import XRD\n\tfrom urllib2 import urlopen\n\treturn XRD(urlopen(host))", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom diaspy import tools\n\tfrom diaspy.models import XRD\n\n\txrd = tools.url_retrieve_to_xrd(host + '/.well-known/host-meta')\n\tif not xrd:\n\t\traise Exception('Could not retrieve host-meta from ' + host)\n\treturn XRD(xrd)\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\ttry:\n\t\timport xrd\n\texcept ImportError:\n\t\traise ImportError('You need xrd-python installed to use this function')\n\n\ttry:\n\t\tfrom xrd import open_string\n\texcept ImportError:\n\t\tfrom xrd.open_string import open_string\n\n\treturn open_string(host)\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\n\txrd_url = host + '/.well-known/host-meta'\n\n\treturn xrd.parse(xrd_url)\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\timport urllib2\n\timport logging\n\tlogger = logging.getLogger()\n\tlogger.info('Retrieving host-meta from {0}'.format(host))\n\ttry:\n\t\treq = urllib2.Request('http://{0}/.well-known/host-meta'.format(host))\n\t\tresponse = urllib2.urlopen(req)\n\t\tdata = response.read()\n\t\tresponse.close()\n\texcept urllib2.HTTPError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\texcept urllib2.URLError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\texcept IOError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\texcept ValueError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\treturn xrd.XRD(data)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport requests\n\tfrom pyxrd.xrd import XRD\n\tresp = requests.get(host)\n\txrd = XRD.fromXML(resp.text)\n\treturn xrd", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\timport urllib2\n\ttry:\n\t\tif not host.startswith(\"http://\"):\n\t\t\thost = \"http://\" + host\n\t\tresponse = urllib2.urlopen(host + \"/.well-known/host-meta\")\n\texcept urllib2.HTTPError:\n\t\traise urllib2.URLError(\"Diaspora host-meta not found\")\n\tdoc = xrd.Document(response.read())\n\tdoc.close()\n\treturn doc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrd import XRD\n\tfrom urllib2 import urlopen\n\n\turl = 'https://' + host + '/.well-known/host-meta'\n\treq = urlopen(url)\n\tdata = req.read()\n\treq.close()\n\n\treturn XRD(data)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrd.client import XRD\n\txrd = XRD()\n\txrd.retrieve(host + '/.well-known/host-meta')\n\treturn xrd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\ttry:\n\t\txrd_url = host + '/host-meta'\n\t\treturn xrd.XRD(xrd_url)\n\texcept:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrd.xrd import XRD\n\tfrom urllib2 import urlopen\n\treturn XRD(urlopen(host))", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom diaspy import tools\n\tfrom diaspy.models import XRD\n\n\txrd = tools.url_retrieve_to_xrd(host + '/.well-known/host-meta')\n\tif not xrd:\n\t\traise Exception('Could not retrieve host-meta from ' + host)\n\treturn XRD(xrd)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\ttry:\n\t\timport xrd\n\texcept ImportError:\n\t\traise ImportError('You need xrd-python installed to use this function')\n\n\ttry:\n\t\tfrom xrd import open_string\n\texcept ImportError:\n\t\tfrom xrd.open_string import open_string\n\n\treturn open_string(host)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\n\txrd_url = host + '/.well-known/host-meta'\n\n\treturn xrd.parse(xrd_url)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport xrd\n\timport urllib2\n\timport logging\n\tlogger = logging.getLogger()\n\tlogger.info('Retrieving host-meta from {0}'.format(host))\n\ttry:\n\t\treq = urllib2.Request('http://{0}/.well-known/host-meta'.format(host))\n\t\tresponse = urllib2.urlopen(req)\n\t\tdata = response.read()\n\t\tresponse.close()\n\texcept urllib2.HTTPError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\texcept urllib2.URLError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\texcept IOError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\texcept ValueError, e:\n\t\tlogger.warn('{0} - {1}'.format(e, host))\n\t\treturn\n\treturn xrd.XRD(data)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tfrom requests import post\n\tdata[\"api_url\"] = url\n\ttry:\n\t\tr = post(url, data=data, timeout=timeout)\n\texcept Exception as e:\n\t\tr = None\n\treturn r\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\tresponse = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n\treturn response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\tfrom requests.auth import HTTPBasicAuth\n\t\n\ttry:\n\t\tauth = kwargs[\"auth\"]\n\t\tauth = HTTPBasicAuth(auth[\"username\"], auth[\"password\"])\n\texcept KeyError:\n\t\tauth = None\n\t\n\ttry:\n\t\theaders = kwargs[\"headers\"]\n\texcept KeyError:\n\t\theaders = {}\n\t\n\ttry:\n\t\theaders[\"Content-Type\"] = \"application/json\"\n\texcept KeyError:\n\t\tpass\n\t\n\ttry:\n\t\theaders[\"Accept\"] = \"application/json\"\n\texcept KeyError:\n\t\tpass\n\t\n\ttry:\n\t\theaders[\"User-Agent\"] = \"l33t-h4x0rz-cli\"\n\texcept KeyError:\n\t\tpass\n\t\n\tresponse = requests.request(\n\t\tmethod=method,\n\t\turl=url,\n\t\tdata=data,\n\t\tauth=auth,\n\t\theaders=headers,\n\t\ttimeout=timeout,\n\t\t*args,\n\t\t**kwargs\n\t)\n\t\n\treturn response.json()", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tfrom . import send_response\n\tfrom ..response import Response\n\timport requests\n\n\tresp = Response(url, method=method, *args, **kwargs)\n\n\ttry:\n\t\tr = requests.post(url,\n\t\t\t\t\t\t  data=data,\n\t\t\t\t\t\t  timeout=timeout)\n\texcept Exception as e:\n\t\tresp.error = str(e)\n\t\treturn resp\n\n\tresp.status_code = r.status_code\n\tresp.reason = r.reason\n\tresp.text = r.text\n\n\treturn resp", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\trequests.post(url, data=data, *args, **kwargs)\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\timport requests\n\t\n\tif not url:\n\t\traise ValueError(\"url must be specified\")\n\t\n\t\n\ttry:\n\t\tsession = requests.Session()\n\t\tresponse = session.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n\t\treturn response\n\texcept:\n\t\tprint(\"Error sending document to url: {}\".format(url))", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\timport os\n\timport sys\n\tfrom PIL import Image\n\n\ttry:\n\t\timg = Image.open(data)\n\texcept:\n\t\tprint(\"Error:\", sys.exc_info()[0])\n\t\treturn False\n\n\tif img.size[0] > 1400:\n\t\timg.thumbnail((1400, img.size[1]))\n\timg.save(data.name)\n\n\tfiles = {\n\t\t\"file\": open(data.name, \"rb\")\n\t}\n\n\ttry:\n\t\tr = requests.post(url, files=files, timeout=timeout, *args, **kwargs)\n\t\tos.remove(data.name)\n\t\treturn r\n\texcept:\n\t\tprint(\"Error:\", sys.exc_info()[0])\n\t\treturn False\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tresponse = requests.request(method=method, url=url, data=data, timeout=timeout, *args, **kwargs)\n\treturn response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\t\n\tr = requests.post(url, data, timeout, *args, **kwargs)\n\t\n\treturn r.text\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tfrom requests import post\n\tfrom requests.exceptions import ConnectionError\n\tfrom requests.exceptions import ReadTimeout\n\n\ttry:\n\t\tpost(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept ConnectionError:\n\t\treturn False\n\texcept ReadTimeout:\n\t\treturn False\n\t\n\treturn True"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tfrom requests import post\n\tdata[\"api_url\"] = url\n\ttry:\n\t\tr = post(url, data=data, timeout=timeout)\n\texcept Exception as e:\n\t\tr = None\n\treturn r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\tresponse = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\tfrom requests.auth import HTTPBasicAuth\n\t\n\ttry:\n\t\tauth = kwargs[\"auth\"]\n\t\tauth = HTTPBasicAuth(auth[\"username\"], auth[\"password\"])\n\texcept KeyError:\n\t\tauth = None\n\t\n\ttry:\n\t\theaders = kwargs[\"headers\"]\n\texcept KeyError:\n\t\theaders = {}\n\t\n\ttry:\n\t\theaders[\"Content-Type\"] = \"application/json\"\n\texcept KeyError:\n\t\tpass\n\t\n\ttry:\n\t\theaders[\"Accept\"] = \"application/json\"\n\texcept KeyError:\n\t\tpass\n\t\n\ttry:\n\t\theaders[\"User-Agent\"] = \"l33t-h4x0rz-cli\"\n\texcept KeyError:\n\t\tpass\n\t\n\tresponse = requests.request(\n\t\tmethod=method,\n\t\turl=url,\n\t\tdata=data,\n\t\tauth=auth,\n\t\theaders=headers,\n\t\ttimeout=timeout,\n\t\t*args,\n\t\t**kwargs\n\t)\n\t\n\treturn response.json()", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tfrom . import send_response\n\tfrom ..response import Response\n\timport requests\n\n\tresp = Response(url, method=method, *args, **kwargs)\n\n\ttry:\n\t\tr = requests.post(url,\n\t\t\t\t\t\t  data=data,\n\t\t\t\t\t\t  timeout=timeout)\n\texcept Exception as e:\n\t\tresp.error = str(e)\n\t\treturn resp\n\n\tresp.status_code = r.status_code\n\tresp.reason = r.reason\n\tresp.text = r.text\n\n\treturn resp", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\trequests.post(url, data=data, *args, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\timport requests\n\t\n\tif not url:\n\t\traise ValueError(\"url must be specified\")\n\t\n\t\n\ttry:\n\t\tsession = requests.Session()\n\t\tresponse = session.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n\t\treturn response\n\texcept:\n\t\tprint(\"Error sending document to url: {}\".format(url))", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\timport os\n\timport sys\n\tfrom PIL import Image\n\n\ttry:\n\t\timg = Image.open(data)\n\texcept:\n\t\tprint(\"Error:\", sys.exc_info()[0])\n\t\treturn False\n\n\tif img.size[0] > 1400:\n\t\timg.thumbnail((1400, img.size[1]))\n\timg.save(data.name)\n\n\tfiles = {\n\t\t\"file\": open(data.name, \"rb\")\n\t}\n\n\ttry:\n\t\tr = requests.post(url, files=files, timeout=timeout, *args, **kwargs)\n\t\tos.remove(data.name)\n\t\treturn r\n\texcept:\n\t\tprint(\"Error:\", sys.exc_info()[0])\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tresponse = requests.request(method=method, url=url, data=data, timeout=timeout, *args, **kwargs)\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\t\n\tr = requests.post(url, data, timeout, *args, **kwargs)\n\t\n\treturn r.text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tfrom requests import post\n\tfrom requests.exceptions import ConnectionError\n\tfrom requests.exceptions import ReadTimeout\n\n\ttry:\n\t\tpost(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept ConnectionError:\n\t\treturn False\n\texcept ReadTimeout:\n\t\treturn False\n\t\n\treturn True", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = dic.get(key, {})\n\tif keys.__len__():\n\t\tdict_insert(dic[key], val, *keys)\n\telse:\n\t\tdic[key] = val", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = dict_insert(dic.get(key, {}), val, *keys) if keys else val", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttarget = dic\n\tfor i, k in enumerate(keys):\n\t\tif i != len(keys) - 1:\n\t\t\tif k not in target:\n\t\t\t\ttarget[k] = {}\n\t\t\ttarget = target[k]\n\t\telse:\n\t\t\ttarget[k] = val", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tif not keys:\n\t\tdic[key] = val\n\telse:\n\t\tdict_insert(dic.setdefault(key, {}), val, *keys)\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\td = dic\n\tl = [key]\n\tl.extend(keys)\n\tfor k in l[:-1]:\n\t\tif k not in d:\n\t\t\td[k] = {}\n\t\td = d[k]\n\td[l[-1]] = val\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkeys = [key] + list(keys)\n\tkeys_chain = keys[:-1]\n\tkey = keys[-1]\n\n\tfor k in keys_chain:\n\t\tdic = dic.setdefault(k, {})\n\n\tdic[key] = val", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = dict_insert(dic.get(key, {}), val, *keys) if keys else val\n\treturn dic", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkeys = list(keys)\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn dic\n\n\tkey_to_insert = keys.pop(0)\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, key_to_insert, *keys)\n\n\treturn dic", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttry:\n\t\tcurrent = dic[key]\n\texcept KeyError:\n\t\tcurrent = {}\n\t\tif keys:\n\t\t\tcurrent = dict_insert(current, None, *keys)\n\t\tdic[key] = current\n\tif keys:\n\t\tcurrent = dict_insert(current, val, *keys)\n\telse:\n\t\tcurrent[key] = val\n\treturn dic", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\td = dic\n\tfor k in keys:\n\t\tif k not in d:\n\t\t\td[k] = {}\n\t\td = d[k]\n\td[key] = val\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = dic.get(key, {})\n\tif keys.__len__():\n\t\tdict_insert(dic[key], val, *keys)\n\telse:\n\t\tdic[key] = val", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = dict_insert(dic.get(key, {}), val, *keys) if keys else val", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttarget = dic\n\tfor i, k in enumerate(keys):\n\t\tif i != len(keys) - 1:\n\t\t\tif k not in target:\n\t\t\t\ttarget[k] = {}\n\t\t\ttarget = target[k]\n\t\telse:\n\t\t\ttarget[k] = val", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tif not keys:\n\t\tdic[key] = val\n\telse:\n\t\tdict_insert(dic.setdefault(key, {}), val, *keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\td = dic\n\tl = [key]\n\tl.extend(keys)\n\tfor k in l[:-1]:\n\t\tif k not in d:\n\t\t\td[k] = {}\n\t\td = d[k]\n\td[l[-1]] = val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkeys = [key] + list(keys)\n\tkeys_chain = keys[:-1]\n\tkey = keys[-1]\n\n\tfor k in keys_chain:\n\t\tdic = dic.setdefault(k, {})\n\n\tdic[key] = val", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic[key] = dict_insert(dic.get(key, {}), val, *keys) if keys else val\n\treturn dic", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkeys = list(keys)\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn dic\n\n\tkey_to_insert = keys.pop(0)\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, key_to_insert, *keys)\n\n\treturn dic", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttry:\n\t\tcurrent = dic[key]\n\texcept KeyError:\n\t\tcurrent = {}\n\t\tif keys:\n\t\t\tcurrent = dict_insert(current, None, *keys)\n\t\tdic[key] = current\n\tif keys:\n\t\tcurrent = dict_insert(current, val, *keys)\n\telse:\n\t\tcurrent[key] = val\n\treturn dic", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\td = dic\n\tfor k in keys:\n\t\tif k not in d:\n\t\t\td[k] = {}\n\t\td = d[k]\n\td[key] = val\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\timport cli\n\tfilenames = cli.ListOfFileNames()\n\tfilenames.set_value_from_string(settings_dirs)\n\treturn filenames\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom . import cli\n\timport os\n\tif spec_option == \"\":\n\t\traise ValueError(\"spec option is mandatory.\")\n\n\tcli.ListOfFileNames(settings_dirs).run()\n\t\n\tfrom . import initypes\n\t\n\tfiles = os.listdir(os.path.join(settings_dirs, \"settings\"))\n\t\n\tfile_names = []\n\tfor file in files:\n\t\tif file.endswith(spec_option):\n\t\t\tfile_names.append(file)\n\t\n\treturn initypes.IniType(cli.ListOfFileNames(settings_dirs).return_value, file_names)", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\timport cli\n\t\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom pydm import ini_file_types\n\tfrom pydm.widgets.fields import Field\n\tfrom pydm.widgets.display_area import DisplayArea\n\n\tfield = Field()\n\tfield.set_display_area(DisplayArea())\n\n\treturn ini_file_types.ListOfFileNames(field, settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tcli = settings_dirs.cli\n\tlist_of_file_names = cli.ListOfFileNames()\n\tif spec_option:\n\t\tlist_of_file_names.set_spec_option(spec_option)\n\treturn list_of_file_names", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom . import _cli\n\tlist_of_file_names_type = _cli.ListOfFileNames(spec_option)\n\treturn list_of_file_names_type(settings_dirs)", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\t\n\tcli_list_of_file_names = ListOfFileNames(settings_dirs, spec_option)\n\treturn cli_list_of_file_names"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\timport cli\n\tfilenames = cli.ListOfFileNames()\n\tfilenames.set_value_from_string(settings_dirs)\n\treturn filenames\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom . import cli\n\timport os\n\tif spec_option == \"\":\n\t\traise ValueError(\"spec option is mandatory.\")\n\n\tcli.ListOfFileNames(settings_dirs).run()\n\t\n\tfrom . import initypes\n\t\n\tfiles = os.listdir(os.path.join(settings_dirs, \"settings\"))\n\t\n\tfile_names = []\n\tfor file in files:\n\t\tif file.endswith(spec_option):\n\t\t\tfile_names.append(file)\n\t\n\treturn initypes.IniType(cli.ListOfFileNames(settings_dirs).return_value, file_names)", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\timport cli\n\t\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom pydm import ini_file_types\n\tfrom pydm.widgets.fields import Field\n\tfrom pydm.widgets.display_area import DisplayArea\n\n\tfield = Field()\n\tfield.set_display_area(DisplayArea())\n\n\treturn ini_file_types.ListOfFileNames(field, settings_dirs, spec_option)", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tcli = settings_dirs.cli\n\tlist_of_file_names = cli.ListOfFileNames()\n\tif spec_option:\n\t\tlist_of_file_names.set_spec_option(spec_option)\n\treturn list_of_file_names", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom . import _cli\n\tlist_of_file_names_type = _cli.ListOfFileNames(spec_option)\n\treturn list_of_file_names_type(settings_dirs)", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\t\n\tcli_list_of_file_names = ListOfFileNames(settings_dirs, spec_option)\n\treturn cli_list_of_file_names", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service is None:\n\t\treturn None\n\treturn service.ANSIBLE_CONFIG_MANAGER", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tcls._get_service()\n\treturn cls.__ansible_config_manager\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tansible_config_manager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\tif ansible_config_manager is None:\n\t\traise InvalidConfigurationError(\"Service {} not found\".format(ServiceName.ANSIBLE_CONFIG_MANAGER))\n\treturn ansible_config_manager\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tcls._get_service()\n\treturn cls._get_service().ANSIBLE_CONFIG_MANAGER\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tcls._get_service()\n\treturn cls.get_service(cls.ANSIBLE_CONFIG_MANAGER)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tpass\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom ansible_service_broker.config_manager import ConfigManager\n\treturn ConfigManager(cls._get_service().ANSIBLE_CONFIG_MANAGER)", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\ttry:\n\t\tconfig_manager = service.ANSIBLE_CONFIG_MANAGER\n\texcept AttributeError:\n\t\traise Exception(\"No config manager found for this service\")\n\treturn config_manager", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service:\n\t\treturn service.get(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\treturn None\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice_name = cls._get_service()\n\treturn cls._get_service_manager(service_name.ANSIBLE_CONFIG_MANAGER)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service is None:\n\t\treturn None\n\treturn service.ANSIBLE_CONFIG_MANAGER", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tcls._get_service()\n\treturn cls.__ansible_config_manager\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tansible_config_manager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\tif ansible_config_manager is None:\n\t\traise InvalidConfigurationError(\"Service {} not found\".format(ServiceName.ANSIBLE_CONFIG_MANAGER))\n\treturn ansible_config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tcls._get_service()\n\treturn cls._get_service().ANSIBLE_CONFIG_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tcls._get_service()\n\treturn cls.get_service(cls.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom ansible_service_broker.config_manager import ConfigManager\n\treturn ConfigManager(cls._get_service().ANSIBLE_CONFIG_MANAGER)", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\ttry:\n\t\tconfig_manager = service.ANSIBLE_CONFIG_MANAGER\n\texcept AttributeError:\n\t\traise Exception(\"No config manager found for this service\")\n\treturn config_manager", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service:\n\t\treturn service.get(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice_name = cls._get_service()\n\treturn cls._get_service_manager(service_name.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service(cls.SERVICE_NAME.WORKSPACE_MANAGER)\n\treturn service\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\twsm = cls._get_service(cls.WORKSPACE_MANAGER_NAME)\n\treturn wsm\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom com.sun.star.frame.Desktop import getCurrentDesktop\n\tdesktop = getCurrentDesktop()\n\treturn desktop.getComponentFromURL(\n\t\t\"service:org.openoffice.Office.WorkspaceManager\",\n\t\tcls._get_service(), 0)", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service(cls.ServiceName.WORKSPACE_MANAGER)\n\treturn service.workspace_manager\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom workspace_manager import WorkspaceManager\n\tfrom zope.interface import implements\n\tclass WorkspaceManager(WorkspaceManager):\n\t\timplements(IWorkspaceManager)\n\treturn WorkspaceManager\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice_name = ServiceName.WORKSPACE_MANAGER\n\ttry:\n\t\tworkspace_manager = cls._get_service(service_name)\n\texcept KeyError:\n\t\traise ServiceNotFoundError(\"workspace manager not found\") from None\n\treturn workspace_manager\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom enamlnative.android.app import Activity\n\tif Activity._get_service(cls, \"WORKSPACE_MANAGER\") is None:\n\t\traise Exception(\"WORKSPACE_MANAGER not found in cls._get_service()\")\n\treturn Activity._get_service(cls, \"WORKSPACE_MANAGER\")\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service('WORKSPACE_MANAGER')\n\treturn service.workspace_manager\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom com.sun.star.frame.Desktop:\n\tdef getCurrentWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getCurrentWorkspace()\n\tdef getWorkspaces():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaces()\n\tdef getDefaultWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getDefaultWorkspace()\n\tdef getWorkspaces():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaces()\n\tdef getCurrentWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getCurrentWorkspace()\n\tdef getWorkspaceByName(name):\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaceByName(name)\n\tdef getDefaultWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getDefaultWorkspace()\n\tdef getWorkspaces():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaces()\n\tdef getCurrentWorkspace():\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tsm = cls._get_service()\n\tsm.workspace_manager\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service(cls.SERVICE_NAME.WORKSPACE_MANAGER)\n\treturn service\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\twsm = cls._get_service(cls.WORKSPACE_MANAGER_NAME)\n\treturn wsm\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom com.sun.star.frame.Desktop import getCurrentDesktop\n\tdesktop = getCurrentDesktop()\n\treturn desktop.getComponentFromURL(\n\t\t\"service:org.openoffice.Office.WorkspaceManager\",\n\t\tcls._get_service(), 0)", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service(cls.ServiceName.WORKSPACE_MANAGER)\n\treturn service.workspace_manager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom workspace_manager import WorkspaceManager\n\tfrom zope.interface import implements\n\tclass WorkspaceManager(WorkspaceManager):\n\t\timplements(IWorkspaceManager)\n\treturn WorkspaceManager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice_name = ServiceName.WORKSPACE_MANAGER\n\ttry:\n\t\tworkspace_manager = cls._get_service(service_name)\n\texcept KeyError:\n\t\traise ServiceNotFoundError(\"workspace manager not found\") from None\n\treturn workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom enamlnative.android.app import Activity\n\tif Activity._get_service(cls, \"WORKSPACE_MANAGER\") is None:\n\t\traise Exception(\"WORKSPACE_MANAGER not found in cls._get_service()\")\n\treturn Activity._get_service(cls, \"WORKSPACE_MANAGER\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service('WORKSPACE_MANAGER')\n\treturn service.workspace_manager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom com.sun.star.frame.Desktop:\n\tdef getCurrentWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getCurrentWorkspace()\n\tdef getWorkspaces():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaces()\n\tdef getDefaultWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getDefaultWorkspace()\n\tdef getWorkspaces():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaces()\n\tdef getCurrentWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getCurrentWorkspace()\n\tdef getWorkspaceByName(name):\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaceByName(name)\n\tdef getDefaultWorkspace():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getDefaultWorkspace()\n\tdef getWorkspaces():\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER).getWorkspaces()\n\tdef getCurrentWorkspace():\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tsm = cls._get_service()\n\tsm.workspace_manager\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif not service:\n\t\treturn\n\treturn service.PLUGINS_MANAGER\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service and hasattr(service, \"PLUGINS_MANAGER\"):\n\t\treturn service.PLUGINS_MANAGER\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom zope.component import getUtility\n\tfrom collective.js.jqueryui.interfaces import IPluginsManager\n\treturn getUtility(IPluginsManager, name=cls._get_service())\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom lutris.plugins.manager import PluginsManager\n\treturn PluginsManager(cls._get_service())\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice_name = cls._get_service()\n\n\tif service_name:\n\t\treturn frappe.get_singleton(service_name.PLUGINS_MANAGER)\n\n\treturn None\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom trac.core import Component\n\t\n\tclass PluginManagerComponent(Component):\n\t\tdef __init__(self):\n\t\t\tComponent.__init__(self)\n\t\t\tself.plugin_manager = None\n\t\t\t\n\t\tdef setup(self):\n\t\t\tComponent.setup(self)\n\t\t\tself.plugin_manager = cls._get_service(ServiceName.PLUGINS_MANAGER)\n\t\n\tPluginManagerComponent.__name__ = str(cls.__name__) + \"_PluginManagerComponent\"\n\treturn PluginManagerComponent", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom zope.component import getUtility\n\tfrom zope.interface import implements\n\tfrom zope.component.interfaces import ComponentLookupError\n\n\ttry:\n\t\tplugins_manager = getUtility(cls._get_service())\n\texcept ComponentLookupError:\n\t\tplugins_manager = cls._get_service()()\n\t\tplugins_manager.implements(IExtensionsPluginManager)\n\n\treturn plugins_manager\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif not service:\n\t\treturn\n\treturn service.PLUGINS_MANAGER\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service and hasattr(service, \"PLUGINS_MANAGER\"):\n\t\treturn service.PLUGINS_MANAGER\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom zope.component import getUtility\n\tfrom collective.js.jqueryui.interfaces import IPluginsManager\n\treturn getUtility(IPluginsManager, name=cls._get_service())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom lutris.plugins.manager import PluginsManager\n\treturn PluginsManager(cls._get_service())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice_name = cls._get_service()\n\n\tif service_name:\n\t\treturn frappe.get_singleton(service_name.PLUGINS_MANAGER)\n\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom trac.core import Component\n\t\n\tclass PluginManagerComponent(Component):\n\t\tdef __init__(self):\n\t\t\tComponent.__init__(self)\n\t\t\tself.plugin_manager = None\n\t\t\t\n\t\tdef setup(self):\n\t\t\tComponent.setup(self)\n\t\t\tself.plugin_manager = cls._get_service(ServiceName.PLUGINS_MANAGER)\n\t\n\tPluginManagerComponent.__name__ = str(cls.__name__) + \"_PluginManagerComponent\"\n\treturn PluginManagerComponent", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom zope.component import getUtility\n\tfrom zope.interface import implements\n\tfrom zope.component.interfaces import ComponentLookupError\n\n\ttry:\n\t\tplugins_manager = getUtility(cls._get_service())\n\texcept ComponentLookupError:\n\t\tplugins_manager = cls._get_service()()\n\t\tplugins_manager.implements(IExtensionsPluginManager)\n\n\treturn plugins_manager\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = cls.load_data(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException(str(e))\n\n\tcls.validate_spec_data(spec_data)\n\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tpass", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\timport yaml\n\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\texcept yaml.YAMLError as e:\n\t\traise IRValidatorException(\n\t\t\t\"Could not parse YAML from spec file\",\n\t\t\te\n\t\t)\n\n\tif not 'name' in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'name' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'description' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'description' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'version' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'version' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'source' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'source' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'maintainer' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'maintainer' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'maintainer_email' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_dict = cls.load_from_content(spec_content)\n\tcls.validate(spec_dict)\n\treturn spec_dict\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_dict = {}\n\tfor field in cls.mandatory_fields:\n\t\tif field not in spec_content:\n\t\t\traise IRValidatorException('%s is a mandatory field' % field)\n\t\tspec_dict[field] = spec_content[field]\n\treturn spec_dict", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom IRValidatorException import IRValidatorException\n\tspec_data = cls.load_yaml(spec_content)\n\tcls.validate_spec_data(spec_data)\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfields = cls._get_fields()\n\ttry:\n\t\tspec_dict = yaml.load(spec_content)\n\t\tvalidate_spec_dict(spec_dict, fields)\n\texcept Exception as e:\n\t\traise IRValidatorException(\n\t\t\t\"Spec file is not valid: {}\".format(str(e)))\n\treturn spec_dict", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\timport yaml\n\texcept ImportError:\n\t\traise ImportError('PyYAML is a required dependency for this module to work')\n\n\tspec_data = None\n\tif spec_content:\n\t\ttry:\n\t\t\tspec_data = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as yaml_error:\n\t\t\traise IRValidatorException(yaml_error)\n\n\tif not spec_data:\n\t\traise IRValidatorException('Spec file content is missing')\n\n\t# validate spec file data\n\terrors = cls.validate_from_dict(spec_data)\n\n\tif errors:\n\t\traise IRValidatorException(errors)", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tpass\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = cls.load(spec_content)\n\tcls.validate(spec)\n\treturn spec\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = cls.load_data(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException(str(e))\n\n\tcls.validate_spec_data(spec_data)\n\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\timport yaml\n\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\texcept yaml.YAMLError as e:\n\t\traise IRValidatorException(\n\t\t\t\"Could not parse YAML from spec file\",\n\t\t\te\n\t\t)\n\n\tif not 'name' in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'name' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'description' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'description' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'version' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'version' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'source' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'source' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'maintainer' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'maintainer' is missing from spec file\",\n\t\t\tNone\n\t\t)\n\n\tif 'maintainer_email' not in spec_data:\n\t\traise IRValidatorException(\n\t\t\t\"'", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_dict = cls.load_from_content(spec_content)\n\tcls.validate(spec_dict)\n\treturn spec_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_dict = {}\n\tfor field in cls.mandatory_fields:\n\t\tif field not in spec_content:\n\t\t\traise IRValidatorException('%s is a mandatory field' % field)\n\t\tspec_dict[field] = spec_content[field]\n\treturn spec_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom IRValidatorException import IRValidatorException\n\tspec_data = cls.load_yaml(spec_content)\n\tcls.validate_spec_data(spec_data)\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfields = cls._get_fields()\n\ttry:\n\t\tspec_dict = yaml.load(spec_content)\n\t\tvalidate_spec_dict(spec_dict, fields)\n\texcept Exception as e:\n\t\traise IRValidatorException(\n\t\t\t\"Spec file is not valid: {}\".format(str(e)))\n\treturn spec_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\timport yaml\n\texcept ImportError:\n\t\traise ImportError('PyYAML is a required dependency for this module to work')\n\n\tspec_data = None\n\tif spec_content:\n\t\ttry:\n\t\t\tspec_data = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as yaml_error:\n\t\t\traise IRValidatorException(yaml_error)\n\n\tif not spec_data:\n\t\traise IRValidatorException('Spec file content is missing')\n\n\t# validate spec file data\n\terrors = cls.validate_from_dict(spec_data)\n\n\tif errors:\n\t\traise IRValidatorException(errors)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = cls.load(spec_content)\n\tcls.validate(spec)\n\treturn spec\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tpass\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tfrom yaml import load\n\t\tfrom yaml import YAMLError\n\texcept ImportError:\n\t\traise ImportError('Please install PyYAML')\n\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = load(f)\n\texcept IOError as e:\n\t\traise IRValidatorException(\n\t\t\t'File {} does not exist'.format(yaml_file))\n\texcept YAMLError as e:\n\t\traise IRValidatorException(\n\t\t\t'Error with YAML file {}: {}'.format(yaml_file, e.message))\n\n\tif not data:\n\t\traise IRValidatorException(\n\t\t\t'File {} is empty'.format(yaml_file))\n\n\t# Validate that all required fields are present\n\tfor field in cls.REQUIRED_FIELDS:\n\t\tif field not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Field {} is required'.format(field))\n\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file) as f:\n\t\tyaml_data = yaml.load(f, Loader=yaml.SafeLoader)\n\n\treturn cls.validate_from_dict(yaml_data)\n\n\tdef validate_from_dict(cls, yaml_data):\n\t\t\"\"\"\n\t\tValidates that a YAML data structure has all required fields\n\n:param yaml_data: Dictionary with data to be validated\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\t\"\"\"\n\t\tcls._validate_required_fields(yaml_data)\n\t\treturn yaml_data\n\n\tdef _validate_required_fields(self, yaml_data):\n\t\t\"\"\"\n\t\tValidates that a YAML data structure has all required fields\n\n:param yaml_data: Dictionary with data to be validated\n:raise IRValidatorException: when mandatory data is missing in file\n\t\t\"\"\"\n\t\tmissing_fields = self._get_missing_fields(yaml_data)\n\n\t\tif missing_fields:\n\t\t\tmessage = f\"Following fields are missing: {', '.join(missing_fields)}\"\n\t\t\traise IRValidatorException(message)\n\n\tdef _", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\traise NotImplementedError()\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as f:\n\t\tdata = yaml.load(f, Loader=yaml.FullLoader)\n\treturn cls.validate(data)", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom ir_validator.exceptions import IRValidatorException\n\timport yaml\n\n\tif not yaml_file:\n\t\traise IRValidatorException(\"No YAML file provided to load\")\n\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_data = yaml.safe_load(f)\n\texcept Exception as e:\n\t\traise IRValidatorException(f\"Error opening YAML file: {e}\")\n\n\tcls.validate_data(yaml_data)\n\n\treturn yaml_data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom ir_validator.exceptions import IRValidatorException\n\tfrom ir_validator.utils import load_yaml\n\n\ttry:\n\t\tdata = load_yaml(yaml_file)\n\t\tcls(data)\n\texcept IRValidatorException as e:\n\t\traise e\n\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tpass\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\timport yaml\n\twith open(yaml_file) as f:\n\t\tdata = yaml.load(f, Loader=yaml.Loader)\n\t\tdata = cls(**data)\n\t\tdata.validate()\n\t\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\timport yaml\n\n\tif yaml_file is not None:\n\t\twith open(yaml_file) as f:\n\t\t\tdata = yaml.safe_load(f)\n\t\t\treturn cls(**data)\n\n\treturn cls()"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tfrom yaml import load\n\t\tfrom yaml import YAMLError\n\texcept ImportError:\n\t\traise ImportError('Please install PyYAML')\n\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = load(f)\n\texcept IOError as e:\n\t\traise IRValidatorException(\n\t\t\t'File {} does not exist'.format(yaml_file))\n\texcept YAMLError as e:\n\t\traise IRValidatorException(\n\t\t\t'Error with YAML file {}: {}'.format(yaml_file, e.message))\n\n\tif not data:\n\t\traise IRValidatorException(\n\t\t\t'File {} is empty'.format(yaml_file))\n\n\t# Validate that all required fields are present\n\tfor field in cls.REQUIRED_FIELDS:\n\t\tif field not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Field {} is required'.format(field))\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file) as f:\n\t\tyaml_data = yaml.load(f, Loader=yaml.SafeLoader)\n\n\treturn cls.validate_from_dict(yaml_data)\n\n\tdef validate_from_dict(cls, yaml_data):\n\t\t\"\"\"\n\t\tValidates that a YAML data structure has all required fields\n\n:param yaml_data: Dictionary with data to be validated\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\t\"\"\"\n\t\tcls._validate_required_fields(yaml_data)\n\t\treturn yaml_data\n\n\tdef _validate_required_fields(self, yaml_data):\n\t\t\"\"\"\n\t\tValidates that a YAML data structure has all required fields\n\n:param yaml_data: Dictionary with data to be validated\n:raise IRValidatorException: when mandatory data is missing in file\n\t\t\"\"\"\n\t\tmissing_fields = self._get_missing_fields(yaml_data)\n\n\t\tif missing_fields:\n\t\t\tmessage = f\"Following fields are missing: {', '.join(missing_fields)}\"\n\t\t\traise IRValidatorException(message)\n\n\tdef _", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as f:\n\t\tdata = yaml.load(f, Loader=yaml.FullLoader)\n\treturn cls.validate(data)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom ir_validator.exceptions import IRValidatorException\n\timport yaml\n\n\tif not yaml_file:\n\t\traise IRValidatorException(\"No YAML file provided to load\")\n\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_data = yaml.safe_load(f)\n\texcept Exception as e:\n\t\traise IRValidatorException(f\"Error opening YAML file: {e}\")\n\n\tcls.validate_data(yaml_data)\n\n\treturn yaml_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom ir_validator.exceptions import IRValidatorException\n\tfrom ir_validator.utils import load_yaml\n\n\ttry:\n\t\tdata = load_yaml(yaml_file)\n\t\tcls(data)\n\texcept IRValidatorException as e:\n\t\traise e\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\timport yaml\n\twith open(yaml_file) as f:\n\t\tdata = yaml.load(f, Loader=yaml.Loader)\n\t\tdata = cls(**data)\n\t\tdata.validate()\n\t\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\timport yaml\n\n\tif yaml_file is not None:\n\t\twith open(yaml_file) as f:\n\t\t\tdata = yaml.safe_load(f)\n\t\t\treturn cls(**data)\n\n\treturn cls()", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroups = parser_dict.get('include')\n\tif not groups:\n\t\treturn\n\n\tdef _parse_include(group_name):\n\t\t\"\"\"\n\t\tFinds the included group and merges it into the\n\t\tcurrent group.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tgroup_dict = self._groups[group_name]\n\t\texcept KeyError:\n\t\t\traise ParserError('Group \"{}\" included in \"{}\" not found'.format(\n\t\t\t\tgroup_name, self._current_group_name\n\t\t\t))\n\n\t\t# Merge the include into the current group\n\t\tself._current_group_dict.update(group_dict)\n\n\tfor group_name in groups:\n\t\t_parse_include(group_name)\n\n\treturn True", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\timport os\n\n\tinclude_dict = parser_dict['include']\n\n\tif not include_dict:\n\t\treturn\n\n\tinclude_groups = include_dict.get('groups')\n\tif include_groups:\n\t\tparser_dict['groups'] = self._include_groups_in_dict(parser_dict['groups'], include_groups)\n\n\tinclude_rules = include_dict.get('rules')\n\tif include_rules:\n\t\tparser_dict['rules'] = self._include_rules_in_dict(parser_dict['rules'], include_rules)\n\n\tinclude_macros = include_dict.get('macros')\n\tif include_macros:\n\t\tparser_dict['macros'] = self._include_macros_in_dict(parser_dict['macros'], include_macros)\n\n\tinclude_files = include_dict.get('files')\n\tif include_files:\n\t\tparser_dict['files'] = self._include_files_in_dict(parser_dict['files'], include_files)\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.include_groups = parser_dict.get('include_groups', [])\n\n\tgroups = []\n\tgroup_files = []\n\tfor group in self.include_groups:\n\t\tgroup_file = self.root_dir / 'specs' / (group + '.yml')\n\t\tgroup_files.append(group_file)\n\t\tif not group_file.is_file():\n\t\t\traise ValueError('Group file does not exist: {}'.format(group_file))\n\n\t\twith open(group_file, 'r') as f:\n\t\t\tgroup_dict = yaml.safe_load(f)\n\n\t\tgroups.append(group_dict)\n\n\tself._update_specs(groups)\n\tself.group_files = group_files", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tincludes = parser_dict['includes']\n\t\n\tfor include in includes:\n\t\tgroup = self._parse_file(include['file'], include['line'])\n\t\tself._include_groups([group])\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfrom .. import spec\n\n\tspec_dir = self.spec_dir\n\tinclude_groups = parser_dict.get(\"include_groups\", [])\n\tinclude_groups = [parser_dict[\"include_groups\"]] if isinstance(include_groups, str) else include_groups\n\tgroups_to_include = []\n\n\tfor group in include_groups:\n\t\ttry:\n\t\t\tif isinstance(group, list):\n\t\t\t\tgroups_to_include.extend(group)\n\t\t\telse:\n\t\t\t\tgroups_to_include.append(group)\n\t\texcept KeyError:\n\t\t\tself.logger.error(f\"Include group {group} does not exist.\")\n\n\tfor group in groups_to_include:\n\t\tinclude_path = os.path.join(spec_dir, f\"{group}.yaml\")\n\t\twith open(include_path, \"r\") as f:\n\t\t\tspec_str = f.read()\n\n\t\tspec_yaml = spec.loads(spec_str)\n\t\tself.logger.info(f\"Including spec group {group}\")\n\t\tparser_dict.update(spec_yaml)\n\n\tspec_yaml = self.resolve_includes(parser_dict)\n\tspec_yaml = self.resolve_variables", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get('include_groups')\n\tif not include_groups:\n\t\treturn\n\n\tself.include_groups = include_groups\n\tself.groups = {}\n\n\tfor group_name in include_groups:\n\t\tself.groups[group_name] = {}\n\n\t\t# Add all of the groups that are needed for this group\n\t\tgroup = self.get_group_spec(group_name)\n\t\tinclude_groups = group.get('include_groups')\n\t\tfor i_group in include_groups:\n\t\t\tself.include_groups.append(i_group)\n\t\t\tself.groups[group_name][i_group] = self.get_group_spec(i_group)\n\n\t\t# Add all of the variables that are needed for this group\n\t\tself.groups[group_name]['variables'] = {}\n\t\tvariables = group.get('variables')\n\t\tif variables is None:\n\t\t\tvariables = []\n\t\tfor var_name in variables:\n\t\t\tself.groups[group_name]['variables'][var_name] = self.get_variable_spec(var_name)\n\n\t\t# Add all of the commands that are needed for this group\n\t\tself.groups[group_name]['commands'] =", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroup_name = parser_dict['name']\n\tinclude_groups = parser_dict['include']\n\t\n\tspec = self._spec\n\tspec_dict = spec.get_dict()\n\t\n\tif group_name not in spec_dict:\n\t\tspec_dict[group_name] = {}\n\t\n\tfor include_group_name in include_groups:\n\t\tgroup = spec.get_group(include_group_name)\n\t\tif group is None:\n\t\t\traise self.error('Spec group \"{}\" does not exist.'.format(include_group_name))\n\t\t\n\t\tgroup_spec_dict = group.get_dict()\n\t\tspec_dict[group_name].update(group_spec_dict)\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroups = []\n\ttry:\n\t\tinclude = parser_dict['include']\n\texcept KeyError:\n\t\treturn groups\n\n\tif not isinstance(include, list):\n\t\tinclude = [include]\n\n\tfor group_name in include:\n\t\tgroup_name = group_name.rstrip('.yml')\n\t\tif group_name not in groups:\n\t\t\tgroups.append(group_name)\n\n\tfor group in groups:\n\t\tgroup_path = os.path.join(self.config_dir, group + '.yml')\n\t\twith open(group_path, 'r') as f:\n\t\t\tgroup_dict = yaml.safe_load(f)\n\t\t\tself._include_groups(group_dict)\n\t\t\tself._process_group(group_dict, group)\n\n\treturn groups\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroup_files = parser_dict.get('include', [])\n\tfor group_file in group_files:\n\t\tself._merge_parser_dict(group_file)", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdef _include(include_file):\n\t\tgroup_name = include_file.split('.')[0]\n\t\tgroup_dict = parser_dict.get(group_name, {})\n\t\tgroup_dict.update(self._load_spec(include_file))\n\t\treturn group_dict\n\tself._include_groups = [_include(x) for x in self.get_includes()]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroups = parser_dict.get('include')\n\tif not groups:\n\t\treturn\n\n\tdef _parse_include(group_name):\n\t\t\"\"\"\n\t\tFinds the included group and merges it into the\n\t\tcurrent group.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tgroup_dict = self._groups[group_name]\n\t\texcept KeyError:\n\t\t\traise ParserError('Group \"{}\" included in \"{}\" not found'.format(\n\t\t\t\tgroup_name, self._current_group_name\n\t\t\t))\n\n\t\t# Merge the include into the current group\n\t\tself._current_group_dict.update(group_dict)\n\n\tfor group_name in groups:\n\t\t_parse_include(group_name)\n\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\timport os\n\n\tinclude_dict = parser_dict['include']\n\n\tif not include_dict:\n\t\treturn\n\n\tinclude_groups = include_dict.get('groups')\n\tif include_groups:\n\t\tparser_dict['groups'] = self._include_groups_in_dict(parser_dict['groups'], include_groups)\n\n\tinclude_rules = include_dict.get('rules')\n\tif include_rules:\n\t\tparser_dict['rules'] = self._include_rules_in_dict(parser_dict['rules'], include_rules)\n\n\tinclude_macros = include_dict.get('macros')\n\tif include_macros:\n\t\tparser_dict['macros'] = self._include_macros_in_dict(parser_dict['macros'], include_macros)\n\n\tinclude_files = include_dict.get('files')\n\tif include_files:\n\t\tparser_dict['files'] = self._include_files_in_dict(parser_dict['files'], include_files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.include_groups = parser_dict.get('include_groups', [])\n\n\tgroups = []\n\tgroup_files = []\n\tfor group in self.include_groups:\n\t\tgroup_file = self.root_dir / 'specs' / (group + '.yml')\n\t\tgroup_files.append(group_file)\n\t\tif not group_file.is_file():\n\t\t\traise ValueError('Group file does not exist: {}'.format(group_file))\n\n\t\twith open(group_file, 'r') as f:\n\t\t\tgroup_dict = yaml.safe_load(f)\n\n\t\tgroups.append(group_dict)\n\n\tself._update_specs(groups)\n\tself.group_files = group_files", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tincludes = parser_dict['includes']\n\t\n\tfor include in includes:\n\t\tgroup = self._parse_file(include['file'], include['line'])\n\t\tself._include_groups([group])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfrom .. import spec\n\n\tspec_dir = self.spec_dir\n\tinclude_groups = parser_dict.get(\"include_groups\", [])\n\tinclude_groups = [parser_dict[\"include_groups\"]] if isinstance(include_groups, str) else include_groups\n\tgroups_to_include = []\n\n\tfor group in include_groups:\n\t\ttry:\n\t\t\tif isinstance(group, list):\n\t\t\t\tgroups_to_include.extend(group)\n\t\t\telse:\n\t\t\t\tgroups_to_include.append(group)\n\t\texcept KeyError:\n\t\t\tself.logger.error(f\"Include group {group} does not exist.\")\n\n\tfor group in groups_to_include:\n\t\tinclude_path = os.path.join(spec_dir, f\"{group}.yaml\")\n\t\twith open(include_path, \"r\") as f:\n\t\t\tspec_str = f.read()\n\n\t\tspec_yaml = spec.loads(spec_str)\n\t\tself.logger.info(f\"Including spec group {group}\")\n\t\tparser_dict.update(spec_yaml)\n\n\tspec_yaml = self.resolve_includes(parser_dict)\n\tspec_yaml = self.resolve_variables", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get('include_groups')\n\tif not include_groups:\n\t\treturn\n\n\tself.include_groups = include_groups\n\tself.groups = {}\n\n\tfor group_name in include_groups:\n\t\tself.groups[group_name] = {}\n\n\t\t# Add all of the groups that are needed for this group\n\t\tgroup = self.get_group_spec(group_name)\n\t\tinclude_groups = group.get('include_groups')\n\t\tfor i_group in include_groups:\n\t\t\tself.include_groups.append(i_group)\n\t\t\tself.groups[group_name][i_group] = self.get_group_spec(i_group)\n\n\t\t# Add all of the variables that are needed for this group\n\t\tself.groups[group_name]['variables'] = {}\n\t\tvariables = group.get('variables')\n\t\tif variables is None:\n\t\t\tvariables = []\n\t\tfor var_name in variables:\n\t\t\tself.groups[group_name]['variables'][var_name] = self.get_variable_spec(var_name)\n\n\t\t# Add all of the commands that are needed for this group\n\t\tself.groups[group_name]['commands'] =", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroup_name = parser_dict['name']\n\tinclude_groups = parser_dict['include']\n\t\n\tspec = self._spec\n\tspec_dict = spec.get_dict()\n\t\n\tif group_name not in spec_dict:\n\t\tspec_dict[group_name] = {}\n\t\n\tfor include_group_name in include_groups:\n\t\tgroup = spec.get_group(include_group_name)\n\t\tif group is None:\n\t\t\traise self.error('Spec group \"{}\" does not exist.'.format(include_group_name))\n\t\t\n\t\tgroup_spec_dict = group.get_dict()\n\t\tspec_dict[group_name].update(group_spec_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroups = []\n\ttry:\n\t\tinclude = parser_dict['include']\n\texcept KeyError:\n\t\treturn groups\n\n\tif not isinstance(include, list):\n\t\tinclude = [include]\n\n\tfor group_name in include:\n\t\tgroup_name = group_name.rstrip('.yml')\n\t\tif group_name not in groups:\n\t\t\tgroups.append(group_name)\n\n\tfor group in groups:\n\t\tgroup_path = os.path.join(self.config_dir, group + '.yml')\n\t\twith open(group_path, 'r') as f:\n\t\t\tgroup_dict = yaml.safe_load(f)\n\t\t\tself._include_groups(group_dict)\n\t\t\tself._process_group(group_dict, group)\n\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tgroup_files = parser_dict.get('include', [])\n\tfor group_file in group_files:\n\t\tself._merge_parser_dict(group_file)", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdef _include(include_file):\n\t\tgroup_name = include_file.split('.')[0]\n\t\tgroup_dict = parser_dict.get(group_name, {})\n\t\tgroup_dict.update(self._load_spec(include_file))\n\t\treturn group_dict\n\tself._include_groups = [_include(x) for x in self.get_includes()]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tself._get_defaults()\n\treturn self.defaults", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self._get_spec()\n\tdefaults = {}\n\tfor k, v in spec.items():\n\t\tdefaults[k] = self._get_defaults(k, v)\n\treturn defaults", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tself._spec.resolve_defaults(self)\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor arg in self.spec.args:\n\t\targ_name = arg.name\n\t\tif arg_name not in self.kwargs:\n\t\t\tif arg.default is not None:\n\t\t\t\targ_value = arg.default\n\t\t\telif arg.default_value is not None:\n\t\t\t\targ_value = self._get_default_value(arg.default_value)\n\t\t\telse:\n\t\t\t\traise ArgumentError(\n\t\t\t\t\targ_name,\n\t\t\t\t\t\"argument is not specified and has no default value\")\n\t\t\tdefaults[arg_name] = arg_value\n\t\telse:\n\t\t\targ_value = self.kwargs[arg_name]\n\t\t\tif arg.default is not None:\n\t\t\t\tif arg_value != arg.default:\n\t\t\t\t\traise ArgumentError(\n\t\t\t\t\t\targ_name,\n\t\t\t\t\t\t\"value does not match argument's default value\")\n\t\t\telif arg.default_value is not None:\n\t\t\t\targ_value = self._get_default_value(arg.default_value)\n\t\t\tdefaults[arg_name] = arg_value\n\tif self.spec.return_arg:\n\t\treturn_arg_name = self.spec.return_arg.name\n\t\tif return_arg_name not in self.kwargs:\n\t\t\tif self.spec.", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tdefaults = {}\n\tif spec:\n\t\tdefaults.update(self._get_defaults(spec))\n\tdefaults.update(self._get_defaults(self.args))\n\tdefaults.update(self._get_defaults(self.kwargs))\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self._get_spec()\n\tdefaults = {}\n\tfor name, arg in spec.items():\n\t\tdefaults[name] = self._get_defaults(arg)\n\tif self.defaults is not None:\n\t\tdefaults.update(self.defaults)\n\tself.defaults = defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor spec in self.specs:\n\t\tdefaults.update(self._get_defaults(spec))\n\tfor source in self.sources:\n\t\tdefaults.update(self._get_defaults(source))\n\treturn defaults\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tself.defaults = self.get_spec_defaults()\n\tif self.defaults is None:\n\t\traise Exception(\"get_spec_defaults() returned None\")\n\tself._get_defaults()\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tdefaults = {}\n\tfor name in spec.keys():\n\t\tdefaults[name] = self.spec[name]\n\t\tif name in self.defaults:\n\t\t\tdefaults[name] = self.defaults[name]\n\t\telse:\n\t\t\ttry:\n\t\t\t\tdefaults[name] = spec[name][\"default\"]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\n\tdefaults[\"_name\"] = self.name\n\tdefaults[\"_task\"] = self.task\n\t\n\treturn defaults\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor arg in self.args:\n\t\tdefaults[arg.name] = self._get_arg_default(arg)\n\treturn defaults\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tself._get_defaults()\n\treturn self.defaults", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self._get_spec()\n\tdefaults = {}\n\tfor k, v in spec.items():\n\t\tdefaults[k] = self._get_defaults(k, v)\n\treturn defaults", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tself._spec.resolve_defaults(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor arg in self.spec.args:\n\t\targ_name = arg.name\n\t\tif arg_name not in self.kwargs:\n\t\t\tif arg.default is not None:\n\t\t\t\targ_value = arg.default\n\t\t\telif arg.default_value is not None:\n\t\t\t\targ_value = self._get_default_value(arg.default_value)\n\t\t\telse:\n\t\t\t\traise ArgumentError(\n\t\t\t\t\targ_name,\n\t\t\t\t\t\"argument is not specified and has no default value\")\n\t\t\tdefaults[arg_name] = arg_value\n\t\telse:\n\t\t\targ_value = self.kwargs[arg_name]\n\t\t\tif arg.default is not None:\n\t\t\t\tif arg_value != arg.default:\n\t\t\t\t\traise ArgumentError(\n\t\t\t\t\t\targ_name,\n\t\t\t\t\t\t\"value does not match argument's default value\")\n\t\t\telif arg.default_value is not None:\n\t\t\t\targ_value = self._get_default_value(arg.default_value)\n\t\t\tdefaults[arg_name] = arg_value\n\tif self.spec.return_arg:\n\t\treturn_arg_name = self.spec.return_arg.name\n\t\tif return_arg_name not in self.kwargs:\n\t\t\tif self.spec.", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tdefaults = {}\n\tif spec:\n\t\tdefaults.update(self._get_defaults(spec))\n\tdefaults.update(self._get_defaults(self.args))\n\tdefaults.update(self._get_defaults(self.kwargs))\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self._get_spec()\n\tdefaults = {}\n\tfor name, arg in spec.items():\n\t\tdefaults[name] = self._get_defaults(arg)\n\tif self.defaults is not None:\n\t\tdefaults.update(self.defaults)\n\tself.defaults = defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor spec in self.specs:\n\t\tdefaults.update(self._get_defaults(spec))\n\tfor source in self.sources:\n\t\tdefaults.update(self._get_defaults(source))\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tself.defaults = self.get_spec_defaults()\n\tif self.defaults is None:\n\t\traise Exception(\"get_spec_defaults() returned None\")\n\tself._get_defaults()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tdefaults = {}\n\tfor name in spec.keys():\n\t\tdefaults[name] = self.spec[name]\n\t\tif name in self.defaults:\n\t\t\tdefaults[name] = self.defaults[name]\n\t\telse:\n\t\t\ttry:\n\t\t\t\tdefaults[name] = spec[name][\"default\"]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\n\tdefaults[\"_name\"] = self.name\n\tdefaults[\"_task\"] = self.task\n\t\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor arg in self.args:\n\t\tdefaults[arg.name] = self._get_arg_default(arg)\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor k, v in self.deprecated_options.items():\n\t\tif k in self.spec_helper.option_spec:\n\t\t\tdeprecated_args[k] = v\n\treturn deprecated_args", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated = {}\n\toptions = self.spec_helper.iterate_option_specs()\n\tfor option in options:\n\t\tif option.deprecated:\n\t\t\tdeprecated[option.name] = option.deprecated\n\treturn deprecated\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tself.spec_helper.iterate_option_specs()\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tself.spec_helper.iterate_option_specs()\n\t\tdeprecated_args = {}\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['deprecated']:\n\t\t\t\tdeprecated_args[option['name']] = option['deprecated']\n\n\t\treturn deprecated_args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tself.spec_helper.iterate_option_specs()\n\t\tdeprecated_args = {}\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['deprecated']:\n\t\t\t\tdeprecated_args[option['name']] = option['deprecated']\n\n\t\treturn deprecated_args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_options = {}\n\t\n\tfor option, option_spec in self.spec_helper.iterate_option_specs():\n\t\tif \"deprecated\" in option_spec:\n\t\t\tdeprecated_options[option] = option_spec[\"deprecated\"]\n\t\n\treturn deprecated_options", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\n\tfor option_name, option_value in self.spec_helper.iterate_option_specs():\n\t\tif option_value.get('deprecated'):\n\t\t\tdeprecated_args[option_name] = option_value\n\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option.deprecated:\n\t\t\tdeprecated_args[option.name] = option", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option_name, option in self.spec_helper.iterate_option_specs():\n\t\tif option.deprecated_options:\n\t\t\tfor deprecated_option_name in option.deprecated_options:\n\t\t\t\tdeprecated_args[deprecated_option_name] = option_name\n\treturn deprecated_args\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif spec.get(\"deprecated_for\"):\n\t\t\tdeprecated_args[spec.get(\"name\")] = spec.get(\"deprecated_for\")\n\treturn deprecated_args", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated = {}\n\tfor option, value in self.spec_helper.iterate_option_specs():\n\t\tif isinstance(value, dict) and 'deprecated' in value:\n\t\t\tdeprecated[option] = value['deprecated']\n\treturn deprecated", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option_name, option_info in self.spec_helper.iterate_option_specs():\n\t\tif option_info.get(\"deprecated\"):\n\t\t\tdeprecated_args[option_name] = option_info[\"deprecated\"]\n\treturn deprecated_args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor k, v in self.deprecated_options.items():\n\t\tif k in self.spec_helper.option_spec:\n\t\t\tdeprecated_args[k] = v\n\treturn deprecated_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated = {}\n\toptions = self.spec_helper.iterate_option_specs()\n\tfor option in options:\n\t\tif option.deprecated:\n\t\t\tdeprecated[option.name] = option.deprecated\n\treturn deprecated\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tself.spec_helper.iterate_option_specs()\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tself.spec_helper.iterate_option_specs()\n\t\tdeprecated_args = {}\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['deprecated']:\n\t\t\t\tdeprecated_args[option['name']] = option['deprecated']\n\n\t\treturn deprecated_args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tself.spec_helper.iterate_option_specs()\n\t\tdeprecated_args = {}\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['deprecated']:\n\t\t\t\tdeprecated_args[option['name']] = option['deprecated']\n\n\t\treturn deprecated_args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_options = {}\n\t\n\tfor option, option_spec in self.spec_helper.iterate_option_specs():\n\t\tif \"deprecated\" in option_spec:\n\t\t\tdeprecated_options[option] = option_spec[\"deprecated\"]\n\t\n\treturn deprecated_options", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\n\tfor option_name, option_value in self.spec_helper.iterate_option_specs():\n\t\tif option_value.get('deprecated'):\n\t\t\tdeprecated_args[option_name] = option_value\n\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option.deprecated:\n\t\t\tdeprecated_args[option.name] = option", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option_name, option in self.spec_helper.iterate_option_specs():\n\t\tif option.deprecated_options:\n\t\t\tfor deprecated_option_name in option.deprecated_options:\n\t\t\t\tdeprecated_args[deprecated_option_name] = option_name\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif spec.get(\"deprecated_for\"):\n\t\t\tdeprecated_args[spec.get(\"name\")] = spec.get(\"deprecated_for\")\n\treturn deprecated_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated = {}\n\tfor option, value in self.spec_helper.iterate_option_specs():\n\t\tif isinstance(value, dict) and 'deprecated' in value:\n\t\t\tdeprecated[option] = value['deprecated']\n\treturn deprecated", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option_name, option_info in self.spec_helper.iterate_option_specs():\n\t\tif option_info.get(\"deprecated\"):\n\t\t\tdeprecated_args[option_name] = option_info[\"deprecated\"]\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t'catalog_path': 'catalog_dir',\n\t\t'catalog_dir': 'catalog_dir',\n\t\t'config_dir': 'config_path',\n\t\t'config_path': 'config_path',\n\t\t'config_dir': 'config_path',\n\t\t'config_file': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path':", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t'image_id': 'image',\n\t\t'image_name': 'image',\n\t\t'image_tag': 'image',\n\t\t'image_type': 'image',\n\t\t'image_version': 'image',\n\t\t'region': 'cluster_region',\n\t\t'region_id': 'cluster_region',\n\t\t'region_name': 'cluster_region',\n\t\t'cluster_id': 'cluster_name',\n\t\t'cluster_name': 'cluster_name',\n\t\t'cluster_url': 'cluster_name',\n\t\t'namespace': 'namespace',\n\t\t'namespace_id': 'namespace',\n\t\t'namespace_name': 'namespace',\n\t\t'project_id': 'project_name',\n\t\t'project_name': 'project_name',\n\t\t'project_url': 'project_name',\n\t\t'project_private_key': 'private_key_file',\n\t\t'project_ssh_key': 'private_key_file',\n\t\t'ssh_key': 'private_key_file',\n\t\t'private_key': 'private_key_file',\n\t\t'private_key_file': 'private_key_file',\n\t\t'private_key_id': 'private_key_file',\n\t\t'private_key_name': 'private_key_file',\n\t\t'", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = deprecated_args_list()\n\tfor arg in cli_args:\n\t\tif arg in deprecated_args:\n\t\t\tprint(f\"WARNING: -{arg} is no longer supported. Please use -{deprecated_args[arg]} instead.\")\n\tfor arg in answer_file_args:\n\t\tif arg in deprecated_args:\n\t\t\tprint(f\"WARNING: {arg} is no longer supported. Please use {deprecated_args[arg]} instead.\")\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tself.deprecated_arguments = []\n\tself.deprecated_arguments_answers = []\n\tfor arg in self.all_deprecated_arguments:\n\t\tif arg in cli_args or arg in answer_file_args:\n\t\t\tself.deprecated_arguments.append(arg)\n\t\t\tif arg in cli_args:\n\t\t\t\tself.deprecated_arguments_answers.append(cli_args[arg])\n\t\t\telse:\n\t\t\t\tself.deprecated_arguments_answers.append(answer_file_args[arg])\n\n\tif len(self.deprecated_arguments) > 0:\n\t\tlogger.warning(f\"\\nThe following arguments are deprecated and will be removed in the next minor release:\")\n\t\tfor arg in self.deprecated_arguments:\n\t\t\tlogger.warning(\"  - \" + arg)\n\t\tlogger.warning(\"Please update your answer file with the new argument name and re-run the installation script.\")\n\n\t\tif len(self.deprecated_arguments_answers) > 0:\n\t\t\tlogger.warning(\"The following arguments have been used from your answer file:\")\n\t\t\tfor arg in self.deprecated_arguments_answers:\n\t\t\t\tlogger.warning(\"  - \" + arg)\n\t\t\tlogger.warning(\"Please update your answer file with the new argument name and re-run the", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = list(self.deprecated_args.keys())\n\tdeprecated_args.sort()\n\n\tdeprecated_args_cli = list(\n\t\tfilter(lambda arg: arg in cli_args, deprecated_args))\n\tdeprecated_args_files = list(\n\t\tfilter(lambda arg: arg in answer_file_args, deprecated_args))\n\n\tif len(deprecated_args_cli) > 0 or len(deprecated_args_files) > 0:\n\t\tprint('\\nDeprecated arguments:')\n\t\tif len(deprecated_args_cli) > 0:\n\t\t\tprint('\\tCLI:')\n\t\t\tfor arg in deprecated_args_cli:\n\t\t\t\tprint('\\t\\t{}={}'.format(arg, cli_args[arg]))\n\n\t\tif len(deprecated_args_files) > 0:\n\t\t\tprint('\\tAnswer file:')\n\t\t\tfor arg in deprecated_args_files:\n\t\t\t\tprint('\\t\\t{}={}'.format(arg, answer_file_args[arg]))\n\n\t\tprint('\\nThe deprecated arguments are no longer supported.')\n\t\tprint('Please refer to the documentation for the latest arguments.')\n\t\tprint('')\n\t\tsys.exit(1)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint_deprecated_args(cli_args, answer_file_args)\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = get_deprecated_args(self.config_path)\n\n\tdeprecated_args.update(get_deprecated_args(self.user_config_path))\n\n\tif answer_file_args is not None:\n\t\tdeprecated_args.update(get_deprecated_args(answer_file_args))\n\n\tfor arg in deprecated_args:\n\t\tif arg in cli_args:\n\t\t\tself.logger.warning(f\"deprecated argument {arg} is ignored\")\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = self.get_deprecation_dict()\n\tdeprecated_args_keys = list(deprecated_args.keys())\n\tfor arg in deprecated_args_keys:\n\t\tif (arg in cli_args or arg in answer_file_args):\n\t\t\tprint(f\"Argument {arg} is deprecated.\")\n\t\t\tif (deprecated_args[arg] == \"none\"):\n\t\t\t\tprint(f\"Argument {arg} is no longer supported.\")\n\t\t\telse:\n\t\t\t\tprint(f\"Argument {arg} is no longer supported. Use {deprecated_args[arg]} instead.\")\n\t\t\tprint()\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t'use_cgroup_driver': 'cgroup-driver',\n\t\t'storage_driver': 'storage-driver',\n\t\t'selinux_enabled': 'selinux-enabled',\n\t\t'selinux_mode': 'selinux-mode',\n\t\t'selinux_type': 'selinux-type',\n\t\t'ip_masq': 'ip-masq',\n\t\t'iptables': 'iptables',\n\t\t'ip_forward': 'ip-forward',\n\t\t'bridge': 'bridge',\n\t\t'fixed_cidr': 'fixed-cidr',\n\t\t'fixed_cidr_v6': 'fixed-cidr-v6',\n\t\t'log_driver': 'log-driver',\n\t\t'log_opts': 'log-opts',\n\t\t'insecure_registry': 'insecure-registry',\n\t\t'registry_mirror': 'registry-mirror',\n\t\t'exec_driver': 'exec-driver',\n\t\t'exec_opts': 'exec-opts',\n\t\t'userns_remap': 'userns-remap',\n\t\t'dns': 'dns',\n\t\t'dns_opt': 'dns-opt',\n\t\t'dns_search': 'dns-search',\n\t\t'dns_opt': 'dns", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = []\n\tself.logger.info(\"Validating deprecated arguments\")\n\tfor arg in cli_args:\n\t\tif arg in DEPRECATED_ARGS:\n\t\t\tdeprecated_args.append(arg)\n\n\tfor arg in answer_file_args:\n\t\tif arg in DEPRECATED_ARGS:\n\t\t\tdeprecated_args.append(arg)\n\n\tself.logger.info(\"Deprecated arguments found\")\n\tif len(deprecated_args) > 0:\n\t\traise ValueError(\n\t\t\t\"Deprecated arguments {0} found in your arguments. Please remove them from the input file. \"\n\t\t\t\"You can find more details about deprecated arguments in the documentation\".format(deprecated_args))\n\n\nDEPRECATED_ARGS = [\"cluster_type\", \"vpc_id\", \"vpc_name\", \"vpc_subnet_id\", \"vpc_subnet_name\",\n                   \"vpc_security_group_ids\", \"vpc_security_group_names\", \"vpc_security_group_id\", \"vpc_security_group_name\",\n                   \"vpc_security_group_id_1\", \"vpc_security_group_id_2\", \"vpc_security_group_id_3\", \"vpc_security_group_name_"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t'catalog_path': 'catalog_dir',\n\t\t'catalog_dir': 'catalog_dir',\n\t\t'config_dir': 'config_path',\n\t\t'config_path': 'config_path',\n\t\t'config_dir': 'config_path',\n\t\t'config_file': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path': 'config_path',\n\t\t'config_file_path':", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t'image_id': 'image',\n\t\t'image_name': 'image',\n\t\t'image_tag': 'image',\n\t\t'image_type': 'image',\n\t\t'image_version': 'image',\n\t\t'region': 'cluster_region',\n\t\t'region_id': 'cluster_region',\n\t\t'region_name': 'cluster_region',\n\t\t'cluster_id': 'cluster_name',\n\t\t'cluster_name': 'cluster_name',\n\t\t'cluster_url': 'cluster_name',\n\t\t'namespace': 'namespace',\n\t\t'namespace_id': 'namespace',\n\t\t'namespace_name': 'namespace',\n\t\t'project_id': 'project_name',\n\t\t'project_name': 'project_name',\n\t\t'project_url': 'project_name',\n\t\t'project_private_key': 'private_key_file',\n\t\t'project_ssh_key': 'private_key_file',\n\t\t'ssh_key': 'private_key_file',\n\t\t'private_key': 'private_key_file',\n\t\t'private_key_file': 'private_key_file',\n\t\t'private_key_id': 'private_key_file',\n\t\t'private_key_name': 'private_key_file',\n\t\t'", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = deprecated_args_list()\n\tfor arg in cli_args:\n\t\tif arg in deprecated_args:\n\t\t\tprint(f\"WARNING: -{arg} is no longer supported. Please use -{deprecated_args[arg]} instead.\")\n\tfor arg in answer_file_args:\n\t\tif arg in deprecated_args:\n\t\t\tprint(f\"WARNING: {arg} is no longer supported. Please use {deprecated_args[arg]} instead.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tself.deprecated_arguments = []\n\tself.deprecated_arguments_answers = []\n\tfor arg in self.all_deprecated_arguments:\n\t\tif arg in cli_args or arg in answer_file_args:\n\t\t\tself.deprecated_arguments.append(arg)\n\t\t\tif arg in cli_args:\n\t\t\t\tself.deprecated_arguments_answers.append(cli_args[arg])\n\t\t\telse:\n\t\t\t\tself.deprecated_arguments_answers.append(answer_file_args[arg])\n\n\tif len(self.deprecated_arguments) > 0:\n\t\tlogger.warning(f\"\\nThe following arguments are deprecated and will be removed in the next minor release:\")\n\t\tfor arg in self.deprecated_arguments:\n\t\t\tlogger.warning(\"  - \" + arg)\n\t\tlogger.warning(\"Please update your answer file with the new argument name and re-run the installation script.\")\n\n\t\tif len(self.deprecated_arguments_answers) > 0:\n\t\t\tlogger.warning(\"The following arguments have been used from your answer file:\")\n\t\t\tfor arg in self.deprecated_arguments_answers:\n\t\t\t\tlogger.warning(\"  - \" + arg)\n\t\t\tlogger.warning(\"Please update your answer file with the new argument name and re-run the", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = list(self.deprecated_args.keys())\n\tdeprecated_args.sort()\n\n\tdeprecated_args_cli = list(\n\t\tfilter(lambda arg: arg in cli_args, deprecated_args))\n\tdeprecated_args_files = list(\n\t\tfilter(lambda arg: arg in answer_file_args, deprecated_args))\n\n\tif len(deprecated_args_cli) > 0 or len(deprecated_args_files) > 0:\n\t\tprint('\\nDeprecated arguments:')\n\t\tif len(deprecated_args_cli) > 0:\n\t\t\tprint('\\tCLI:')\n\t\t\tfor arg in deprecated_args_cli:\n\t\t\t\tprint('\\t\\t{}={}'.format(arg, cli_args[arg]))\n\n\t\tif len(deprecated_args_files) > 0:\n\t\t\tprint('\\tAnswer file:')\n\t\t\tfor arg in deprecated_args_files:\n\t\t\t\tprint('\\t\\t{}={}'.format(arg, answer_file_args[arg]))\n\n\t\tprint('\\nThe deprecated arguments are no longer supported.')\n\t\tprint('Please refer to the documentation for the latest arguments.')\n\t\tprint('')\n\t\tsys.exit(1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint_deprecated_args(cli_args, answer_file_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = get_deprecated_args(self.config_path)\n\n\tdeprecated_args.update(get_deprecated_args(self.user_config_path))\n\n\tif answer_file_args is not None:\n\t\tdeprecated_args.update(get_deprecated_args(answer_file_args))\n\n\tfor arg in deprecated_args:\n\t\tif arg in cli_args:\n\t\t\tself.logger.warning(f\"deprecated argument {arg} is ignored\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = self.get_deprecation_dict()\n\tdeprecated_args_keys = list(deprecated_args.keys())\n\tfor arg in deprecated_args_keys:\n\t\tif (arg in cli_args or arg in answer_file_args):\n\t\t\tprint(f\"Argument {arg} is deprecated.\")\n\t\t\tif (deprecated_args[arg] == \"none\"):\n\t\t\t\tprint(f\"Argument {arg} is no longer supported.\")\n\t\t\telse:\n\t\t\t\tprint(f\"Argument {arg} is no longer supported. Use {deprecated_args[arg]} instead.\")\n\t\t\tprint()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t'use_cgroup_driver': 'cgroup-driver',\n\t\t'storage_driver': 'storage-driver',\n\t\t'selinux_enabled': 'selinux-enabled',\n\t\t'selinux_mode': 'selinux-mode',\n\t\t'selinux_type': 'selinux-type',\n\t\t'ip_masq': 'ip-masq',\n\t\t'iptables': 'iptables',\n\t\t'ip_forward': 'ip-forward',\n\t\t'bridge': 'bridge',\n\t\t'fixed_cidr': 'fixed-cidr',\n\t\t'fixed_cidr_v6': 'fixed-cidr-v6',\n\t\t'log_driver': 'log-driver',\n\t\t'log_opts': 'log-opts',\n\t\t'insecure_registry': 'insecure-registry',\n\t\t'registry_mirror': 'registry-mirror',\n\t\t'exec_driver': 'exec-driver',\n\t\t'exec_opts': 'exec-opts',\n\t\t'userns_remap': 'userns-remap',\n\t\t'dns': 'dns',\n\t\t'dns_opt': 'dns-opt',\n\t\t'dns_search': 'dns-search',\n\t\t'dns_opt': 'dns", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = []\n\tself.logger.info(\"Validating deprecated arguments\")\n\tfor arg in cli_args:\n\t\tif arg in DEPRECATED_ARGS:\n\t\t\tdeprecated_args.append(arg)\n\n\tfor arg in answer_file_args:\n\t\tif arg in DEPRECATED_ARGS:\n\t\t\tdeprecated_args.append(arg)\n\n\tself.logger.info(\"Deprecated arguments found\")\n\tif len(deprecated_args) > 0:\n\t\traise ValueError(\n\t\t\t\"Deprecated arguments {0} found in your arguments. Please remove them from the input file. \"\n\t\t\t\"You can find more details about deprecated arguments in the documentation\".format(deprecated_args))\n\n\nDEPRECATED_ARGS = [\"cluster_type\", \"vpc_id\", \"vpc_name\", \"vpc_subnet_id\", \"vpc_subnet_name\",\n                   \"vpc_security_group_ids\", \"vpc_security_group_names\", \"vpc_security_group_id\", \"vpc_security_group_name\",\n                   \"vpc_security_group_id_1\", \"vpc_security_group_id_2\", \"vpc_security_group_id_3\", \"vpc_security_group_name_", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tself.log.info(\"Getting parser option specs for %s\" % command_name)\n\tparser = self.get_parser(command_name)\n\toptions = list()\n\tif parser:\n\t\toptions = list(parser._actions)\n\treturn options", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\traise NotImplementedError()", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tfrom .get_parser_option_specs_impl import get_parser_option_specs_impl\n\treturn get_parser_option_specs_impl(self, command_name)", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tself.logger.debug(\"get_parser_option_specs(%s)\", command_name)\n\tself.get_command_parser(command_name)\n\treturn self.command_parsers[command_name].option_list\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tself.log.info(\"Getting parser option specs for %s\" % command_name)\n\tparser = self.get_parser(command_name)\n\toptions = list()\n\tif parser:\n\t\toptions = list(parser._actions)\n\treturn options", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tfrom .get_parser_option_specs_impl import get_parser_option_specs_impl\n\treturn get_parser_option_specs_impl(self, command_name)", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tself.logger.debug(\"get_parser_option_specs(%s)\", command_name)\n\tself.get_command_parser(command_name)\n\treturn self.command_parsers[command_name].option_list\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tparser_option_specs = self.get_parser_option_specs()\n\tif not command_name in parser_option_specs:\n\t\treturn {}\n\tparser_option_spec = parser_option_specs[command_name]\n\tif not argument_name in parser_option_spec:\n\t\treturn {}\n\toption_spec = parser_option_spec[argument_name]\n\treturn option_spec", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tself.get_parser_option_specs(command_name)\n\t\n\toption_spec = self.specs.get(argument_name)\n\treturn option_spec", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspec = None\n\tif command_name in self.command_specs:\n\t\tspec = self.command_specs[command_name].get(argument_name, None)\n\treturn spec\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tparser_option_specs = self.get_parser_option_specs()\n\tif not command_name in parser_option_specs:\n\t\treturn {}\n\tparser_option_spec = parser_option_specs[command_name]\n\tif not argument_name in parser_option_spec:\n\t\treturn {}\n\toption_spec = parser_option_spec[argument_name]\n\treturn option_spec", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tself.get_parser_option_specs(command_name)\n\t\n\toption_spec = self.specs.get(argument_name)\n\treturn option_spec", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspec = None\n\tif command_name in self.command_specs:\n\t\tspec = self.command_specs[command_name].get(argument_name, None)\n\treturn spec\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-'):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\ttry:\n\t\t\tif arg.startswith(\"-\"):\n\t\t\t\targ = arg[1:]\n\t\texcept:\n\t\t\tpass\n\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = {}\n\tfor arg in args.split(\",\"):\n\t\tif arg.startswith(\"--\"):\n\t\t\tsilent_args[arg] = \"\"\n\t\telse:\n\t\t\tsilent_args[\"--\" + arg] = \"\"\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-'):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\n\tfor arg in args:\n\t\targ = arg.strip()\n\t\tif len(arg) == 0:\n\t\t\tcontinue\n\t\tif arg[0] == '-':\n\t\t\tsilent_args.append(arg)\n\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\n\tfor arg in args:\n\t\tif (arg.startswith(\"-\") or arg.startswith(\"--\")) and arg[1] not in self.options:\n\t\t\tsilent.append(arg)\n\n\treturn silent\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor i, arg in enumerate(args):\n\t\tif self.arg_type(arg) == \"silent\":\n\t\t\tsilent_args.append(i)\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tif args.silent:\n\t\tsilent_args.append(\"silent\")\n\treturn silent_args", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tif args.silent:\n\t\tif args.silent == 'all':\n\t\t\tsilent = ['info', 'error', 'debug']\n\t\telse:\n\t\t\tsilent = args.silent.split(',')\n\n\treturn silent", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-') and arg.count('=') == 0:\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-'):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\ttry:\n\t\t\tif arg.startswith(\"-\"):\n\t\t\t\targ = arg[1:]\n\t\texcept:\n\t\t\tpass\n\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = {}\n\tfor arg in args.split(\",\"):\n\t\tif arg.startswith(\"--\"):\n\t\t\tsilent_args[arg] = \"\"\n\t\telse:\n\t\t\tsilent_args[\"--\" + arg] = \"\"\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-'):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\n\tfor arg in args:\n\t\targ = arg.strip()\n\t\tif len(arg) == 0:\n\t\t\tcontinue\n\t\tif arg[0] == '-':\n\t\t\tsilent_args.append(arg)\n\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\n\tfor arg in args:\n\t\tif (arg.startswith(\"-\") or arg.startswith(\"--\")) and arg[1] not in self.options:\n\t\t\tsilent.append(arg)\n\n\treturn silent\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor i, arg in enumerate(args):\n\t\tif self.arg_type(arg) == \"silent\":\n\t\t\tsilent_args.append(i)\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tif args.silent:\n\t\tsilent_args.append(\"silent\")\n\treturn silent_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tif args.silent:\n\t\tif args.silent == 'all':\n\t\t\tsilent = ['info', 'error', 'debug']\n\t\telse:\n\t\t\tsilent = args.silent.split(',')\n\n\treturn silent", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-') and arg.count('=') == 0:\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_keys = self.get_required_args()\n\tfor key in required_keys:\n\t\tif key not in args:\n\t\t\traise ValueError('Missing required argument: {}'.format(key))\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = [\"username\", \"password\", \"title\", \"content\", \"category\"]\n\tfor arg in required_args:\n\t\tif arg not in args:\n\t\t\traise Exception(f\"Missing required argument: {arg}\")\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = set(self.required_args)\n\tprovided = set(args.keys())\n\tif required.issubset(provided):\n\t\treturn", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing = []\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tmissing.append(arg)\n\tif missing:\n\t\tmsg = \"Missing required arguments: %s\" % \", \".join(missing)\n\t\traise Exception(msg)\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing_args = []\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tmissing_args.append(arg)\n\tif missing_args:\n\t\traise Exception(\"Missing required arguments: {}\".format(missing_args))\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_to_check = ['infile', 'outfile', 'idtype', 'idpos']\n\n\tfor arg in args_to_check:\n\t\tif arg not in args:\n\t\t\traise Exception(\"Argument {} not found.\".format(arg))\n\n\ttry:\n\t\tself.infile = open(args['infile'], 'r')\n\texcept IOError:\n\t\traise Exception(\"File {} not found.\".format(args['infile']))\n\n\tif args['outfile'] == '-':\n\t\tself.outfile = sys.stdout\n\telse:\n\t\tself.outfile = open(args['outfile'], 'w')", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tnargs = len(args)\n\tif nargs < self.min_args:\n\t\traise ValueError(\"Not enough arguments provided.\")\n\tif nargs > self.max_args:\n\t\traise ValueError(\"Too many arguments provided.\")\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = [\n\t\t'name',\n\t\t'version',\n\t\t'description',\n\t\t'author',\n\t\t'license'\n\t]\n\tfor arg in required_args:\n\t\tif not getattr(args, arg):\n\t\t\tprint('Error: Argument --{} is required'.format(arg))\n\t\t\texit(1)", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_list = [\n\t\t'username',\n\t\t'password',\n\t\t'path'\n\t]\n\n\tfor arg in args_list:\n\t\tif not args.get(arg):\n\t\t\tself.log('error', 'Missing parameter: %s' % arg)\n\t\t\tsys.exit(1)\n\n\treturn True", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing_args = []\n\tfor arg in args:\n\t\tif not getattr(self, arg, False):\n\t\t\tmissing_args.append(arg)\n\tif missing_args:\n\t\traise ValueError(\n\t\t\t\"Missing required arguments: %s\" % \", \".join(missing_args)\n\t\t)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_keys = self.get_required_args()\n\tfor key in required_keys:\n\t\tif key not in args:\n\t\t\traise ValueError('Missing required argument: {}'.format(key))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = [\"username\", \"password\", \"title\", \"content\", \"category\"]\n\tfor arg in required_args:\n\t\tif arg not in args:\n\t\t\traise Exception(f\"Missing required argument: {arg}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = set(self.required_args)\n\tprovided = set(args.keys())\n\tif required.issubset(provided):\n\t\treturn", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing = []\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tmissing.append(arg)\n\tif missing:\n\t\tmsg = \"Missing required arguments: %s\" % \", \".join(missing)\n\t\traise Exception(msg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing_args = []\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tmissing_args.append(arg)\n\tif missing_args:\n\t\traise Exception(\"Missing required arguments: {}\".format(missing_args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_to_check = ['infile', 'outfile', 'idtype', 'idpos']\n\n\tfor arg in args_to_check:\n\t\tif arg not in args:\n\t\t\traise Exception(\"Argument {} not found.\".format(arg))\n\n\ttry:\n\t\tself.infile = open(args['infile'], 'r')\n\texcept IOError:\n\t\traise Exception(\"File {} not found.\".format(args['infile']))\n\n\tif args['outfile'] == '-':\n\t\tself.outfile = sys.stdout\n\telse:\n\t\tself.outfile = open(args['outfile'], 'w')", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tnargs = len(args)\n\tif nargs < self.min_args:\n\t\traise ValueError(\"Not enough arguments provided.\")\n\tif nargs > self.max_args:\n\t\traise ValueError(\"Too many arguments provided.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = [\n\t\t'name',\n\t\t'version',\n\t\t'description',\n\t\t'author',\n\t\t'license'\n\t]\n\tfor arg in required_args:\n\t\tif not getattr(args, arg):\n\t\t\tprint('Error: Argument --{} is required'.format(arg))\n\t\t\texit(1)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_list = [\n\t\t'username',\n\t\t'password',\n\t\t'path'\n\t]\n\n\tfor arg in args_list:\n\t\tif not args.get(arg):\n\t\t\tself.log('error', 'Missing parameter: %s' % arg)\n\t\t\tsys.exit(1)\n\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing_args = []\n\tfor arg in args:\n\t\tif not getattr(self, arg, False):\n\t\t\tmissing_args.append(arg)\n\tif missing_args:\n\t\traise ValueError(\n\t\t\t\"Missing required arguments: %s\" % \", \".join(missing_args)\n\t\t)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tmatched_args = []\n\tfor arg_name, arg in options_spec.items():\n\t\tif arg.get('required_when', {}).get(command_name):\n\t\t\tmatched_args.append(arg_name)\n\treturn matched_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg in options_spec:\n\t\tif arg.get('required_when', {}).get(command_name, False):\n\t\t\trequired_args.append(arg.get('name', ''))\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor arg_name in args:\n\t\tif arg_name in options_spec:\n\t\t\targ_spec = options_spec[arg_name]\n\t\t\tif 'required_when' in arg_spec:\n\t\t\t\tcondition = arg_spec['required_when']\n\t\t\t\tif callable(condition):\n\t\t\t\t\tcondition = condition(args, command_name)\n\t\t\t\tif condition:\n\t\t\t\t\tresult.append(arg_name)\n\treturn result\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tmatched_args = []\n\tfor arg in args:\n\t\tfor option in options_spec:\n\t\t\tif option.get(\"name\") == arg:\n\t\t\t\tif option.get(\"required_when\"):\n\t\t\t\t\tmatched_args.append(arg)\n\treturn matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_match_args = []\n\tfor arg in options_spec:\n\t\trequired_when = arg.get('required_when')\n\t\tif not required_when:\n\t\t\tcontinue\n\t\trequired_when_match = True\n\t\trequired_when_args = required_when.split(',')\n\t\tfor required_when_arg in required_when_args:\n\t\t\twhen_arg = required_when_arg.split(':')\n\t\t\twhen_arg_name = when_arg[0].strip()\n\t\t\twhen_arg_value = when_arg[1].strip()\n\t\t\twhen_arg_value = when_arg_value.replace('\"', '')\n\t\t\twhen_arg_value = when_arg_value.replace(\"'\", '')\n\t\t\tif when_arg_name not in args:\n\t\t\t\trequired_when_match = False\n\t\t\t\tbreak\n\t\t\targ_value = args[when_arg_name]\n\t\t\tif when_arg_value != arg_value:\n\t\t\t\trequired_when_match = False\n\t\t\t\tbreak\n\t\tif required_when_match:\n\t\t\trequired_when_match_args.append(arg['name'])\n\n\treturn required_when_match_args\n\n\nif __name__ == \"__main__\":\n\tprint(_get", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tconditionally_required_args = []\n\tfor arg in options_spec:\n\t\targ_name = arg['name']\n\t\targ_required_when = arg.get('required_when')\n\t\tif arg_required_when:\n\t\t\targ_required_when_value = self.config.get(arg_required_when, None)\n\t\t\tif arg_required_when_value:\n\t\t\t\targ_required_when_value = arg_required_when_value.split(',')\n\t\t\t\tif arg_name in args:\n\t\t\t\t\tcontinue\n\t\t\t\tif arg_name in arg_required_when_value:\n\t\t\t\t\tconditionally_required_args.append(arg_name)\n\treturn conditionally_required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor opt in options_spec:\n\t\tif 'required_when' in opt:\n\t\t\tif opt['required_when'] in args:\n\t\t\t\tif 'value_list' in opt:\n\t\t\t\t\treq_values = opt['value_list']\n\t\t\t\telse:\n\t\t\t\t\treq_values = [opt['required_when']]\n\t\t\t\tfor value in req_values:\n\t\t\t\t\tif value not in args:\n\t\t\t\t\t\tresult.append(opt['name'])\n\treturn result\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\targs_required_when_matched = []\n\tfor arg_name, arg_spec in options_spec.items():\n\t\tif arg_name in args:\n\t\t\tcontinue\n\t\trequired_when = arg_spec.get('required_when')\n\t\tif required_when is None:\n\t\t\tcontinue\n\t\trequired_when_command = required_when.get('command')\n\t\trequired_when_args = required_when.get('args')\n\t\tif required_when_command is None and required_when_args is None:\n\t\t\tcontinue\n\t\tif required_when_command is None or required_when_command == command_name:\n\t\t\tif required_when_args is None or self._validate_args(required_when_args, args):\n\t\t\t\targs_required_when_matched.append(arg_name)\n\treturn args_required_when_matched\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired = []\n\tfor arg in args:\n\t\targ_name = arg[0]\n\t\tif arg_name in options_spec:\n\t\t\targ_spec = options_spec[arg_name]\n\t\t\tif 'required_when' in arg_spec:\n\t\t\t\trequired_when = arg_spec.get('required_when')\n\t\t\t\tif required_when:\n\t\t\t\t\tmatch = re.match(required_when, args[command_name])\n\t\t\t\t\tif match:\n\t\t\t\t\t\trequired.append(arg_name)\n\treturn required", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg, arg_spec in options_spec.items():\n\t\tif not arg.startswith('-') and arg_spec.get('required_when'):\n\t\t\tif arg_spec['required_when'](args):\n\t\t\t\trequired_args.append(arg)\n\treturn required_args\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tmatched_args = []\n\tfor arg_name, arg in options_spec.items():\n\t\tif arg.get('required_when', {}).get(command_name):\n\t\t\tmatched_args.append(arg_name)\n\treturn matched_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg in options_spec:\n\t\tif arg.get('required_when', {}).get(command_name, False):\n\t\t\trequired_args.append(arg.get('name', ''))\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor arg_name in args:\n\t\tif arg_name in options_spec:\n\t\t\targ_spec = options_spec[arg_name]\n\t\t\tif 'required_when' in arg_spec:\n\t\t\t\tcondition = arg_spec['required_when']\n\t\t\t\tif callable(condition):\n\t\t\t\t\tcondition = condition(args, command_name)\n\t\t\t\tif condition:\n\t\t\t\t\tresult.append(arg_name)\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tmatched_args = []\n\tfor arg in args:\n\t\tfor option in options_spec:\n\t\t\tif option.get(\"name\") == arg:\n\t\t\t\tif option.get(\"required_when\"):\n\t\t\t\t\tmatched_args.append(arg)\n\treturn matched_args", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_match_args = []\n\tfor arg in options_spec:\n\t\trequired_when = arg.get('required_when')\n\t\tif not required_when:\n\t\t\tcontinue\n\t\trequired_when_match = True\n\t\trequired_when_args = required_when.split(',')\n\t\tfor required_when_arg in required_when_args:\n\t\t\twhen_arg = required_when_arg.split(':')\n\t\t\twhen_arg_name = when_arg[0].strip()\n\t\t\twhen_arg_value = when_arg[1].strip()\n\t\t\twhen_arg_value = when_arg_value.replace('\"', '')\n\t\t\twhen_arg_value = when_arg_value.replace(\"'\", '')\n\t\t\tif when_arg_name not in args:\n\t\t\t\trequired_when_match = False\n\t\t\t\tbreak\n\t\t\targ_value = args[when_arg_name]\n\t\t\tif when_arg_value != arg_value:\n\t\t\t\trequired_when_match = False\n\t\t\t\tbreak\n\t\tif required_when_match:\n\t\t\trequired_when_match_args.append(arg['name'])\n\n\treturn required_when_match_args\n\n\nif __name__ == \"__main__\":\n\tprint(_get", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tconditionally_required_args = []\n\tfor arg in options_spec:\n\t\targ_name = arg['name']\n\t\targ_required_when = arg.get('required_when')\n\t\tif arg_required_when:\n\t\t\targ_required_when_value = self.config.get(arg_required_when, None)\n\t\t\tif arg_required_when_value:\n\t\t\t\targ_required_when_value = arg_required_when_value.split(',')\n\t\t\t\tif arg_name in args:\n\t\t\t\t\tcontinue\n\t\t\t\tif arg_name in arg_required_when_value:\n\t\t\t\t\tconditionally_required_args.append(arg_name)\n\treturn conditionally_required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor opt in options_spec:\n\t\tif 'required_when' in opt:\n\t\t\tif opt['required_when'] in args:\n\t\t\t\tif 'value_list' in opt:\n\t\t\t\t\treq_values = opt['value_list']\n\t\t\t\telse:\n\t\t\t\t\treq_values = [opt['required_when']]\n\t\t\t\tfor value in req_values:\n\t\t\t\t\tif value not in args:\n\t\t\t\t\t\tresult.append(opt['name'])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\targs_required_when_matched = []\n\tfor arg_name, arg_spec in options_spec.items():\n\t\tif arg_name in args:\n\t\t\tcontinue\n\t\trequired_when = arg_spec.get('required_when')\n\t\tif required_when is None:\n\t\t\tcontinue\n\t\trequired_when_command = required_when.get('command')\n\t\trequired_when_args = required_when.get('args')\n\t\tif required_when_command is None and required_when_args is None:\n\t\t\tcontinue\n\t\tif required_when_command is None or required_when_command == command_name:\n\t\t\tif required_when_args is None or self._validate_args(required_when_args, args):\n\t\t\t\targs_required_when_matched.append(arg_name)\n\treturn args_required_when_matched\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired = []\n\tfor arg in args:\n\t\targ_name = arg[0]\n\t\tif arg_name in options_spec:\n\t\t\targ_spec = options_spec[arg_name]\n\t\t\tif 'required_when' in arg_spec:\n\t\t\t\trequired_when = arg_spec.get('required_when')\n\t\t\t\tif required_when:\n\t\t\t\t\tmatch = re.match(required_when, args[command_name])\n\t\t\t\t\tif match:\n\t\t\t\t\t\trequired.append(arg_name)\n\treturn required", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg, arg_spec in options_spec.items():\n\t\tif not arg.startswith('-') and arg_spec.get('required_when'):\n\t\t\tif arg_spec['required_when'](args):\n\t\t\t\trequired_args.append(arg)\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = args.length\n\n\tif length is None:\n\t\tlength = 1\n\n\tlength = int(length)\n\n\tfor arg in args.args:\n\t\tif len(arg) > length:\n\t\t\traise ValueError(f\"Argument {arg} is longer than the specified length of {length}\")\n\n\treturn args", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.length\n\tself.length = None\n\n\tfor arg in args:\n\t\tif len(arg) > length:\n\t\t\traise ValueError(f\"Length of the value is higher than the specified. ({len(arg)} > {length})\")\n\n\tself.length = length", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.length\n\targs_length = len(args)\n\tif args_length > length:\n\t\traise ValueError(\"Too many arguments, expected \" + str(length) + \" but got \" + str(args_length))\n\telif args_length < length:\n\t\traise ValueError(\"Too few arguments, expected \" + str(length) + \" but got \" + str(args_length))\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength_allowed = 100\n\targs_length = len(args)\n\tif args_length > length_allowed:\n\t\traise ValueError(f'Too many arguments (max {length_allowed} args)')\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength_args = []\n\tfor arg in args:\n\t\tlength_args.append(len(arg))\n\n\tlength_args_set = set(length_args)\n\n\tif len(length_args_set) > 1:\n\t\treturn False\n\treturn True\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.validate_non_empty_args(args)\n\tif len(args) > self.length:\n\t\traise Exception('The number of args exceeds the expected length')\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = len(args[0])\n\ttry:\n\t\tlength_int = int(args[1])\n\texcept ValueError:\n\t\traise ValueError('Second argument is not an integer')\n\tif length > length_int:\n\t\traise ValueError(f'Length of the given string is longer than the specified one: {length_int}')\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = len(args)\n\tif length == 1:\n\t\treturn True\n\telif length == 2:\n\t\ttry:\n\t\t\tif len(args[1]) == int(args[0]):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tprint(f\"Length of the value is not equal to {args[0]}.\")\n\t\t\t\treturn False\n\t\texcept:\n\t\t\tprint(f\"{args[0]} is not a number.\")\n\t\t\treturn False\n\telse:\n\t\tprint(\"Not enough or too many arguments.\")\n\t\treturn False\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.config[\"length\"]\n\tfor arg in args:\n\t\tif len(arg) > length:\n\t\t\treturn False\n\treturn True"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = args.length\n\n\tif length is None:\n\t\tlength = 1\n\n\tlength = int(length)\n\n\tfor arg in args.args:\n\t\tif len(arg) > length:\n\t\t\traise ValueError(f\"Argument {arg} is longer than the specified length of {length}\")\n\n\treturn args", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.length\n\tself.length = None\n\n\tfor arg in args:\n\t\tif len(arg) > length:\n\t\t\traise ValueError(f\"Length of the value is higher than the specified. ({len(arg)} > {length})\")\n\n\tself.length = length", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.length\n\targs_length = len(args)\n\tif args_length > length:\n\t\traise ValueError(\"Too many arguments, expected \" + str(length) + \" but got \" + str(args_length))\n\telif args_length < length:\n\t\traise ValueError(\"Too few arguments, expected \" + str(length) + \" but got \" + str(args_length))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength_allowed = 100\n\targs_length = len(args)\n\tif args_length > length_allowed:\n\t\traise ValueError(f'Too many arguments (max {length_allowed} args)')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength_args = []\n\tfor arg in args:\n\t\tlength_args.append(len(arg))\n\n\tlength_args_set = set(length_args)\n\n\tif len(length_args_set) > 1:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.validate_non_empty_args(args)\n\tif len(args) > self.length:\n\t\traise Exception('The number of args exceeds the expected length')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = len(args[0])\n\ttry:\n\t\tlength_int = int(args[1])\n\texcept ValueError:\n\t\traise ValueError('Second argument is not an integer')\n\tif length > length_int:\n\t\traise ValueError(f'Length of the given string is longer than the specified one: {length_int}')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = len(args)\n\tif length == 1:\n\t\treturn True\n\telif length == 2:\n\t\ttry:\n\t\t\tif len(args[1]) == int(args[0]):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tprint(f\"Length of the value is not equal to {args[0]}.\")\n\t\t\t\treturn False\n\t\texcept:\n\t\t\tprint(f\"{args[0]} is not a number.\")\n\t\t\treturn False\n\telse:\n\t\tprint(\"Not enough or too many arguments.\")\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.config[\"length\"]\n\tfor arg in args:\n\t\tif len(arg) > length:\n\t\t\treturn False\n\treturn True", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.get_choices()\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\traise ValueError(f\"{arg} is not a valid argument. Please choose one of {choices}.\")", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tfor arg in args:\n\t\tif not arg in choices:\n\t\t\traise ValueError(\n\t\t\t\t'Invalid choice. The value ({}) must be one of the following choices: {}'.format(arg, choices)\n\t\t\t)\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = [\"start\", \"stop\", \"restart\"]\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\tprint(\"Invalid argument: \" + arg)\n\t\t\tsys.exit(1)\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tchoices = choices.split(\",\")\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\traise Exception(\"The value of the argument '\" + arg + \"' is not a valid choice. Possible choices are '\" + choices + \"'.\")", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.args = args\n\tvalid_args = [\"--help\", \"-h\", \"--version\", \"-v\", \"--verbose\", \"-vv\", \"--no-color\", \"--color\", \"--no-ansi\"]\n\tfor arg in args:\n\t\tif arg not in valid_args:\n\t\t\tprint(\"The argument \\\"\" + arg + \"\\\" is not a valid argument\")\n\t\t\tprint(\"The available arguments are: \" + str(valid_args))\n\t\t\texit()\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\targs_to_check = self.get_args_to_check()\n\tchoices = self.get_choices()\n\n\tfor arg in args_to_check:\n\t\tif args[arg] not in choices:\n\t\t\traise argparse.ArgumentTypeError(\n\t\t\t\tself.get_choices_error_message(arg, choices))\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.logger.debug(f'Validating choices for arguments: {args}')\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise ValueError(f'{arg} is not a valid choice.')\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\n\tif args.get('choices') is not None and isinstance(args.get('choices'), list) and args.get('value') is not None:\n\t\tif args.get('value') not in args.get('choices'):\n\t\t\traise ValueError(\n\t\t\t\tf\"{args.get('name')} is not a valid choice. Valid choices are {args.get('choices')}\"\n\t\t\t)", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\toptions = self.get_options()\n\tfor arg in args:\n\t\tif arg in options:\n\t\t\tcontinue\n\t\traise Exception(f'Invalid choice: {arg}')\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tif choices is None:\n\t\treturn\n\telif hasattr(choices, '__iter__'):\n\t\tif args not in choices:\n\t\t\traise ValueError(f\"Value {args} is not one of the available choices: {choices}\")\n\telif isinstance(choices, dict):\n\t\tif args not in choices.keys():\n\t\t\traise ValueError(f\"Key {args} is not one of the available choices: {choices.keys()}\")\n\telse:\n\t\traise ValueError(\"Wrong choices type.\")\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.get_choices()\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\traise ValueError(f\"{arg} is not a valid argument. Please choose one of {choices}.\")", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tfor arg in args:\n\t\tif not arg in choices:\n\t\t\traise ValueError(\n\t\t\t\t'Invalid choice. The value ({}) must be one of the following choices: {}'.format(arg, choices)\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = [\"start\", \"stop\", \"restart\"]\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\tprint(\"Invalid argument: \" + arg)\n\t\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tchoices = choices.split(\",\")\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\traise Exception(\"The value of the argument '\" + arg + \"' is not a valid choice. Possible choices are '\" + choices + \"'.\")", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.args = args\n\tvalid_args = [\"--help\", \"-h\", \"--version\", \"-v\", \"--verbose\", \"-vv\", \"--no-color\", \"--color\", \"--no-ansi\"]\n\tfor arg in args:\n\t\tif arg not in valid_args:\n\t\t\tprint(\"The argument \\\"\" + arg + \"\\\" is not a valid argument\")\n\t\t\tprint(\"The available arguments are: \" + str(valid_args))\n\t\t\texit()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\targs_to_check = self.get_args_to_check()\n\tchoices = self.get_choices()\n\n\tfor arg in args_to_check:\n\t\tif args[arg] not in choices:\n\t\t\traise argparse.ArgumentTypeError(\n\t\t\t\tself.get_choices_error_message(arg, choices))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.logger.debug(f'Validating choices for arguments: {args}')\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise ValueError(f'{arg} is not a valid choice.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\n\tif args.get('choices') is not None and isinstance(args.get('choices'), list) and args.get('value') is not None:\n\t\tif args.get('value') not in args.get('choices'):\n\t\t\traise ValueError(\n\t\t\t\tf\"{args.get('name')} is not a valid choice. Valid choices are {args.get('choices')}\"\n\t\t\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\toptions = self.get_options()\n\tfor arg in args:\n\t\tif arg in options:\n\t\t\tcontinue\n\t\traise Exception(f'Invalid choice: {arg}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tif choices is None:\n\t\treturn\n\telif hasattr(choices, '__iter__'):\n\t\tif args not in choices:\n\t\t\traise ValueError(f\"Value {args} is not one of the available choices: {choices}\")\n\telif isinstance(choices, dict):\n\t\tif args not in choices.keys():\n\t\t\traise ValueError(f\"Key {args} is not one of the available choices: {choices.keys()}\")\n\telse:\n\t\traise ValueError(\"Wrong choices type.\")\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\tmin_value = args.get('min_value')\n\tmax_value = args.get('max_value')\n\t\n\tvalue = args.get('value')\n\t\n\tif min_value is not None:\n\t\tif value < min_value:\n\t\t\treturn {\n\t\t\t\t'code': 400,\n\t\t\t\t'message': 'Value is less than minimum value'\n\t\t\t}\n\t\t\n\tif max_value is not None:\n\t\tif value > max_value:\n\t\t\treturn {\n\t\t\t\t'code': 400,\n\t\t\t\t'message': 'Value is greater than maximum value'\n\t\t\t}\n\t\n\treturn True", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = args.min_value\n\tmax_value = args.max_value\n\t\n\tif min_value > max_value:\n\t\traise Exception(\"Minimum value cannot be greater than maximum value.\")\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = self.min_value\n\tmax_value = self.max_value\n\tvalue = args[0]\n\tif value < min_value or value > max_value:\n\t\treturn False\n\treturn True\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tresult = True\n\tif args is not None:\n\t\tfor arg in args:\n\t\t\tif arg.get('value') is not None:\n\t\t\t\tresult = result and self.validate_min_max(arg.get('value'), arg.get('min'), arg.get('max'))\n\treturn result\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tis_valid = True\n\tis_valid = is_valid and (args.get('min') is None or args.get('min') <= args.get('value'))\n\tis_valid = is_valid and (args.get('max') is None or args.get('max') >= args.get('value'))\n\treturn is_valid", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value, max_value = args\n\tif self.value < min_value or self.value > max_value:\n\t\traise ValueError(f\"{self.name} value is out of range\")\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\tmin_val = args[\"min\"]\n\texcept KeyError:\n\t\tmin_val = None\n\ttry:\n\t\tmax_val = args[\"max\"]\n\texcept KeyError:\n\t\tmax_val = None\n\n\tif min_val is not None and max_val is not None:\n\t\tif not min_val <= max_val:\n\t\t\traise ValueError(\"min value must be smaller or equal to max value\")\n\telif min_val is not None:\n\t\tself.logger.info(\"min_val is not None, check for max_val\")\n\t\tif not min_val >= max_val:\n\t\t\traise ValueError(\"min value must be smaller or equal to max value\")\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tminimum = args.get('min')\n\tmaximum = args.get('max')\n\tvalue = args.get('value')\n\tif minimum is not None and maximum is not None:\n\t\tif value < minimum:\n\t\t\traise forms.ValidationError('Value should be greater than or equal to %s' % minimum)\n\t\telif value > maximum:\n\t\t\traise forms.ValidationError('Value should be less than or equal to %s' % maximum)\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tminimum = args.get('minimum', None)\n\tmaximum = args.get('maximum', None)\n\tvalue = args.get('value', None)\n\terror = None\n\n\tif minimum is not None and maximum is not None:\n\t\tif value < minimum or value > maximum:\n\t\t\terror = 'Value ({}) should be between {} and {}'.format(value, minimum, maximum)\n\n\tif minimum is not None and value < minimum:\n\t\terror = 'Value ({}) should be greater than {}'.format(value, minimum)\n\n\tif maximum is not None and value > maximum:\n\t\terror = 'Value ({}) should be less than {}'.format(value, maximum)\n\n\treturn error\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = self.get_min_value()\n\tmax_value = self.get_max_value()\n\tvalue = args[0]\n\t\n\tif value < min_value or value > max_value:\n\t\traise ValueError('value must be between %s and %s' % (min_value, max_value))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\tmin_value = args.get('min_value')\n\tmax_value = args.get('max_value')\n\t\n\tvalue = args.get('value')\n\t\n\tif min_value is not None:\n\t\tif value < min_value:\n\t\t\treturn {\n\t\t\t\t'code': 400,\n\t\t\t\t'message': 'Value is less than minimum value'\n\t\t\t}\n\t\t\n\tif max_value is not None:\n\t\tif value > max_value:\n\t\t\treturn {\n\t\t\t\t'code': 400,\n\t\t\t\t'message': 'Value is greater than maximum value'\n\t\t\t}\n\t\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = args.min_value\n\tmax_value = args.max_value\n\t\n\tif min_value > max_value:\n\t\traise Exception(\"Minimum value cannot be greater than maximum value.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = self.min_value\n\tmax_value = self.max_value\n\tvalue = args[0]\n\tif value < min_value or value > max_value:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tresult = True\n\tif args is not None:\n\t\tfor arg in args:\n\t\t\tif arg.get('value') is not None:\n\t\t\t\tresult = result and self.validate_min_max(arg.get('value'), arg.get('min'), arg.get('max'))\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tis_valid = True\n\tis_valid = is_valid and (args.get('min') is None or args.get('min') <= args.get('value'))\n\tis_valid = is_valid and (args.get('max') is None or args.get('max') >= args.get('value'))\n\treturn is_valid", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value, max_value = args\n\tif self.value < min_value or self.value > max_value:\n\t\traise ValueError(f\"{self.name} value is out of range\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\tmin_val = args[\"min\"]\n\texcept KeyError:\n\t\tmin_val = None\n\ttry:\n\t\tmax_val = args[\"max\"]\n\texcept KeyError:\n\t\tmax_val = None\n\n\tif min_val is not None and max_val is not None:\n\t\tif not min_val <= max_val:\n\t\t\traise ValueError(\"min value must be smaller or equal to max value\")\n\telif min_val is not None:\n\t\tself.logger.info(\"min_val is not None, check for max_val\")\n\t\tif not min_val >= max_val:\n\t\t\traise ValueError(\"min value must be smaller or equal to max value\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tminimum = args.get('min')\n\tmaximum = args.get('max')\n\tvalue = args.get('value')\n\tif minimum is not None and maximum is not None:\n\t\tif value < minimum:\n\t\t\traise forms.ValidationError('Value should be greater than or equal to %s' % minimum)\n\t\telif value > maximum:\n\t\t\traise forms.ValidationError('Value should be less than or equal to %s' % maximum)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tminimum = args.get('minimum', None)\n\tmaximum = args.get('maximum', None)\n\tvalue = args.get('value', None)\n\terror = None\n\n\tif minimum is not None and maximum is not None:\n\t\tif value < minimum or value > maximum:\n\t\t\terror = 'Value ({}) should be between {} and {}'.format(value, minimum, maximum)\n\n\tif minimum is not None and value < minimum:\n\t\terror = 'Value ({}) should be greater than {}'.format(value, minimum)\n\n\tif maximum is not None and value > maximum:\n\t\terror = 'Value ({}) should be less than {}'.format(value, maximum)\n\n\treturn error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = self.get_min_value()\n\tmax_value = self.get_max_value()\n\tvalue = args[0]\n\t\n\tif value < min_value or value > max_value:\n\t\traise ValueError('value must be between %s and %s' % (min_value, max_value))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif complex_action is None:\n\t    raise ValueError(\"Unknown type name %s\" % type_name)\n\treturn complex_action(self, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise ValueError('Unknown complex type %s' % type_name)\n\ttry:\n\t\tgetattr(self, option_name)\n\texcept AttributeError:\n\t\tself.add_option(option_name, None)\n\tcomplex_action(self, subcommand, option_name, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tdef complex_action(self, option_name, (vars, defaults, plugin_path), subcommand, spec_option):\n\t\t\"\"\"\n\t\tComplex action function for creating complex argument types.\n\t\t\"\"\"\n\t\tdef inner_action(option_name, (vars, defaults, plugin_path), subcommand, spec_option):\n\t\t\t\"\"\"\n\t\t\tComplex action function for creating complex argument types.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\ttype_info = COMPLEX_TYPES[type_name]\n\t\t\texcept KeyError:\n\t\t\t\traise Exception('Unknown type: %r' % type_name)\n\t\t\ttype_class = type_info[0]\n\t\t\tspec_class = type_info[1]\n\t\t\tspec_kwargs = type_info[2]\n\t\t\tspec = spec_class(**spec_kwargs)\n\t\t\ttry:\n\t\t\t\tvalue = spec.read_value_from_string(spec_option.dest, spec_option.default,\n\t\t\t\t                                    spec_option.source, spec_option.envvar,\n\t\t\t\t                                    spec_option.conflict_handler)\n\t\t\texcept SpecError as e:\n\t\t\t\traise SpecError(option_name, e.msg)\n\t\t\tsubcommand.add_option(Option(option_name, type", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tfrom os import path\n\tfrom json import load\n\twith open(path.join(path.dirname(path.abspath(__file__)), 'complex_types.json'), 'r') as f:\n\t\tcomplex_types = load(f)\n\n\tcomplex_action = complex_types[type_name]\n\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif complex_action:\n\t\ttry:\n\t\t\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\t\texcept TypeError as e:\n\t\t\traise PluginError(self, 'Error in complex argument type \"{}\" ({})'.format(type_name, e))\n\traise PluginError(self, 'Unknown argument type \"{}\"'.format(type_name))", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttype_name = type_name.lower()\n\tif type_name in COMPLEX_TYPES:\n\t\t# we are good, just call the function\n\t\treturn COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n\n\traise NotImplementedError(\"The type '%s' for the option '%s' is not implemented. Please make sure that the type is valid\" % (type_name, option_name))", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_type = COMPLEX_TYPES.get(type_name)\n\treturn complex_type(self, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif not complex_action:\n\t\traise TypeError(\"Complex argument type '%s' not supported\" % type_name)\n\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\timport sys\n\timport os\n\timport imp\n\tfrom . import COMPLEX_TYPES\n\tfrom . import Argumet\n\tfrom . import get_path_of_plugin\n\tfrom . import get_path_of_module\n\tfrom . import get_path_of_file\n\tfrom . import get_path_of_dir\n\tfrom . import get_path_of_main_py\n\tfrom . import get_path_of_main_py_dir\n\tfrom . import get_path_of_main_py_file\n\tfrom . import get_path_of_main_py_dir_file\n\tfrom . import get_path_of_main_py_dir_file_dir\n\tfrom . import get_path_of_main_py_dir_file_dir_file\n\n\tpath_of_plugin = get_path_of_plugin(self.plugin_path)\n\n\tpath_of_module = get_path_of_module(path_of_plugin, type_name)\n\n\tpath_of_file = get_path_of_file(path_of_module, type_name)\n\n\tpath_of_dir = get_path_of_dir(path_of_file)\n\n\tpath_of_main_py = get_path_", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES[type_name][0]\n\tnew_function = partial(complex_action, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\treturn new_function"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif complex_action is None:\n\t    raise ValueError(\"Unknown type name %s\" % type_name)\n\treturn complex_action(self, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise ValueError('Unknown complex type %s' % type_name)\n\ttry:\n\t\tgetattr(self, option_name)\n\texcept AttributeError:\n\t\tself.add_option(option_name, None)\n\tcomplex_action(self, subcommand, option_name, spec_option)", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tdef complex_action(self, option_name, (vars, defaults, plugin_path), subcommand, spec_option):\n\t\t\"\"\"\n\t\tComplex action function for creating complex argument types.\n\t\t\"\"\"\n\t\tdef inner_action(option_name, (vars, defaults, plugin_path), subcommand, spec_option):\n\t\t\t\"\"\"\n\t\t\tComplex action function for creating complex argument types.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\ttype_info = COMPLEX_TYPES[type_name]\n\t\t\texcept KeyError:\n\t\t\t\traise Exception('Unknown type: %r' % type_name)\n\t\t\ttype_class = type_info[0]\n\t\t\tspec_class = type_info[1]\n\t\t\tspec_kwargs = type_info[2]\n\t\t\tspec = spec_class(**spec_kwargs)\n\t\t\ttry:\n\t\t\t\tvalue = spec.read_value_from_string(spec_option.dest, spec_option.default,\n\t\t\t\t                                    spec_option.source, spec_option.envvar,\n\t\t\t\t                                    spec_option.conflict_handler)\n\t\t\texcept SpecError as e:\n\t\t\t\traise SpecError(option_name, e.msg)\n\t\t\tsubcommand.add_option(Option(option_name, type", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tfrom os import path\n\tfrom json import load\n\twith open(path.join(path.dirname(path.abspath(__file__)), 'complex_types.json'), 'r') as f:\n\t\tcomplex_types = load(f)\n\n\tcomplex_action = complex_types[type_name]\n\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif complex_action:\n\t\ttry:\n\t\t\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\t\texcept TypeError as e:\n\t\t\traise PluginError(self, 'Error in complex argument type \"{}\" ({})'.format(type_name, e))\n\traise PluginError(self, 'Unknown argument type \"{}\"'.format(type_name))", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttype_name = type_name.lower()\n\tif type_name in COMPLEX_TYPES:\n\t\t# we are good, just call the function\n\t\treturn COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n\n\traise NotImplementedError(\"The type '%s' for the option '%s' is not implemented. Please make sure that the type is valid\" % (type_name, option_name))", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_type = COMPLEX_TYPES.get(type_name)\n\treturn complex_type(self, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif not complex_action:\n\t\traise TypeError(\"Complex argument type '%s' not supported\" % type_name)\n\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\timport sys\n\timport os\n\timport imp\n\tfrom . import COMPLEX_TYPES\n\tfrom . import Argumet\n\tfrom . import get_path_of_plugin\n\tfrom . import get_path_of_module\n\tfrom . import get_path_of_file\n\tfrom . import get_path_of_dir\n\tfrom . import get_path_of_main_py\n\tfrom . import get_path_of_main_py_dir\n\tfrom . import get_path_of_main_py_file\n\tfrom . import get_path_of_main_py_dir_file\n\tfrom . import get_path_of_main_py_dir_file_dir\n\tfrom . import get_path_of_main_py_dir_file_dir_file\n\n\tpath_of_plugin = get_path_of_plugin(self.plugin_path)\n\n\tpath_of_module = get_path_of_module(path_of_plugin, type_name)\n\n\tpath_of_file = get_path_of_file(path_of_module, type_name)\n\n\tpath_of_dir = get_path_of_dir(path_of_file)\n\n\tpath_of_main_py = get_path_", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES[type_name][0]\n\tnew_function = partial(complex_action, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\treturn new_function", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith((\"-\", \"--\")):\n\t\t\tkv = arg.split(\"=\", 1)\n\t\t\tif len(kv) > 1:\n\t\t\t\tk, v = kv\n\t\t\t\tif k in CONTROL_ARGS:\n\t\t\t\t\tcontrol_args[k] = v\n\t\t\t\telif k in CUSTOM_ARGS:\n\t\t\t\t\tcustom_args[k] = v\n\t\t\t\telse:\n\t\t\t\t\tnested_args[k] = v\n\t\t\telse:\n\t\t\t\tcontrol_args[arg] = True\n\n\treturn control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg, val in args.items():\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = val\n\t\telif arg in self.nested_args:\n\t\t\tnested_args[arg] = val\n\t\telse:\n\t\t\tnested_args[arg] = val\n\n\treturn control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\t# Split args to control_args, nested_args, custom_args\n\tfor arg in args:\n\t\tif arg.startswith(\"--\"):\n\t\t\targ = arg[2:]\n\t\telif arg.startswith(\"-\"):\n\t\t\targ = arg[1:]\n\t\tif arg.startswith(\"ansible-\"):\n\t\t\tcustom_args[arg[8:]] = args[arg]\n\t\telif arg.startswith(\"nested-\"):\n\t\t\tnested_args[arg[7:]] = args[arg]\n\t\telse:\n\t\t\tcontrol_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = dict()\n\tnested_args = dict()\n\tfor key, value in args.items():\n\t\tif key.startswith(\"--\"):\n\t\t\tcontrol_args[key] = value\n\t\telse:\n\t\t\tnested_args[key] = value\n\treturn control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--\"):\n\t\t\tcontrol_args[arg.replace(\"--\", \"\")] = True\n\t\telif arg.startswith(\"--custom\"):\n\t\t\tcustom_args[arg.split(\"=\")[0].replace(\"--custom\", \"\")] = arg.split(\"=\")[1]\n\t\telse:\n\t\t\tnested_args[arg.split(\"=\")[0].replace(\"--\", \"\")] = arg.split(\"=\")[1]\n\treturn control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif args[arg] is not None:\n\t\t\tif isinstance(args[arg], list):\n\t\t\t\t# If the arg is a list, it's probably a list of strings\n\t\t\t\t# The custom args are used as lists of strings\n\t\t\t\t# The nested args are used as lists of dictionaries\n\t\t\t\tcustom_args[arg] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tcustom_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tkey = arg[0]\n\t\tvalue = arg[1]\n\n\t\tif key.startswith('--'):  # control args\n\t\t\tcontrol_args[key] = value\n\t\telif key.startswith('-'):  # nested args\n\t\t\tif ',' in value:\n\t\t\t\tnested_args[key] = value.split(',')\n\t\t\telse:\n\t\t\t\tnested_args[key] = value\n\t\telse:  # custom args\n\t\t\tcustom_args[key] = value\n\n\treturn control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {\n\t\t\"verbose\": False,\n\t\t\"quiet\": False,\n\t\t\"no_prompt\": False,\n\t\t\"no_clean\": False,\n\t\t\"clean\": True,\n\t\t\"dry_run\": False,\n\t}\n\tcustom_args = {\n\t\t\"clean_before\": False,\n\t\t\"clean_after\": False,\n\t}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in control_args.keys():\n\t\t\tcontrol_args[arg] = True\n\t\telif arg in custom_args.keys():\n\t\t\tcustom_args[arg] = True\n\t\telse:\n\t\t\tnested_args[arg] = True\n\treturn control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg[0] in control_args:\n\t\t\tcontrol_args[arg[0]].append(arg[1])\n\t\telse:\n\t\t\tcontrol_args[arg[0]] = [arg[1]]\n\t\tif arg[0].startswith('nested'):\n\t\t\tnested_args[arg[0][len('nested'):]] = arg[1]\n\t\telif arg[0].startswith('custom'):\n\t\t\tcustom_args[arg[0][len('custom'):]] = arg[1]\n\treturn control_args, nested_args, custom_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = get_control_args(args)\n\tnested_args = get_nested_args(args)\n\tcustom_args = get_custom_args(args)\n\treturn control_args, nested_args, custom_args\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith((\"-\", \"--\")):\n\t\t\tkv = arg.split(\"=\", 1)\n\t\t\tif len(kv) > 1:\n\t\t\t\tk, v = kv\n\t\t\t\tif k in CONTROL_ARGS:\n\t\t\t\t\tcontrol_args[k] = v\n\t\t\t\telif k in CUSTOM_ARGS:\n\t\t\t\t\tcustom_args[k] = v\n\t\t\t\telse:\n\t\t\t\t\tnested_args[k] = v\n\t\t\telse:\n\t\t\t\tcontrol_args[arg] = True\n\n\treturn control_args, nested_args, custom_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg, val in args.items():\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = val\n\t\telif arg in self.nested_args:\n\t\t\tnested_args[arg] = val\n\t\telse:\n\t\t\tnested_args[arg] = val\n\n\treturn control_args, nested_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\t# Split args to control_args, nested_args, custom_args\n\tfor arg in args:\n\t\tif arg.startswith(\"--\"):\n\t\t\targ = arg[2:]\n\t\telif arg.startswith(\"-\"):\n\t\t\targ = arg[1:]\n\t\tif arg.startswith(\"ansible-\"):\n\t\t\tcustom_args[arg[8:]] = args[arg]\n\t\telif arg.startswith(\"nested-\"):\n\t\t\tnested_args[arg[7:]] = args[arg]\n\t\telse:\n\t\t\tcontrol_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = dict()\n\tnested_args = dict()\n\tfor key, value in args.items():\n\t\tif key.startswith(\"--\"):\n\t\t\tcontrol_args[key] = value\n\t\telse:\n\t\t\tnested_args[key] = value\n\treturn control_args, nested_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--\"):\n\t\t\tcontrol_args[arg.replace(\"--\", \"\")] = True\n\t\telif arg.startswith(\"--custom\"):\n\t\t\tcustom_args[arg.split(\"=\")[0].replace(\"--custom\", \"\")] = arg.split(\"=\")[1]\n\t\telse:\n\t\t\tnested_args[arg.split(\"=\")[0].replace(\"--\", \"\")] = arg.split(\"=\")[1]\n\treturn control_args, nested_args, custom_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif args[arg] is not None:\n\t\t\tif isinstance(args[arg], list):\n\t\t\t\t# If the arg is a list, it's probably a list of strings\n\t\t\t\t# The custom args are used as lists of strings\n\t\t\t\t# The nested args are used as lists of dictionaries\n\t\t\t\tcustom_args[arg] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args, custom_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tcustom_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tkey = arg[0]\n\t\tvalue = arg[1]\n\n\t\tif key.startswith('--'):  # control args\n\t\t\tcontrol_args[key] = value\n\t\telif key.startswith('-'):  # nested args\n\t\t\tif ',' in value:\n\t\t\t\tnested_args[key] = value.split(',')\n\t\t\telse:\n\t\t\t\tnested_args[key] = value\n\t\telse:  # custom args\n\t\t\tcustom_args[key] = value\n\n\treturn control_args, nested_args, custom_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {\n\t\t\"verbose\": False,\n\t\t\"quiet\": False,\n\t\t\"no_prompt\": False,\n\t\t\"no_clean\": False,\n\t\t\"clean\": True,\n\t\t\"dry_run\": False,\n\t}\n\tcustom_args = {\n\t\t\"clean_before\": False,\n\t\t\"clean_after\": False,\n\t}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in control_args.keys():\n\t\t\tcontrol_args[arg] = True\n\t\telif arg in custom_args.keys():\n\t\t\tcustom_args[arg] = True\n\t\telse:\n\t\t\tnested_args[arg] = True\n\treturn control_args, nested_args, custom_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg[0] in control_args:\n\t\t\tcontrol_args[arg[0]].append(arg[1])\n\t\telse:\n\t\t\tcontrol_args[arg[0]] = [arg[1]]\n\t\tif arg[0].startswith('nested'):\n\t\t\tnested_args[arg[0][len('nested'):]] = arg[1]\n\t\telif arg[0].startswith('custom'):\n\t\t\tcustom_args[arg[0][len('custom'):]] = arg[1]\n\treturn control_args, nested_args, custom_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = get_control_args(args)\n\tnested_args = get_nested_args(args)\n\tcustom_args = get_custom_args(args)\n\treturn control_args, nested_args, custom_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor extra_var in extra_vars:\n\t\t# TODO: handle more than one '='\n\t\tkey, value = extra_var.split('=', 1)\n\t\tvars_dict[key] = value", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif \"=\" in extra_var:\n\t\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise ValueError(\"extra-var {0} is invalid\".format(extra_var))", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not isinstance(vars_dict, dict):\n\t\traise TypeError('vars_dict must be a dict')\n\t\n\tif extra_vars:\n\t\tif not isinstance(extra_vars, list):\n\t\t\traise TypeError('extra_vars must be a list')\n\t\t\n\t\tfor extra_var in extra_vars:\n\t\t\tif extra_var.find('=') > 0:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\tvars_dict[extra_var] = True\n\t\n\treturn vars_dict", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not extra_vars:\n\t\treturn\n\t\n\tfor extra_var in extra_vars:\n\t\ttry:\n\t\t\tkey, value = extra_var.split('=', 1)\n\t\texcept ValueError:\n\t\t\traise Exception('Invalid extra-var: {0}'.format(extra_var))\n\t\t\n\t\tif value.startswith('@') and os.path.isfile(value[1:]):\n\t\t\tvalue = open(value[1:]).read().strip()\n\t\t\n\t\tvars_dict[key] = value", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor item in extra_vars:\n\t\tkey, value = item.split(\"=\", 1)\n\t\tvars_dict[key] = value", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfrom itertools import chain\n\n\tif not extra_vars:\n\t\treturn vars_dict\n\n\textra_vars_dict = dict(item.split('=', 1) for item in extra_vars)\n\tvars_dict.update(extra_vars_dict)\n\treturn vars_dict", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfrom ansible import utils\n\n\tif extra_vars:\n\t\tfor var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise AnsibleError('malformed \"extra-vars\", expecting key=value: %s' % var)\n\t\t\tutils.extend_vars(vars_dict, {key: value})", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict = dict(vars_dict)\n\tif extra_vars:\n\t\textra_vars = shlex_split(extra_vars)\n\t\tif len(extra_vars) % 2 == 1:\n\t\t\traise AnsibleError(\n\t\t\t\t'Extra variables (-e) require a key=value format, got %s' % (\n\t\t\t\t\textra_vars.pop(),\n\t\t\t\t))\n\t\tvars_dict.update(zip(extra_vars[::2], extra_vars[1::2]))\n\treturn vars_dict", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict = vars_dict.copy()\n\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\tvars_dict[key] = value\n\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\n\t# if extra-vars is a string, split it\n\tif isinstance(extra_vars, str):\n\t\textra_vars = extra_vars.split(',')\n\n\tfor var in extra_vars:\n\t\ttry:\n\t\t\tkey, value = var.split('=')\n\t\t\tvars_dict[key] = value\n\t\texcept ValueError:\n\t\t\tpass\n\n\treturn vars_dict"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor extra_var in extra_vars:\n\t\t# TODO: handle more than one '='\n\t\tkey, value = extra_var.split('=', 1)\n\t\tvars_dict[key] = value", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif \"=\" in extra_var:\n\t\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise ValueError(\"extra-var {0} is invalid\".format(extra_var))", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not isinstance(vars_dict, dict):\n\t\traise TypeError('vars_dict must be a dict')\n\t\n\tif extra_vars:\n\t\tif not isinstance(extra_vars, list):\n\t\t\traise TypeError('extra_vars must be a list')\n\t\t\n\t\tfor extra_var in extra_vars:\n\t\t\tif extra_var.find('=') > 0:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\tvars_dict[extra_var] = True\n\t\n\treturn vars_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not extra_vars:\n\t\treturn\n\t\n\tfor extra_var in extra_vars:\n\t\ttry:\n\t\t\tkey, value = extra_var.split('=', 1)\n\t\texcept ValueError:\n\t\t\traise Exception('Invalid extra-var: {0}'.format(extra_var))\n\t\t\n\t\tif value.startswith('@') and os.path.isfile(value[1:]):\n\t\t\tvalue = open(value[1:]).read().strip()\n\t\t\n\t\tvars_dict[key] = value", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor item in extra_vars:\n\t\tkey, value = item.split(\"=\", 1)\n\t\tvars_dict[key] = value", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfrom itertools import chain\n\n\tif not extra_vars:\n\t\treturn vars_dict\n\n\textra_vars_dict = dict(item.split('=', 1) for item in extra_vars)\n\tvars_dict.update(extra_vars_dict)\n\treturn vars_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfrom ansible import utils\n\n\tif extra_vars:\n\t\tfor var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise AnsibleError('malformed \"extra-vars\", expecting key=value: %s' % var)\n\t\t\tutils.extend_vars(vars_dict, {key: value})", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict = dict(vars_dict)\n\tif extra_vars:\n\t\textra_vars = shlex_split(extra_vars)\n\t\tif len(extra_vars) % 2 == 1:\n\t\t\traise AnsibleError(\n\t\t\t\t'Extra variables (-e) require a key=value format, got %s' % (\n\t\t\t\t\textra_vars.pop(),\n\t\t\t\t))\n\t\tvars_dict.update(zip(extra_vars[::2], extra_vars[1::2]))\n\treturn vars_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict = vars_dict.copy()\n\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\tvars_dict[key] = value\n\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\n\t# if extra-vars is a string, split it\n\tif isinstance(extra_vars, str):\n\t\textra_vars = extra_vars.split(',')\n\n\tfor var in extra_vars:\n\t\ttry:\n\t\t\tkey, value = var.split('=')\n\t\t\tvars_dict[key] = value\n\t\texcept ValueError:\n\t\t\tpass\n\n\treturn vars_dict", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tfrom infrared.core import AnsibleRunner\n\n\t# Build the extra-vars dict.\n\textra_vars = extra_vars or {}\n\n\tansible_args = ansible_args or {}\n\n\tansible_args['extra_vars'] = extra_vars\n\n\tif verbose:\n\t\tansible_args['verbose'] = verbose\n\n\tansible_runner = AnsibleRunner(ir_workspace, playbook_path,\n\t                               **ansible_args)\n\n\treturn ansible_runner.run()", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\textra_vars = extra_vars or {}\n\tif verbose and verbose != 0:\n\t\tansible_args['-v'] = verbose\n\tif extra_vars:\n\t\tansible_args['-e'] = extra_vars\n\treturn ir_workspace.execute(\n\t\t('ansible-playbook', playbook_path),\n\t\t**ansible_args\n\t)", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\tfrom ansible.cli.playbook import PlaybookCLI\n\tfrom ansible.inventory.manager import InventoryManager\n\tfrom ansible.vars.manager import VariableManager\n\tfrom ansible.utils.display import Display\n\n\tdisplay = Display()\n\n\t# if verbose is not provided, use the default from the plugin.\n\tif verbose is None:\n\t\tverbose = ir_plugin.get_config_value('verbose')\n\n\tansible_args = ansible_args or {}\n\tansible_args.update({\n\t\t'v': verbose,\n\t\t'i': os.path.join(ir_workspace.path, 'hosts.ini'),\n\t\t'e': os.path.join(ir_workspace.path, 'vars.yml'),\n\t\t'c': os.path.join(ir_workspace.path, 'credentials.yml'),\n\t})\n\n\tinventory_manager = InventoryManager(loader=ir_workspace.loader,\n\t                                     sources=ir_workspace.get_hosts())\n\tvariable_manager = VariableManager(loader=ir_workspace.loader,\n\t                                   inventory=inventory_manager)\n\n\tif extra_vars is not None:\n\t\t", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tverbose = verbose or ansible_args.get('verbose', 0)\n\n\tansible_args.update({\n\t\t'verbose': verbose,\n\t\t'extra-vars': extra_vars,\n\t\t'playbook': playbook_path,\n\t})\n\n\tir_workspace.execute_ansible(playbook_path, **ansible_args)", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tfrom infrared.cli import AnsiblePlaybookCLI\n\n\tcmd = AnsiblePlaybookCLI(\n\t\tworkspace=ir_workspace,\n\t\tplugin=ir_plugin,\n\t\tplaybook_path=playbook_path,\n\t\tverbose=verbose,\n\t\textra_vars=extra_vars,\n\t\tansible_args=ansible_args\n\t)\n\treturn cmd()", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tenv = {}\n\tif extra_vars:\n\t\tenv['EXTRA_VARS'] = extra_vars\n\tif ansible_args:\n\t\tenv['ANSIBLE_ARGS'] = ansible_args\n\treturn ir_workspace.invoke(\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t\tir_plugin.path,\n\t\tenv=env,\n\t\tverbose=verbose,\n\t)", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tfrom infrared.lib.ansible_wrapper import AnsibleWrapper\n\tansible_wrapper = AnsibleWrapper(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n\treturn ansible_wrapper.run()", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport sys\n\tfrom infrared.utils import ansible_run\n\n\tansible_args = ansible_args or {}\n\tansible_args['playbook_path'] = os.path.abspath(playbook_path)\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['verbose'] = verbose\n\tansible_args['extra_env'] = {\n\t\t'ANSIBLE_ROLES_PATH': os.path.join(os.getcwd(), 'roles'),\n\t}\n\tansible_args['inventory_path'] = os.path.join(\n\t\tos.getcwd(), 'inventory')\n\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['extra_vars']['workspace'] = ir_workspace.workspace_dir\n\tansible_args['extra_vars']['plugin'] = ir_plugin\n\tansible_args['extra_vars']['plugin_dir'] = os.path.abspath(\n\t\tos.path.dirname(ir_plugin.plugin_file))\n\tansible_args['extra_vars']['plugin_name'] = os.path.basename(\n\t\tir_plugin.plugin_file)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom distutils.spawn import find_executable\n\n\tif find_executable('ansible-playbook') is None:\n\t\traise Exception('ansible-playbook not found in PATH')\n\n\tansible_args = ansible_args or {}\n\tansible_args.setdefault('verbosity', verbose)\n\tansible_args.setdefault('extra_vars', extra_vars)\n\n\tenv = os.environ.copy()\n\tenv['ANSIBLE_FORCE_COLOR'] = '1'\n\n\targs = ['ansible-playbook']\n\tif 'verbosity' in ansible_args:\n\t\targs.append('-%sv' % ansible_args['verbosity'])\n\t\tdel ansible_args['verbosity']\n\n\tif 'extra_vars' in ansible_args:\n\t\tfor key, val in ansible_args['extra_vars'].iteritems():\n\t\t\targs.append('-e')\n\t\t\targs.append('%s=%s' % (key, val))\n\t\tdel ansible_args['extra_vars']\n\n\targs.extend(ansible_args.keys())\n\tfor value in ansible_args.values():\n\t\targs.", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport sys\n\timport ansible_runner\n\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\tif verbose:\n\t\tansible_args['verbose'] = verbose\n\n\tif extra_vars:\n\t\tansible_args['extra_vars'] = extra_vars\n\n\tproject_dir = ir_workspace.workspace_path\n\tplaybook_path = os.path.join(project_dir, playbook_path)\n\trunner = ansible_runner.run(private_data_dir=ir_plugin.tmp_path,\n\t                            playbook=playbook_path, **ansible_args)\n\n\tsys.exit(runner.rc)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tfrom infrared.core import AnsibleRunner\n\n\t# Build the extra-vars dict.\n\textra_vars = extra_vars or {}\n\n\tansible_args = ansible_args or {}\n\n\tansible_args['extra_vars'] = extra_vars\n\n\tif verbose:\n\t\tansible_args['verbose'] = verbose\n\n\tansible_runner = AnsibleRunner(ir_workspace, playbook_path,\n\t                               **ansible_args)\n\n\treturn ansible_runner.run()", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\textra_vars = extra_vars or {}\n\tif verbose and verbose != 0:\n\t\tansible_args['-v'] = verbose\n\tif extra_vars:\n\t\tansible_args['-e'] = extra_vars\n\treturn ir_workspace.execute(\n\t\t('ansible-playbook', playbook_path),\n\t\t**ansible_args\n\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\tfrom ansible.cli.playbook import PlaybookCLI\n\tfrom ansible.inventory.manager import InventoryManager\n\tfrom ansible.vars.manager import VariableManager\n\tfrom ansible.utils.display import Display\n\n\tdisplay = Display()\n\n\t# if verbose is not provided, use the default from the plugin.\n\tif verbose is None:\n\t\tverbose = ir_plugin.get_config_value('verbose')\n\n\tansible_args = ansible_args or {}\n\tansible_args.update({\n\t\t'v': verbose,\n\t\t'i': os.path.join(ir_workspace.path, 'hosts.ini'),\n\t\t'e': os.path.join(ir_workspace.path, 'vars.yml'),\n\t\t'c': os.path.join(ir_workspace.path, 'credentials.yml'),\n\t})\n\n\tinventory_manager = InventoryManager(loader=ir_workspace.loader,\n\t                                     sources=ir_workspace.get_hosts())\n\tvariable_manager = VariableManager(loader=ir_workspace.loader,\n\t                                   inventory=inventory_manager)\n\n\tif extra_vars is not None:\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tverbose = verbose or ansible_args.get('verbose', 0)\n\n\tansible_args.update({\n\t\t'verbose': verbose,\n\t\t'extra-vars': extra_vars,\n\t\t'playbook': playbook_path,\n\t})\n\n\tir_workspace.execute_ansible(playbook_path, **ansible_args)", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tfrom infrared.cli import AnsiblePlaybookCLI\n\n\tcmd = AnsiblePlaybookCLI(\n\t\tworkspace=ir_workspace,\n\t\tplugin=ir_plugin,\n\t\tplaybook_path=playbook_path,\n\t\tverbose=verbose,\n\t\textra_vars=extra_vars,\n\t\tansible_args=ansible_args\n\t)\n\treturn cmd()", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tenv = {}\n\tif extra_vars:\n\t\tenv['EXTRA_VARS'] = extra_vars\n\tif ansible_args:\n\t\tenv['ANSIBLE_ARGS'] = ansible_args\n\treturn ir_workspace.invoke(\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t\tir_plugin.path,\n\t\tenv=env,\n\t\tverbose=verbose,\n\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tfrom infrared.lib.ansible_wrapper import AnsibleWrapper\n\tansible_wrapper = AnsibleWrapper(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n\treturn ansible_wrapper.run()", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport sys\n\tfrom infrared.utils import ansible_run\n\n\tansible_args = ansible_args or {}\n\tansible_args['playbook_path'] = os.path.abspath(playbook_path)\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['verbose'] = verbose\n\tansible_args['extra_env'] = {\n\t\t'ANSIBLE_ROLES_PATH': os.path.join(os.getcwd(), 'roles'),\n\t}\n\tansible_args['inventory_path'] = os.path.join(\n\t\tos.getcwd(), 'inventory')\n\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['extra_vars']['workspace'] = ir_workspace.workspace_dir\n\tansible_args['extra_vars']['plugin'] = ir_plugin\n\tansible_args['extra_vars']['plugin_dir'] = os.path.abspath(\n\t\tos.path.dirname(ir_plugin.plugin_file))\n\tansible_args['extra_vars']['plugin_name'] = os.path.basename(\n\t\tir_plugin.plugin_file)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom distutils.spawn import find_executable\n\n\tif find_executable('ansible-playbook') is None:\n\t\traise Exception('ansible-playbook not found in PATH')\n\n\tansible_args = ansible_args or {}\n\tansible_args.setdefault('verbosity', verbose)\n\tansible_args.setdefault('extra_vars', extra_vars)\n\n\tenv = os.environ.copy()\n\tenv['ANSIBLE_FORCE_COLOR'] = '1'\n\n\targs = ['ansible-playbook']\n\tif 'verbosity' in ansible_args:\n\t\targs.append('-%sv' % ansible_args['verbosity'])\n\t\tdel ansible_args['verbosity']\n\n\tif 'extra_vars' in ansible_args:\n\t\tfor key, val in ansible_args['extra_vars'].iteritems():\n\t\t\targs.append('-e')\n\t\t\targs.append('%s=%s' % (key, val))\n\t\tdel ansible_args['extra_vars']\n\n\targs.extend(ansible_args.keys())\n\tfor value in ansible_args.values():\n\t\targs.", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport sys\n\timport ansible_runner\n\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\tif verbose:\n\t\tansible_args['verbose'] = verbose\n\n\tif extra_vars:\n\t\tansible_args['extra_vars'] = extra_vars\n\n\tproject_dir = ir_workspace.workspace_path\n\tplaybook_path = os.path.join(project_dir, playbook_path)\n\trunner = ansible_runner.run(private_data_dir=ir_plugin.tmp_path,\n\t                            playbook=playbook_path, **ansible_args)\n\n\tsys.exit(runner.rc)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible_runner\n\tfrom ansible_runner.config.runner import RunnerConfig\n\tfrom ansible_runner.runner import Runner\n\n\t# Create the default config\n\tconfig = RunnerConfig(\n\t\tproject_dir=ir_workspace.get_project_dir(),\n\t\tinventory=ir_workspace.get_inventory_path(),\n\t\textravars=vars_dict,\n\t\thost_pattern=ir_plugin.get_config_option('host_pattern'),\n\t\tmodule_name=ir_plugin.get_config_option('module_name'),\n\t\tmodule_args=ir_plugin.get_config_option('module_args'),\n\t\tmodule_kwargs=ir_plugin.get_config_option('module_kwargs'),\n\t\tskip_tags=ir_plugin.get_config_option('skip_tags'),\n\t\tverbosity=ir_plugin.get_config_option('verbosity'),\n\t\tcheck=ir_plugin.get_config_option('check'),\n\t\tdiff=ir_plugin.get_config_option('diff'),\n\t\tlisthosts=ir_plugin.get_config_option('listhosts'),\n\t\tlisttasks=ir_plugin.get_config_option('listtasks'),\n\t", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport os\n\tfrom infrared.utils.ansible import run_ansible_playbook\n\n\tansible_playbook = os.path.join(\n\tir_plugin.get_plugin_path(),\n\t'playbook.yml')\n\n\treturn run_ansible_playbook(\n\tansible_playbook,\n\tvars_dict,\n\tcli_args,\n\tir_workspace)\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli import CLI\n\tfrom ansible.cli.adhoc import AdHocCLI\n\tCLI.playbooks = CLI.module_chooser(AdHocCLI.playbooks)\n\toptions = cli_args.copy()\n\toptions.append('--extra-vars')\n\toptions.append('{%s}' % json.dumps(vars_dict))\n\toptions.append(ir_workspace.get_workspace_path())\n\tcli = CLI(options=options)\n\treturn cli.run()\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tansible_cli = cli_args.get('ansible_cli')\n\tif not ansible_cli:\n\t\traise Exception(\"ansible_cli is not set\")\n\tansible_playbook_path = cli_args.get('ansible_playbook_path')\n\tif not ansible_playbook_path:\n\t\traise Exception(\"ansible_playbook_path is not set\")\n\tansible_vars_path = vars_dict.get('ansible_vars_path')\n\tvars_files = vars_dict.get('vars_files')\n\textra_vars = vars_dict.get('extra_vars')\n\tlimit = vars_dict.get('limit')\n\tcheck = vars_dict.get('check')\n\tverbosity = vars_dict.get('verbosity')\n\tinventory = vars_dict.get('inventory')\n\tskip_tags = vars_dict.get('skip_tags')\n\tstart_at_task = vars_dict.get('start_at_task')\n\tstop_at_task = vars_dict.get('stop_at_task')\n\tskip_tags = vars_dict.get('skip_tags')\n\tonly_tags = vars_dict.get('only_", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli.playbook import PlaybookCLI\n\tfrom ansible.inventory.manager import InventoryManager\n\tfrom ansible.vars.manager import VariableManager\n\tfrom ansible.executor.playbook_executor import PlaybookExecutor\n\tfrom ansible import context\n\tfrom ansible.plugins.loader import plugin_loader\n\timport sys\n\tfrom ansible import utils\n\tfrom ansible import errors\n\tfrom ansible import callbacks\n\timport subprocess\n\n\tplaybook_path = ir_plugin.plugin_path + \"/playbook.yml\"\n\tsys.argv = [\"ansible-playbook\", \"--flush-cache\", \"-vvvv\", playbook_path]\n\n\tcontext.CLIARGS = utils.parse_cli_args(sys.argv, cli_args)\n\tcontext.CLIARGS.tags = context.CLIARGS.tags or ['all']\n\n\tstats = callbacks.AggregateStats()\n\tplaybook_cb = callbacks.PlaybookCallbacks(verbose=utils.VERBOSITY)\n\tresults_callback = callbacks.JsonCallback(\n\t\tout_file=None,\n\t\tcallback_whitelist=[\"runner_on_ok\",", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli.playbook import PlaybookCLI\n\tfrom ansible.parsing.dataloader import DataLoader\n\tfrom ansible.vars.manager import VariableManager\n\tfrom ansible.inventory.manager import InventoryManager\n\tfrom ansible.playbook.play import Play\n\tfrom ansible.executor.task_queue_manager import TaskQueueManager\n\tfrom ansible.plugins.callback import CallbackBase\n\n\tloader = DataLoader()\n\tpasswords = dict()\n\n\tinventory = InventoryManager(loader=loader, sources=cli_args.inventory)\n\n\tvariable_manager = VariableManager(loader=loader, inventory=inventory)\n\tvariable_manager.extra_vars = vars_dict\n\n\tplay_source = dict(\n\t\tname=\"deploy\",\n\t\thosts=\"all\",\n\t\ttasks=[{'action': {'module': 'ping'}}]\n\t)\n\tplay = Play().load(play_source, variable_manager=variable_manager, loader=loader)\n\n\tresults_callback = ResultsCollector()\n\n\trunner = TaskQueueManager(\n\t\tinventory=inventory,\n\t\tvariable_manager=variable_manager,\n\t\t", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tansible_runner = cli_args['ansible_runner']\n\textra_vars = dict(vars_dict)\n\textra_vars['ir_workspace'] = ir_workspace\n\textra_vars['ir_plugin'] = ir_plugin\n\textra_vars['ir_plugin_name'] = ir_plugin.name\n\textra_vars['ir_plugin_path'] = ir_plugin.path\n\textra_vars['ir_plugin_version'] = ir_plugin.version\n\textra_vars['ir_plugin_base_dir'] = ir_plugin.base_dir\n\n\tansible_config_path = cli_args['ansible_config_path']\n\tansible_config = None\n\tif ansible_config_path:\n\t\tansible_config = os.path.join(ansible_config_path, 'ansible.cfg')\n\t\tansible_runner.config.environ.update(ANSIBLE_CONFIG=ansible_config)\n\t\textra_vars['ansible_config'] = ansible_config\n\n\tansible_runner.environ.update(\n\t\tANSIBLE_HOST_KEY_CHECKING='False',\n\t)\n\tansible_runner.environ.update(\n\t\tANSIBLE_", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible_runner\n\tfrom os.path import join\n\n\tlogger = ir_plugin.get_logger()\n\tlogger.info(\"Running playbook\")\n\n\tworking_dir = join(ir_workspace.workspace_path, 'ansible')\n\n\trunner = ansible_runner.run(playbook='ansible/playbook.yml',\n\t                            inventory='ansible/hosts',\n\t                            extravars=vars_dict,\n\t                            playbook_dir=working_dir,\n\t                            verbosity=3,\n\t                            suppress_log_file=True,\n\t                            suppress_env_file=False,\n\t                            suppress_plugin_warnings=True,\n\t                            suppress_unreachable=True,\n\t                            suppress_diff=False,\n\t                            cli_args=cli_args)\n\n\tresults = runner.results\n\n\treturn results\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli import CLI\n\tfrom ansible.inventory import Inventory\n\tfrom ansible.playbook import Playbook\n\n\targs = cli_args + ['--extra-vars', json.dumps(vars_dict)]\n\tcli = CLI(args=args)\n\tplaybook = Playbook.load(\n\t\tplaybook_path=ir_workspace.playbook_path,\n\t\tinventory=Inventory(host_list=cli.options.inventory),\n\t\tvariable_manager=cli.variable_manager,\n\t\tpasswords=cli.passwords,\n\t)\n\treturn playbook.run()", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom infrared.utils import ansible\n\tfrom infrared.exceptions import InfraredError\n\tfrom infrared.utils.log import logger\n\n\tpb_args = list(cli_args)\n\tvars_file_value = vars_dict.get('vars_file', None)\n\tif vars_file_value:\n\t\tpb_args += ['--vars-file', vars_file_value]\n\n\tlogger.debug('running playbook with command line args %s', pb_args)\n\tlogger.debug('running playbook with vars_dict %s', vars_dict)\n\tplaybook_result = ansible.run_playbook(\n\t\tpb_args,\n\t\tvars_dict,\n\t\tir_workspace.get_playbook_dir(),\n\t\tir_workspace.get_ansible_extra_vars_dir(),\n\t\tir_workspace.get_ansible_extra_vars_file(),\n\t\tir_workspace.get_inventory_file(),\n\t\tir_workspace.get_ssh_config_file(),\n\t\tir_workspace.get_ansible_config_file(),\n\t\tir_workspace.get_ansible_cache_dir(),\n\t\tir_workspace.get_ans"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible_runner\n\tfrom ansible_runner.config.runner import RunnerConfig\n\tfrom ansible_runner.runner import Runner\n\n\t# Create the default config\n\tconfig = RunnerConfig(\n\t\tproject_dir=ir_workspace.get_project_dir(),\n\t\tinventory=ir_workspace.get_inventory_path(),\n\t\textravars=vars_dict,\n\t\thost_pattern=ir_plugin.get_config_option('host_pattern'),\n\t\tmodule_name=ir_plugin.get_config_option('module_name'),\n\t\tmodule_args=ir_plugin.get_config_option('module_args'),\n\t\tmodule_kwargs=ir_plugin.get_config_option('module_kwargs'),\n\t\tskip_tags=ir_plugin.get_config_option('skip_tags'),\n\t\tverbosity=ir_plugin.get_config_option('verbosity'),\n\t\tcheck=ir_plugin.get_config_option('check'),\n\t\tdiff=ir_plugin.get_config_option('diff'),\n\t\tlisthosts=ir_plugin.get_config_option('listhosts'),\n\t\tlisttasks=ir_plugin.get_config_option('listtasks'),\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport os\n\tfrom infrared.utils.ansible import run_ansible_playbook\n\n\tansible_playbook = os.path.join(\n\tir_plugin.get_plugin_path(),\n\t'playbook.yml')\n\n\treturn run_ansible_playbook(\n\tansible_playbook,\n\tvars_dict,\n\tcli_args,\n\tir_workspace)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli import CLI\n\tfrom ansible.cli.adhoc import AdHocCLI\n\tCLI.playbooks = CLI.module_chooser(AdHocCLI.playbooks)\n\toptions = cli_args.copy()\n\toptions.append('--extra-vars')\n\toptions.append('{%s}' % json.dumps(vars_dict))\n\toptions.append(ir_workspace.get_workspace_path())\n\tcli = CLI(options=options)\n\treturn cli.run()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tansible_cli = cli_args.get('ansible_cli')\n\tif not ansible_cli:\n\t\traise Exception(\"ansible_cli is not set\")\n\tansible_playbook_path = cli_args.get('ansible_playbook_path')\n\tif not ansible_playbook_path:\n\t\traise Exception(\"ansible_playbook_path is not set\")\n\tansible_vars_path = vars_dict.get('ansible_vars_path')\n\tvars_files = vars_dict.get('vars_files')\n\textra_vars = vars_dict.get('extra_vars')\n\tlimit = vars_dict.get('limit')\n\tcheck = vars_dict.get('check')\n\tverbosity = vars_dict.get('verbosity')\n\tinventory = vars_dict.get('inventory')\n\tskip_tags = vars_dict.get('skip_tags')\n\tstart_at_task = vars_dict.get('start_at_task')\n\tstop_at_task = vars_dict.get('stop_at_task')\n\tskip_tags = vars_dict.get('skip_tags')\n\tonly_tags = vars_dict.get('only_", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli.playbook import PlaybookCLI\n\tfrom ansible.inventory.manager import InventoryManager\n\tfrom ansible.vars.manager import VariableManager\n\tfrom ansible.executor.playbook_executor import PlaybookExecutor\n\tfrom ansible import context\n\tfrom ansible.plugins.loader import plugin_loader\n\timport sys\n\tfrom ansible import utils\n\tfrom ansible import errors\n\tfrom ansible import callbacks\n\timport subprocess\n\n\tplaybook_path = ir_plugin.plugin_path + \"/playbook.yml\"\n\tsys.argv = [\"ansible-playbook\", \"--flush-cache\", \"-vvvv\", playbook_path]\n\n\tcontext.CLIARGS = utils.parse_cli_args(sys.argv, cli_args)\n\tcontext.CLIARGS.tags = context.CLIARGS.tags or ['all']\n\n\tstats = callbacks.AggregateStats()\n\tplaybook_cb = callbacks.PlaybookCallbacks(verbose=utils.VERBOSITY)\n\tresults_callback = callbacks.JsonCallback(\n\t\tout_file=None,\n\t\tcallback_whitelist=[\"runner_on_ok\",", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli.playbook import PlaybookCLI\n\tfrom ansible.parsing.dataloader import DataLoader\n\tfrom ansible.vars.manager import VariableManager\n\tfrom ansible.inventory.manager import InventoryManager\n\tfrom ansible.playbook.play import Play\n\tfrom ansible.executor.task_queue_manager import TaskQueueManager\n\tfrom ansible.plugins.callback import CallbackBase\n\n\tloader = DataLoader()\n\tpasswords = dict()\n\n\tinventory = InventoryManager(loader=loader, sources=cli_args.inventory)\n\n\tvariable_manager = VariableManager(loader=loader, inventory=inventory)\n\tvariable_manager.extra_vars = vars_dict\n\n\tplay_source = dict(\n\t\tname=\"deploy\",\n\t\thosts=\"all\",\n\t\ttasks=[{'action': {'module': 'ping'}}]\n\t)\n\tplay = Play().load(play_source, variable_manager=variable_manager, loader=loader)\n\n\tresults_callback = ResultsCollector()\n\n\trunner = TaskQueueManager(\n\t\tinventory=inventory,\n\t\tvariable_manager=variable_manager,\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tansible_runner = cli_args['ansible_runner']\n\textra_vars = dict(vars_dict)\n\textra_vars['ir_workspace'] = ir_workspace\n\textra_vars['ir_plugin'] = ir_plugin\n\textra_vars['ir_plugin_name'] = ir_plugin.name\n\textra_vars['ir_plugin_path'] = ir_plugin.path\n\textra_vars['ir_plugin_version'] = ir_plugin.version\n\textra_vars['ir_plugin_base_dir'] = ir_plugin.base_dir\n\n\tansible_config_path = cli_args['ansible_config_path']\n\tansible_config = None\n\tif ansible_config_path:\n\t\tansible_config = os.path.join(ansible_config_path, 'ansible.cfg')\n\t\tansible_runner.config.environ.update(ANSIBLE_CONFIG=ansible_config)\n\t\textra_vars['ansible_config'] = ansible_config\n\n\tansible_runner.environ.update(\n\t\tANSIBLE_HOST_KEY_CHECKING='False',\n\t)\n\tansible_runner.environ.update(\n\t\tANSIBLE_", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible_runner\n\tfrom os.path import join\n\n\tlogger = ir_plugin.get_logger()\n\tlogger.info(\"Running playbook\")\n\n\tworking_dir = join(ir_workspace.workspace_path, 'ansible')\n\n\trunner = ansible_runner.run(playbook='ansible/playbook.yml',\n\t                            inventory='ansible/hosts',\n\t                            extravars=vars_dict,\n\t                            playbook_dir=working_dir,\n\t                            verbosity=3,\n\t                            suppress_log_file=True,\n\t                            suppress_env_file=False,\n\t                            suppress_plugin_warnings=True,\n\t                            suppress_unreachable=True,\n\t                            suppress_diff=False,\n\t                            cli_args=cli_args)\n\n\tresults = runner.results\n\n\treturn results\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli import CLI\n\tfrom ansible.inventory import Inventory\n\tfrom ansible.playbook import Playbook\n\n\targs = cli_args + ['--extra-vars', json.dumps(vars_dict)]\n\tcli = CLI(args=args)\n\tplaybook = Playbook.load(\n\t\tplaybook_path=ir_workspace.playbook_path,\n\t\tinventory=Inventory(host_list=cli.options.inventory),\n\t\tvariable_manager=cli.variable_manager,\n\t\tpasswords=cli.passwords,\n\t)\n\treturn playbook.run()", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom infrared.utils import ansible\n\tfrom infrared.exceptions import InfraredError\n\tfrom infrared.utils.log import logger\n\n\tpb_args = list(cli_args)\n\tvars_file_value = vars_dict.get('vars_file', None)\n\tif vars_file_value:\n\t\tpb_args += ['--vars-file', vars_file_value]\n\n\tlogger.debug('running playbook with command line args %s', pb_args)\n\tlogger.debug('running playbook with vars_dict %s', vars_dict)\n\tplaybook_result = ansible.run_playbook(\n\t\tpb_args,\n\t\tvars_dict,\n\t\tir_workspace.get_playbook_dir(),\n\t\tir_workspace.get_ansible_extra_vars_dir(),\n\t\tir_workspace.get_ansible_extra_vars_file(),\n\t\tir_workspace.get_inventory_file(),\n\t\tir_workspace.get_ssh_config_file(),\n\t\tir_workspace.get_ansible_config_file(),\n\t\tir_workspace.get_ansible_cache_dir(),\n\t\tir_workspace.get_ans", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalue_to_type = {\n\t\t\"virsh\": {\n\t\t\t\"debug\": int,\n\t\t\t\"trace\": int,\n\t\t\t\"quiet\": int,\n\t\t\t\"readonly\": int,\n\t\t\t\"connect\": str,\n\t\t\t\"connect_timeout\": int,\n\t\t\t\"daemon\": int,\n\t\t\t\"daemon_pid\": int,\n\t\t\t\"daemon_timeout\": int,\n\t\t\t\"debug_appliance\": int,\n\t\t\t\"quiet_appliance\": int,\n\t\t\t\"readonly_appliance\": int,\n\t\t\t\"session\": int,\n\t\t\t\"session_timeout\": int,\n\t\t\t\"virttype\": str,\n\t\t\t\"virtxen\": str,\n\t\t},\n\t\t\"ospd\": {\n\t\t\t\"debug\": int,\n\t\t\t\"trace\": int,\n\t\t\t\"quiet\": int,\n\t\t\t\"readonly\": int,\n\t\t\t\"config_file\": str,\n\t\t\t\"config_file_path\": str,\n\t\t\t\"config_file_path_name\": str,\n\t\t\t\"config_file_path_prefix\": str,\n\t\t\t\"config_file_name\": str,\n\t\t\t\"config_file_prefix\": str,\n\t\t\t\"config_file_suffix\": str,\n\t\t\t\"pid_file\": str,\n\t\t\t\"pid_file_path\": str,\n\t\t\t\"pid_file", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalue_to_type = {\n\t\t\"virsh\": {\n\t\t\t\"debug\": int,\n\t\t\t\"trace\": int,\n\t\t\t\"quiet\": int,\n\t\t\t\"readonly\": int,\n\t\t\t\"connect\": str,\n\t\t\t\"connect_timeout\": int,\n\t\t\t\"daemon\": int,\n\t\t\t\"daemon_pid\": int,\n\t\t\t\"daemon_timeout\": int,\n\t\t\t\"debug_appliance\": int,\n\t\t\t\"quiet_appliance\": int,\n\t\t\t\"readonly_appliance\": int,\n\t\t\t\"session\": int,\n\t\t\t\"session_timeout\": int,\n\t\t\t\"virttype\": str,\n\t\t\t\"virtxen\": str,\n\t\t},\n\t\t\"ospd\": {\n\t\t\t\"debug\": int,\n\t\t\t\"trace\": int,\n\t\t\t\"quiet\": int,\n\t\t\t\"readonly\": int,\n\t\t\t\"config_file\": str,\n\t\t\t\"config_file_path\": str,\n\t\t\t\"config_file_path_name\": str,\n\t\t\t\"config_file_path_prefix\": str,\n\t\t\t\"config_file_name\": str,\n\t\t\t\"config_file_prefix\": str,\n\t\t\t\"config_file_suffix\": str,\n\t\t\t\"pid_file\": str,\n\t\t\t\"pid_file_path\": str,\n\t\t\t\"pid_file", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin_spec.yml\")\n\t\n\twith open(plugin_spec_file) as f:\n\t\tplugin_spec = yaml.safe_load(f)\n\t\n\tflattened_dict = flatten_dict(plugin_spec)\n\t\n\treturn flattened_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_file_path = plugin_dir + \"/spec.yaml\"\n\twith open(spec_file_path) as f:\n\t\tspec_dict = yaml.load(f, Loader=yaml.FullLoader)\n\treturn spec_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tfrom os import path\n\timport yaml\n\twith open(path.join(plugin_dir, 'plugin.yaml')) as f:\n\t\tplugin_spec = yaml.load(f)\n\t\tif plugin_spec is None:\n\t\t\tplugin_spec = {}\n\t\tplugin_spec['plugin_dir'] = plugin_dir\n\treturn plugin_spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tfrom yaml import load\n\n\twith open(plugin_dir, 'r') as f:\n\t\tspec = load(f)\n\n\treturn spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport os\n\timport yaml\n\t\n\tplugin_spec_flatten = {}\n\t\n\t# read plugin_spec.yml file\n\twith open(f\"{plugin_dir}/plugin_spec.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_spec\"] = yaml.safe_load(f)\n\t\t\n\t# read plugin_spec.yml file\n\twith open(f\"{plugin_dir}/plugin_config.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_config\"] = yaml.safe_load(f)\n\t\n\t# read README.md file\n\twith open(f\"{plugin_dir}/README.md\") as f:\n\t\tplugin_spec_flatten[\"plugin_readme\"] = f.read()\n\t\n\t# read plugin_spec_schema.yml file\n\twith open(f\"{plugin_dir}/plugin_spec_schema.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_spec_schema\"] = yaml.safe_load(f)\n\t\n\t# read plugin_config_schema.yml file\n\twith open(f\"{plugin_dir}/plugin_config_schema.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_config", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\timport os\n\timport logging\n\tfrom pathlib import Path\n\n\t# Get the absolute path of the plugin directory.\n\tplugin_dir = os.path.abspath(plugin_dir)\n\n\t# Check if plugin_dir is a valid directory.\n\tif not os.path.isdir(plugin_dir):\n\t\tlogging.error(\"Error: Plugin directory not found.\")\n\t\treturn None\n\n\t# Get the path to the plugin_spec.yaml file.\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin_spec.yaml\")\n\n\t# Check if plugin_spec.yaml exists.\n\tif not os.path.isfile(plugin_spec_file):\n\t\tlogging.error(\"Error: Plugin spec file not found.\")\n\t\treturn None\n\n\t# Read the plugin spec file.\n\twith open(plugin_spec_file, \"r\") as f:\n\t\tplugin_spec = yaml.safe_load(f)\n\n\t# Check if the plugin spec file is valid.\n\tif plugin_spec is None:\n\t\tlogging.error(\"Error: Plugin spec file is invalid.\")\n\t\treturn None\n\n\t# Flatten the plugin spec dictionary.\n\tflattened_plugin_spec = {}\n\tfor key, value in plugin_spec.items():", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\twith open(plugin_dir, 'r') as f:\n\t\tplugin_spec = yaml.load(f, Loader=yaml.FullLoader)\n\n\tflatten_dict = {}\n\tflatten_dict['name'] = plugin_spec['name']\n\n\tif 'version' in plugin_spec:\n\t\tflatten_dict['version'] = plugin_spec['version']\n\n\tif 'description' in plugin_spec:\n\t\tflatten_dict['description'] = plugin_spec['description']\n\n\tif 'documentation' in plugin_spec:\n\t\tflatten_dict['documentation'] = plugin_spec['documentation']\n\n\tif 'maintainers' in plugin_spec:\n\t\tflatten_dict['maintainers'] = plugin_spec['maintainers']\n\n\tif 'tags' in plugin_spec:\n\t\tflatten_dict['tags'] = plugin_spec['tags']\n\n\tif 'dependencies' in plugin_spec:\n\t\tflatten_dict['dependencies'] = plugin_spec['dependencies']\n\n\treturn flatten_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec_full_path = os.path.join(plugin_dir, 'plugin_spec.yaml')\n\twith open(plugin_spec_full_path, 'r') as f:\n\t\tplugin_spec_dict = yaml.load(f)\n\n\t# Flatten the dictionary.\n\tplugin_spec_flatten_dict = {}\n\tfor key, value in plugin_spec_dict.items():\n\t\tplugin_spec_flatten_dict[key] = value\n\n\treturn plugin_spec_flatten_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\tspec = {}\n\twith open(plugin_dir + \"/plugin.yaml\", \"r\") as f:\n\t\tspec = yaml.load(f)\n\twith open(plugin_dir + \"/plugin_config.yaml\", \"r\") as f:\n\t\tspec.update(yaml.load(f))\n\treturn spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\twith open(plugin_dir + \"/plugin.yaml\") as f:\n\t\tplugin_spec = yaml.load(f)\n\t\treturn plugin_spec\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin_spec.yml\")\n\t\n\twith open(plugin_spec_file) as f:\n\t\tplugin_spec = yaml.safe_load(f)\n\t\n\tflattened_dict = flatten_dict(plugin_spec)\n\t\n\treturn flattened_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_file_path = plugin_dir + \"/spec.yaml\"\n\twith open(spec_file_path) as f:\n\t\tspec_dict = yaml.load(f, Loader=yaml.FullLoader)\n\treturn spec_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tfrom os import path\n\timport yaml\n\twith open(path.join(plugin_dir, 'plugin.yaml')) as f:\n\t\tplugin_spec = yaml.load(f)\n\t\tif plugin_spec is None:\n\t\t\tplugin_spec = {}\n\t\tplugin_spec['plugin_dir'] = plugin_dir\n\treturn plugin_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tfrom yaml import load\n\n\twith open(plugin_dir, 'r') as f:\n\t\tspec = load(f)\n\n\treturn spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport os\n\timport yaml\n\t\n\tplugin_spec_flatten = {}\n\t\n\t# read plugin_spec.yml file\n\twith open(f\"{plugin_dir}/plugin_spec.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_spec\"] = yaml.safe_load(f)\n\t\t\n\t# read plugin_spec.yml file\n\twith open(f\"{plugin_dir}/plugin_config.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_config\"] = yaml.safe_load(f)\n\t\n\t# read README.md file\n\twith open(f\"{plugin_dir}/README.md\") as f:\n\t\tplugin_spec_flatten[\"plugin_readme\"] = f.read()\n\t\n\t# read plugin_spec_schema.yml file\n\twith open(f\"{plugin_dir}/plugin_spec_schema.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_spec_schema\"] = yaml.safe_load(f)\n\t\n\t# read plugin_config_schema.yml file\n\twith open(f\"{plugin_dir}/plugin_config_schema.yml\") as f:\n\t\tplugin_spec_flatten[\"plugin_config", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\timport os\n\timport logging\n\tfrom pathlib import Path\n\n\t# Get the absolute path of the plugin directory.\n\tplugin_dir = os.path.abspath(plugin_dir)\n\n\t# Check if plugin_dir is a valid directory.\n\tif not os.path.isdir(plugin_dir):\n\t\tlogging.error(\"Error: Plugin directory not found.\")\n\t\treturn None\n\n\t# Get the path to the plugin_spec.yaml file.\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin_spec.yaml\")\n\n\t# Check if plugin_spec.yaml exists.\n\tif not os.path.isfile(plugin_spec_file):\n\t\tlogging.error(\"Error: Plugin spec file not found.\")\n\t\treturn None\n\n\t# Read the plugin spec file.\n\twith open(plugin_spec_file, \"r\") as f:\n\t\tplugin_spec = yaml.safe_load(f)\n\n\t# Check if the plugin spec file is valid.\n\tif plugin_spec is None:\n\t\tlogging.error(\"Error: Plugin spec file is invalid.\")\n\t\treturn None\n\n\t# Flatten the plugin spec dictionary.\n\tflattened_plugin_spec = {}\n\tfor key, value in plugin_spec.items():", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\twith open(plugin_dir, 'r') as f:\n\t\tplugin_spec = yaml.load(f, Loader=yaml.FullLoader)\n\n\tflatten_dict = {}\n\tflatten_dict['name'] = plugin_spec['name']\n\n\tif 'version' in plugin_spec:\n\t\tflatten_dict['version'] = plugin_spec['version']\n\n\tif 'description' in plugin_spec:\n\t\tflatten_dict['description'] = plugin_spec['description']\n\n\tif 'documentation' in plugin_spec:\n\t\tflatten_dict['documentation'] = plugin_spec['documentation']\n\n\tif 'maintainers' in plugin_spec:\n\t\tflatten_dict['maintainers'] = plugin_spec['maintainers']\n\n\tif 'tags' in plugin_spec:\n\t\tflatten_dict['tags'] = plugin_spec['tags']\n\n\tif 'dependencies' in plugin_spec:\n\t\tflatten_dict['dependencies'] = plugin_spec['dependencies']\n\n\treturn flatten_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec_full_path = os.path.join(plugin_dir, 'plugin_spec.yaml')\n\twith open(plugin_spec_full_path, 'r') as f:\n\t\tplugin_spec_dict = yaml.load(f)\n\n\t# Flatten the dictionary.\n\tplugin_spec_flatten_dict = {}\n\tfor key, value in plugin_spec_dict.items():\n\t\tplugin_spec_flatten_dict[key] = value\n\n\treturn plugin_spec_flatten_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\tspec = {}\n\twith open(plugin_dir + \"/plugin.yaml\", \"r\") as f:\n\t\tspec = yaml.load(f)\n\twith open(plugin_dir + \"/plugin_config.yaml\", \"r\") as f:\n\t\tspec.update(yaml.load(f))\n\treturn spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\twith open(plugin_dir + \"/plugin.yaml\") as f:\n\t\tplugin_spec = yaml.load(f)\n\t\treturn plugin_spec\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\timport os\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG_FILE property does not exist in os.environ, set it to self.config_file_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG_FILE' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG_FILE'] = self.config_file_path", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\timport os\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tenviron = os.environ\n\tif self.ansible_config_path:\n\t\tansible_config_path = self.ansible_config_path\n\telse:\n\t\tansible_config_path = 'ansible.cfg'\n\tenviron['ANSIBLE_CONFIG'] = ansible_config_path", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\timport os\n\tansible_config = os.environ.get('ANSIBLE_CONFIG')\n\tif ansible_config is None:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tos.environ[\"ANSIBLE_CONFIG\"]\n\texcept KeyError:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t    os.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\n    def run(self, *args, **kwargs):\n        \"\"\"\n        Synchronously executes the command.  Returns a tuple of (rc, stdout, stderr).\n\n        :rtype:  tuple\n        :return: (rc, stdout, stderr)\n        \"\"\"\n        kwargs.update(self.extra_env)\n\n        if self.ask_sudo_pass:\n            kwargs['ask_sudo_pass'] = True\n\n        if self.ask_vault_pass:\n            kwargs['ask_vault_pass'] = True\n\n        stdout, stderr = self.module.run_command(args, check_rc=False, environ_update=kwargs)\n\n        return stdout, stderr\n\n    def run_bg(self, *args, **kwargs):\n        \"\"\"\n        Asynchronously executes the command.  Returns a tuple of (p, stdout, stderr), where p is a\n        subprocess.Popen object.\n\n        :rtype:  tuple\n        :return: (p, stdout, stderr)\n        \"\"\"\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tansible_config_path = self.ansible_config_path\n\tansible_config_path_key = 'ANSIBLE_CONFIG'\n\tif ansible_config_path_key not in os.environ:\n\t\tos.environ[ansible_config_path_key] = ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config = self.ansible_config_path\n\tself.ansible_config_file = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tos.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\tos.environ.setdefault('ANSIBLE_CONFIG', self.ansible_config_path)"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\timport os\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG_FILE property does not exist in os.environ, set it to self.config_file_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG_FILE' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG_FILE'] = self.config_file_path", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\timport os\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tenviron = os.environ\n\tif self.ansible_config_path:\n\t\tansible_config_path = self.ansible_config_path\n\telse:\n\t\tansible_config_path = 'ansible.cfg'\n\tenviron['ANSIBLE_CONFIG'] = ansible_config_path", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\timport os\n\tansible_config = os.environ.get('ANSIBLE_CONFIG')\n\tif ansible_config is None:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tos.environ[\"ANSIBLE_CONFIG\"]\n\texcept KeyError:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t    os.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\n    def run(self, *args, **kwargs):\n        \"\"\"\n        Synchronously executes the command.  Returns a tuple of (rc, stdout, stderr).\n\n        :rtype:  tuple\n        :return: (rc, stdout, stderr)\n        \"\"\"\n        kwargs.update(self.extra_env)\n\n        if self.ask_sudo_pass:\n            kwargs['ask_sudo_pass'] = True\n\n        if self.ask_vault_pass:\n            kwargs['ask_vault_pass'] = True\n\n        stdout, stderr = self.module.run_command(args, check_rc=False, environ_update=kwargs)\n\n        return stdout, stderr\n\n    def run_bg(self, *args, **kwargs):\n        \"\"\"\n        Asynchronously executes the command.  Returns a tuple of (p, stdout, stderr), where p is a\n        subprocess.Popen object.\n\n        :rtype:  tuple\n        :return: (p, stdout, stderr)\n        \"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tansible_config_path = self.ansible_config_path\n\tansible_config_path_key = 'ANSIBLE_CONFIG'\n\tif ansible_config_path_key not in os.environ:\n\t\tos.environ[ansible_config_path_key] = ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config = self.ansible_config_path\n\tself.ansible_config_file = self.ansible_config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tos.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\tos.environ.setdefault('ANSIBLE_CONFIG', self.ansible_config_path)", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser('spec', help='specification tools')\n\tspec_parser.add_argument('-f', type=str, dest='spec_file', help='set the spec file')\n\n\tspec_subparser = spec_parser.add_subparsers(title='subcommands', dest='spec_subcommand')\n\n\t# Subparsers for spec\n\tspec_subparser.add_parser('list', help='list all specs')\n\n\tspec_subparser.add_parser('new', help='create a new spec')\n\n\tspec_subparser.add_parser('generate', help='generate the spec')\n\n\tspec_subparser.add_parser('validate', help='validate the spec')\n\n\tspec_subparser.add_parser('deploy', help='deploy the spec')\n\n\tspec_subparser.add_parser('delete', help='delete the spec')", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser(\"spec\", help=\"Manage spec files.\")\n\tspec_parser.add_argument(\"--show\", action=\"store_true\", help=\"Show spec files.\")\n\tspec_parser.add_argument(\"--edit\", action=\"store_true\", help=\"Edit spec files.\")\n\tspec_parser.add_argument(\"--validate\", action=\"store_true\", help=\"Validate spec files.\")\n\tspec_parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile spec files.\")\n\tspec_parser.set_defaults(func=self.spec)\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp='Generate a spec file from a python script.',\n\t\tdescription='Generate a spec file from a python script.',\n\t)\n\tspec_parser.add_argument(\n\t\t'script',\n\t\thelp='The python script to generate a spec file for.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-n',\n\t\t'--name',\n\t\thelp='The name of the spec file to output.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-o',\n\t\t'--outdir',\n\t\thelp='The directory to output the spec file to.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-c',\n\t\t'--class',\n\t\thelp='The class name of the spec file to output.',\n\t\tdest='class_name',\n\t)\n\tspec_parser.add_argument(\n\t\t'-p',\n\t\t'--path',\n\t\thelp='The directory to output the spec file to.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-d',\n\t\t'--doc',\n\t\thelp='The documentation for the", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp='spec related actions')\n\n\tparser.add_argument(\n\t\t'spec',\n\t\thelp='the spec to modify')\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_add_spec = subparsers.add_parser(\n\t\t'add',\n\t\thelp='add a spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--url',\n\t\thelp='the url to the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--branch',\n\t\thelp='the branch to use for the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--tag',\n\t\thelp='the tag to use for the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--commit',\n\t\thelp='the commit to use for the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--commit-range',\n\t\thelp='the commit range to use for the new spec')\n\n\tparser = subparsers.add_parser(\n\t\t'list',\n\t\thelp='list", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser('spec', help='Specification related operations')\n\tspec_subparsers = spec_parser.add_subparsers()\n\n\tspec_parser.set_defaults(func=lambda args: spec_parser.print_help())\n\n\tspec_new_parser = spec_subparsers.add_parser('new', help='Creates a specification file')\n\tspec_new_parser.add_argument('spec_file', help='The specification file to create')\n\tspec_new_parser.add_argument('-n', '--name', help='The name of the specification')\n\tspec_new_parser.add_argument('-v', '--version', help='The version of the specification')\n\tspec_new_parser.add_argument('-d', '--description', help='A description of the specification')\n\tspec_new_parser.add_argument('-x', '--schema', help='The schema to use for the specification')\n\tspec_new_parser.add_argument('-s', '--schema_version', help='The version of the schema to use for the specification')\n\tspec_new_parser.set_defaults(func=new_spec)\n\n\tspec_validate_parser", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\timport argparse\n\tspec_parser = root_subparsers.add_parser( \"spec\", help=\"Spec related commands.\" )\n\tspec_subparsers = spec_parser.add_subparsers( title=\"Spec commands\", dest=\"spec_command\", help=\"Spec related commands.\" )\n\n\t# spec add\n\tspec_add_parser = spec_subparsers.add_parser( \"add\", help=\"Add a spec from a path.\" )\n\tspec_add_parser.add_argument( \"spec_path\", help=\"Path to the spec to add.\" )\n\tspec_add_parser.add_argument( \"--namespace\", help=\"The namespace of the spec.\", default=\"default\" )\n\tspec_add_parser.add_argument( \"--name\", help=\"The name of the spec.\", default=None )\n\n\t# spec list\n\tspec_list_parser = spec_subparsers.add_parser( \"list\", help=\"List specs.\" )\n\tspec_list_parser.add_argument( \"--namespace\", help=\"The namespace of the spec.\", default=\"default\" )\n\tspec_list_parser.add_argument( \"--regex\", help=\"Regex to filter the specs with.\", default=\".*\" )\n\tspec_list", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser('extends',\n\t\t\t\t\t\t\t\t\t\tdescription='Extends the current spec.',\n\t\t\t\t\t\t\t\t\t\thelp='Extends the current spec.')\n\tparser.add_argument('--add',\n\t\t\t\t\t\tdest='add',\n\t\t\t\t\t\tnargs='+',\n\t\t\t\t\t\thelp='adds one or more specs')\n\tparser.add_argument('--delete',\n\t\t\t\t\t\tdest='delete',\n\t\t\t\t\t\tnargs='+',\n\t\t\t\t\t\thelp='deletes one or more specs')\n\tparser.add_argument('--list',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\thelp='lists all specs')\n\tparser.add_argument('--list-extended',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\thelp='lists all specs and their extended specs')\n\tparser.set_defaults(func=self.extends)\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser('spec')\n\tspec_parser.add_argument('--target', choices=['cli', 'yaml', 'json'], default='yaml')\n\tspec_parser.add_argument('-v', '--verbose', action='store_true')\n\tspec_parser.add_argument('--no-sort', action='store_true')\n\tspec_parser.add_argument('--raw', action='store_true')\n\tspec_parser.add_argument('--hide-empty', action='store_true')\n\tspec_parser.add_argument('--hide-private', action='store_true')\n\tspec_parser.add_argument('--hide-hidden', action='store_true')\n\tspec_parser.add_argument('--hide-type', action='store_true')\n\tspec_parser.add_argument('--hide-default', action='store_true')\n\tspec_parser.add_argument('--hide-desc', action='store_true')\n\tspec_parser.add_argument('--hide-required', action='store_true')\n\tspec_parser.add_argument('--hide-empty-required', action='store_true')\n\tspec_parser.add", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparsers = {}\n\troot_subparsers.add_parser(\"spec\", help=\"Specification related commands.\")\n\n\tparsers['spec'] = root_subparsers.add_parser(\"spec\", help=\"Specification related commands.\")\n\tparsers['specsubparser'] = parsers['spec'].add_subparsers(help=\"Subcommand help.\")\n\n\tparsers['specsubparser'].add_parser(\"add\", help=\"Add a new specification.\")\n\n\tparsers['specsubparser'].add_parser(\"remove\", help=\"Remove a specification.\")\n\n\tparsers['specsubparser'].add_parser(\"list\", help=\"List all specifications.\")\n\n\tparsers['specsubparser'].add_parser(\"get\", help=\"Get a specification.\")", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_subparser = root_subparsers.add_parser('spec')\n\tspec_subparser.add_argument('spec_file', type=str, help='the spec file to create')\n\tspec_subparser.set_defaults(func=self.generate_spec)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser('spec', help='specification tools')\n\tspec_parser.add_argument('-f', type=str, dest='spec_file', help='set the spec file')\n\n\tspec_subparser = spec_parser.add_subparsers(title='subcommands', dest='spec_subcommand')\n\n\t# Subparsers for spec\n\tspec_subparser.add_parser('list', help='list all specs')\n\n\tspec_subparser.add_parser('new', help='create a new spec')\n\n\tspec_subparser.add_parser('generate', help='generate the spec')\n\n\tspec_subparser.add_parser('validate', help='validate the spec')\n\n\tspec_subparser.add_parser('deploy', help='deploy the spec')\n\n\tspec_subparser.add_parser('delete', help='delete the spec')", "is_pass": true, "return_code": 0}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser(\"spec\", help=\"Manage spec files.\")\n\tspec_parser.add_argument(\"--show\", action=\"store_true\", help=\"Show spec files.\")\n\tspec_parser.add_argument(\"--edit\", action=\"store_true\", help=\"Edit spec files.\")\n\tspec_parser.add_argument(\"--validate\", action=\"store_true\", help=\"Validate spec files.\")\n\tspec_parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile spec files.\")\n\tspec_parser.set_defaults(func=self.spec)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp='Generate a spec file from a python script.',\n\t\tdescription='Generate a spec file from a python script.',\n\t)\n\tspec_parser.add_argument(\n\t\t'script',\n\t\thelp='The python script to generate a spec file for.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-n',\n\t\t'--name',\n\t\thelp='The name of the spec file to output.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-o',\n\t\t'--outdir',\n\t\thelp='The directory to output the spec file to.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-c',\n\t\t'--class',\n\t\thelp='The class name of the spec file to output.',\n\t\tdest='class_name',\n\t)\n\tspec_parser.add_argument(\n\t\t'-p',\n\t\t'--path',\n\t\thelp='The directory to output the spec file to.',\n\t)\n\tspec_parser.add_argument(\n\t\t'-d',\n\t\t'--doc',\n\t\thelp='The documentation for the", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp='spec related actions')\n\n\tparser.add_argument(\n\t\t'spec',\n\t\thelp='the spec to modify')\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_add_spec = subparsers.add_parser(\n\t\t'add',\n\t\thelp='add a spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--url',\n\t\thelp='the url to the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--branch',\n\t\thelp='the branch to use for the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--tag',\n\t\thelp='the tag to use for the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--commit',\n\t\thelp='the commit to use for the new spec')\n\n\tparser_add_spec.add_argument(\n\t\t'--commit-range',\n\t\thelp='the commit range to use for the new spec')\n\n\tparser = subparsers.add_parser(\n\t\t'list',\n\t\thelp='list", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser('spec', help='Specification related operations')\n\tspec_subparsers = spec_parser.add_subparsers()\n\n\tspec_parser.set_defaults(func=lambda args: spec_parser.print_help())\n\n\tspec_new_parser = spec_subparsers.add_parser('new', help='Creates a specification file')\n\tspec_new_parser.add_argument('spec_file', help='The specification file to create')\n\tspec_new_parser.add_argument('-n', '--name', help='The name of the specification')\n\tspec_new_parser.add_argument('-v', '--version', help='The version of the specification')\n\tspec_new_parser.add_argument('-d', '--description', help='A description of the specification')\n\tspec_new_parser.add_argument('-x', '--schema', help='The schema to use for the specification')\n\tspec_new_parser.add_argument('-s', '--schema_version', help='The version of the schema to use for the specification')\n\tspec_new_parser.set_defaults(func=new_spec)\n\n\tspec_validate_parser", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\timport argparse\n\tspec_parser = root_subparsers.add_parser( \"spec\", help=\"Spec related commands.\" )\n\tspec_subparsers = spec_parser.add_subparsers( title=\"Spec commands\", dest=\"spec_command\", help=\"Spec related commands.\" )\n\n\t# spec add\n\tspec_add_parser = spec_subparsers.add_parser( \"add\", help=\"Add a spec from a path.\" )\n\tspec_add_parser.add_argument( \"spec_path\", help=\"Path to the spec to add.\" )\n\tspec_add_parser.add_argument( \"--namespace\", help=\"The namespace of the spec.\", default=\"default\" )\n\tspec_add_parser.add_argument( \"--name\", help=\"The name of the spec.\", default=None )\n\n\t# spec list\n\tspec_list_parser = spec_subparsers.add_parser( \"list\", help=\"List specs.\" )\n\tspec_list_parser.add_argument( \"--namespace\", help=\"The namespace of the spec.\", default=\"default\" )\n\tspec_list_parser.add_argument( \"--regex\", help=\"Regex to filter the specs with.\", default=\".*\" )\n\tspec_list", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser('extends',\n\t\t\t\t\t\t\t\t\t\tdescription='Extends the current spec.',\n\t\t\t\t\t\t\t\t\t\thelp='Extends the current spec.')\n\tparser.add_argument('--add',\n\t\t\t\t\t\tdest='add',\n\t\t\t\t\t\tnargs='+',\n\t\t\t\t\t\thelp='adds one or more specs')\n\tparser.add_argument('--delete',\n\t\t\t\t\t\tdest='delete',\n\t\t\t\t\t\tnargs='+',\n\t\t\t\t\t\thelp='deletes one or more specs')\n\tparser.add_argument('--list',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\thelp='lists all specs')\n\tparser.add_argument('--list-extended',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\thelp='lists all specs and their extended specs')\n\tparser.set_defaults(func=self.extends)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_parser = root_subparsers.add_parser('spec')\n\tspec_parser.add_argument('--target', choices=['cli', 'yaml', 'json'], default='yaml')\n\tspec_parser.add_argument('-v', '--verbose', action='store_true')\n\tspec_parser.add_argument('--no-sort', action='store_true')\n\tspec_parser.add_argument('--raw', action='store_true')\n\tspec_parser.add_argument('--hide-empty', action='store_true')\n\tspec_parser.add_argument('--hide-private', action='store_true')\n\tspec_parser.add_argument('--hide-hidden', action='store_true')\n\tspec_parser.add_argument('--hide-type', action='store_true')\n\tspec_parser.add_argument('--hide-default', action='store_true')\n\tspec_parser.add_argument('--hide-desc', action='store_true')\n\tspec_parser.add_argument('--hide-required', action='store_true')\n\tspec_parser.add_argument('--hide-empty-required', action='store_true')\n\tspec_parser.add", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparsers = {}\n\troot_subparsers.add_parser(\"spec\", help=\"Specification related commands.\")\n\n\tparsers['spec'] = root_subparsers.add_parser(\"spec\", help=\"Specification related commands.\")\n\tparsers['specsubparser'] = parsers['spec'].add_subparsers(help=\"Subcommand help.\")\n\n\tparsers['specsubparser'].add_parser(\"add\", help=\"Add a new specification.\")\n\n\tparsers['specsubparser'].add_parser(\"remove\", help=\"Remove a specification.\")\n\n\tparsers['specsubparser'].add_parser(\"list\", help=\"List all specifications.\")\n\n\tparsers['specsubparser'].add_parser(\"get\", help=\"Get a specification.\")", "is_pass": true, "return_code": 0}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_subparser = root_subparsers.add_parser('spec')\n\tspec_subparser.add_argument('spec_file', type=str, help='the spec file to create')\n\tspec_subparser.set_defaults(func=self.generate_spec)\n", "is_pass": false, "return_code": 1}]}
