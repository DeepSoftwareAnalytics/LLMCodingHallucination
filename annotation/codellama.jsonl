{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif tz is not None:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9, tz=tz)\n\telse:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif tz is None:\n\t\treturn time.strftime(\"%H:%M:%S\", time.gmtime(nanoseconds/1000000000.0))\n\telse:\n\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds/1000000000.0))\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif not nanoseconds:\n\t\treturn ''\n\tif not tz:\n\t\ttz = 'UTC'\n\ttime = datetime.datetime.fromtimestamp(nanoseconds / 1000000000.0)\n\treturn time.strftime('%Y-%m-%dT%H:%M:%S') + '.' + str(nanoseconds)[-9:] + 'Z'\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tseconds = nanoseconds / 1e9\n\tnanoseconds = nanoseconds % 1e9\n\tif tz is None:\n\t\treturn datetime.time(seconds=seconds, microseconds=nanoseconds)\n\telse:\n\t\treturn datetime.time(seconds=seconds, microseconds=nanoseconds, tzinfo=tz)\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t# TODO: handle negative values\n\tif tz:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9, tz=tz)\n\telse:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9)\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tnanoseconds = int(nanoseconds)\n\tseconds = nanoseconds / 1000000000\n\tnanoseconds = nanoseconds % 1000000000\n\tif tz:\n\t\treturn datetime.datetime.utcfromtimestamp(seconds).replace(tzinfo=pytz.timezone(tz)).time()\n\telse:\n\t\treturn datetime.datetime.utcfromtimestamp(seconds).time()\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif nanoseconds is None:\n\t\tnanoseconds = 0\n\ttime = timedelta(nanoseconds / 1000000000)\n\tif tz:\n\t\ttime = datetime.fromtimestamp(time.total_seconds(), tz)\n\treturn time\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif nanoseconds is None:\n\t\treturn None\n\telse:\n\t\tif tz is not None:\n\t\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds / 1000000000 + tz))\n\t\telse:\n\t\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds / 1000000000))\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif tz is None:\n\t\ttz = \"UTC\"\n\thour = nanoseconds // 60**2 // 60 // 10**9\n\tnanoseconds -= hour * 60**2 * 60 * 10**9\n\tminute = nanoseconds // 60**2 // 10**9\n\tnanoseconds -= minute * 60**2 * 10**9\n\tsecond = nanoseconds // 60**2 // 10**6\n\tnanoseconds -= second * 60**2 * 10**6\n\tmicrosecond = nanoseconds // 10**3\n\tnanosecond = nanoseconds - microsecond * 10**6\n\treturn \"{}:{:02}:{:02}.{:06}.{:09}\".format(hour, minute, second, microsecond, nanosecond)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\tif nanoseconds is None:\n\t\treturn None\n\t\n\tseconds = int(nanoseconds / 1000000000)\n\tnanoseconds -= seconds * 1000000000\n\t\n\tif seconds < 0:\n\t\tseconds = -seconds\n\t\tnanoseconds = -nanoseconds\n\t\t\n\tif tz is not None:\n\t\tnanoseconds += tz.utcoffset(datetime.datetime.now()).total_seconds() * 1000000000\n\t\n\treturn \"%02d:%02d:%02d.%09d\" % (seconds / 3600, (seconds / 60) % 60, seconds % 60, nanoseconds)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif tz is not None:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9, tz=tz)\n\telse:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif tz is None:\n\t\treturn time.strftime(\"%H:%M:%S\", time.gmtime(nanoseconds/1000000000.0))\n\telse:\n\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds/1000000000.0))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif not nanoseconds:\n\t\treturn ''\n\tif not tz:\n\t\ttz = 'UTC'\n\ttime = datetime.datetime.fromtimestamp(nanoseconds / 1000000000.0)\n\treturn time.strftime('%Y-%m-%dT%H:%M:%S') + '.' + str(nanoseconds)[-9:] + 'Z'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tseconds = nanoseconds / 1e9\n\tnanoseconds = nanoseconds % 1e9\n\tif tz is None:\n\t\treturn datetime.time(seconds=seconds, microseconds=nanoseconds)\n\telse:\n\t\treturn datetime.time(seconds=seconds, microseconds=nanoseconds, tzinfo=tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t# TODO: handle negative values\n\tif tz:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9, tz=tz)\n\telse:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tnanoseconds = int(nanoseconds)\n\tseconds = nanoseconds / 1000000000\n\tnanoseconds = nanoseconds % 1000000000\n\tif tz:\n\t\treturn datetime.datetime.utcfromtimestamp(seconds).replace(tzinfo=pytz.timezone(tz)).time()\n\telse:\n\t\treturn datetime.datetime.utcfromtimestamp(seconds).time()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif nanoseconds is None:\n\t\tnanoseconds = 0\n\ttime = timedelta(nanoseconds / 1000000000)\n\tif tz:\n\t\ttime = datetime.fromtimestamp(time.total_seconds(), tz)\n\treturn time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif nanoseconds is None:\n\t\treturn None\n\telse:\n\t\tif tz is not None:\n\t\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds / 1000000000 + tz))\n\t\telse:\n\t\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds / 1000000000))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tif tz is None:\n\t\ttz = \"UTC\"\n\thour = nanoseconds // 60**2 // 60 // 10**9\n\tnanoseconds -= hour * 60**2 * 60 * 10**9\n\tminute = nanoseconds // 60**2 // 10**9\n\tnanoseconds -= minute * 60**2 * 10**9\n\tsecond = nanoseconds // 60**2 // 10**6\n\tnanoseconds -= second * 60**2 * 10**6\n\tmicrosecond = nanoseconds // 10**3\n\tnanosecond = nanoseconds - microsecond * 10**6\n\treturn \"{}:{:02}:{:02}.{:06}.{:09}\".format(hour, minute, second, microsecond, nanosecond)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\tif nanoseconds is None:\n\t\treturn None\n\t\n\tseconds = int(nanoseconds / 1000000000)\n\tnanoseconds -= seconds * 1000000000\n\t\n\tif seconds < 0:\n\t\tseconds = -seconds\n\t\tnanoseconds = -nanoseconds\n\t\t\n\tif tz is not None:\n\t\tnanoseconds += tz.utcoffset(datetime.datetime.now()).total_seconds() * 1000000000\n\t\n\treturn \"%02d:%02d:%02d.%09d\" % (seconds / 3600, (seconds / 60) % 60, seconds % 60, nanoseconds)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn timedelta(\n\t\tdays=value.days,\n\t\thours=value.seconds // 3600,\n\t\tminutes=(value.seconds // 60) % 60,\n\t\tseconds=value.seconds % 60,\n\t)\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn Structure(\n\t\t{\n\t\t\t'days': value.days,\n\t\t\t'seconds': value.seconds,\n\t\t\t'microseconds': value.microseconds,\n\t\t}\n\t)\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\n\tfrom datetime import timedelta\n\treturn timedelta(**value)\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn {\n\t\t'days': value.days,\n\t\t'seconds': value.seconds,\n\t\t'microseconds': value.microseconds,\n\t}\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif not value:\n\t\treturn None\n\treturn timedelta(hours=value.hours, minutes=value.minutes, seconds=value.seconds)\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\tif isinstance(value, datetime.timedelta):\n\t\treturn value.total_seconds()\n\treturn None\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn cls(timedelta=value)\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn {\n\t\t'days': value.days,\n\t\t'seconds': value.seconds,\n\t\t'microseconds': value.microseconds\n\t}\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif value.microseconds == 0:\n\t\treturn timedelta(days=value.days, seconds=value.seconds)\n\telse:\n\t\treturn timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn Structure(\n\t\tdays=value.days,\n\t\tseconds=value.seconds,\n\t\tmicroseconds=value.microseconds\n\t)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn timedelta(\n\t\tdays=value.days,\n\t\thours=value.seconds // 3600,\n\t\tminutes=(value.seconds // 60) % 60,\n\t\tseconds=value.seconds % 60,\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn Structure(\n\t\t{\n\t\t\t'days': value.days,\n\t\t\t'seconds': value.seconds,\n\t\t\t'microseconds': value.microseconds,\n\t\t}\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\n\tfrom datetime import timedelta\n\treturn timedelta(**value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn {\n\t\t'days': value.days,\n\t\t'seconds': value.seconds,\n\t\t'microseconds': value.microseconds,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif not value:\n\t\treturn None\n\treturn timedelta(hours=value.hours, minutes=value.minutes, seconds=value.seconds)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\tif isinstance(value, datetime.timedelta):\n\t\treturn value.total_seconds()\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn cls(timedelta=value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn {\n\t\t'days': value.days,\n\t\t'seconds': value.seconds,\n\t\t'microseconds': value.microseconds\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tif value.microseconds == 0:\n\t\treturn timedelta(days=value.days, seconds=value.seconds)\n\telse:\n\t\treturn timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\treturn Structure(\n\t\tdays=value.days,\n\t\tseconds=value.seconds,\n\t\tmicroseconds=value.microseconds\n\t)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\treturn value.ticks\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t# If the time was given as a tuple, convert it to a time object.\n\tif isinstance(value, tuple):\n\t\tvalue = datetime.time(hour=value[0], minute=value[1], second=value[2])\n\t\n\t# If the value is a time object, convert it to a dict.\n\tif isinstance(value, datetime.time):\n\t\treturn {\n\t\t\t'hour': value.hour,\n\t\t\t'minute': value.minute,\n\t\t\t'second': value.second,\n\t\t}\n\t\n\t# If the value is a dict, return it.\n\telif isinstance(value, dict):\n\t\treturn value\n\t\n\t# If none of the above, raise an exception.\n\telse:\n\t\traise Exception('Invalid argument to dehydrate_time(): %s' % value)\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn Time(value)\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif not isinstance(value, Time):\n\t\traise ValueError('Value must be a Time instance')\n\n\treturn Time(value.ticks, value.format)\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn value\n\telse:\n\t\treturn Time(value)\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value:\n\t\treturn Time(time=value)\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\treturn value.ticks\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\treturn Time.fromTicks(value)\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value == None:\n\t\treturn None\n\treturn Time(value)\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\n\tif isinstance(value, (int, long)):\n\t\treturn Time.from_ticks(value)\n\n\tif isinstance(value, datetime):\n\t\treturn Time.from_datetime(value)\n\n\treturn Time.from_iso8601(str(value))\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\treturn value.ticks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t# If the time was given as a tuple, convert it to a time object.\n\tif isinstance(value, tuple):\n\t\tvalue = datetime.time(hour=value[0], minute=value[1], second=value[2])\n\t\n\t# If the value is a time object, convert it to a dict.\n\tif isinstance(value, datetime.time):\n\t\treturn {\n\t\t\t'hour': value.hour,\n\t\t\t'minute': value.minute,\n\t\t\t'second': value.second,\n\t\t}\n\t\n\t# If the value is a dict, return it.\n\telif isinstance(value, dict):\n\t\treturn value\n\t\n\t# If none of the above, raise an exception.\n\telse:\n\t\traise Exception('Invalid argument to dehydrate_time(): %s' % value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn Time(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif not isinstance(value, Time):\n\t\traise ValueError('Value must be a Time instance')\n\n\treturn Time(value.ticks, value.format)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn value\n\telse:\n\t\treturn Time(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value:\n\t\treturn Time(time=value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\treturn value.ticks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\treturn Time.fromTicks(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value == None:\n\t\treturn None\n\treturn Time(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\n\tif isinstance(value, (int, long)):\n\t\treturn Time.from_ticks(value)\n\n\tif isinstance(value, datetime):\n\t\treturn Time.from_datetime(value)\n\n\treturn Time.from_iso8601(str(value))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point(*value)\n\telse:\n\t\treturn Point(*value[1:3])\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telse:\n\t\treturn Point(value[0], value[1], value[2])\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point2D(value)\n\telif len(value) == 3:\n\t\treturn Point3D(value)\n\telif len(value) == 4:\n\t\treturn Point4D(value)\n\telse:\n\t\traise AttributeError('Invalid point: %s' % value)\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn {\n\t\t4: Point,\n\t\t8: Point3D,\n\t}[len(value)](*value)\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 3:\n\t\treturn Point(value[0], value[1], value[2])\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\treturn Point(value)\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 0:\n\t\treturn None\n\telif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telse:\n\t\traise ValueError(\"Unknown point length: %s\" % len(value))\n\n\nclass Point(object):\n\tdef __init__(self, x, y):\n\t\tself.x = x\n\t\tself.y = y\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Point):\n\t\t\treturn self.x == other.x and self.y == other.y\n\t\treturn False\n\n\tdef __getitem__(self, index):\n\t\treturn [self.x, self.y][index]\n\n\tdef __hash__(self):\n\t\treturn hash((self.x, self.y))\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __str__(self):\n\t\treturn \"(%s, %s)\" % (self.x, self.y)\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn Point3D(value[0], value[1], value[2])\n\telse:\n\t\traise ValueError(\"Invalid Point length: {}\".format(len(value)))\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif value == None:\n\t\treturn None\n\n\tif isinstance(value, (list, tuple)):\n\t\treturn value\n\n\tif isinstance(value, dict):\n\t\treturn value\n\n\tif len(value) == 1:\n\t\treturn value[0]\n\n\tif len(value) == 2:\n\t\treturn value\n\n\tif len(value) == 3:\n\t\treturn value\n\n\treturn value\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point(value)\n\telif len(value) == 3:\n\t\treturn Point3D(value)\n\n\traise Exception(\"Point: length {} not recognized\".format(len(value)))\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn GeoPoint(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn GeoPoint(value[0], value[1], value[2])\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point(*value)\n\telse:\n\t\treturn Point(*value[1:3])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telse:\n\t\treturn Point(value[0], value[1], value[2])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point2D(value)\n\telif len(value) == 3:\n\t\treturn Point3D(value)\n\telif len(value) == 4:\n\t\treturn Point4D(value)\n\telse:\n\t\traise AttributeError('Invalid point: %s' % value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn {\n\t\t4: Point,\n\t\t8: Point3D,\n\t}[len(value)](*value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 3:\n\t\treturn Point(value[0], value[1], value[2])\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\treturn Point(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 0:\n\t\treturn None\n\telif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telse:\n\t\traise ValueError(\"Unknown point length: %s\" % len(value))\n\n\nclass Point(object):\n\tdef __init__(self, x, y):\n\t\tself.x = x\n\t\tself.y = y\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Point):\n\t\t\treturn self.x == other.x and self.y == other.y\n\t\treturn False\n\n\tdef __getitem__(self, index):\n\t\treturn [self.x, self.y][index]\n\n\tdef __hash__(self):\n\t\treturn hash((self.x, self.y))\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __str__(self):\n\t\treturn \"(%s, %s)\" % (self.x, self.y)\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn Point3D(value[0], value[1], value[2])\n\telse:\n\t\traise ValueError(\"Invalid Point length: {}\".format(len(value)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif value == None:\n\t\treturn None\n\n\tif isinstance(value, (list, tuple)):\n\t\treturn value\n\n\tif isinstance(value, dict):\n\t\treturn value\n\n\tif len(value) == 1:\n\t\treturn value[0]\n\n\tif len(value) == 2:\n\t\treturn value\n\n\tif len(value) == 3:\n\t\treturn value\n\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn Point(value)\n\telif len(value) == 3:\n\t\treturn Point3D(value)\n\n\traise Exception(\"Point: length {} not recognized\".format(len(value)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tif len(value) == 2:\n\t\treturn GeoPoint(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn GeoPoint(value[0], value[1], value[2])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__dict__['__keys'])\n\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n\n\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tl=[]\n\tfor key in self.__keys:\n\t\tl.append(key)\n\treturn l\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn self.__keys\n\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self._keys.keys())\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__dict__['__keys'])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tl=[]\n\tfor key in self.__keys:\n\t\tl.append(key)\n\treturn l\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn self.__keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self._keys.keys())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn list(self.__keys)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = cls._default_protocol_version()\n\tif protocol_version == 1:\n\t\tfrom neo4j._packstream import PackStreamHandlerV1\n\t\treturn PackStreamHandlerV1\n\telif protocol_version == 2:\n\t\tfrom neo4j._packstream import PackStreamHandlerV2\n\t\treturn PackStreamHandlerV2\n\telse:\n\t\traise ValueError(\"Unknown protocol version %s\" % (protocol_version,))\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\ttry:\n\t\tprotocol_version = int(protocol_version)\n\texcept TypeError:\n\t\tprotocol_version = None\n\n\tif protocol_version is None:\n\t\treturn cls.default_handlers\n\telif protocol_version <= 0:\n\t\traise ValueError(\"Bolt protocol version must be greater than 0\")\n\telif protocol_version == 1:\n\t\treturn cls.v1_handlers\n\telif protocol_version == 2:\n\t\treturn cls.v2_handlers\n\telif protocol_version == 3:\n\t\treturn cls.v3_handlers\n\telif protocol_version == 4:\n\t\treturn cls.v4_handlers\n\telif protocol_version == 5:\n\t\treturn cls.v5_handlers\n\telse:\n\t\traise ValueError(\"Unknown Bolt protocol version: %s\" % protocol_version)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version and protocol_version >= 4:\n\t\treturn Bolt4\n\telse:\n\t\treturn Bolt1\n\n\nclass Bolt1:\n\t@classmethod\n\tdef init(cls, **kwargs):\n\t\treturn Bolt1Session(**kwargs)\n\n\nclass Bolt4(Bolt1):\n\t@classmethod\n\tdef init(cls, **kwargs):\n\t\treturn Bolt4Session(**kwargs)\n\n\nclass Bolt1Session(Session):\n\t\"\"\"\n\tBolt 1-specific implementation of the :class:`.Session` interface.\n\t\"\"\"\n\n\tdef __init__(self, socket, address, user_agent=None, max_connection_lifetime=None,\n\t\t\t\t max_transaction_retry_time=None, max_unbound_transaction_lifetime=None,\n\t\t\t\t max_stream_retry_time=None, **kwargs):\n\n\t\tsuper(Bolt1Session, self).__init__(socket, address, user_agent, max_connection_lifetime,\n\t\t\t\t\t\t\t\t\t\t  max_transaction_retry_time, max_unbound_transaction_lifetime,\n\t\t\t\t\t\t\t\t\t\t  max_stream_re", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls\n\telif protocol_version < 4:\n\t\treturn v3.handlers\n\telif protocol_version == 4:\n\t\treturn v4.handlers\n\telse:\n\t\traise ValueError(\"Unsupported Bolt protocol version: %s\" % protocol_version)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\t# This value is set by the driver's constructor. It is None when the protocol version is not\n\t\t# known yet. This is the case when the driver is being instantiated.\n\t\treturn cls.HANDLERS_UNVERSIONED\n\tif protocol_version == \"1.0\":\n\t\treturn cls.HANDLERS_1_0\n\tif protocol_version == \"2.0\":\n\t\treturn cls.HANDLERS_2_0\n\tif protocol_version == \"3.0\":\n\t\treturn cls.HANDLERS_3_0\n\tif protocol_version == \"4.0\":\n\t\treturn cls.HANDLERS_4_0\n\tif protocol_version == \"4.1\":\n\t\treturn cls.HANDLERS_4_1\n\tif protocol_version == \"4.2\":\n\t\treturn cls.HANDLERS_4_2\n\traise ValueError(\"Unsupported protocol version: %s\" % protocol_version)\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\treturn {\n\t\t1: Bolt1,\n\t\t2: Bolt2,\n\t\t3: Bolt3,\n\t\t4: Bolt4\n\t}.get(protocol_version, Bolt1)\n\n\nclass Bolt(object):\n\t\"\"\"\n\tBase class for all Bolt protocols, defining the interface for low-level communication with neo4j.\n\t\"\"\"\n\n\tdef __init__(self, address, auth=None, **kwargs):\n\t\t\"\"\"\n\t\t:param address: The address to connect to.\n\t\t:type address: tuple(str, int)\n\t\t:param auth: The authentication mechanism to use.\n\t\t:type auth: :py:class:`.auth.Auth`\n\t\t\"\"\"\n\t\tself._address = address\n\t\tself._auth = auth\n\t\tself._config = Config(**kwargs)\n\t\tself._encrypted_address = None\n\n\tdef open(self):\n\t\t\"\"\"\n\t\tOpens the connection.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef close(self):\n\t\t\"\"\"\n\t\tCloses the connection.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef begin(self):\n\t\t\"\"\"\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls.protocol_handlers_v1\n\telif protocol_version == 1:\n\t\treturn cls.protocol_handlers_v1\n\telif protocol_version == 2:\n\t\treturn cls.protocol_handlers_v2\n\telse:\n\t\traise ValueError(\"Unknown protocol version: {}\".format(protocol_version))\n\n\nclass Connection(object):\n\n\tprotocol_handlers_v1 = {\n\t\t\"init\": Bolt1,\n\t\t\"run\": Bolt1,\n\t\t\"pull\": Bolt1,\n\t\t\"discard\": Bolt1,\n\t\t\"begin\": Bolt1,\n\t\t\"commit\": Bolt1,\n\t\t\"rollback\": Bolt1,\n\t\t\"pull_all\": Bolt1,\n\t\t\"begin_transaction\": Bolt1,\n\t\t\"commit_transaction\": Bolt1,\n\t\t\"rollback_transaction\": Bolt1,\n\t\t\"discard_all\": Bolt1,\n\t\t\"reset\": Bolt1,\n\t\t\"field\": Bolt1,\n\t\t\"ignore\": Bolt1,\n\t\t\"failure\": Bolt1,\n\t\t\"success\": Bolt1,\n\t\t", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version in (1, 2, 3, 4, 5):\n\t\treturn cls.v1_handlers()\n\telif protocol_version == 6:\n\t\treturn cls.v1_handlers() + cls.v6_handlers()\n\telif protocol_version == 7:\n\t\treturn cls.v1_handlers() + cls.v6_handlers() + cls.v7_handlers()\n\telse:\n\t\traise ValueError(\"Unknown protocol version: %s\" % protocol_version)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\tcls.protocol_version = None\n\t\tcls.response_handler = _v1.Response\n\telif protocol_version == 1:\n\t\tcls.protocol_version = 1\n\t\tcls.response_handler = _v1.Response\n\telif protocol_version == 2:\n\t\tcls.protocol_version = 2\n\t\tcls.response_handler = _v2.Response\n\telif protocol_version == 3:\n\t\tcls.protocol_version = 3\n\t\tcls.response_handler = _v3.Response\n\telse:\n\t\traise RuntimeError(\n\t\t\t\"Unsupported Bolt protocol version: %s\" % protocol_version)\n\n\nclass Bolt(object):\n\t\"\"\"\n\tInterface for sending requests to a Neo4j server and processing the responses.\n\t\"\"\"\n\n\t#: The default request timeout in seconds\n\tDEFAULT_TIMEOUT = 5\n\n\t#: The default maximum number of retries\n\tDEFAULT_MAX_RETRIES = 3\n\n\t@classmethod\n\tdef open(cls, bolt_url, **config):\n\t\t\"\"\"\n\t\tOpen a connection to a Neo4j server.\n\n\t\t:param bolt_url:", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef get_handlers(protocol_version):\n\t\t\"\"\"\n\t\tReturns the handlers for the given version of the Bolt protocol.\n\t\t\"\"\"\n\t\tif protocol_version == 4:\n\t\t\treturn cls.V4\n\t\tif protocol_version == 3:\n\t\t\treturn cls.V3\n\t\treturn cls.V1\n\n\tif protocol_version is None:\n\t\treturn get_handlers(cls.DEFAULT_PROTOCOL_VERSION)\n\telse:\n\t\treturn get_handlers(protocol_version)\n\n\nclass BoltProtocol(object):\n\t\"\"\"\n\tBase class for all Bolt protocols.\n\t\"\"\"\n\n\tDEFAULT_PROTOCOL_VERSION = 4\n\t\"\"\"\n\tDefault protocol version to use if none is set.\n\t\"\"\"\n\n\t#: Bolt protocol handlers for Bolt v1.x.\n\tV1 = None\n\t#: Bolt protocol handlers for Bolt v2.x.\n\tV2 = None\n\t#: Bolt protocol handlers for Bolt v3.x.\n\tV3 = None\n\t#: Bolt protocol handlers for Bolt v4.x.\n\tV4 = None\n\n\t#: The version of the Bolt protocol this object is"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = cls._default_protocol_version()\n\tif protocol_version == 1:\n\t\tfrom neo4j._packstream import PackStreamHandlerV1\n\t\treturn PackStreamHandlerV1\n\telif protocol_version == 2:\n\t\tfrom neo4j._packstream import PackStreamHandlerV2\n\t\treturn PackStreamHandlerV2\n\telse:\n\t\traise ValueError(\"Unknown protocol version %s\" % (protocol_version,))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\ttry:\n\t\tprotocol_version = int(protocol_version)\n\texcept TypeError:\n\t\tprotocol_version = None\n\n\tif protocol_version is None:\n\t\treturn cls.default_handlers\n\telif protocol_version <= 0:\n\t\traise ValueError(\"Bolt protocol version must be greater than 0\")\n\telif protocol_version == 1:\n\t\treturn cls.v1_handlers\n\telif protocol_version == 2:\n\t\treturn cls.v2_handlers\n\telif protocol_version == 3:\n\t\treturn cls.v3_handlers\n\telif protocol_version == 4:\n\t\treturn cls.v4_handlers\n\telif protocol_version == 5:\n\t\treturn cls.v5_handlers\n\telse:\n\t\traise ValueError(\"Unknown Bolt protocol version: %s\" % protocol_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version and protocol_version >= 4:\n\t\treturn Bolt4\n\telse:\n\t\treturn Bolt1\n\n\nclass Bolt1:\n\t@classmethod\n\tdef init(cls, **kwargs):\n\t\treturn Bolt1Session(**kwargs)\n\n\nclass Bolt4(Bolt1):\n\t@classmethod\n\tdef init(cls, **kwargs):\n\t\treturn Bolt4Session(**kwargs)\n\n\nclass Bolt1Session(Session):\n\t\"\"\"\n\tBolt 1-specific implementation of the :class:`.Session` interface.\n\t\"\"\"\n\n\tdef __init__(self, socket, address, user_agent=None, max_connection_lifetime=None,\n\t\t\t\t max_transaction_retry_time=None, max_unbound_transaction_lifetime=None,\n\t\t\t\t max_stream_retry_time=None, **kwargs):\n\n\t\tsuper(Bolt1Session, self).__init__(socket, address, user_agent, max_connection_lifetime,\n\t\t\t\t\t\t\t\t\t\t  max_transaction_retry_time, max_unbound_transaction_lifetime,\n\t\t\t\t\t\t\t\t\t\t  max_stream_re", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls\n\telif protocol_version < 4:\n\t\treturn v3.handlers\n\telif protocol_version == 4:\n\t\treturn v4.handlers\n\telse:\n\t\traise ValueError(\"Unsupported Bolt protocol version: %s\" % protocol_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\t# This value is set by the driver's constructor. It is None when the protocol version is not\n\t\t# known yet. This is the case when the driver is being instantiated.\n\t\treturn cls.HANDLERS_UNVERSIONED\n\tif protocol_version == \"1.0\":\n\t\treturn cls.HANDLERS_1_0\n\tif protocol_version == \"2.0\":\n\t\treturn cls.HANDLERS_2_0\n\tif protocol_version == \"3.0\":\n\t\treturn cls.HANDLERS_3_0\n\tif protocol_version == \"4.0\":\n\t\treturn cls.HANDLERS_4_0\n\tif protocol_version == \"4.1\":\n\t\treturn cls.HANDLERS_4_1\n\tif protocol_version == \"4.2\":\n\t\treturn cls.HANDLERS_4_2\n\traise ValueError(\"Unsupported protocol version: %s\" % protocol_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\treturn {\n\t\t1: Bolt1,\n\t\t2: Bolt2,\n\t\t3: Bolt3,\n\t\t4: Bolt4\n\t}.get(protocol_version, Bolt1)\n\n\nclass Bolt(object):\n\t\"\"\"\n\tBase class for all Bolt protocols, defining the interface for low-level communication with neo4j.\n\t\"\"\"\n\n\tdef __init__(self, address, auth=None, **kwargs):\n\t\t\"\"\"\n\t\t:param address: The address to connect to.\n\t\t:type address: tuple(str, int)\n\t\t:param auth: The authentication mechanism to use.\n\t\t:type auth: :py:class:`.auth.Auth`\n\t\t\"\"\"\n\t\tself._address = address\n\t\tself._auth = auth\n\t\tself._config = Config(**kwargs)\n\t\tself._encrypted_address = None\n\n\tdef open(self):\n\t\t\"\"\"\n\t\tOpens the connection.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef close(self):\n\t\t\"\"\"\n\t\tCloses the connection.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef begin(self):\n\t\t\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls.protocol_handlers_v1\n\telif protocol_version == 1:\n\t\treturn cls.protocol_handlers_v1\n\telif protocol_version == 2:\n\t\treturn cls.protocol_handlers_v2\n\telse:\n\t\traise ValueError(\"Unknown protocol version: {}\".format(protocol_version))\n\n\nclass Connection(object):\n\n\tprotocol_handlers_v1 = {\n\t\t\"init\": Bolt1,\n\t\t\"run\": Bolt1,\n\t\t\"pull\": Bolt1,\n\t\t\"discard\": Bolt1,\n\t\t\"begin\": Bolt1,\n\t\t\"commit\": Bolt1,\n\t\t\"rollback\": Bolt1,\n\t\t\"pull_all\": Bolt1,\n\t\t\"begin_transaction\": Bolt1,\n\t\t\"commit_transaction\": Bolt1,\n\t\t\"rollback_transaction\": Bolt1,\n\t\t\"discard_all\": Bolt1,\n\t\t\"reset\": Bolt1,\n\t\t\"field\": Bolt1,\n\t\t\"ignore\": Bolt1,\n\t\t\"failure\": Bolt1,\n\t\t\"success\": Bolt1,\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version in (1, 2, 3, 4, 5):\n\t\treturn cls.v1_handlers()\n\telif protocol_version == 6:\n\t\treturn cls.v1_handlers() + cls.v6_handlers()\n\telif protocol_version == 7:\n\t\treturn cls.v1_handlers() + cls.v6_handlers() + cls.v7_handlers()\n\telse:\n\t\traise ValueError(\"Unknown protocol version: %s\" % protocol_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tif protocol_version is None:\n\t\tcls.protocol_version = None\n\t\tcls.response_handler = _v1.Response\n\telif protocol_version == 1:\n\t\tcls.protocol_version = 1\n\t\tcls.response_handler = _v1.Response\n\telif protocol_version == 2:\n\t\tcls.protocol_version = 2\n\t\tcls.response_handler = _v2.Response\n\telif protocol_version == 3:\n\t\tcls.protocol_version = 3\n\t\tcls.response_handler = _v3.Response\n\telse:\n\t\traise RuntimeError(\n\t\t\t\"Unsupported Bolt protocol version: %s\" % protocol_version)\n\n\nclass Bolt(object):\n\t\"\"\"\n\tInterface for sending requests to a Neo4j server and processing the responses.\n\t\"\"\"\n\n\t#: The default request timeout in seconds\n\tDEFAULT_TIMEOUT = 5\n\n\t#: The default maximum number of retries\n\tDEFAULT_MAX_RETRIES = 3\n\n\t@classmethod\n\tdef open(cls, bolt_url, **config):\n\t\t\"\"\"\n\t\tOpen a connection to a Neo4j server.\n\n\t\t:param bolt_url:", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef get_handlers(protocol_version):\n\t\t\"\"\"\n\t\tReturns the handlers for the given version of the Bolt protocol.\n\t\t\"\"\"\n\t\tif protocol_version == 4:\n\t\t\treturn cls.V4\n\t\tif protocol_version == 3:\n\t\t\treturn cls.V3\n\t\treturn cls.V1\n\n\tif protocol_version is None:\n\t\treturn get_handlers(cls.DEFAULT_PROTOCOL_VERSION)\n\telse:\n\t\treturn get_handlers(protocol_version)\n\n\nclass BoltProtocol(object):\n\t\"\"\"\n\tBase class for all Bolt protocols.\n\t\"\"\"\n\n\tDEFAULT_PROTOCOL_VERSION = 4\n\t\"\"\"\n\tDefault protocol version to use if none is set.\n\t\"\"\"\n\n\t#: Bolt protocol handlers for Bolt v1.x.\n\tV1 = None\n\t#: Bolt protocol handlers for Bolt v2.x.\n\tV2 = None\n\t#: Bolt protocol handlers for Bolt v3.x.\n\tV3 = None\n\t#: Bolt protocol handlers for Bolt v4.x.\n\tV4 = None\n\n\t#: The version of the Bolt protocol this object is", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn lambda f: f(metadata=metadata, timeout=timeout)\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn func(*args, metadata=metadata, timeout=timeout, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tdef wrapped_function(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrapped_function.metadata = metadata\n\t\twrapped_function.timeout = timeout\n\t\treturn wrapped_function\n\treturn wrapper\n\n\n@unit_of_work(timeout=5)\ndef long_running_operation():\n\timport time\n\ttime.sleep(10)\n\tprint(\"Finished long operation...\")\n\treturn \"result\"\n\n\n@unit_of_work()\ndef quick_operation():\n\tprint(\"Finished quick operation...\")\n\treturn \"result\"\n\n\nprint(long_running_operation.timeout)\nprint(long_running_operation.metadata)\nprint(quick_operation.timeout)\nprint(quick_operation.metadata)\n\nquick_operation()\nlong_running_operation()\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrap(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrap.metadata = metadata\n\t\twrap.timeout = timeout\n\t\treturn wrap\n\treturn decorator\n\n@unit_of_work(metadata=[\"test_metadata\"], timeout=30)\ndef test_function():\n\tprint \"test_function\"\n\nif __name__ == \"__main__\":\n\ttest_function()\n\n# Output\n# test_function\n# <function test_function at 0x10a8ca930>\n# <function test_function at 0x10a8ca930>.metadata\n# ['test_metadata']\n# <function test_function at 0x10a8ca930>.timeout\n# 30\n\n# If you want to access the metadata and timeout of a function, you can use the following code:\nprint test_function.metadata\nprint test_function.timeout\n\n# Output\n# ['test_metadata']\n# 30\n\n# If you want to use a decorator with parameters, you can pass the parameters in the decorator.\n@unit_of_work(metadata=[\"test_metadata\"], timeout=30)\ndef test_function2(arg1, arg2", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator function.\n\t\t\"\"\"\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn UnitOfWork(metadata, timeout)\n\n\nclass UnitOfWork(object):\n\t\"\"\"\n\tDecorator that wraps the function to which it is attached. It starts a\n\tunit of work before calling the function and commits or rollbacks the\n\tunit of work after calling the function.\n\t\"\"\"\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\n\tdef __call__(self, f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tuow = unit_of_work_factory(self.metadata, self.timeout)\n\t\t\tresult = f(*args, **kwargs)\n\t\t\tuow.commit()\n\t\t\treturn result\n\t\treturn wrapper\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn lambda func: unit_of_work(metadata=metadata, timeout=timeout)(func)\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn f(*args, **kwargs)\n\t\twrapper.metadata = metadata\n\t\twrapper.timeout = timeout\n\t\treturn wrapper\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn WorkDecorator(metadata, timeout)\n\nclass WorkDecorator:\n\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\n\tdef __call__(self, func):\n\t\tfunc.metadata = self.metadata\n\t\tfunc.timeout = self.timeout\n\t\treturn func\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\traise UnitOfWorkError(e, metadata, timeout)\n\t\treturn wrapper\n\treturn decorator\n\nclass UnitOfWorkError(Exception):\n\tdef __init__(self, e, metadata, timeout):\n\t\tself.e = e\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\tdef __str__(self):\n\t\treturn 'An error occurred in the unit of work with the following metadata: \\n' \\\n\t\t\t\t'{0}\\n' \\\n\t\t\t\t'and the following timeout: \\n' \\\n\t\t\t\t'{1}'.format(self.metadata, self.timeout)\n\nclass UnitOfWork(object):\n\tdef __init__(self, metadata, timeout):\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\t\tself.start_time = None\n\t\tself.end_time = None\n\t\tself.duration = None\n\t\tself.status = None\n\n\tdef __enter__(self):\n\t\tself.start_time = datetime.datetime.now()\n\t\treturn self\n"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn lambda f: f(metadata=metadata, timeout=timeout)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn func(*args, metadata=metadata, timeout=timeout, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tdef wrapped_function(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrapped_function.metadata = metadata\n\t\twrapped_function.timeout = timeout\n\t\treturn wrapped_function\n\treturn wrapper\n\n\n@unit_of_work(timeout=5)\ndef long_running_operation():\n\timport time\n\ttime.sleep(10)\n\tprint(\"Finished long operation...\")\n\treturn \"result\"\n\n\n@unit_of_work()\ndef quick_operation():\n\tprint(\"Finished quick operation...\")\n\treturn \"result\"\n\n\nprint(long_running_operation.timeout)\nprint(long_running_operation.metadata)\nprint(quick_operation.timeout)\nprint(quick_operation.metadata)\n\nquick_operation()\nlong_running_operation()\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(long_running_operation.metadata)\n\nprint(quick_operation.metadata)\nprint(", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrap(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrap.metadata = metadata\n\t\twrap.timeout = timeout\n\t\treturn wrap\n\treturn decorator\n\n@unit_of_work(metadata=[\"test_metadata\"], timeout=30)\ndef test_function():\n\tprint \"test_function\"\n\nif __name__ == \"__main__\":\n\ttest_function()\n\n# Output\n# test_function\n# <function test_function at 0x10a8ca930>\n# <function test_function at 0x10a8ca930>.metadata\n# ['test_metadata']\n# <function test_function at 0x10a8ca930>.timeout\n# 30\n\n# If you want to access the metadata and timeout of a function, you can use the following code:\nprint test_function.metadata\nprint test_function.timeout\n\n# Output\n# ['test_metadata']\n# 30\n\n# If you want to use a decorator with parameters, you can pass the parameters in the decorator.\n@unit_of_work(metadata=[\"test_metadata\"], timeout=30)\ndef test_function2(arg1, arg2", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator function.\n\t\t\"\"\"\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn UnitOfWork(metadata, timeout)\n\n\nclass UnitOfWork(object):\n\t\"\"\"\n\tDecorator that wraps the function to which it is attached. It starts a\n\tunit of work before calling the function and commits or rollbacks the\n\tunit of work after calling the function.\n\t\"\"\"\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\n\tdef __call__(self, f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tuow = unit_of_work_factory(self.metadata, self.timeout)\n\t\t\tresult = f(*args, **kwargs)\n\t\t\tuow.commit()\n\t\t\treturn result\n\t\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn lambda func: unit_of_work(metadata=metadata, timeout=timeout)(func)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn f(*args, **kwargs)\n\t\twrapper.metadata = metadata\n\t\twrapper.timeout = timeout\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\treturn WorkDecorator(metadata, timeout)\n\nclass WorkDecorator:\n\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\n\tdef __call__(self, func):\n\t\tfunc.metadata = self.metadata\n\t\tfunc.timeout = self.timeout\n\t\treturn func\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\traise UnitOfWorkError(e, metadata, timeout)\n\t\treturn wrapper\n\treturn decorator\n\nclass UnitOfWorkError(Exception):\n\tdef __init__(self, e, metadata, timeout):\n\t\tself.e = e\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\tdef __str__(self):\n\t\treturn 'An error occurred in the unit of work with the following metadata: \\n' \\\n\t\t\t\t'{0}\\n' \\\n\t\t\t\t'and the following timeout: \\n' \\\n\t\t\t\t'{1}'.format(self.metadata, self.timeout)\n\nclass UnitOfWork(object):\n\tdef __init__(self, metadata, timeout):\n\t\tself.metadata = metadata\n\t\tself.timeout = timeout\n\t\tself.start_time = None\n\t\tself.end_time = None\n\t\tself.duration = None\n\t\tself.status = None\n\n\tdef __enter__(self):\n\t\tself.start_time = datetime.datetime.now()\n\t\treturn self\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key + 1\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn int(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tif key in self.table:\n\t\treturn self.table[key]\n\treturn None\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn int(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn int(str(key))\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key % self.num_buckets\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn int(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tif key in self.table:\n\t\treturn self.table[key]\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn int(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn int(str(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key % self.num_buckets\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn key\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self[key] for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self[key] for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\toutput = []\n\tfor key in keys:\n\t\toutput.append(self.data[key][self.index])\n\treturn output\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.get(key) for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.values_dict[key] for key in keys]\n\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.index[key] for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.index[k] for k in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.data[key] for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tif not keys:\n\t\treturn list(self.keys())\n\treturn [self[key] for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self[key] for key in keys]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self[key] for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self[key] for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\toutput = []\n\tfor key in keys:\n\t\toutput.append(self.data[key][self.index])\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.get(key) for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.values_dict[key] for key in keys]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.index[key] for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.index[k] for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self.data[key] for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tif not keys:\n\t\treturn list(self.keys())\n\treturn [self[key] for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\treturn [self[key] for key in keys]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn self.keys\n\telse:\n\t\treturn [self.keys[key] for key in keys]\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.records.data(*keys)\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tif self.is_none():\n\t\treturn []\n\telif keys:\n\t\treturn [self[key] for key in keys]\n\telse:\n\t\treturn list(self.keys())\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.record_exporter.data(*keys)\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn self._data.keys()\n\telse:\n\t\treturn self._data[keys[0]]\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self._data.get(*keys)\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.data_\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.exporter.data(*keys)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn self.keys\n\telse:\n\t\treturn [self.keys[key] for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.records.data(*keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tif self.is_none():\n\t\treturn []\n\telif keys:\n\t\treturn [self[key] for key in keys]\n\telse:\n\t\treturn list(self.keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.record_exporter.data(*keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tif len(keys) == 0:\n\t\treturn self._data.keys()\n\telse:\n\t\treturn self._data[keys[0]]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self._data.get(*keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.data_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\treturn self.exporter.data(*keys)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\told_data = self.data\n\tself.data = self.data[:-2]\n\treturn old_data[-2:]\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn_value = self.data.pop()\n\treturn_value += self.data.pop() << 8\n\treturn return_value\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop(), self.data.pop()\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop()\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data.pop()\n\tself.data.pop()\n\treturn self.data\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t# return the last two elements from the list\n\treturn self.data.pop(), self.data.pop()\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tretval = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn retval\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop(), self.data.pop()\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop() << 8 | self.data.pop()\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop(), self.data.pop()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\told_data = self.data\n\tself.data = self.data[:-2]\n\treturn old_data[-2:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn_value = self.data.pop()\n\treturn_value += self.data.pop() << 8\n\treturn return_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop(), self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tself.data.pop()\n\tself.data.pop()\n\treturn self.data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t# return the last two elements from the list\n\treturn self.data.pop(), self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tretval = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn retval\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop(), self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop() << 8 | self.data.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop(), self.data.pop()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif qid == -1:\n\t\tqid = len(self.queries) - 1\n\tif qid < 0:\n\t\traise ValueError(\"Query ID should be >= 0\")\n\tself.queries[qid]._discard(n, dehydration_hooks, hydration_hooks)\n\tself.queries[qid]._append_to_output_queue(DISCARD)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._output_queue.append((DISCARD, n, qid, dehydration_hooks,\n\t                           hydration_hooks, handlers))\n\treturn Response(self)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn Response(message(self, MessageType.DISCARD, n, qid, **handlers), dehydration_hooks, hydration_hooks)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn self._append_message(\n\t    messages.DiscardMessage(n, qid), dehydration_hooks, hydration_hooks,\n\t    **handlers)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._send_message(\n\t\tMessage(\n\t\t\tDISCARD, None,\n\t\t\tn=n, qid=qid,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks,\n\t\t\thandlers=handlers\n\t\t)\n\t)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._send_message(Message.DISCARD, n=n, qid=qid, **handlers)\n\tresponse = Response(self, dehydration_hooks=dehydration_hooks,\n\t                    hydration_hooks=hydration_hooks)\n\tresponse.wait()\n\treturn response\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif qid == -1:\n\t\tqid = self._last_qid\n\t\tif qid is None:\n\t\t\traise RuntimeError(\"No previous query, cannot discard\")\n\tqid = int(qid)\n\tif n == -1:\n\t\tn = 0\n\tn = int(n)\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tcmd = _DISCARD_MSG % (qid, n)\n\treturn self._send_message(cmd, dehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.send_message(\n\t    DISCARD,\n\t    n=n,\n\t    qid=qid,\n\t    dehydration_hooks=dehydration_hooks,\n\t    hydration_hooks=hydration_hooks,\n\t    **handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif n == 0:\n\t\traise ValueError(\"Can't discard 0 records\")\n\tmessage = Message(DISCARD, n, qid, dehydration_hooks, hydration_hooks,\n\t\t\t\t\t  **handlers)\n\tself.__queue.append(message)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn self.__send(self._discard_msg(n, qid), **handlers)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif qid == -1:\n\t\tqid = len(self.queries) - 1\n\tif qid < 0:\n\t\traise ValueError(\"Query ID should be >= 0\")\n\tself.queries[qid]._discard(n, dehydration_hooks, hydration_hooks)\n\tself.queries[qid]._append_to_output_queue(DISCARD)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._output_queue.append((DISCARD, n, qid, dehydration_hooks,\n\t                           hydration_hooks, handlers))\n\treturn Response(self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn Response(message(self, MessageType.DISCARD, n, qid, **handlers), dehydration_hooks, hydration_hooks)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn self._append_message(\n\t    messages.DiscardMessage(n, qid), dehydration_hooks, hydration_hooks,\n\t    **handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._send_message(\n\t\tMessage(\n\t\t\tDISCARD, None,\n\t\t\tn=n, qid=qid,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks,\n\t\t\thandlers=handlers\n\t\t)\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._send_message(Message.DISCARD, n=n, qid=qid, **handlers)\n\tresponse = Response(self, dehydration_hooks=dehydration_hooks,\n\t                    hydration_hooks=hydration_hooks)\n\tresponse.wait()\n\treturn response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif qid == -1:\n\t\tqid = self._last_qid\n\t\tif qid is None:\n\t\t\traise RuntimeError(\"No previous query, cannot discard\")\n\tqid = int(qid)\n\tif n == -1:\n\t\tn = 0\n\tn = int(n)\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tcmd = _DISCARD_MSG % (qid, n)\n\treturn self._send_message(cmd, dehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.send_message(\n\t    DISCARD,\n\t    n=n,\n\t    qid=qid,\n\t    dehydration_hooks=dehydration_hooks,\n\t    hydration_hooks=hydration_hooks,\n\t    **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif n == 0:\n\t\traise ValueError(\"Can't discard 0 records\")\n\tmessage = Message(DISCARD, n, qid, dehydration_hooks, hydration_hooks,\n\t\t\t\t\t  **handlers)\n\tself.__queue.append(message)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn self.__send(self._discard_msg(n, qid), **handlers)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t...\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tif not bookmarks:\n\t\tbookmarks = []\n\tif not metadata:\n\t\tmetadata = {}\n\n\tif imp_user is not None:\n\t\tmetadata[\"bookmark_impersonated_user\"] = imp_user\n\n\tif db is not None:\n\t\tmetadata[\"bookmark_database_name\"] = db\n\n\tif timeout is not None:\n\t\tmetadata[\"timeout\"] = timeout\n\n\tmetadata[\"bookmark\"] = bookmarks\n\tmetadata[\"mode\"] = mode or \"WRITE\"\n\n\tself._send_begin(metadata)\n\treturn self._pull_all(**handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tquery = 'BEGIN'\n\tif mode:\n\t\tquery += ' ' + mode\n\tif bookmarks:\n\t\tquery += ' ' + ' '.join(bookmarks)\n\tif metadata:\n\t\tquery += ' ' + ' '.join(\n\t\t\t[\"{0}: {1}\".format(k, v) for k, v in metadata.items()])\n\tif timeout:\n\t\tquery += ' ' + str(timeout)\n\tif db:\n\t\tquery += ' ' + db\n\tif imp_user:\n\t\tquery += ' ' + imp_user\n\n\treturn self.send_all(query, dehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t hydration_hooks=hydration_hooks, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tif timeout is not None:\n\t\twarnings.warn(\n\t\t\t\"The timeout parameter is deprecated and has no effect\",\n\t\t\tDeprecationWarning,\n\t\t\tstacklevel=2,\n\t\t)\n\n\tif metadata is None:\n\t\tmetadata = {}\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif db is None:\n\t\tdb = self.default_database\n\tif imp_user is None:\n\t\timp_user = self.impersonated_user\n\n\treturn self.send_all(\n\t\tmessages=[\n\t\t\t(1, dict(\n\t\t\t\tmode=mode,\n\t\t\t\tbookmarks=bookmarks,\n\t\t\t\tmetadata=metadata,\n\t\t\t\ttimeout=timeout,\n\t\t\t\tdb=db,\n\t\t\t\timp_user=imp_user,\n\t\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\t\thydration_hooks=hydration_hooks,\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t...\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tif not self.unlocked:\n\t\traise RuntimeError(\"Cannot send BEGIN message while still locked\")\n\n\tmsg = BoltMessage(message_type=MessageType.BEGIN)\n\tif mode is not None:\n\t\tmsg.begin_mode = mode\n\tif bookmarks is not None:\n\t\tmsg.bookmarks = bookmarks\n\tif metadata is not None:\n\t\tmsg.metadata = metadata\n\tif timeout is not None:\n\t\tmsg.timeout = timeout\n\tif db is not None:\n\t\tmsg.database = db\n\tif imp_user is not None:\n\t\tmsg.imp_user = imp_user\n\tif dehydration_hooks is not None:\n\t\tmsg.dehydration_hooks = dehydration_hooks\n\tif hydration_hooks is not None:\n\t\tmsg.hydration_hooks = hydration_hooks\n\tself.send_message(msg)\n\tself.unlocked = False\n\tresponse = self.receive_message()\n\tself.unlocked = True\n\treturn Response(response, **handlers)\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\trequest = BeginMessage(mode, bookmarks, metadata, timeout, db, imp_user)\n\treturn self._send_message(request, **handlers)\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself._append(\n\t\tmessage.BeginMessage(mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t\t\t\t\t\t\t\t\t\t\t dehydration_hooks, hydration_hooks),\n\t\t**handlers)\n\treturn self._pull_all()\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\treturn self.send_all(\n\t\tself._begin_message(mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t                    dehydration_hooks, hydration_hooks),\n\t\t**handlers\n\t)\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\treturn self._append(\n\t\tmessage.BeginMessage(mode, bookmarks, metadata, timeout, db,\n\t\t                     imp_user, dehydration_hooks, hydration_hooks),\n\t\thandlers=handlers)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tif not bookmarks:\n\t\tbookmarks = []\n\tif not metadata:\n\t\tmetadata = {}\n\n\tif imp_user is not None:\n\t\tmetadata[\"bookmark_impersonated_user\"] = imp_user\n\n\tif db is not None:\n\t\tmetadata[\"bookmark_database_name\"] = db\n\n\tif timeout is not None:\n\t\tmetadata[\"timeout\"] = timeout\n\n\tmetadata[\"bookmark\"] = bookmarks\n\tmetadata[\"mode\"] = mode or \"WRITE\"\n\n\tself._send_begin(metadata)\n\treturn self._pull_all(**handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tquery = 'BEGIN'\n\tif mode:\n\t\tquery += ' ' + mode\n\tif bookmarks:\n\t\tquery += ' ' + ' '.join(bookmarks)\n\tif metadata:\n\t\tquery += ' ' + ' '.join(\n\t\t\t[\"{0}: {1}\".format(k, v) for k, v in metadata.items()])\n\tif timeout:\n\t\tquery += ' ' + str(timeout)\n\tif db:\n\t\tquery += ' ' + db\n\tif imp_user:\n\t\tquery += ' ' + imp_user\n\n\treturn self.send_all(query, dehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tif timeout is not None:\n\t\twarnings.warn(\n\t\t\t\"The timeout parameter is deprecated and has no effect\",\n\t\t\tDeprecationWarning,\n\t\t\tstacklevel=2,\n\t\t)\n\n\tif metadata is None:\n\t\tmetadata = {}\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif db is None:\n\t\tdb = self.default_database\n\tif imp_user is None:\n\t\timp_user = self.impersonated_user\n\n\treturn self.send_all(\n\t\tmessages=[\n\t\t\t(1, dict(\n\t\t\t\tmode=mode,\n\t\t\t\tbookmarks=bookmarks,\n\t\t\t\tmetadata=metadata,\n\t\t\t\ttimeout=timeout,\n\t\t\t\tdb=db,\n\t\t\t\timp_user=imp_user,\n\t\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\t\thydration_hooks=hydration_hooks,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t...\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tif not self.unlocked:\n\t\traise RuntimeError(\"Cannot send BEGIN message while still locked\")\n\n\tmsg = BoltMessage(message_type=MessageType.BEGIN)\n\tif mode is not None:\n\t\tmsg.begin_mode = mode\n\tif bookmarks is not None:\n\t\tmsg.bookmarks = bookmarks\n\tif metadata is not None:\n\t\tmsg.metadata = metadata\n\tif timeout is not None:\n\t\tmsg.timeout = timeout\n\tif db is not None:\n\t\tmsg.database = db\n\tif imp_user is not None:\n\t\tmsg.imp_user = imp_user\n\tif dehydration_hooks is not None:\n\t\tmsg.dehydration_hooks = dehydration_hooks\n\tif hydration_hooks is not None:\n\t\tmsg.hydration_hooks = hydration_hooks\n\tself.send_message(msg)\n\tself.unlocked = False\n\tresponse = self.receive_message()\n\tself.unlocked = True\n\treturn Response(response, **handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\trequest = BeginMessage(mode, bookmarks, metadata, timeout, db, imp_user)\n\treturn self._send_message(request, **handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself._append(\n\t\tmessage.BeginMessage(mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t\t\t\t\t\t\t\t\t\t\t dehydration_hooks, hydration_hooks),\n\t\t**handlers)\n\treturn self._pull_all()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\treturn self.send_all(\n\t\tself._begin_message(mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t                    dehydration_hooks, hydration_hooks),\n\t\t**handlers\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\treturn self._append(\n\t\tmessage.BeginMessage(mode, bookmarks, metadata, timeout, db,\n\t\t                     imp_user, dehydration_hooks, hydration_hooks),\n\t\thandlers=handlers)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n == 0.5:\n\t\treturn 0\n\telif n == -0.5:\n\t\treturn 0\n\telif n > 0:\n\t\treturn math.ceil(n - 0.5) if (n - math.floor(n)) > 0 else math.floor(n + 0.5)\n\telse:\n\t\treturn math.ceil(n + 0.5) if (n - math.floor(n)) > 0 else math.floor(n - 0.5)\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n) if n % 2 == 0 else round(n + 0.5)\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n, 1)\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\treturn round(n + 0.5)\n\telse:\n\t\treturn round(n - 0.5)\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n+0.5) if n>=0 else round(n-0.5)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\tn += 0.5\n\telse:\n\t\tn -= 0.5\n\treturn int(n)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\treturn math.floor(n + 0.5)\n\telse:\n\t\treturn math.ceil(n - 0.5)\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn math.floor(n + 0.5) if n.is_integer() else math.floor(n + 0.5) if n % 1 < 0.5 else math.ceil(n - 0.5)\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\treturn round_half_to_even_positive(n)\n\telse:\n\t\treturn -round_half_to_even_positive(-n)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n, 1)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n == 0.5:\n\t\treturn 0\n\telif n == -0.5:\n\t\treturn 0\n\telif n > 0:\n\t\treturn math.ceil(n - 0.5) if (n - math.floor(n)) > 0 else math.floor(n + 0.5)\n\telse:\n\t\treturn math.ceil(n + 0.5) if (n - math.floor(n)) > 0 else math.floor(n - 0.5)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n) if n % 2 == 0 else round(n + 0.5)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n, 1)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\treturn round(n + 0.5)\n\telse:\n\t\treturn round(n - 0.5)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n+0.5) if n>=0 else round(n-0.5)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\tn += 0.5\n\telse:\n\t\tn -= 0.5\n\treturn int(n)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\treturn math.floor(n + 0.5)\n\telse:\n\t\treturn math.ceil(n - 0.5)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn math.floor(n + 0.5) if n.is_integer() else math.floor(n + 0.5) if n % 1 < 0.5 else math.ceil(n - 0.5)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tif n >= 0:\n\t\treturn round_half_to_even_positive(n)\n\telse:\n\t\treturn -round_half_to_even_positive(-n)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\treturn round(n, 1)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\t# Point Class\n\tpoint_class = type(name, (PointBase,), dict(\n\t\tsrid_map=srid_map,\n\t\t__module__=name.__module__,\n\t\t__doc__=name.__doc__,\n\t\t__init__=PointBase.__init__,\n\t\t__new__=PointBase.__new__,\n\t\t__slots__=(\n\t\t\t'__weakref__',\n\t\t\t'srid',\n\t\t\t'x',\n\t\t\t'y',\n\t\t\t'z',\n\t\t\t'm',\n\t\t),\n\t\t__dict__=fields,\n\t))\n\t\n\t# Point Class Properties\n\tpoint_class.x = property(lambda self: self._get_field('x'))\n\tpoint_class.y = property(lambda self: self._get_field('y'))\n\tpoint_class.z = property(lambda self: self._get_field('z'))\n\tpoint_class.m = property(lambda self: self._get_field('m'))\n\t\n\treturn point_class\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(\n\t\tname,\n\t\t(Point,),\n\t\t{\n\t\t\t\"__slots__\": fields,\n\t\t\t\"srid\": srid_map,\n\t\t\t\"__module__\": \"geoalchemy2.types\",\n\t\t\t\"__doc__\": \"A Point type\",\n\t\t},\n\t)\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_class = type(name, (Point,), {})\n\tpoint_class.srid_map = srid_map\n\tpoint_class.fields = fields\n\treturn point_class\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(name, (Point,), {'__slots__': fields, '__srid_map__': srid_map})\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(name, (PointBase,), {\n\t\t\t'__module__': __name__,\n\t\t\t'__slots__': (),\n\t\t\t'__doc__': 'The {0} class.'.format(name),\n\t\t\t'__srid_map__': srid_map,\n\t\t\t'__fields__': fields,\n\t\t\t'__name__': name,\n\t\t\t'__point_type__': True,\n\t\t\t'__srid__': srid_map.get(name, None),\n\t\t\t'__coord_type__': 'Point',\n\t\t})\n\n\nclass PointBase(object):\n\t\"\"\"\n\tThe Base Point class\n\t\"\"\"\n\n\tdef __new__(cls, *args, **kwargs):\n\t\treturn super(PointBase, cls).__new__(cls)\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(PointBase, self).__init__(*args, **kwargs)\n\t\tself.x = None\n\t\tself.y = None\n\t\tself.srid = None\n\t\tself.coord_type = None\n\t\tself.point_type = None\n\t\tself.__fields__ = None\n\t\tself.__srid", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(\n\t\tname,\n\t\t(Point,),\n\t\t{\n\t\t\t'__slots__': (),\n\t\t\t'__module__': 'django.contrib.gis.db.models.fields',\n\t\t\t'__doc__': \"A point geometry object\",\n\t\t\t'deflated': lambda self: self,\n\t\t\t'srid_map': srid_map,\n\t\t\t'get_lat_long': lambda self: (self.y, self.x),\n\t\t\t'get_y': lambda self: self.y,\n\t\t\t'get_x': lambda self: self.x,\n\t\t\t'distance': lambda self, other: distance(self, other),\n\t\t\t'distance_sphere': lambda self, other: distance_sphere(self, other),\n\t\t\t'distance_spheroid': lambda self, other: distance_spheroid(self, other),\n\t\t\t'transform': lambda self, srid, clone=False: transform(self, srid, clone),\n\t\t\t'kml': lambda self: kml(self),\n\t\t\t'wkt': lambda self: wkt(self),\n\t\t\t'geojson': lambda self: geojson(self),\n\t", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Meta:\n\t\tsrid_map = srid_map\n\t\tfields = fields\n\n\tclass _Point(object):\n\t\t__metaclass__ = type\n\t\t__name__ = name\n\t\t__module__ = \"geos.models\"\n\t\t__metaclass__ = Meta\n\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tif args:\n\t\t\t\tif len(args) > 1:\n\t\t\t\t\traise ValueError(\"Point takes only one positional argument\")\n\t\t\t\tif kwargs:\n\t\t\t\t\traise ValueError(\"Point takes either positional or keyword arguments\")\n\t\t\t\telif args[0] is None:\n\t\t\t\t\tself.x, self.y = None, None\n\t\t\t\telse:\n\t\t\t\t\tif isinstance(args[0], (list, tuple)):\n\t\t\t\t\t\tself.x, self.y = args[0][0], args[0][1]\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.x, self.y = args[0].x, args[0].y\n\t\t\telse:\n\t\t\t\tif kwargs:\n\t\t\t\t\tself.x = kwargs.get", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\t\"\"\"\n\t\tPoint Class\n\t\t\"\"\"\n\t\tdef __init__(self, **kwargs):\n\t\t\t\"\"\"\n\t\t\tPoint Constructor\n\t\t\t\"\"\"\n\t\t\tself.name = name\n\t\t\tself.fields = fields\n\t\t\tself.srid_map = srid_map\n\t\t\tself.srid = fields[0].srid\n\t\t\tfor f in fields:\n\t\t\t\tsetattr(self, f.name, kwargs.get(f.name, None))\n\t\tdef __repr__(self):\n\t\t\t\"\"\"\n\t\t\tPoint Representation\n\t\t\t\"\"\"\n\t\t\treturn \"<%s %s>\" % (self.name, self.fields)\n\t\tdef __str__(self):\n\t\t\t\"\"\"\n\t\t\tPoint String Representation\n\t\t\t\"\"\"\n\t\t\treturn str(self.fields)\n\treturn Point\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_type = namedtuple(name, fields)\n\tpoint_type.__new__.__defaults__ = (None,) * len(fields)\n\tpoint_type.__module__ = __name__\n\tpoint_type.__srid_map__ = srid_map\n\treturn point_type\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tdef __init__(self, x, y, srid=4326):\n\t\tself.x, self.y = x, y\n\t\tself.srid = srid\n\n\t\tself._fields = [\n\t\t\t'x',\n\t\t\t'y',\n\t\t\t'srid'\n\t\t]\n\n\t\tself._fields.extend(fields)\n\n\tdef __str__(self):\n\t\treturn '(%s, %s)' % (self.x, self.y)\n\n\tdef __repr__(self):\n\t\treturn '<%s at %s (%s, %s)>' % (self.__class__.__name__,\n\t\t\thex(id(self)), self.x, self.y)\n\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, self.__class__) and \\\n\t\t\t(self.x, self.y, self.srid) == (other.x, other.y, other.srid)\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __hash__(self):\n\t\treturn hash((self.x, self.y, self.srid))\n\n\tdef __getattr__(self, name):\n\t\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\t# Point Class\n\tpoint_class = type(name, (PointBase,), dict(\n\t\tsrid_map=srid_map,\n\t\t__module__=name.__module__,\n\t\t__doc__=name.__doc__,\n\t\t__init__=PointBase.__init__,\n\t\t__new__=PointBase.__new__,\n\t\t__slots__=(\n\t\t\t'__weakref__',\n\t\t\t'srid',\n\t\t\t'x',\n\t\t\t'y',\n\t\t\t'z',\n\t\t\t'm',\n\t\t),\n\t\t__dict__=fields,\n\t))\n\t\n\t# Point Class Properties\n\tpoint_class.x = property(lambda self: self._get_field('x'))\n\tpoint_class.y = property(lambda self: self._get_field('y'))\n\tpoint_class.z = property(lambda self: self._get_field('z'))\n\tpoint_class.m = property(lambda self: self._get_field('m'))\n\t\n\treturn point_class\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(\n\t\tname,\n\t\t(Point,),\n\t\t{\n\t\t\t\"__slots__\": fields,\n\t\t\t\"srid\": srid_map,\n\t\t\t\"__module__\": \"geoalchemy2.types\",\n\t\t\t\"__doc__\": \"A Point type\",\n\t\t},\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_class = type(name, (Point,), {})\n\tpoint_class.srid_map = srid_map\n\tpoint_class.fields = fields\n\treturn point_class\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(name, (Point,), {'__slots__': fields, '__srid_map__': srid_map})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(name, (PointBase,), {\n\t\t\t'__module__': __name__,\n\t\t\t'__slots__': (),\n\t\t\t'__doc__': 'The {0} class.'.format(name),\n\t\t\t'__srid_map__': srid_map,\n\t\t\t'__fields__': fields,\n\t\t\t'__name__': name,\n\t\t\t'__point_type__': True,\n\t\t\t'__srid__': srid_map.get(name, None),\n\t\t\t'__coord_type__': 'Point',\n\t\t})\n\n\nclass PointBase(object):\n\t\"\"\"\n\tThe Base Point class\n\t\"\"\"\n\n\tdef __new__(cls, *args, **kwargs):\n\t\treturn super(PointBase, cls).__new__(cls)\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(PointBase, self).__init__(*args, **kwargs)\n\t\tself.x = None\n\t\tself.y = None\n\t\tself.srid = None\n\t\tself.coord_type = None\n\t\tself.point_type = None\n\t\tself.__fields__ = None\n\t\tself.__srid", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\treturn type(\n\t\tname,\n\t\t(Point,),\n\t\t{\n\t\t\t'__slots__': (),\n\t\t\t'__module__': 'django.contrib.gis.db.models.fields',\n\t\t\t'__doc__': \"A point geometry object\",\n\t\t\t'deflated': lambda self: self,\n\t\t\t'srid_map': srid_map,\n\t\t\t'get_lat_long': lambda self: (self.y, self.x),\n\t\t\t'get_y': lambda self: self.y,\n\t\t\t'get_x': lambda self: self.x,\n\t\t\t'distance': lambda self, other: distance(self, other),\n\t\t\t'distance_sphere': lambda self, other: distance_sphere(self, other),\n\t\t\t'distance_spheroid': lambda self, other: distance_spheroid(self, other),\n\t\t\t'transform': lambda self, srid, clone=False: transform(self, srid, clone),\n\t\t\t'kml': lambda self: kml(self),\n\t\t\t'wkt': lambda self: wkt(self),\n\t\t\t'geojson': lambda self: geojson(self),\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Meta:\n\t\tsrid_map = srid_map\n\t\tfields = fields\n\n\tclass _Point(object):\n\t\t__metaclass__ = type\n\t\t__name__ = name\n\t\t__module__ = \"geos.models\"\n\t\t__metaclass__ = Meta\n\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tif args:\n\t\t\t\tif len(args) > 1:\n\t\t\t\t\traise ValueError(\"Point takes only one positional argument\")\n\t\t\t\tif kwargs:\n\t\t\t\t\traise ValueError(\"Point takes either positional or keyword arguments\")\n\t\t\t\telif args[0] is None:\n\t\t\t\t\tself.x, self.y = None, None\n\t\t\t\telse:\n\t\t\t\t\tif isinstance(args[0], (list, tuple)):\n\t\t\t\t\t\tself.x, self.y = args[0][0], args[0][1]\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.x, self.y = args[0].x, args[0].y\n\t\t\telse:\n\t\t\t\tif kwargs:\n\t\t\t\t\tself.x = kwargs.get", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\t\"\"\"\n\t\tPoint Class\n\t\t\"\"\"\n\t\tdef __init__(self, **kwargs):\n\t\t\t\"\"\"\n\t\t\tPoint Constructor\n\t\t\t\"\"\"\n\t\t\tself.name = name\n\t\t\tself.fields = fields\n\t\t\tself.srid_map = srid_map\n\t\t\tself.srid = fields[0].srid\n\t\t\tfor f in fields:\n\t\t\t\tsetattr(self, f.name, kwargs.get(f.name, None))\n\t\tdef __repr__(self):\n\t\t\t\"\"\"\n\t\t\tPoint Representation\n\t\t\t\"\"\"\n\t\t\treturn \"<%s %s>\" % (self.name, self.fields)\n\t\tdef __str__(self):\n\t\t\t\"\"\"\n\t\t\tPoint String Representation\n\t\t\t\"\"\"\n\t\t\treturn str(self.fields)\n\treturn Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tpoint_type = namedtuple(name, fields)\n\tpoint_type.__new__.__defaults__ = (None,) * len(fields)\n\tpoint_type.__module__ = __name__\n\tpoint_type.__srid_map__ = srid_map\n\treturn point_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tdef __init__(self, x, y, srid=4326):\n\t\tself.x, self.y = x, y\n\t\tself.srid = srid\n\n\t\tself._fields = [\n\t\t\t'x',\n\t\t\t'y',\n\t\t\t'srid'\n\t\t]\n\n\t\tself._fields.extend(fields)\n\n\tdef __str__(self):\n\t\treturn '(%s, %s)' % (self.x, self.y)\n\n\tdef __repr__(self):\n\t\treturn '<%s at %s (%s, %s)>' % (self.__class__.__name__,\n\t\t\thex(id(self)), self.x, self.y)\n\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, self.__class__) and \\\n\t\t\t(self.x, self.y, self.srid) == (other.x, other.y, other.srid)\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __hash__(self):\n\t\treturn hash((self.x, self.y, self.srid))\n\n\tdef __getattr__(self, name):\n\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tA decorator function for deprecating functions and methods.\n\t\t\"\"\"\n\t\tdef newFunc(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe new function which calls the deprecated function.\n\t\t\t\"\"\"\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\tnewFunc.__name__ = func.__name__\n\t\tnewFunc.__doc__ = func.__doc__\n\t\tnewFunc.__dict__.update(func.__dict__)\n\t\treturn newFunc\n\treturn deprecated_decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\timport warnings\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tReturn a wrapper function for the deprecated function or method.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tReturn a function that logs a warning message and then calls the\n\t\t\tdeprecated function or method.\n\t\t\t\"\"\"\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\twrapper.__dict__.update(func.__dict__)\n\t\twrapper.__dict__['deprecated'] = True\n\t\treturn wrapper\n\treturn decorator\n\n#-------------------------------------------------------------------------------\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorate(func):\n\t\t\"\"\"\n\t\tThe decorator function. \n\t\t\"\"\"\n\t\tdef newFunc(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe new function to be called.\n\t\t\t\"\"\"\n\t\t\tprint(\"WARNING: %s is deprecated. %s\" % (func.__name__, message))\n\t\t\treturn func(*args, **kwargs)\n\t\tnewFunc.__name__ = func.__name__\n\t\tnewFunc.__doc__ = func.__doc__\n\t\tnewFunc.__dict__.update(func.__dict__)\n\t\treturn newFunc\n\treturn decorate\n\nclass Deprecated(Exception):\n\t\"\"\"\n\tThe exception raised by deprecated functions and methods.\n\t\"\"\"\n\tpass\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tDecorate a function or method.\n\t\t\"\"\"\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tDeprecated function or method.\n\t\t\t\"\"\"\n\t\t\twarnings.simplefilter('always', DeprecationWarning)  # turn off filter\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\twarnings.simplefilter('default', DeprecationWarning)  # reset filter\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn deprecated_decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tDecorate a function or method with a warning message to the user.\n\t\t\"\"\"\n\t\tdef new_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWarn the user then call the function.\n\t\t\t\"\"\"\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\tnew_func.__name__ = func.__name__\n\t\tnew_func.__doc__ = func.__doc__\n\t\tnew_func.__dict__.update(func.__dict__)\n\t\treturn new_func\n\treturn deprecated_decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint('%s is deprecated: %s' % (func.__name__, message))\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\twrapper.__dict__.update(func.__dict__)\n\t\treturn wrapper\n\treturn decorator\n\n\n@deprecated('use the `deprecated` package instead')\ndef test():\n\t", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tA decorator for deprecating functions and methods.\n\t\t\"\"\"\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe wrapper function for the deprecated function.\n\t\t\t\"\"\"\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\twrapper.__dict__.update(func.__dict__)\n\t\treturn wrapper\n\n\treturn decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tA decorator function for deprecating functions and methods.\n\t\t\"\"\"\n\t\tdef newFunc(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe new function which calls the deprecated function.\n\t\t\t\"\"\"\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\tnewFunc.__name__ = func.__name__\n\t\tnewFunc.__doc__ = func.__doc__\n\t\tnewFunc.__dict__.update(func.__dict__)\n\t\treturn newFunc\n\treturn deprecated_decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\timport warnings\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tReturn a wrapper function for the deprecated function or method.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tReturn a function that logs a warning message and then calls the\n\t\t\tdeprecated function or method.\n\t\t\t\"\"\"\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\twrapper.__dict__.update(func.__dict__)\n\t\twrapper.__dict__['deprecated'] = True\n\t\treturn wrapper\n\treturn decorator\n\n#-------------------------------------------------------------------------------\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorate(func):\n\t\t\"\"\"\n\t\tThe decorator function. \n\t\t\"\"\"\n\t\tdef newFunc(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe new function to be called.\n\t\t\t\"\"\"\n\t\t\tprint(\"WARNING: %s is deprecated. %s\" % (func.__name__, message))\n\t\t\treturn func(*args, **kwargs)\n\t\tnewFunc.__name__ = func.__name__\n\t\tnewFunc.__doc__ = func.__doc__\n\t\tnewFunc.__dict__.update(func.__dict__)\n\t\treturn newFunc\n\treturn decorate\n\nclass Deprecated(Exception):\n\t\"\"\"\n\tThe exception raised by deprecated functions and methods.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tDecorate a function or method.\n\t\t\"\"\"\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tDeprecated function or method.\n\t\t\t\"\"\"\n\t\t\twarnings.simplefilter('always', DeprecationWarning)  # turn off filter\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\twarnings.simplefilter('default', DeprecationWarning)  # reset filter\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn deprecated_decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tDecorate a function or method with a warning message to the user.\n\t\t\"\"\"\n\t\tdef new_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWarn the user then call the function.\n\t\t\t\"\"\"\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\tnew_func.__name__ = func.__name__\n\t\tnew_func.__doc__ = func.__doc__\n\t\tnew_func.__dict__.update(func.__dict__)\n\t\treturn new_func\n\treturn deprecated_decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tprint('%s is deprecated: %s' % (func.__name__, message))\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\twrapper.__dict__.update(func.__dict__)\n\t\treturn wrapper\n\treturn decorator\n\n\n@deprecated('use the `deprecated` package instead')\ndef test():\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tA decorator for deprecating functions and methods.\n\t\t\"\"\"\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe wrapper function for the deprecated function.\n\t\t\t\"\"\"\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\twrapper.__dict__.update(func.__dict__)\n\t\treturn wrapper\n\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn R_SETUP_CODE + code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t\toptions(\n\t\t\terror = function() {{\n\t\t\t\tmessage(\"{code.replace('\"', '\\\\\"')}\")\n\t\t\t\tq()\n\t\t\t\tinvisible()\n\t\t\t}}\n\t\t)\n\t\"\"\"\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# Set warning options\n\tcode += \"options(warn = 2, warn.conflicts = TRUE)\\n\"\n\t# Set RNG seed\n\tcode += \"set.seed(123)\\n\"\n\n\t# Set max.print to 100000\n\tcode += \"options(max.print=100000)\\n\"\n\n\t# Set max.print length to 100000\n\tcode += \"options(width=100000)\\n\"\n\treturn code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t\t# Set up inline R\n\t\tr_setup <- function() {{\n\t\t\t# Set the number of decimal places to display\n\t\t\toptions(digits = {R_DIGITS})\n\t\t}}\n\t\tr_setup()\n\t\t{code}\n\t\"\"\"\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\toptions(\n\t\twarn = -1,\n\t\terror = -1,\n\t\tshow.error.messages = FALSE,\n\t\twarn.conflicts = FALSE,\n\t\twarn.unused.packages = FALSE,\n\t\twarn.unused.arguments = FALSE,\n\t\twarn.implicit.assign = FALSE,\n\t\twarn.restart = FALSE,\n\t\twarn.order = FALSE,\n\t\twarn.packageload = FALSE,\n\t\twarn.empty.cell = FALSE,\n\t\twarn.assign = FALSE,\n\t\twarn.message = FALSE,\n\t\twarn.package.startup = FALSE,\n\t\twarn.deprecated = FALSE,\n\t\twarn.input.format = FALSE,\n\t\twarn.deprecated.graph = FALSE,\n\t\twarn.deprecated.time = FALSE,\n\t\twarn.deprecated.timeDate = FALSE,\n\t\twarn.deprecated.s3 = FALSE,\n\t\twarn.deprecated.s4 = FALSE,\n\t\twarn.deprecated.pkgconfig = FALSE,\n\t\twarn.deprecated.html = FALSE,\n\t\twarn.deprecated.xml = FALSE,\n\t\twarn.deprecated.libxml = FALSE,\n\t", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\noptions({{\n\twarn = -1, # do not warn about anything\n\tprint.eval = FALSE,\n\tprint.eval.print = FALSE,\n\techo = FALSE,\n\tshow.error.messages = FALSE,\n\tshow.warn.messages = FALSE,\n\tshow.error.call = FALSE,\n\tshow.source = FALSE,\n\tshow.output = {config.show_output},\n\tshow.method = {config.show_method},\n\tshow.signif.stars = {config.show_stars},\n\tshow.signif.values = {config.show_values}\n}});\n\"\"\" + code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t{code}\n\toptions(\n\t\twarn = 0,\n\t\terror = 0,\n\t\tuseFancyQuotes = FALSE,\n\t\texpressions = 0\n\t)\n\t\"\"\"\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t\toptions(R_BROWSER='{code}')\n\t\toptions(error=2)\n\t\toptions(repos='https://cran.rstudio.com')\n\t\toptions(warn=2)\n\t\toptions(encoding='UTF-8')\n\t\toptions(useFancyQuotes=FALSE)\n\t\toptions(useWarnings=TRUE)\n\t\toptions(use_tibble=FALSE)\n\t\toptions(warn.input=TRUE)\n\t\toptions(warn.output=TRUE)\n\t\toptions(warn.parse=TRUE)\n\t\toptions(warn.break.pos=TRUE)\n\t\toptions(warn.call=TRUE)\n\t\toptions(warn.order=TRUE)\n\t\toptions(warn.deprecated=TRUE)\n\t\toptions(warn.multiline=TRUE)\n\t\toptions(warn.unused=TRUE)\n\t\toptions(warn.unused.vars=TRUE)\n\t\toptions(warn.unused.args=TRUE)\n\t\toptions(warn.debug=TRUE)\n\t\toptions(echo=TRUE)\n\t\toptions(keep.source=TRUE)\n\t\toptions(keep.source.pkgs=FALSE)\n\t\toptions(keep.data=TRUE)\n\t\toptions(keep", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tresult = \"\"\"\n\t\toptions(\n\t\t\trepos = c(CRAN=repos),\n\t\t\twarn = 2,\n\t\t\terror = function(message, call, ..., immediate.exit=FALSE) {\n\t\t\t\tif (grepl(\"\\\\[WARNING\\\\]\", message) || grepl(\"\\\\[ERROR\\\\]\", message)) {\n\t\t\t\t\t# Do nothing\n\t\t\t\t} else {\n\t\t\t\t\tprint(message)\n\t\t\t\t\tif (grepl(\"\\\\[FATAL\\\\]\", message)) {\n\t\t\t\t\t\tstop(message)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t)\\n\"\"\"\n\treturn result + code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\tR.options(\n\t\twarn = 2,\n\t\terror = 1,\n\t\tdebug = 1,\n\t\tverbose = 1,\n\t\tmessage = 2,\n\t\tshow.error.messages = TRUE,\n\t\tshow.warning.messages = TRUE,\n\t\tshow.messages = TRUE,\n\t\tencoding = \"UTF-8\",\n\t\tresults = \"text\",\n\t\tkeep.source = TRUE,\n\t\tkeep.data = TRUE,\n\t\tkeep.output = TRUE,\n\t\tkeep.empty = TRUE,\n\t\tencoding.warnings = FALSE,\n\t\twarn.globals = TRUE,\n\t\twarn.order = TRUE,\n\t\twarn.conflicts = TRUE,\n\t\twarn.packages = TRUE,\n\t\tkeep.empty.environment = TRUE,\n\t\tno.environ.cache = TRUE,\n\t\tshow.error.calls = TRUE,\n\t\tprompt = \"> \",\n\t\tbrowser = FALSE,\n\t\tbrowser.max.size = 100,\n\t\tbrowser.max.lines = 100,\n\t\tbrowser.max.width = 100,\n\t\tbrowser.max.height = 100,\n\t\tbrowser.max.expr."], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn R_SETUP_CODE + code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t\toptions(\n\t\t\terror = function() {{\n\t\t\t\tmessage(\"{code.replace('\"', '\\\\\"')}\")\n\t\t\t\tq()\n\t\t\t\tinvisible()\n\t\t\t}}\n\t\t)\n\t\"\"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# Set warning options\n\tcode += \"options(warn = 2, warn.conflicts = TRUE)\\n\"\n\t# Set RNG seed\n\tcode += \"set.seed(123)\\n\"\n\n\t# Set max.print to 100000\n\tcode += \"options(max.print=100000)\\n\"\n\n\t# Set max.print length to 100000\n\tcode += \"options(width=100000)\\n\"\n\treturn code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t\t# Set up inline R\n\t\tr_setup <- function() {{\n\t\t\t# Set the number of decimal places to display\n\t\t\toptions(digits = {R_DIGITS})\n\t\t}}\n\t\tr_setup()\n\t\t{code}\n\t\"\"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\toptions(\n\t\twarn = -1,\n\t\terror = -1,\n\t\tshow.error.messages = FALSE,\n\t\twarn.conflicts = FALSE,\n\t\twarn.unused.packages = FALSE,\n\t\twarn.unused.arguments = FALSE,\n\t\twarn.implicit.assign = FALSE,\n\t\twarn.restart = FALSE,\n\t\twarn.order = FALSE,\n\t\twarn.packageload = FALSE,\n\t\twarn.empty.cell = FALSE,\n\t\twarn.assign = FALSE,\n\t\twarn.message = FALSE,\n\t\twarn.package.startup = FALSE,\n\t\twarn.deprecated = FALSE,\n\t\twarn.input.format = FALSE,\n\t\twarn.deprecated.graph = FALSE,\n\t\twarn.deprecated.time = FALSE,\n\t\twarn.deprecated.timeDate = FALSE,\n\t\twarn.deprecated.s3 = FALSE,\n\t\twarn.deprecated.s4 = FALSE,\n\t\twarn.deprecated.pkgconfig = FALSE,\n\t\twarn.deprecated.html = FALSE,\n\t\twarn.deprecated.xml = FALSE,\n\t\twarn.deprecated.libxml = FALSE,\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\noptions({{\n\twarn = -1, # do not warn about anything\n\tprint.eval = FALSE,\n\tprint.eval.print = FALSE,\n\techo = FALSE,\n\tshow.error.messages = FALSE,\n\tshow.warn.messages = FALSE,\n\tshow.error.call = FALSE,\n\tshow.source = FALSE,\n\tshow.output = {config.show_output},\n\tshow.method = {config.show_method},\n\tshow.signif.stars = {config.show_stars},\n\tshow.signif.values = {config.show_values}\n}});\n\"\"\" + code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t{code}\n\toptions(\n\t\twarn = 0,\n\t\terror = 0,\n\t\tuseFancyQuotes = FALSE,\n\t\texpressions = 0\n\t)\n\t\"\"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\t\toptions(R_BROWSER='{code}')\n\t\toptions(error=2)\n\t\toptions(repos='https://cran.rstudio.com')\n\t\toptions(warn=2)\n\t\toptions(encoding='UTF-8')\n\t\toptions(useFancyQuotes=FALSE)\n\t\toptions(useWarnings=TRUE)\n\t\toptions(use_tibble=FALSE)\n\t\toptions(warn.input=TRUE)\n\t\toptions(warn.output=TRUE)\n\t\toptions(warn.parse=TRUE)\n\t\toptions(warn.break.pos=TRUE)\n\t\toptions(warn.call=TRUE)\n\t\toptions(warn.order=TRUE)\n\t\toptions(warn.deprecated=TRUE)\n\t\toptions(warn.multiline=TRUE)\n\t\toptions(warn.unused=TRUE)\n\t\toptions(warn.unused.vars=TRUE)\n\t\toptions(warn.unused.args=TRUE)\n\t\toptions(warn.debug=TRUE)\n\t\toptions(echo=TRUE)\n\t\toptions(keep.source=TRUE)\n\t\toptions(keep.source.pkgs=FALSE)\n\t\toptions(keep.data=TRUE)\n\t\toptions(keep", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tresult = \"\"\"\n\t\toptions(\n\t\t\trepos = c(CRAN=repos),\n\t\t\twarn = 2,\n\t\t\terror = function(message, call, ..., immediate.exit=FALSE) {\n\t\t\t\tif (grepl(\"\\\\[WARNING\\\\]\", message) || grepl(\"\\\\[ERROR\\\\]\", message)) {\n\t\t\t\t\t# Do nothing\n\t\t\t\t} else {\n\t\t\t\t\tprint(message)\n\t\t\t\t\tif (grepl(\"\\\\[FATAL\\\\]\", message)) {\n\t\t\t\t\t\tstop(message)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t)\\n\"\"\"\n\treturn result + code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\n\tR.options(\n\t\twarn = 2,\n\t\terror = 1,\n\t\tdebug = 1,\n\t\tverbose = 1,\n\t\tmessage = 2,\n\t\tshow.error.messages = TRUE,\n\t\tshow.warning.messages = TRUE,\n\t\tshow.messages = TRUE,\n\t\tencoding = \"UTF-8\",\n\t\tresults = \"text\",\n\t\tkeep.source = TRUE,\n\t\tkeep.data = TRUE,\n\t\tkeep.output = TRUE,\n\t\tkeep.empty = TRUE,\n\t\tencoding.warnings = FALSE,\n\t\twarn.globals = TRUE,\n\t\twarn.order = TRUE,\n\t\twarn.conflicts = TRUE,\n\t\twarn.packages = TRUE,\n\t\tkeep.empty.environment = TRUE,\n\t\tno.environ.cache = TRUE,\n\t\tshow.error.calls = TRUE,\n\t\tprompt = \"> \",\n\t\tbrowser = FALSE,\n\t\tbrowser.max.size = 100,\n\t\tbrowser.max.lines = 100,\n\t\tbrowser.max.width = 100,\n\t\tbrowser.max.height = 100,\n\t\tbrowser.max.expr.", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tassert len(varargs) > 0\n\tassert target_concurrency > 0\n\tassert _max_length > 0\n\n\t# split varargs into chunks\n\tvarargs_slice = list(chunked(varargs, _max_length))\n\t# for each chunk, run the command\n\tfor varargs_slice_i in varargs_slice:\n\t\targs = [str(arg) for arg in varargs_slice_i]\n\t\tif color:\n\t\t\targs = [\"--color=always\"] + args\n\t\tcmd_i = cmd + args\n\t\tprint(f\"{cmd_i}\")\n\t\trun_cmd(cmd_i, **kwargs)\n\t\ttime.sleep(0.1)\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif target_concurrency < 1:\n\t    raise ValueError(\"target_concurrency must be a positive integer\")\n\tif _max_length <= 0:\n\t    raise ValueError(\"max_length must be a positive integer\")\n\tif _max_length < len(cmd):\n\t    raise ValueError(\"max_length must be at least as long as the command\")\n\tif len(varargs) == 0:\n\t    return []\n\tif color:\n\t    print(f\"$ {cmd}\", end=\"\", flush=True)\n\tvarargs = list(varargs)\n\tresult = []\n\twhile len(varargs) > 0:\n\t    if color:\n\t\tprint(\" >\", end=\"\", flush=True)\n\t    result.append(\n\t\t    subprocess.run(\n\t\t\t    cmd,\n\t\t\t    input=\"\\n\".join(varargs[:_max_length]).encode(),\n\t\t\t    stdout=subprocess.PIPE,\n\t\t\t    check=True,\n\t\t\t    **kwargs,\n\t\t\t    ).stdout.decode().splitlines()\n\t\t\t)\n\t    varargs = result[-1]\n\t    # print(f\"{result}\")\n\t    if color:\n\t\tprint(\"\\r\\x", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif color:\n\t\tprint(f\"{Fore.MAGENTA}{'xargs':10} {Fore.RESET}\", end='')\n\tif not varargs:\n\t\tprint(f\"{Fore.LIGHTRED_EX}Usage: xargs <cmd> <varargs>...{Fore.RESET}\")\n\t\treturn\n\tif not cmd:\n\t\tprint(f\"{Fore.LIGHTRED_EX}Usage: xargs <cmd> <varargs>...{Fore.RESET}\")\n\t\treturn\n\tif target_concurrency > _max_length:\n\t\tprint(f\"{Fore.LIGHTRED_EX}Error: Target Concurrency > Max Length{Fore.RESET}\")\n\t\treturn\n\ttarget_concurrency = max(target_concurrency, 1)\n\t\n\tdef _worker(varargs: Sequence[str], cmd: tuple[str, ...], **kwargs) -> None:\n\t\tif not varargs:\n\t\t\treturn\n\t\tif not cmd:\n\t\t\treturn\n\t\tif not cmd[0]:\n\t\t\treturn\n\t\tif not cmd[0].strip():\n\t\t\treturn\n\t\tif not cmd[0].strip().split():\n\t\t", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif not varargs:\n\t\traise RuntimeError(f'Varargs cannot be empty')\n\tif not all(isinstance(i, str) for i in varargs):\n\t\traise RuntimeError('Varargs must be strings')\n\tif len(varargs) > _max_length:\n\t\traise RuntimeError(f'Varargs length cannot exceed {_max_length}')\n\n\tif not isinstance(cmd, tuple):\n\t\traise RuntimeError('cmd must be a tuple')\n\tcmd = list(cmd)\n\tif not all(isinstance(i, str) for i in cmd):\n\t\traise RuntimeError('cmd must be a tuple of strings')\n\n\tif color:\n\t\tcmd.append('--color=auto')\n\tif target_concurrency > 1:\n\t\tcmd.append(f'-P{target_concurrency}')\n\tif kwargs:\n\t\tcmd.extend(f'--{k}={v}' for k, v in kwargs.items())\n\n\tcmd.append('--')\n\tcmd.extend(varargs)\n\n\treturn tuple(cmd)\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif target_concurrency > 1:\n\t\traise NotImplementedError(\n\t\t\t'This implementation is not thread-safe and is not implemented'\n\t\t\t' to support concurrency greater than 1')\n\tif len(varargs) > _max_length:\n\t\traise ValueError(f'varargs cannot exceed {_max_length}')\n\tif isinstance(cmd, str):\n\t\tcmd = cmd.split()\n\tif not cmd:\n\t\traise ValueError('cmd must be a non-empty sequence of str')\n\tif not isinstance(cmd[0], str):\n\t\traise ValueError('cmd must be a sequence of str')\n\tif not varargs:\n\t\treturn cmd\n\tif isinstance(varargs, str):\n\t\tvarargs = varargs.split()\n\tif not isinstance(varargs, Sequence):\n\t\traise ValueError('varargs must be a sequence of str')\n\n\tvarargs = list(varargs)\n\tvarargs_length = len(varargs)\n\tcmd_length = len(cmd)\n\tcmd_length_with_varargs = cmd_length + 1\n\tif cmd_length_with_varargs > _max_length:\n\t\traise ValueError(\n\t\t\tf'cmd +", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif color:\n\t\timport colorama\n\t\tcolorama.init()\n\t\tcolor = colorama.Fore.CYAN\n\t\tcolor_end = colorama.Style.RESET_ALL\n\telse:\n\t\tcolor = \"\"\n\t\tcolor_end = \"\"\n\tif isinstance(cmd, list):\n\t\tcmd = tuple(cmd)\n\tif len(cmd) > _max_length:\n\t\traise ValueError(f\"Maximum Length of the Command is {_max_length}, \"\n\t\t\t\t\t\tf\"but you are trying to execute {len(cmd)} args\")\n\tif len(varargs) == 0:\n\t\treturn cmd\n\tif len(varargs) > _max_length:\n\t\traise ValueError(f\"Maximum Length of the Command is {_max_length}, \"\n\t\t\t\t\t\tf\"but you are trying to execute {len(varargs)} args\")\n\tif len(varargs) == 1:\n\t\treturn (cmd + (varargs,))\n\tvarargs = tuple(varargs)\n\tif target_concurrency == 1:\n\t\treturn (cmd + (varargs,))\n\targs = []\n\tfor arg in varargs:\n\t\targs.append(arg)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# Check the number of arguments, if it is less than 2, raise an error\n\tif len(cmd) < 2:\n\t\traise ValueError(\"Atleast 2 Arguments are required\")\n\n\t# Check the target concurrency, if it is less than 1, raise an error\n\tif target_concurrency < 1:\n\t\traise ValueError(\"Target Concurrency must be greater than 0\")\n\n\t# Check the max length, if it is less than 1, raise an error\n\tif _max_length < 1:\n\t\traise ValueError(\"Max Length must be greater than 0\")\n\n\t# Check the max length, if it is less than the minimum, raise an error\n\tif _max_length < 1:\n\t\traise ValueError(\"Max Length must be greater than 0\")\n\n\t# Check the max length, if it is less than the minimum, raise an error\n\tif _max_length < 1:\n\t\traise ValueError(\"Max Length must be greater than 0\")\n\n\t# Create the output pipes\n\tout_r, out_w = os.pipe()\n\terr_r, err_w = os.pipe()\n\n\t# Create the output pipes\n\tout_r, out_w = os.pipe()\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# TODO: Add support for --show-limits\n\t# TODO: Add support for --no-run-if-empty\n\tif not color:\n\t\traise NotImplementedError\n\n\tif target_concurrency < 1:\n\t\traise ValueError(\"target_concurrency must be greater than 0\")\n\n\targs_length = 0\n\tfor arg in cmd:\n\t\targs_length += len(arg) + 1\n\n\tif args_length + len(varargs) > _max_length:\n\t\traise ValueError(\"arguments are too long\")\n\n\tkwargs.update(\n\t\t_xargs_flags(\n\t\t\tcolor,\n\t\t\ttarget_concurrency,\n\t\t)\n\t)\n\n\tcmd_args = [\n\t\t\"xargs\",\n\t\t\"--\",\n\t\t*cmd,\n\t\t*varargs,\n\t]\n\n\treturn cmd_args, kwargs\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# get the max length of the command\n\tcommand_len = _len_of_command(cmd)\n\t# get the max length of the command\n\tvarargs_len = _len_of_args(varargs)\n\t# if the total size of the command and varargs is less than the max size\n\tif command_len + varargs_len <= _max_length:\n\t\t# run the command\n\t\treturn _run(cmd,\n\t\t            varargs,\n\t\t            color=color,\n\t\t            target_concurrency=target_concurrency,\n\t\t            **kwargs)\n\t# else we need to split the command\n\telse:\n\t\t# we split the command using the max length\n\t\tsplit_cmd = _split_command(cmd, _max_length)\n\t\t# we split the varargs using the max length\n\t\tsplit_varargs = _split_args(varargs, _max_length)\n\t\t# we need to run each command inside a thread\n\t\tthreads = []\n\t\t# we need to run each command inside a thread\n\t\tfor cmd, varargs in zip(split_cmd, split_varargs):\n\t\t\t# we create a thread and run the command\n\t\t\tt = Thread(target=_run, args=(cmd, var", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# TODO: Add support for -I flag\n\n\tif target_concurrency > _max_length:\n\t\traise ValueError(f\"target_concurrency cannot be greater than {_max_length}\")\n\n\tif not cmd:\n\t\traise ValueError(\"cmd cannot be empty\")\n\n\tif not isinstance(varargs, list) or not varargs:\n\t\traise ValueError(\"varargs cannot be empty\")\n\n\tif color:\n\t\tcolorize_cmd = _colorize_cmd(cmd)\n\t\tcmd = (colorize_cmd,)\n\n\tvarargs = list(varargs)\n\n\t# TODO: Add support for -n flag\n\t# TODO: Add support for -L flag\n\n\tif target_concurrency == 1:\n\t\treturn _run_all_in_parallel(cmd[0], varargs, **kwargs)\n\n\tif target_concurrency > _max_length:\n\t\treturn _run_all_in_parallel(cmd[0], varargs, **kwargs)\n\n\t# target_concurrency <= _max_length\n\n\t# TODO: Add support for -P flag\n\n\t# TODO: Add support for -t flag\n\n\t# TODO: Add support for -0 flag\n\n\t# TODO: Add support for -a flag\n\n\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tassert len(varargs) > 0\n\tassert target_concurrency > 0\n\tassert _max_length > 0\n\n\t# split varargs into chunks\n\tvarargs_slice = list(chunked(varargs, _max_length))\n\t# for each chunk, run the command\n\tfor varargs_slice_i in varargs_slice:\n\t\targs = [str(arg) for arg in varargs_slice_i]\n\t\tif color:\n\t\t\targs = [\"--color=always\"] + args\n\t\tcmd_i = cmd + args\n\t\tprint(f\"{cmd_i}\")\n\t\trun_cmd(cmd_i, **kwargs)\n\t\ttime.sleep(0.1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif target_concurrency < 1:\n\t    raise ValueError(\"target_concurrency must be a positive integer\")\n\tif _max_length <= 0:\n\t    raise ValueError(\"max_length must be a positive integer\")\n\tif _max_length < len(cmd):\n\t    raise ValueError(\"max_length must be at least as long as the command\")\n\tif len(varargs) == 0:\n\t    return []\n\tif color:\n\t    print(f\"$ {cmd}\", end=\"\", flush=True)\n\tvarargs = list(varargs)\n\tresult = []\n\twhile len(varargs) > 0:\n\t    if color:\n\t\tprint(\" >\", end=\"\", flush=True)\n\t    result.append(\n\t\t    subprocess.run(\n\t\t\t    cmd,\n\t\t\t    input=\"\\n\".join(varargs[:_max_length]).encode(),\n\t\t\t    stdout=subprocess.PIPE,\n\t\t\t    check=True,\n\t\t\t    **kwargs,\n\t\t\t    ).stdout.decode().splitlines()\n\t\t\t)\n\t    varargs = result[-1]\n\t    # print(f\"{result}\")\n\t    if color:\n\t\tprint(\"\\r\\x", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif color:\n\t\tprint(f\"{Fore.MAGENTA}{'xargs':10} {Fore.RESET}\", end='')\n\tif not varargs:\n\t\tprint(f\"{Fore.LIGHTRED_EX}Usage: xargs <cmd> <varargs>...{Fore.RESET}\")\n\t\treturn\n\tif not cmd:\n\t\tprint(f\"{Fore.LIGHTRED_EX}Usage: xargs <cmd> <varargs>...{Fore.RESET}\")\n\t\treturn\n\tif target_concurrency > _max_length:\n\t\tprint(f\"{Fore.LIGHTRED_EX}Error: Target Concurrency > Max Length{Fore.RESET}\")\n\t\treturn\n\ttarget_concurrency = max(target_concurrency, 1)\n\t\n\tdef _worker(varargs: Sequence[str], cmd: tuple[str, ...], **kwargs) -> None:\n\t\tif not varargs:\n\t\t\treturn\n\t\tif not cmd:\n\t\t\treturn\n\t\tif not cmd[0]:\n\t\t\treturn\n\t\tif not cmd[0].strip():\n\t\t\treturn\n\t\tif not cmd[0].strip().split():\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif not varargs:\n\t\traise RuntimeError(f'Varargs cannot be empty')\n\tif not all(isinstance(i, str) for i in varargs):\n\t\traise RuntimeError('Varargs must be strings')\n\tif len(varargs) > _max_length:\n\t\traise RuntimeError(f'Varargs length cannot exceed {_max_length}')\n\n\tif not isinstance(cmd, tuple):\n\t\traise RuntimeError('cmd must be a tuple')\n\tcmd = list(cmd)\n\tif not all(isinstance(i, str) for i in cmd):\n\t\traise RuntimeError('cmd must be a tuple of strings')\n\n\tif color:\n\t\tcmd.append('--color=auto')\n\tif target_concurrency > 1:\n\t\tcmd.append(f'-P{target_concurrency}')\n\tif kwargs:\n\t\tcmd.extend(f'--{k}={v}' for k, v in kwargs.items())\n\n\tcmd.append('--')\n\tcmd.extend(varargs)\n\n\treturn tuple(cmd)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif target_concurrency > 1:\n\t\traise NotImplementedError(\n\t\t\t'This implementation is not thread-safe and is not implemented'\n\t\t\t' to support concurrency greater than 1')\n\tif len(varargs) > _max_length:\n\t\traise ValueError(f'varargs cannot exceed {_max_length}')\n\tif isinstance(cmd, str):\n\t\tcmd = cmd.split()\n\tif not cmd:\n\t\traise ValueError('cmd must be a non-empty sequence of str')\n\tif not isinstance(cmd[0], str):\n\t\traise ValueError('cmd must be a sequence of str')\n\tif not varargs:\n\t\treturn cmd\n\tif isinstance(varargs, str):\n\t\tvarargs = varargs.split()\n\tif not isinstance(varargs, Sequence):\n\t\traise ValueError('varargs must be a sequence of str')\n\n\tvarargs = list(varargs)\n\tvarargs_length = len(varargs)\n\tcmd_length = len(cmd)\n\tcmd_length_with_varargs = cmd_length + 1\n\tif cmd_length_with_varargs > _max_length:\n\t\traise ValueError(\n\t\t\tf'cmd +", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tif color:\n\t\timport colorama\n\t\tcolorama.init()\n\t\tcolor = colorama.Fore.CYAN\n\t\tcolor_end = colorama.Style.RESET_ALL\n\telse:\n\t\tcolor = \"\"\n\t\tcolor_end = \"\"\n\tif isinstance(cmd, list):\n\t\tcmd = tuple(cmd)\n\tif len(cmd) > _max_length:\n\t\traise ValueError(f\"Maximum Length of the Command is {_max_length}, \"\n\t\t\t\t\t\tf\"but you are trying to execute {len(cmd)} args\")\n\tif len(varargs) == 0:\n\t\treturn cmd\n\tif len(varargs) > _max_length:\n\t\traise ValueError(f\"Maximum Length of the Command is {_max_length}, \"\n\t\t\t\t\t\tf\"but you are trying to execute {len(varargs)} args\")\n\tif len(varargs) == 1:\n\t\treturn (cmd + (varargs,))\n\tvarargs = tuple(varargs)\n\tif target_concurrency == 1:\n\t\treturn (cmd + (varargs,))\n\targs = []\n\tfor arg in varargs:\n\t\targs.append(arg)", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# Check the number of arguments, if it is less than 2, raise an error\n\tif len(cmd) < 2:\n\t\traise ValueError(\"Atleast 2 Arguments are required\")\n\n\t# Check the target concurrency, if it is less than 1, raise an error\n\tif target_concurrency < 1:\n\t\traise ValueError(\"Target Concurrency must be greater than 0\")\n\n\t# Check the max length, if it is less than 1, raise an error\n\tif _max_length < 1:\n\t\traise ValueError(\"Max Length must be greater than 0\")\n\n\t# Check the max length, if it is less than the minimum, raise an error\n\tif _max_length < 1:\n\t\traise ValueError(\"Max Length must be greater than 0\")\n\n\t# Check the max length, if it is less than the minimum, raise an error\n\tif _max_length < 1:\n\t\traise ValueError(\"Max Length must be greater than 0\")\n\n\t# Create the output pipes\n\tout_r, out_w = os.pipe()\n\terr_r, err_w = os.pipe()\n\n\t# Create the output pipes\n\tout_r, out_w = os.pipe()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# TODO: Add support for --show-limits\n\t# TODO: Add support for --no-run-if-empty\n\tif not color:\n\t\traise NotImplementedError\n\n\tif target_concurrency < 1:\n\t\traise ValueError(\"target_concurrency must be greater than 0\")\n\n\targs_length = 0\n\tfor arg in cmd:\n\t\targs_length += len(arg) + 1\n\n\tif args_length + len(varargs) > _max_length:\n\t\traise ValueError(\"arguments are too long\")\n\n\tkwargs.update(\n\t\t_xargs_flags(\n\t\t\tcolor,\n\t\t\ttarget_concurrency,\n\t\t)\n\t)\n\n\tcmd_args = [\n\t\t\"xargs\",\n\t\t\"--\",\n\t\t*cmd,\n\t\t*varargs,\n\t]\n\n\treturn cmd_args, kwargs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# get the max length of the command\n\tcommand_len = _len_of_command(cmd)\n\t# get the max length of the command\n\tvarargs_len = _len_of_args(varargs)\n\t# if the total size of the command and varargs is less than the max size\n\tif command_len + varargs_len <= _max_length:\n\t\t# run the command\n\t\treturn _run(cmd,\n\t\t            varargs,\n\t\t            color=color,\n\t\t            target_concurrency=target_concurrency,\n\t\t            **kwargs)\n\t# else we need to split the command\n\telse:\n\t\t# we split the command using the max length\n\t\tsplit_cmd = _split_command(cmd, _max_length)\n\t\t# we split the varargs using the max length\n\t\tsplit_varargs = _split_args(varargs, _max_length)\n\t\t# we need to run each command inside a thread\n\t\tthreads = []\n\t\t# we need to run each command inside a thread\n\t\tfor cmd, varargs in zip(split_cmd, split_varargs):\n\t\t\t# we create a thread and run the command\n\t\t\tt = Thread(target=_run, args=(cmd, var", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t# TODO: Add support for -I flag\n\n\tif target_concurrency > _max_length:\n\t\traise ValueError(f\"target_concurrency cannot be greater than {_max_length}\")\n\n\tif not cmd:\n\t\traise ValueError(\"cmd cannot be empty\")\n\n\tif not isinstance(varargs, list) or not varargs:\n\t\traise ValueError(\"varargs cannot be empty\")\n\n\tif color:\n\t\tcolorize_cmd = _colorize_cmd(cmd)\n\t\tcmd = (colorize_cmd,)\n\n\tvarargs = list(varargs)\n\n\t# TODO: Add support for -n flag\n\t# TODO: Add support for -L flag\n\n\tif target_concurrency == 1:\n\t\treturn _run_all_in_parallel(cmd[0], varargs, **kwargs)\n\n\tif target_concurrency > _max_length:\n\t\treturn _run_all_in_parallel(cmd[0], varargs, **kwargs)\n\n\t# target_concurrency <= _max_length\n\n\t# TODO: Add support for -P flag\n\n\t# TODO: Add support for -t flag\n\n\t# TODO: Add support for -0 flag\n\n\t# TODO: Add support for -a flag\n\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tif len(seq) <= 1:\n\t\treturn list(seq)\n\telse:\n\t\treturn seq[0:1] + _shuffled(seq[1:])\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tshuffled = list(seq)\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(shuffled)\n\treturn shuffled\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(seq)\n\treturn seq\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tif len(seq) <= 1:\n\t\treturn list(seq)\n\telse:\n\t\treturn seq[0:1] + _shuffled(seq[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\tshuffled = list(seq)\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(shuffled)\n\treturn shuffled\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split(\".\"))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(map(int, s.split('.')))\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(x) for x in s.split(\".\"))\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(part) for part in s.split('.'))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(part) for part in s.split(\".\"))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(x) for x in s.split(\".\"))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(map(int, s.split('.')))\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split(\".\"))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(map(int, s.split('.')))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(x) for x in s.split(\".\"))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(part) for part in s.split('.'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(part) for part in s.split(\".\"))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(x) for x in s.split(\".\"))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\treturn tuple(map(int, s.split('.')))\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif len(cmd) == 0:\n\t\traise ValueError('No command to normalize')\n\n\tif cmd[0] == '':\n\t\treturn cmd\n\n\tif '\\\\' in cmd[0]:\n\t\t# Assume this is an absolute path to the executable\n\t\treturn cmd\n\telse:\n\t\t# Assume this is a relative path to the executable\n\t\tif '.' in cmd[0]:\n\t\t\t# Assume this is a relative path to the current directory\n\t\t\treturn (os.path.join(os.getcwd(), cmd[0]), *cmd[1:])\n\t\telse:\n\t\t\t# Assume this is a relative path to the current directory\n\t\t\treturn (os.path.join(os.getcwd(), '.' + cmd[0]), *cmd[1:])\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tif not os.path.isfile(exe) and not os.path.isabs(exe):\n\t\texe = shutil.which(exe)\n\tif exe is None:\n\t\traise ValueError(f\"Cannot find executable {exe!r}\")\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif sys.platform == \"win32\":\n\t\texe_format = \"{}.exe\"\n\telse:\n\t\texe_format = \"{}\"\n\tif cmd[0].endswith(\".exe\"):\n\t\treturn tuple(exe_format.format(part) for part in cmd)\n\telse:\n\t\treturn tuple(exe_format.format(part) for part in cmd if not part.endswith(\".exe\"))\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\treturn tuple(os.path.abspath(cmd[0]) if cmd[0].endswith('.exe') else cmd[0])\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif os.name == 'nt':\n\t\treturn cmd\n\telse:\n\t\treturn (cmd[0],)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif os.name == 'nt':\n\t\tif not os.path.splitext(cmd[0])[1]:\n\t\t\tcmd = (os.path.join(os.environ['SYSTEMROOT'], 'system32', cmd[0] + '.exe'), *cmd[1:])\n\telse:\n\t\tif not os.path.dirname(cmd[0]):\n\t\t\tcmd = (os.path.join('/usr/bin', cmd[0]), *cmd[1:])\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif cmd[0].startswith(\".\"):\n\t\treturn os.path.normpath(os.path.join(os.getcwd(), cmd[0])), *cmd[1:]\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif len(cmd) == 0:\n\t\traise ValueError(\"Command cannot be empty\")\n\tif not isinstance(cmd[0], str):\n\t\traise TypeError(f\"Expected string but got {type(cmd[0])}\")\n\tif cmd[0][0] != \"/\":\n\t\tif cmd[0][0] == \".\":\n\t\t\tcmd = (cmd[0][1:], *cmd[1:])\n\t\t\tif cmd[0] == \"\":\n\t\t\t\traise ValueError(\"Command's first argument cannot be empty\")\n\t\t\tif cmd[0][0] == \"/\":\n\t\t\t\traise ValueError(\"Command's first argument cannot start with a slash\")\n\t\tcmd = (os.path.join(os.getcwd(), cmd[0]), *cmd[1:])\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif sys.platform == 'win32':\n\t\timport os\n\t\tif ' ' in cmd[0]:\n\t\t\tcmd = (r'\"' + cmd[0] + r'\"', *cmd[1:])\n\t\tif not os.path.isabs(cmd[0]):\n\t\t\tcmd = (os.path.join(sys.exec_prefix, \"", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif not cmd or not isinstance(cmd, tuple):\n\t\treturn tuple()\n\texe = cmd[0]\n\ttry:\n\t\timport os\n\t\tif os.path.isfile(exe):\n\t\t\treturn cmd\n\t\telse:\n\t\t\timport sys\n\t\t\tif os.path.isfile(exe = f\".{os.sep}{exe}\"):\n\t\t\t\treturn (f\".{os.sep}{exe}\",) + cmd[1:]\n\t\t\telse:\n\t\t\t\texe = os.path.join(sys.prefix, exe)\n\t\t\t\tif os.path.isfile(exe):\n\t\t\t\t\treturn (exe,) + cmd[1:]\n\t\t\t\telse:\n\t\t\t\t\timport warnings\n\t\t\t\t\twarnings.warn(f\"Cannot find '{exe}' in $PATH or $PYTHONPATH\")\n\t\t\t\t\treturn cmd\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn cmd\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif len(cmd) == 0:\n\t\traise ValueError('No command to normalize')\n\n\tif cmd[0] == '':\n\t\treturn cmd\n\n\tif '\\\\' in cmd[0]:\n\t\t# Assume this is an absolute path to the executable\n\t\treturn cmd\n\telse:\n\t\t# Assume this is a relative path to the executable\n\t\tif '.' in cmd[0]:\n\t\t\t# Assume this is a relative path to the current directory\n\t\t\treturn (os.path.join(os.getcwd(), cmd[0]), *cmd[1:])\n\t\telse:\n\t\t\t# Assume this is a relative path to the current directory\n\t\t\treturn (os.path.join(os.getcwd(), '.' + cmd[0]), *cmd[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tif not os.path.isfile(exe) and not os.path.isabs(exe):\n\t\texe = shutil.which(exe)\n\tif exe is None:\n\t\traise ValueError(f\"Cannot find executable {exe!r}\")\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif sys.platform == \"win32\":\n\t\texe_format = \"{}.exe\"\n\telse:\n\t\texe_format = \"{}\"\n\tif cmd[0].endswith(\".exe\"):\n\t\treturn tuple(exe_format.format(part) for part in cmd)\n\telse:\n\t\treturn tuple(exe_format.format(part) for part in cmd if not part.endswith(\".exe\"))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\treturn tuple(os.path.abspath(cmd[0]) if cmd[0].endswith('.exe') else cmd[0])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif os.name == 'nt':\n\t\treturn cmd\n\telse:\n\t\treturn (cmd[0],)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif os.name == 'nt':\n\t\tif not os.path.splitext(cmd[0])[1]:\n\t\t\tcmd = (os.path.join(os.environ['SYSTEMROOT'], 'system32', cmd[0] + '.exe'), *cmd[1:])\n\telse:\n\t\tif not os.path.dirname(cmd[0]):\n\t\t\tcmd = (os.path.join('/usr/bin', cmd[0]), *cmd[1:])\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif cmd[0].startswith(\".\"):\n\t\treturn os.path.normpath(os.path.join(os.getcwd(), cmd[0])), *cmd[1:]\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif len(cmd) == 0:\n\t\traise ValueError(\"Command cannot be empty\")\n\tif not isinstance(cmd[0], str):\n\t\traise TypeError(f\"Expected string but got {type(cmd[0])}\")\n\tif cmd[0][0] != \"/\":\n\t\tif cmd[0][0] == \".\":\n\t\t\tcmd = (cmd[0][1:], *cmd[1:])\n\t\t\tif cmd[0] == \"\":\n\t\t\t\traise ValueError(\"Command's first argument cannot be empty\")\n\t\t\tif cmd[0][0] == \"/\":\n\t\t\t\traise ValueError(\"Command's first argument cannot start with a slash\")\n\t\tcmd = (os.path.join(os.getcwd(), cmd[0]), *cmd[1:])\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif sys.platform == 'win32':\n\t\timport os\n\t\tif ' ' in cmd[0]:\n\t\t\tcmd = (r'\"' + cmd[0] + r'\"', *cmd[1:])\n\t\tif not os.path.isabs(cmd[0]):\n\t\t\tcmd = (os.path.join(sys.exec_prefix, \"", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tif not cmd or not isinstance(cmd, tuple):\n\t\treturn tuple()\n\texe = cmd[0]\n\ttry:\n\t\timport os\n\t\tif os.path.isfile(exe):\n\t\t\treturn cmd\n\t\telse:\n\t\t\timport sys\n\t\t\tif os.path.isfile(exe = f\".{os.sep}{exe}\"):\n\t\t\t\treturn (f\".{os.sep}{exe}\",) + cmd[1:]\n\t\t\telse:\n\t\t\t\texe = os.path.join(sys.prefix, exe)\n\t\t\t\tif os.path.isfile(exe):\n\t\t\t\t\treturn (exe,) + cmd[1:]\n\t\t\t\telse:\n\t\t\t\t\timport warnings\n\t\t\t\t\twarnings.warn(f\"Cannot find '{exe}' in $PATH or $PYTHONPATH\")\n\t\t\t\t\treturn cmd\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn cmd\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tif lock is None:\n\t\t\tlock = Lock()\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, value)\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith cache.lock(lock) if lock else nullcontext():\n\t\t\t\tif key in cache:\n\t\t\t\t\treturn cache[key]\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tlock.acquire()\n\t\t\t\tif cache.exists(key):\n\t\t\t\t\treturn cache.get(key)\n\t\t\t\telse:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, value)\n\t\t\t\t\treturn value\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\thashkey = key(func, args, kwargs)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tvalue = cache.get(hashkey)\n\t\t\texcept Exception:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\traise\n\n\t\t\tif value is None:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache.set(hashkey, value)\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tif lock is None:\n\t\tlock = threading.Lock()\n\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tDecorates a function to cache its results\n\t\t\"\"\"\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function to cache results\n\t\t\t\"\"\"\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache.set(key, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorate\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key(*args, **kwargs))\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Error in cache\", e)\n\t\t\t\tresult = None\n\t\t\tif result:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\t\telse:\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tcache.set(key(*args, **kwargs), result)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorate(function):\n\t\t@functools.wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t_key = key(*args, **kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tret = cache.get(_key)\n\t\t\texcept:\n\t\t\t\tret = None\n\t\t\tif ret is not None:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn ret\n\t\t\tret = function(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tcache.set(_key, ret)\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\t\t\treturn ret\n\t\treturn wrapper\n\treturn decorate\n\nclass MemoryCache(object):\n\t\"\"\"\n\tSimple in-memory cache for small data.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._cache = {}\n\t\tself._lock = threading.Lock()\n\n\tdef set(self, key, value):\n\t\t\"\"\"\n\t\tSet a value\n\t\t\"\"\"\n\t", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorate(f):\n\t\tdef decorated(*args, **kwargs):\n\t\t\tkey = key(args, kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = f(*args, **kwargs)\n\t\t\t\t\tcache.set(key, value)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn decorated\n\treturn decorate\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef decorated_function(*args, **kwargs):\n\t\t\tkey_ = key(*args, **kwargs)\n\t\t\twith cache.lock(key_, lock) as lock:\n\t\t\t\tif cache.has(key_):\n\t\t\t\t\treturn cache.get(key_)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(key_, result)\n\t\t\t\treturn result\n\t\treturn decorated_function\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef _decorator(func):\n\t\tdef _wrapper(*args, **kwargs):\n\t\t\tif lock is None:\n\t\t\t\tlock = threading.Lock()\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key(*args, **kwargs))\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key(*args, **kwargs), result)\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn _wrapper\n\treturn _decorator\n\n\n# @cached(cache=Cache(1000))\ndef hashkey(*args, **kwargs):\n\t\"\"\"\n\tReturns a hash key for the function\n\t\"\"\"\n\treturn hashlib.md5(pickle.dumps((args, kwargs), protocol=-1)).hexdigest()\n\n\nclass Cache(object):\n\t\"\"\"\n\tCache object that stores and retrieves data from a cache\n\t\"\"\"\n\tdef __init__(self, size=0):\n\t\t\"\"\"\n\t\tCreates a new Cache\n\n\t\t:param size: Number of elements to store\n\t\t\"\"\"\n\t\tself"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tif lock is None:\n\t\t\tlock = Lock()\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, value)\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith cache.lock(lock) if lock else nullcontext():\n\t\t\t\tif key in cache:\n\t\t\t\t\treturn cache[key]\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tlock.acquire()\n\t\t\t\tif cache.exists(key):\n\t\t\t\t\treturn cache.get(key)\n\t\t\t\telse:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, value)\n\t\t\t\t\treturn value\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\thashkey = key(func, args, kwargs)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tvalue = cache.get(hashkey)\n\t\t\texcept Exception:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\traise\n\n\t\t\tif value is None:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache.set(hashkey, value)\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tif lock is None:\n\t\tlock = threading.Lock()\n\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tDecorates a function to cache its results\n\t\t\"\"\"\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function to cache results\n\t\t\t\"\"\"\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache.set(key, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorate\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key(*args, **kwargs))\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Error in cache\", e)\n\t\t\t\tresult = None\n\t\t\tif result:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\t\telse:\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tcache.set(key(*args, **kwargs), result)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorate(function):\n\t\t@functools.wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t_key = key(*args, **kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tret = cache.get(_key)\n\t\t\texcept:\n\t\t\t\tret = None\n\t\t\tif ret is not None:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn ret\n\t\t\tret = function(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tcache.set(_key, ret)\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\t\t\treturn ret\n\t\treturn wrapper\n\treturn decorate\n\nclass MemoryCache(object):\n\t\"\"\"\n\tSimple in-memory cache for small data.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._cache = {}\n\t\tself._lock = threading.Lock()\n\n\tdef set(self, key, value):\n\t\t\"\"\"\n\t\tSet a value\n\t\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorate(f):\n\t\tdef decorated(*args, **kwargs):\n\t\t\tkey = key(args, kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = f(*args, **kwargs)\n\t\t\t\t\tcache.set(key, value)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn decorated\n\treturn decorate\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef decorated_function(*args, **kwargs):\n\t\t\tkey_ = key(*args, **kwargs)\n\t\t\twith cache.lock(key_, lock) as lock:\n\t\t\t\tif cache.has(key_):\n\t\t\t\t\treturn cache.get(key_)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(key_, result)\n\t\t\t\treturn result\n\t\treturn decorated_function\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef _decorator(func):\n\t\tdef _wrapper(*args, **kwargs):\n\t\t\tif lock is None:\n\t\t\t\tlock = threading.Lock()\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key(*args, **kwargs))\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key(*args, **kwargs), result)\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn _wrapper\n\treturn _decorator\n\n\n# @cached(cache=Cache(1000))\ndef hashkey(*args, **kwargs):\n\t\"\"\"\n\tReturns a hash key for the function\n\t\"\"\"\n\treturn hashlib.md5(pickle.dumps((args, kwargs), protocol=-1)).hexdigest()\n\n\nclass Cache(object):\n\t\"\"\"\n\tCache object that stores and retrieves data from a cache\n\t\"\"\"\n\tdef __init__(self, size=0):\n\t\t\"\"\"\n\t\tCreates a new Cache\n\n\t\t:param size: Number of elements to store\n\t\t\"\"\"\n\t\tself", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t# Reference implementation for the above documentation.\n\t#\n\t# class LRUCache(dict):\n\t# \tdef __init__(self, maxsize, ttl, timer=time.monotonic, typed=False):\n\t# \t\tself.maxsize = maxsize\n\t# \t\tself.ttl = ttl\n\t# \t\tself.timer = timer\n\t# \t\tself.typed = typed\n\t# \t\tself.root = []\n\t# \t\tself.stats = collections.Counter()\n\t#\n\t# \tdef get(self, key, default=None):\n\t# \t\tif key not in self:\n\t# \t\t\treturn default\n\t# \t\tvalue = self[key]\n\t# \t\tself.move_to_end(key)\n\t# \t\treturn value\n\t#\n\t# \tdef __getitem__(self, key):\n\t# \t\tvalue = super().__getitem__(key)\n\t# \t\tself.move_to_end(key)\n\t# \t\treturn value\n\t#\n\t# \tdef __setitem__(self, key, value):\n\t# \t\tif key in self:\n\t# \t\t\tsuper().__del", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tassert maxsize >= 1, \"maxsize must be >= 1\"\n\tassert ttl >= 1, \"ttl must be >= 1\"\n\tassert timer is not None, \"timer must not be None\"\n\n\tif typed:\n\t\tcache = TTLCache(maxsize, ttl, timer, key=type)\n\telse:\n\t\tcache = TTLCache(maxsize, ttl, timer)\n\n\tdef decorating_function(*args, **kwargs):\n\t\tkey = args if typed else (args, frozenset(kwargs.items()))\n\t\ttry:\n\t\t\treturn cache[key]\n\t\texcept KeyError:\n\t\t\tvalue = cache[key] = func(*args, **kwargs)\n\t\t\treturn value\n\t\texcept TypeError:\n\t\t\t\"unsupported operand type(s) for +: 'int' and 'tuple'\"\n\t\t\treturn func(*args, **kwargs)\n\n\tdef cache_info():\n\t\treturn (len(cache), maxsize, len(list(cache.values())))\n\n\tdef cache_clear():\n\t\tcache.clear()\n\n\tdecorating_function.cache_info = cache_info\n\tdecorating_function.cache_clear = cache_clear\n\n\treturn decorating_function\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper_function(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tif key in cache:\n\t\t\t\thits += 1\n\t\t\t\treturn cache[key]\n\t\t\tmisses += 1\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\twrapper_function.cache_info = lambda: CacheInfo(hits, misses, maxsize, timer() - start_time)\n\t\twrapper_function.cache_clear = cache.clear\n\t\treturn wrapper_function\n\treturn decorating_function\n\n\n\nclass LRUCache(MutableMapping):\n\t\"\"\"\n\tLeast-Recently-Used cache implementation.\n\t\n\tThis implementation is based on the `LRU list`_ recipe in the\n\t*Python Cookbook*.\n\t\n\t.. _LRU list: https://code.activestate.com/recipes/578197/\n\t\n\t:param maxsize: maximum", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache_attr = '_cache_%s' % id(ttl_cache)\n\t_ttl_caches = {}\n\n\tdef decorating_function(user_function):\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# cache_attr name is bound to the instance, but the decorator\n\t\t\t# definition is executed in the module scope, so we need to use\n\t\t\t# `getattr` to keep it working in both scenarios.\n\t\t\tcache = getattr(wrapper, cache_attr, None)\n\t\t\tif cache is not None:\n\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\t\tresult = cache.get(key, _miss)\n\t\t\t\tif result is not _miss:\n\t\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\tif cache is None:\n\t\t\t\tcache = wrapper.cache = TTLCache(maxsize, ttl, timer, typed)\n\t\t\t\tsetattr(wrapper, cache_attr, cache)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\nclass TTLCache(", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(func, *args, **kwargs):\n\t\ttry:\n\t\t\tmemo = _TTLCache(maxsize, ttl, timer, typed)\n\t\texcept TypeError as e:\n\t\t\traise TypeError(f\"{func.__name__}() {e}\") from None\n\t\twrapped_func = _TTLCache__wrapper(func, memo)\n\t\twrapped_func.cache_info = _TTLCache__cache_info(memo)\n\t\twrapped_func.cache_clear = _TTLCache__cache_clear(memo)\n\t\treturn wrapped_func\n\n\treturn wrapper\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = TTLCache(maxsize, ttl, timer)\n\thits = _CacheMisses()\n\t\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tkey = args if typed else (args, frozenset(kwargs.items()))\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\t\thits.incr()\n\t\t\treturn result\n\t\texcept KeyError:\n\t\t\tpass\n\t\t\n\t\tresult = func(*args, **kwargs)\n\t\tcache[key] = result\n\t\treturn result\n\t\n\twrapper.cache_info = cache.cache_info\n\twrapper.cache_clear = cache.cache_clear\n\twrapper.cache_stats = cache.cache_stats\n\twrapper.hits = hits\n\twrapper.__wrapped__ = func\n\treturn wrapper\n\n\nclass _CacheMisses:\n\t\n\tdef __init__(self):\n\t\tself._lock = threading.Lock()\n\t\tself._value = 0\n\t\n\tdef incr(self):\n\t\twith self._lock:\n\t\t\tself._value += 1\n\t\n\t@property\n\tdef value(self):\n\t\twith self._lock:\n\t\t\tv = self._value\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = TTLCache(maxsize, ttl, timer, typed)\n\n\tdef decorator(user_function: Callable) -> Callable:\n\t\t\"\"\"Returns a wrapped version of `user_function`\"\"\"\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tdata = cache.get(key, None)\n\t\t\tif data is None:\n\t\t\t\tdata = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = data\n\t\t\treturn data\n\n\t\treturn wrapper\n\n\treturn decorator\n\nclass TTLCache(MutableMapping):\n\t\"\"\"\n\tCache that stores items based on a least-recently-used (LRU) algorithm with a per-item\ntime-to-live (TTL) value.\n\t\"\"\"\n\tdef __init__(self, maxsize: int, ttl: float, timer: Callable[[], float], typed: bool):\n\t\tself.maxsize = maxsize\n\t\tself.ttl = ttl\n\t\tself.timer = timer\n\t\tself.typed = typed\n\n\t\tself.root = []\n\t\t", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\treturn _Cache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = TTLCache(maxsize, ttl, timer=timer, typed=typed)\n\n\t@functools.wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tkey = _make_key(args, kwargs, typed)\n\t\tresult = cache.get(key)\n\t\tif result is None:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\treturn result\n\n\treturn wrapper\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\tcache = TTLCache(maxsize, ttl, timer=timer, typed=typed)\n\t\n\tdef decorator(user_function):\n\t\treturn _TTLCacheCatcher(cache, user_function)\n\t\n\treturn decorator\n\n\nclass _TTLCacheCatcher:\n\tdef __init__(self, cache, user_function):\n\t\tself.cache = cache\n\t\tself.user_function = user_function\n\t\tself.__wrapped__ = user_function\n\t\t\n\tdef __repr__(self):\n\t\treturn repr(self.user_function)\n\t\n\tdef __get__(self, instance, cls):\n\t\tif instance is None:\n\t\t\treturn self\n\t\telse:\n\t\t\treturn _TTLCacheCatcher(self.cache,\n\t\t\t\tpartial(self.user_function, instance))\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\tkey = self.cache.make_key(args, kwargs, typed=self.cache.typed)\n\t\tresult, expiry, _ = self.cache.get(key, None, None)\n\t\tif expiry is None:\n\t\t\tresult = self.user_function(*args, **kwargs)"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t# Reference implementation for the above documentation.\n\t#\n\t# class LRUCache(dict):\n\t# \tdef __init__(self, maxsize, ttl, timer=time.monotonic, typed=False):\n\t# \t\tself.maxsize = maxsize\n\t# \t\tself.ttl = ttl\n\t# \t\tself.timer = timer\n\t# \t\tself.typed = typed\n\t# \t\tself.root = []\n\t# \t\tself.stats = collections.Counter()\n\t#\n\t# \tdef get(self, key, default=None):\n\t# \t\tif key not in self:\n\t# \t\t\treturn default\n\t# \t\tvalue = self[key]\n\t# \t\tself.move_to_end(key)\n\t# \t\treturn value\n\t#\n\t# \tdef __getitem__(self, key):\n\t# \t\tvalue = super().__getitem__(key)\n\t# \t\tself.move_to_end(key)\n\t# \t\treturn value\n\t#\n\t# \tdef __setitem__(self, key, value):\n\t# \t\tif key in self:\n\t# \t\t\tsuper().__del", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tassert maxsize >= 1, \"maxsize must be >= 1\"\n\tassert ttl >= 1, \"ttl must be >= 1\"\n\tassert timer is not None, \"timer must not be None\"\n\n\tif typed:\n\t\tcache = TTLCache(maxsize, ttl, timer, key=type)\n\telse:\n\t\tcache = TTLCache(maxsize, ttl, timer)\n\n\tdef decorating_function(*args, **kwargs):\n\t\tkey = args if typed else (args, frozenset(kwargs.items()))\n\t\ttry:\n\t\t\treturn cache[key]\n\t\texcept KeyError:\n\t\t\tvalue = cache[key] = func(*args, **kwargs)\n\t\t\treturn value\n\t\texcept TypeError:\n\t\t\t\"unsupported operand type(s) for +: 'int' and 'tuple'\"\n\t\t\treturn func(*args, **kwargs)\n\n\tdef cache_info():\n\t\treturn (len(cache), maxsize, len(list(cache.values())))\n\n\tdef cache_clear():\n\t\tcache.clear()\n\n\tdecorating_function.cache_info = cache_info\n\tdecorating_function.cache_clear = cache_clear\n\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper_function(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tif key in cache:\n\t\t\t\thits += 1\n\t\t\t\treturn cache[key]\n\t\t\tmisses += 1\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\twrapper_function.cache_info = lambda: CacheInfo(hits, misses, maxsize, timer() - start_time)\n\t\twrapper_function.cache_clear = cache.clear\n\t\treturn wrapper_function\n\treturn decorating_function\n\n\n\nclass LRUCache(MutableMapping):\n\t\"\"\"\n\tLeast-Recently-Used cache implementation.\n\t\n\tThis implementation is based on the `LRU list`_ recipe in the\n\t*Python Cookbook*.\n\t\n\t.. _LRU list: https://code.activestate.com/recipes/578197/\n\t\n\t:param maxsize: maximum", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache_attr = '_cache_%s' % id(ttl_cache)\n\t_ttl_caches = {}\n\n\tdef decorating_function(user_function):\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# cache_attr name is bound to the instance, but the decorator\n\t\t\t# definition is executed in the module scope, so we need to use\n\t\t\t# `getattr` to keep it working in both scenarios.\n\t\t\tcache = getattr(wrapper, cache_attr, None)\n\t\t\tif cache is not None:\n\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\t\tresult = cache.get(key, _miss)\n\t\t\t\tif result is not _miss:\n\t\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\tif cache is None:\n\t\t\t\tcache = wrapper.cache = TTLCache(maxsize, ttl, timer, typed)\n\t\t\t\tsetattr(wrapper, cache_attr, cache)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\nclass TTLCache(", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(func, *args, **kwargs):\n\t\ttry:\n\t\t\tmemo = _TTLCache(maxsize, ttl, timer, typed)\n\t\texcept TypeError as e:\n\t\t\traise TypeError(f\"{func.__name__}() {e}\") from None\n\t\twrapped_func = _TTLCache__wrapper(func, memo)\n\t\twrapped_func.cache_info = _TTLCache__cache_info(memo)\n\t\twrapped_func.cache_clear = _TTLCache__cache_clear(memo)\n\t\treturn wrapped_func\n\n\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = TTLCache(maxsize, ttl, timer)\n\thits = _CacheMisses()\n\t\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tkey = args if typed else (args, frozenset(kwargs.items()))\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\t\thits.incr()\n\t\t\treturn result\n\t\texcept KeyError:\n\t\t\tpass\n\t\t\n\t\tresult = func(*args, **kwargs)\n\t\tcache[key] = result\n\t\treturn result\n\t\n\twrapper.cache_info = cache.cache_info\n\twrapper.cache_clear = cache.cache_clear\n\twrapper.cache_stats = cache.cache_stats\n\twrapper.hits = hits\n\twrapper.__wrapped__ = func\n\treturn wrapper\n\n\nclass _CacheMisses:\n\t\n\tdef __init__(self):\n\t\tself._lock = threading.Lock()\n\t\tself._value = 0\n\t\n\tdef incr(self):\n\t\twith self._lock:\n\t\t\tself._value += 1\n\t\n\t@property\n\tdef value(self):\n\t\twith self._lock:\n\t\t\tv = self._value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = TTLCache(maxsize, ttl, timer, typed)\n\n\tdef decorator(user_function: Callable) -> Callable:\n\t\t\"\"\"Returns a wrapped version of `user_function`\"\"\"\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tdata = cache.get(key, None)\n\t\t\tif data is None:\n\t\t\t\tdata = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = data\n\t\t\treturn data\n\n\t\treturn wrapper\n\n\treturn decorator\n\nclass TTLCache(MutableMapping):\n\t\"\"\"\n\tCache that stores items based on a least-recently-used (LRU) algorithm with a per-item\ntime-to-live (TTL) value.\n\t\"\"\"\n\tdef __init__(self, maxsize: int, ttl: float, timer: Callable[[], float], typed: bool):\n\t\tself.maxsize = maxsize\n\t\tself.ttl = ttl\n\t\tself.timer = timer\n\t\tself.typed = typed\n\n\t\tself.root = []\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\treturn _Cache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = TTLCache(maxsize, ttl, timer=timer, typed=typed)\n\n\t@functools.wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tkey = _make_key(args, kwargs, typed)\n\t\tresult = cache.get(key)\n\t\tif result is None:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\treturn result\n\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\tcache = TTLCache(maxsize, ttl, timer=timer, typed=typed)\n\t\n\tdef decorator(user_function):\n\t\treturn _TTLCacheCatcher(cache, user_function)\n\t\n\treturn decorator\n\n\nclass _TTLCacheCatcher:\n\tdef __init__(self, cache, user_function):\n\t\tself.cache = cache\n\t\tself.user_function = user_function\n\t\tself.__wrapped__ = user_function\n\t\t\n\tdef __repr__(self):\n\t\treturn repr(self.user_function)\n\t\n\tdef __get__(self, instance, cls):\n\t\tif instance is None:\n\t\t\treturn self\n\t\telse:\n\t\t\treturn _TTLCacheCatcher(self.cache,\n\t\t\t\tpartial(self.user_function, instance))\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\tkey = self.cache.make_key(args, kwargs, typed=self.cache.typed)\n\t\tresult, expiry, _ = self.cache.get(key, None, None)\n\t\tif expiry is None:\n\t\t\tresult = self.user_function(*args, **kwargs)", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tif maxsize is None or maxsize < 0:\n\t\traise ValueError(\"Expected maxsize to be a non-negative integer\")\n\n\tcache = OrderedDict()\n\n\tdef decorator(func):\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\ttry:\n\t\t\t\tvalue = cache.pop(key)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tcache[key] = value\n\t\t\tif len(cache) > maxsize:\n\t\t\t\tcache.popitem(last=False)\n\n\t\t\tvalue = func(*args, **kwargs)\n\t\t\tcache[key] = value\n\t\t\treturn value\n\n\t\tdef clear():\n\t\t\tnonlocal cache\n\t\t\tcache.clear()\n\n\t\tdef get(key, default=None):\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn default\n\n\t\tdef pop(key, default=None):\n\t\t\ttry:\n\t\t\t\tvalue = cache.pop(key)\n\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = Misses = 0\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tglobal hits, misses\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tif key in cache:\n\t\t\t\thits += 1\n\t\t\t\tvalue = cache[key]\n\t\t\telse:\n\t\t\t\tmisses += 1\n\t\t\t\tvalue = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\t\tif maxsize is not None:\n\t\t\t\t\twhile len(cache) > maxsize:\n\t\t\t\t\t\tcache.popitem(last=False)\n\t\t\treturn value\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report the number of hits and misses.\"\"\"\n\t\t\treturn hits, misses, len(cache)\n\n\t\tdef clear():\n\t\t\t\"\"\"Clear the cache and cache statistics.\"\"\"\n\t\t\tcache.clear()\n\t\t\thits = misses = 0\n\t\t\twrapper.cache_info = cache_info\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\t\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = user_function(*args, **kwargs)\n\t\t\t\t\n\t\t\t\t# Remove the oldest item to make room for the newest one,\n\t\t\t\t# if we're at capacity.\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.pop(list(cache.keys())[0])\n\t\t\t\n\t\t\t# Move the most recently accessed item to the front of the queue.\n\t\t\tcache.move_to_end(key)\n\t\t\t\n\t\t\treturn cache[key]\n\t\t\n\t\twrapper.cache = cache\n\t\twrapper.maxsize = maxsize\n\t\twrapper.typed = typed\n\t\twrapper.cache_info = lambda: (len(wrapper.cache), maxsize, len(wrapper.cache), None)\n\t\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = _Missing = object()\n\t\tif maxsize is None:\n\t\t\treturn _unlimited_cache(cache, user_function,\n\t\t\t\t\t\t\t\t\thits, misses, cache_clear, cache_info, cache_keys, cache_values)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tif not cache:\n\t\t\t\thits = misses = 0\n\t\t\t\tcache[args] = cache[kwds] = _Missing\n\t\t\tkey = args if kwds else kwds\n\t\t\tvalue = cache.get(key, _Missing)\n\t\t\tif value is not _Missing:\n\t\t\t\thits += 1\n\t\t\t\treturn value\n\t\t\tmisses += 1\n\t\t\tvalue = user_function(*args, **kwds)\n\t\t\tcache[key] = value\n\t\t\tif len(cache) > maxsize:\n\t\t\t\t_remove_oldest(cache)\n\t\t\treturn value\n\t\tdef cache_clear():\n\t\t\tnonlocal hits, misses\n\t\t\thits = misses = 0\n\t\t\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t# Adapted from functools.lru_cache in the Python 2.7 standard library.\n\tdef _mru_cache(user_function):\n\t\tcache = MRUCache(user_function, maxsize)\n\t\t# We want to be able to set the maxsize at runtime also. So we can't\n\t\t# use functools.update_wrapper here.\n\t\tdef _mru_cache_wrapper(*args, **kwds):\n\t\t\treturn cache.get(*args, **kwds)\n\t\t_mru_cache_wrapper.__wrapped__ = user_function\n\t\treturn _mru_cache_wrapper\n\treturn _mru_cache\n\nclass MRUCache(object):\n\t\"\"\"\n\tContainer class that implements a Most Recently Used (MRU) cache.\n\t\"\"\"\n\tdef __init__(self, user_function, maxsize):\n\t\tself.user_function = user_function\n\t\tself.maxsize = maxsize\n\t\tself.hits = 0\n\t\tself.misses = 0\n\t\tself.cache = {}\n\t\tself.lru_list = []\n\n\tdef get(self, *args, **kwds):\n\t\tkey = self._make_cache_key(args", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = Misses = 0\n\t\tcache_get = cache.get\n\t\tlock = RLock()\n\t\twrapper = _CacheWrapper(user_function, cache, lock, cache_get,\n\t\t\t\t\t\t\t\thits, Misses)\n\t\tif typed:\n\t\t\twrapper = _CacheWrapper(wrapper, {}, lock, cache_get, hits,\n\t\t\t\t\t\t\t\t\tMisses)\n\t\twrapper.cache_info = lambda: CacheInfo(hits, Misses, maxsize,\n\t\t\t\t\t\t\t\t\t\t\t  len(cache))\n\t\twrapper.cache_clear = lambda: _CacheCleared(cache, lock)\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__doc__ = user_function.__doc__\n\t\twrapper._get_cache_statistics = lambda: (hits, Misses)\n\t\twrapper.get_cache_info = lambda: wrapper.cache_info()\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\nclass _CacheCleared(object):\n\tdef __init__(self, cache, lock):\n\t\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\treturn lru_cache(maxsize=maxsize, typed=typed, cache_get=_mru_cache_get,\n\t\t\t\t\tcache_put=_mru_cache_put, cache_discard=_mru_cache_discard,\n\t\t\t\t\tcache_clear=_mru_cache_clear)\n\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tmaxsize = maxsize if maxsize > 0 else None\n\tcache = dict()\n\tcache_get = cache.get\n\tdef decorator(user_function):\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\tresult = cache_get(key)\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif maxsize is not None:\n\t\t\t\t\t_prune_cache(cache, maxsize)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\tstats = [0]\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed=typed)\n\t\t\tif key in cache:\n\t\t\t\t# Move the found item to the front of the order list.\n\t\t\t\tdel cache[key]\n\t\t\t\tcache[key] = user_function(*args, **kwargs)\n\t\t\t\tstats[0] += 1\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\telse:\n\t\t\t\tcache[key] = user_function(*args, **kwargs)\n\t\t\t\tstats[0] += 1\n\n\t\t\treturn cache[key]\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report the number of cached items.\"\"\"\n\t\t\treturn len(cache)\n\n\t\tdef cache_clear():\n\t\t\t\"\"\"Remove all items from the cache.\"\"\"\n\t\t\tcache.clear()\n\n\t\twrapper.cache_info = cache_info\n\t\twrapper.", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = OrderedDict()\n\n\tdef wrapper(*args, **kwargs):\n\t\tkey = _make_key(args, kwargs, typed)\n\t\ttry:\n\t\t\tvalue = cache.pop(key)\n\t\texcept KeyError:\n\t\t\tif len(cache) >= maxsize:\n\t\t\t\tcache.popitem(last=False)\n\t\t\tvalue = func(*args, **kwargs)\n\t\t\tcache[key] = value\n\t\treturn value\n\n\tdef cache_info():\n\t\treturn len(cache), maxsize, len(cache), cache.keys()\n\n\tdef cache_clear():\n\t\tcache.clear()\n\n\tdef _make_key(args, kwargs, typed):\n\t\tkey = args\n\t\tif kwargs:\n\t\t\tkey += (kwargs,)\n\t\tif typed:\n\t\t\tkey += tuple(type(v) for v in args)\n\t\treturn key\n\n\twrapper.cache_info = cache_info\n\twrapper.cache_clear = cache_clear\n\treturn wrapper\n\n\nif __name__ == \"__main__\":\n\t@mru_cache()\n\tdef fib(n):\n\t\tif n < 2:\n\t\t\treturn n\n\t\treturn fib(n -"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tif maxsize is None or maxsize < 0:\n\t\traise ValueError(\"Expected maxsize to be a non-negative integer\")\n\n\tcache = OrderedDict()\n\n\tdef decorator(func):\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\ttry:\n\t\t\t\tvalue = cache.pop(key)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tcache[key] = value\n\t\t\tif len(cache) > maxsize:\n\t\t\t\tcache.popitem(last=False)\n\n\t\t\tvalue = func(*args, **kwargs)\n\t\t\tcache[key] = value\n\t\t\treturn value\n\n\t\tdef clear():\n\t\t\tnonlocal cache\n\t\t\tcache.clear()\n\n\t\tdef get(key, default=None):\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\treturn default\n\n\t\tdef pop(key, default=None):\n\t\t\ttry:\n\t\t\t\tvalue = cache.pop(key)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = Misses = 0\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tglobal hits, misses\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tif key in cache:\n\t\t\t\thits += 1\n\t\t\t\tvalue = cache[key]\n\t\t\telse:\n\t\t\t\tmisses += 1\n\t\t\t\tvalue = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\t\tif maxsize is not None:\n\t\t\t\t\twhile len(cache) > maxsize:\n\t\t\t\t\t\tcache.popitem(last=False)\n\t\t\treturn value\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report the number of hits and misses.\"\"\"\n\t\t\treturn hits, misses, len(cache)\n\n\t\tdef clear():\n\t\t\t\"\"\"Clear the cache and cache statistics.\"\"\"\n\t\t\tcache.clear()\n\t\t\thits = misses = 0\n\t\t\twrapper.cache_info = cache_info\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\t\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = user_function(*args, **kwargs)\n\t\t\t\t\n\t\t\t\t# Remove the oldest item to make room for the newest one,\n\t\t\t\t# if we're at capacity.\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.pop(list(cache.keys())[0])\n\t\t\t\n\t\t\t# Move the most recently accessed item to the front of the queue.\n\t\t\tcache.move_to_end(key)\n\t\t\t\n\t\t\treturn cache[key]\n\t\t\n\t\twrapper.cache = cache\n\t\twrapper.maxsize = maxsize\n\t\twrapper.typed = typed\n\t\twrapper.cache_info = lambda: (len(wrapper.cache), maxsize, len(wrapper.cache), None)\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = _Missing = object()\n\t\tif maxsize is None:\n\t\t\treturn _unlimited_cache(cache, user_function,\n\t\t\t\t\t\t\t\t\thits, misses, cache_clear, cache_info, cache_keys, cache_values)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tif not cache:\n\t\t\t\thits = misses = 0\n\t\t\t\tcache[args] = cache[kwds] = _Missing\n\t\t\tkey = args if kwds else kwds\n\t\t\tvalue = cache.get(key, _Missing)\n\t\t\tif value is not _Missing:\n\t\t\t\thits += 1\n\t\t\t\treturn value\n\t\t\tmisses += 1\n\t\t\tvalue = user_function(*args, **kwds)\n\t\t\tcache[key] = value\n\t\t\tif len(cache) > maxsize:\n\t\t\t\t_remove_oldest(cache)\n\t\t\treturn value\n\t\tdef cache_clear():\n\t\t\tnonlocal hits, misses\n\t\t\thits = misses = 0\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t# Adapted from functools.lru_cache in the Python 2.7 standard library.\n\tdef _mru_cache(user_function):\n\t\tcache = MRUCache(user_function, maxsize)\n\t\t# We want to be able to set the maxsize at runtime also. So we can't\n\t\t# use functools.update_wrapper here.\n\t\tdef _mru_cache_wrapper(*args, **kwds):\n\t\t\treturn cache.get(*args, **kwds)\n\t\t_mru_cache_wrapper.__wrapped__ = user_function\n\t\treturn _mru_cache_wrapper\n\treturn _mru_cache\n\nclass MRUCache(object):\n\t\"\"\"\n\tContainer class that implements a Most Recently Used (MRU) cache.\n\t\"\"\"\n\tdef __init__(self, user_function, maxsize):\n\t\tself.user_function = user_function\n\t\tself.maxsize = maxsize\n\t\tself.hits = 0\n\t\tself.misses = 0\n\t\tself.cache = {}\n\t\tself.lru_list = []\n\n\tdef get(self, *args, **kwds):\n\t\tkey = self._make_cache_key(args", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = Misses = 0\n\t\tcache_get = cache.get\n\t\tlock = RLock()\n\t\twrapper = _CacheWrapper(user_function, cache, lock, cache_get,\n\t\t\t\t\t\t\t\thits, Misses)\n\t\tif typed:\n\t\t\twrapper = _CacheWrapper(wrapper, {}, lock, cache_get, hits,\n\t\t\t\t\t\t\t\t\tMisses)\n\t\twrapper.cache_info = lambda: CacheInfo(hits, Misses, maxsize,\n\t\t\t\t\t\t\t\t\t\t\t  len(cache))\n\t\twrapper.cache_clear = lambda: _CacheCleared(cache, lock)\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__doc__ = user_function.__doc__\n\t\twrapper._get_cache_statistics = lambda: (hits, Misses)\n\t\twrapper.get_cache_info = lambda: wrapper.cache_info()\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\nclass _CacheCleared(object):\n\tdef __init__(self, cache, lock):\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\treturn lru_cache(maxsize=maxsize, typed=typed, cache_get=_mru_cache_get,\n\t\t\t\t\tcache_put=_mru_cache_put, cache_discard=_mru_cache_discard,\n\t\t\t\t\tcache_clear=_mru_cache_clear)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tmaxsize = maxsize if maxsize > 0 else None\n\tcache = dict()\n\tcache_get = cache.get\n\tdef decorator(user_function):\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\tresult = cache_get(key)\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif maxsize is not None:\n\t\t\t\t\t_prune_cache(cache, maxsize)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\tstats = [0]\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed=typed)\n\t\t\tif key in cache:\n\t\t\t\t# Move the found item to the front of the order list.\n\t\t\t\tdel cache[key]\n\t\t\t\tcache[key] = user_function(*args, **kwargs)\n\t\t\t\tstats[0] += 1\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\telse:\n\t\t\t\tcache[key] = user_function(*args, **kwargs)\n\t\t\t\tstats[0] += 1\n\n\t\t\treturn cache[key]\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report the number of cached items.\"\"\"\n\t\t\treturn len(cache)\n\n\t\tdef cache_clear():\n\t\t\t\"\"\"Remove all items from the cache.\"\"\"\n\t\t\tcache.clear()\n\n\t\twrapper.cache_info = cache_info\n\t\twrapper.", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = OrderedDict()\n\n\tdef wrapper(*args, **kwargs):\n\t\tkey = _make_key(args, kwargs, typed)\n\t\ttry:\n\t\t\tvalue = cache.pop(key)\n\t\texcept KeyError:\n\t\t\tif len(cache) >= maxsize:\n\t\t\t\tcache.popitem(last=False)\n\t\t\tvalue = func(*args, **kwargs)\n\t\t\tcache[key] = value\n\t\treturn value\n\n\tdef cache_info():\n\t\treturn len(cache), maxsize, len(cache), cache.keys()\n\n\tdef cache_clear():\n\t\tcache.clear()\n\n\tdef _make_key(args, kwargs, typed):\n\t\tkey = args\n\t\tif kwargs:\n\t\t\tkey += (kwargs,)\n\t\tif typed:\n\t\t\tkey += tuple(type(v) for v in args)\n\t\treturn key\n\n\twrapper.cache_info = cache_info\n\twrapper.cache_clear = cache_clear\n\treturn wrapper\n\n\nif __name__ == \"__main__\":\n\t@mru_cache()\n\tdef fib(n):\n\t\tif n < 2:\n\t\t\treturn n\n\t\treturn fib(n -", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t# @lru_cache(maxsize=maxsize, typed=typed)\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = misses = 0\n\t\t\n\t\t# @functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\t\n\t\t\tif key not in cache:\n\t\t\t\tmisses += 1\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tdel cache[list(cache.keys())[0]]\n\t\t\telse:\n\t\t\t\thits += 1\n\t\t\t\tresult = cache[key]\n\t\t\treturn result\n\t\t\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn (len(cache), hits, misses, maxsize)\n\t\t\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\t\thits = misses = 0\n\t\t\t\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tassert maxsize > 0, 'maxsize must be greater than 0'\n\thits = [0]\n\tmisses = [0]\n\tif maxsize == 1:\n\t\tcache = OrderedDict()\n\t\tcache_get = cache.get\n\t\tdel cache\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# @lru_cache can be used on functions which\n\t\t\t# take either no arguments or a single hashable\n\t\t\t# argument\n\t\t\tif len(args) != 1:\n\t\t\t\tkey = args\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\telse:\n\t\t\t\t\tkey = None\n\t\t\telse:\n\t\t\t\tkey = args[0]\n\t\t\tif key in wrapper.cache:\n\t\t\t\thits[0] += 1\n\t\t\t\treturn wrapper.cache[key]\n\t\t\tmisses[0] += 1\n\t\t\tresult = wrapper.user_function(*args, **kwds)\n\t\t\twrapper.cache[key] = result\n\t\t\twrapper.cache.move_to_end(key)\n\t\t\treturn result\n\n\t\twrapper", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n\t# lru_cache_timer = Timer(maxsize)\n\t\n\tdef decorating_function(user_function):\n\t\t\n\t\tcache = dict()\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\tif kwargs:\n\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = args\n\t\t\t\n\t\t\tif key in cache:\n\t\t\t\tvalue = cache[key]\n\t\t\t\t# lru_cache_timer.check_hit(key, value)\n\t\t\t\treturn value\n\t\t\t\n\t\t\tvalue = user_function(*args, **kwargs)\n\t\t\t# lru_cache_timer.check_miss(key, value)\n\t\t\t\n\t\t\twhile len(cache) >= maxsize:\n\t\t\t\t# lru_cache_timer.check_eject()\n\t\t\t\t# lru_cache_timer.check_miss(min(cache, key=lambda k: cache[k]),\n\t\t\t\t#                            value)\n\t\t\t\tdel cache[min(cache, key=lambda k: cache[k", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\tstats = [0, 0]  # Hits and misses.\n\t\tlock = RLock()\n\t\troot = []\n\t\tget = root.append\n\t\troot[:] = cache  # Clearing the list each time resets the cache.\n\t\tif maxsize == 0:\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tif typed:\n\t\t\t\t\tkey = args, tuple(sorted(kwds.items()))\n\t\t\t\telse:\n\t\t\t\t\tkey = args, kwds\n\t\t\t\treturn user_function(*key[0], **key[1])\n\t\t\treturn update_wrapper(wrapper, user_function)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# Keyed on args and kwds, since we have to hash them.\n\t\t\tif typed:\n\t\t\t\tkey = args, tuple(sorted(kwds.items()))\n\t\t\telse:\n\t\t\t\tkey = args, kwds\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tval = cache.pop(key)\n\t\t\t\texcept KeyError:\n\t\t\t", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tassert maxsize > 0\n\t_warn_on_cache_miss = True\n\t\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# if kwargs:\n\t\t\t# \targs = (args,)\n\t\t\t# \tkey = (kwargs,)\n\t\t\t# else:\n\t\t\t# \tkey = args\n\t\t\t# \tif not typed:\n\t\t\t# \t\tkey = tuple(map(hashable_key, key))\n\t\t\tkey = args\n\t\t\tif not typed:\n\t\t\t\tkey = tuple(map(hashable_key, key))\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\tresult = user_function(*args)\n\t\t\tif maxsize == 1:\n\t\t\t\tcache.clear()\n\t\t\telif len(cache) >= maxsize:\n\t\t\t\tdel cache[_lru_key(cache)]\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tmaxsize = maxsize or sys.maxsize\n\tcache = OrderedDict()\n\thits = misses = 0\n\n\tdef wrapped_fn(*args, **kwargs):\n\t\tglobal hits, misses\n\t\tkey = \"\".join([str(x) for x in args]) + str(kwargs)\n\t\tif key in cache:\n\t\t\tvalue = cache.pop(key)\n\t\t\tcache[key] = value\n\t\t\thits += 1\n\t\t\treturn value\n\t\telse:\n\t\t\tmisses += 1\n\t\t\tvalue = fn(*args, **kwargs)\n\t\t\tif len(cache) >= maxsize:\n\t\t\t\tcache.popitem(last=False)\n\t\t\tcache[key] = value\n\t\t\treturn value\n\n\tdef cache_info():\n\t\treturn hits, misses, maxsize, cache.items()\n\n\tdef cache_clear():\n\t\tcache.clear()\n\t\thits = misses = 0\n\n\twrapped_fn.cache_info = cache_info\n\twrapped_fn.cache_clear = cache_clear\n\twrapped_fn.cache = cache\n\treturn wrapped_fn\n\n\n# lru_cache decorator\n@lru_", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t# users should only access the lru_cache through its public API:\n\t#       cache_info, cache_clear, and the wrapped function\n\t# the internals of the lru_cache are encapsulated for thread safety and\n\t# to allow the implementation to change (but the interface to remain\n\t# the same)\n\n\t# this should be in a utils module in Python 3\n\ttry:\n\t\tfrom _thread import RLock\n\texcept ImportError:\n\t\tfrom threading import RLock\n\t_missing = object()\n\tmake_key = _make_key = lambda args, kwds, typed: _make_key(args, kwds, typed, 1)\n\n\tdef _get_cache_statistics_info():\n\t\twith _cache_lock:\n\t\t\treturn len(_cache), _cache.get('hits', 0), _cache.get('misses', 0)\n\n\tdef _cache_clear():\n\t\twith _cache_lock:\n\t\t\t_cache.clear()\n\n\tdef wrapper(*args, **kwds):\n\t\t# cache_key = make_key(args, kwds, typed)\n\t\tcache_key = _make_key(args, kwds, typed, 1)\n\t\twith _cache_lock:", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t# pylint:disable=invalid-name\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = misses = 0\n\t\tfull_hash = False\n\t\ttyped_wrapper = None\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal full_hash\n\t\t\tif kwargs and typed_wrapper is None:\n\t\t\t\ttyped_wrapper = lru_cache_wrapper(*args, **kwargs)\n\t\t\t\treturn typed_wrapper\n\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif len(key) != len(args):\n\t\t\t\t\tfull_hash = True\n\n\t\t\tif full_hash:\n\t\t\t\twrapper_hash = hash(key)\n\t\t\telse:\n\t\t\t\twrapper_hash = wrapper_hash_key(key)\n\n\t\t\tnow = time.perf_counter()\n\t\t\tif wrapper_hash in cache:\n\t\t\t\thits += 1\n\t\t\t\tif cache[wrapper_hash][0] < now:\n\t\t\t\t\tdel cache[wrapper_hash]\n\t\t\t\t", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tassert isinstance(maxsize, (int, long))\n\tassert maxsize >= 0\n\n\t# Handle unlimited cache size.\n\tif maxsize == 0:\n\t\treturn lambda f: f\n\n\t# Implementation detail: The cache is stored as a dictionary that\n\t# mapps the arguments to the cached values. This is a dictionary\n\t# because the ordering is needed, and we can't trust that the\n\t# ordering of the arguments is going to be the same between\n\t# invocations.\n\tcache = {}\n\n\tdef wrapper(*args):\n\t\t# Get the key from the arguments.\n\t\tkey = args\n\t\tif typed:\n\t\t\t# If the function is typed, we need to use the\n\t\t\t# unhashable type to lookup in the cache.\n\t\t\tkey = tuple(args)\n\n\t\t# Get the cached value or compute it if it's not cached.\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\texcept KeyError:\n\t\t\t# Call the function.\n\t\t\tresult = f(*args)\n\t\t\tif maxsize == 1:\n\t\t\t\t# Sets are not ordered, so we clear the cache\n\t\t\t\t# before storing the result.\n\t\t\t\tcache", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n\t# order dictionary to store the cache\n\tclass OrderedDict(dict):\n\t\tdef __init__(self, maxlen, *args, **kwds):\n\t\t\tself.maxlen = maxlen\n\t\t\tsuper().__init__(*args, **kwds)\n\t\t\tself.counter = 0\n\t\t\tself.ordered = []\n\t\t\n\t\tdef __getitem__(self, key):\n\t\t\tvalue = super().__getitem__(key)\n\t\t\tself.ordered.append(key)\n\t\t\tself.counter += 1\n\t\t\treturn value\n\t\t\n\t\tdef __setitem__(self, key, value):\n\t\t\tif len(self) >= self.maxlen:\n\t\t\t\tself.pop(self.ordered[0])\n\t\t\t\tself.ordered.pop(0)\n\t\t\t\tself.counter -= 1\n\t\t\tsuper().__setitem__(key, value)\n\t\t\tself.ordered.append(key)\n\t\t\tself.counter += 1\n\t\t\n\t\tdef __delitem__(self, key):\n\t\t\tsuper().__delitem__(key)\n\t\t\tself.ordered.remove(key)\n\t\t\tself.counter -= 1\n\t\t\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t# @lru_cache(maxsize=maxsize, typed=typed)\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = misses = 0\n\t\t\n\t\t# @functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = _make_key(args, kwargs, typed)\n\t\t\t\n\t\t\tif key not in cache:\n\t\t\t\tmisses += 1\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tdel cache[list(cache.keys())[0]]\n\t\t\telse:\n\t\t\t\thits += 1\n\t\t\t\tresult = cache[key]\n\t\t\treturn result\n\t\t\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn (len(cache), hits, misses, maxsize)\n\t\t\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\t\thits = misses = 0\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tassert maxsize > 0, 'maxsize must be greater than 0'\n\thits = [0]\n\tmisses = [0]\n\tif maxsize == 1:\n\t\tcache = OrderedDict()\n\t\tcache_get = cache.get\n\t\tdel cache\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# @lru_cache can be used on functions which\n\t\t\t# take either no arguments or a single hashable\n\t\t\t# argument\n\t\t\tif len(args) != 1:\n\t\t\t\tkey = args\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\telse:\n\t\t\t\t\tkey = None\n\t\t\telse:\n\t\t\t\tkey = args[0]\n\t\t\tif key in wrapper.cache:\n\t\t\t\thits[0] += 1\n\t\t\t\treturn wrapper.cache[key]\n\t\t\tmisses[0] += 1\n\t\t\tresult = wrapper.user_function(*args, **kwds)\n\t\t\twrapper.cache[key] = result\n\t\t\twrapper.cache.move_to_end(key)\n\t\t\treturn result\n\n\t\twrapper", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n\t# lru_cache_timer = Timer(maxsize)\n\t\n\tdef decorating_function(user_function):\n\t\t\n\t\tcache = dict()\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\tif kwargs:\n\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = args\n\t\t\t\n\t\t\tif key in cache:\n\t\t\t\tvalue = cache[key]\n\t\t\t\t# lru_cache_timer.check_hit(key, value)\n\t\t\t\treturn value\n\t\t\t\n\t\t\tvalue = user_function(*args, **kwargs)\n\t\t\t# lru_cache_timer.check_miss(key, value)\n\t\t\t\n\t\t\twhile len(cache) >= maxsize:\n\t\t\t\t# lru_cache_timer.check_eject()\n\t\t\t\t# lru_cache_timer.check_miss(min(cache, key=lambda k: cache[k]),\n\t\t\t\t#                            value)\n\t\t\t\tdel cache[min(cache, key=lambda k: cache[k", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\tstats = [0, 0]  # Hits and misses.\n\t\tlock = RLock()\n\t\troot = []\n\t\tget = root.append\n\t\troot[:] = cache  # Clearing the list each time resets the cache.\n\t\tif maxsize == 0:\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tif typed:\n\t\t\t\t\tkey = args, tuple(sorted(kwds.items()))\n\t\t\t\telse:\n\t\t\t\t\tkey = args, kwds\n\t\t\t\treturn user_function(*key[0], **key[1])\n\t\t\treturn update_wrapper(wrapper, user_function)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# Keyed on args and kwds, since we have to hash them.\n\t\t\tif typed:\n\t\t\t\tkey = args, tuple(sorted(kwds.items()))\n\t\t\telse:\n\t\t\t\tkey = args, kwds\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tval = cache.pop(key)\n\t\t\t\texcept KeyError:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tassert maxsize > 0\n\t_warn_on_cache_miss = True\n\t\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# if kwargs:\n\t\t\t# \targs = (args,)\n\t\t\t# \tkey = (kwargs,)\n\t\t\t# else:\n\t\t\t# \tkey = args\n\t\t\t# \tif not typed:\n\t\t\t# \t\tkey = tuple(map(hashable_key, key))\n\t\t\tkey = args\n\t\t\tif not typed:\n\t\t\t\tkey = tuple(map(hashable_key, key))\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\tresult = user_function(*args)\n\t\t\tif maxsize == 1:\n\t\t\t\tcache.clear()\n\t\t\telif len(cache) >= maxsize:\n\t\t\t\tdel cache[_lru_key(cache)]\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tmaxsize = maxsize or sys.maxsize\n\tcache = OrderedDict()\n\thits = misses = 0\n\n\tdef wrapped_fn(*args, **kwargs):\n\t\tglobal hits, misses\n\t\tkey = \"\".join([str(x) for x in args]) + str(kwargs)\n\t\tif key in cache:\n\t\t\tvalue = cache.pop(key)\n\t\t\tcache[key] = value\n\t\t\thits += 1\n\t\t\treturn value\n\t\telse:\n\t\t\tmisses += 1\n\t\t\tvalue = fn(*args, **kwargs)\n\t\t\tif len(cache) >= maxsize:\n\t\t\t\tcache.popitem(last=False)\n\t\t\tcache[key] = value\n\t\t\treturn value\n\n\tdef cache_info():\n\t\treturn hits, misses, maxsize, cache.items()\n\n\tdef cache_clear():\n\t\tcache.clear()\n\t\thits = misses = 0\n\n\twrapped_fn.cache_info = cache_info\n\twrapped_fn.cache_clear = cache_clear\n\twrapped_fn.cache = cache\n\treturn wrapped_fn\n\n\n# lru_cache decorator\n@lru_", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t# users should only access the lru_cache through its public API:\n\t#       cache_info, cache_clear, and the wrapped function\n\t# the internals of the lru_cache are encapsulated for thread safety and\n\t# to allow the implementation to change (but the interface to remain\n\t# the same)\n\n\t# this should be in a utils module in Python 3\n\ttry:\n\t\tfrom _thread import RLock\n\texcept ImportError:\n\t\tfrom threading import RLock\n\t_missing = object()\n\tmake_key = _make_key = lambda args, kwds, typed: _make_key(args, kwds, typed, 1)\n\n\tdef _get_cache_statistics_info():\n\t\twith _cache_lock:\n\t\t\treturn len(_cache), _cache.get('hits', 0), _cache.get('misses', 0)\n\n\tdef _cache_clear():\n\t\twith _cache_lock:\n\t\t\t_cache.clear()\n\n\tdef wrapper(*args, **kwds):\n\t\t# cache_key = make_key(args, kwds, typed)\n\t\tcache_key = _make_key(args, kwds, typed, 1)\n\t\twith _cache_lock:", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t# pylint:disable=invalid-name\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\n\t\thits = misses = 0\n\t\tfull_hash = False\n\t\ttyped_wrapper = None\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal full_hash\n\t\t\tif kwargs and typed_wrapper is None:\n\t\t\t\ttyped_wrapper = lru_cache_wrapper(*args, **kwargs)\n\t\t\t\treturn typed_wrapper\n\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif len(key) != len(args):\n\t\t\t\t\tfull_hash = True\n\n\t\t\tif full_hash:\n\t\t\t\twrapper_hash = hash(key)\n\t\t\telse:\n\t\t\t\twrapper_hash = wrapper_hash_key(key)\n\n\t\t\tnow = time.perf_counter()\n\t\t\tif wrapper_hash in cache:\n\t\t\t\thits += 1\n\t\t\t\tif cache[wrapper_hash][0] < now:\n\t\t\t\t\tdel cache[wrapper_hash]\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tassert isinstance(maxsize, (int, long))\n\tassert maxsize >= 0\n\n\t# Handle unlimited cache size.\n\tif maxsize == 0:\n\t\treturn lambda f: f\n\n\t# Implementation detail: The cache is stored as a dictionary that\n\t# mapps the arguments to the cached values. This is a dictionary\n\t# because the ordering is needed, and we can't trust that the\n\t# ordering of the arguments is going to be the same between\n\t# invocations.\n\tcache = {}\n\n\tdef wrapper(*args):\n\t\t# Get the key from the arguments.\n\t\tkey = args\n\t\tif typed:\n\t\t\t# If the function is typed, we need to use the\n\t\t\t# unhashable type to lookup in the cache.\n\t\t\tkey = tuple(args)\n\n\t\t# Get the cached value or compute it if it's not cached.\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\texcept KeyError:\n\t\t\t# Call the function.\n\t\t\tresult = f(*args)\n\t\t\tif maxsize == 1:\n\t\t\t\t# Sets are not ordered, so we clear the cache\n\t\t\t\t# before storing the result.\n\t\t\t\tcache", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n\t# order dictionary to store the cache\n\tclass OrderedDict(dict):\n\t\tdef __init__(self, maxlen, *args, **kwds):\n\t\t\tself.maxlen = maxlen\n\t\t\tsuper().__init__(*args, **kwds)\n\t\t\tself.counter = 0\n\t\t\tself.ordered = []\n\t\t\n\t\tdef __getitem__(self, key):\n\t\t\tvalue = super().__getitem__(key)\n\t\t\tself.ordered.append(key)\n\t\t\tself.counter += 1\n\t\t\treturn value\n\t\t\n\t\tdef __setitem__(self, key, value):\n\t\t\tif len(self) >= self.maxlen:\n\t\t\t\tself.pop(self.ordered[0])\n\t\t\t\tself.ordered.pop(0)\n\t\t\t\tself.counter -= 1\n\t\t\tsuper().__setitem__(key, value)\n\t\t\tself.ordered.append(key)\n\t\t\tself.counter += 1\n\t\t\n\t\tdef __delitem__(self, key):\n\t\t\tsuper().__delitem__(key)\n\t\t\tself.ordered.remove(key)\n\t\t\tself.counter -= 1\n\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = _LFUCache(maxsize, typed)\n\n\tdef decorating_function(*args, **kwargs):\n\t\treturn cache[args, kwargs]\n\n\treturn decorating_function\n\n\nclass _LFUCache(dict):\n\t\"\"\"\n\tAn LFUCache is a specialization of the DictCache class that adds an\nordering based on frequency of use, allowing the eviction of the\nleast frequently used items first.\n\t\"\"\"\n\tdef __init__(self, maxsize=128, typed=False):\n\t\tsuper().__init__()\n\t\tself.maxsize = maxsize\n\t\tself.counter = collections.Counter()\n\t\tself.typed = typed\n\n\tdef __getitem__(self, key):\n\t\ttry:\n\t\t\tvalue = super().__getitem__(key)\n\t\t\tself.counter[key] += 1\n\t\t\treturn value\n\t\texcept KeyError:\n\t\t\traise KeyError('Key is not in the cache.')\n\n\tdef __setitem__(self, key, value):\n\t\tif len(self) >= self.maxsize:\n\t\t\tfor _ in range(self.maxsize):\n\t\t\t\tkey_to_evict, _ = self.counter.", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = _LFUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper_function(*args):\n\t\t\t# if self.cache_clear:\n\t\t\t# \tcache.clear()\n\t\t\t# else:\n\t\t\t# \tif self.cache_get:\n\t\t\t# \t\ttry:\n\t\t\t# \t\t\treturn cache[args]\n\t\t\t# \t\texcept KeyError:\n\t\t\t# \t\t\tpass\n\t\t\t# \tif self.cache_set:\n\t\t\t# \t\tresult = user_function(*args)\n\t\t\t# \t\tcache[args] = result\n\t\t\t# \t\treturn result\n\t\t\t# \telse:\n\t\t\t# \t\treturn user_function(*args)\n\n\t\t\t# if self.cache_clear:\n\t\t\t# \tself.cache.clear()\n\t\t\t# else:\n\t\t\t# \tif self.cache_get:\n\t\t\t# \t\ttry:\n\t\t\t# \t\t\treturn self.cache[args]\n\t\t\t# \t", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t# TODO: Implement LFU cache.\n\t# TODO: Add support for typed=True.\n\tpass\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tclass _Cache(dict):\n\t\tdef __init__(self, maxsize, typed):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.hits = 0\n\t\t\tself.misses = 0\n\t\t\tself.currsize = 0\n\t\t\tself.typed = typed\n\t\t\tself.mapping = {}\n\t\t\tself.queue = []\n\t\n\t\tdef __len__(self):\n\t\t\treturn self.currsize\n\t\n\t\tdef __getitem__(self, key):\n\t\t\tself.hits += 1\n\t\t\treturn dict.__getitem__(self, key)\n\t\n\t\tdef __setitem__(self, key, value):\n\t\t\tif self.currsize >= self.maxsize:\n\t\t\t\twhile len(self.queue) and self.queue[0][1] < self.currsize - self.maxsize:\n\t\t\t\t\tdel self.mapping[self.queue.pop(0)[0]]\n\t\t\t\t\tself.currsize -= 1\n\t\n\t\t\toldsize = self.currsize\n\t\t\tdict.__setitem__(self, key, value)\n\t\t\tself.currsize = oldsize + 1\n\t\n\t\t\tif key", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tdef decorator(user_function):\n\t\t\n\t\tcache = {}\n\t\thits = [0] * maxsize\n\t\tmisses = [0] * maxsize\n\t\tfull = False\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\t# UserSays:\n\t\t\t# I can't find any documentation on how this works, can someone explain it?\n\t\t\t# I've been using this decorator for years, and I've never seen this line before.\n\t\t\t# What does it do?\n\t\t\t\n\t\t\tif full and len(cache) == maxsize:\n\t\t\t\t# Remove the least frequently used cache entry\n\t\t\t\t# (which is guaranteed to be the oldest entry due to the\n\t\t\t\t# Least Frequently Used algorithm).\n\t\t\t\t# The following line is equivalent to:\n\t\t\t\t#     lru = hits.index(min(hits))\n\t\t\t\t# But since we only have 256 possible hits values, we can use\n\t\t\t\t# a linear scan to find the minimum (or the first hit value\n\t\t\t\t# found, which is the same since the hits are monotonically\n\t\t\t\t# increasing).\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tclass _LFUCache(dict):\n\t\tdef __init__(self, maxsize, typed, *args, **kwds):\n\t\t\tif maxsize < 0:\n\t\t\t\traise ValueError('maxsize must be non-negative')\n\t\t\tself.maxsize = maxsize\n\t\t\tself.typed = typed\n\t\t\tself.hits = self.misses = self.currsize = 0\n\t\t\tself.queue = list()\n\t\t\tself.mapping = dict()\n\t\t\tsuper(_LFUCache, self).__init__(*args, **kwds)\n\t\t\t\n\t\tdef __delitem__(self, key, *args, **kwds):\n\t\t\tsuper(_LFUCache, self).__delitem__(key, *args, **kwds)\n\t\t\tself._remove_from_queue(key)\n\t\t\t\n\t\tdef __getitem__(self, key, *args, **kwds):\n\t\t\tself.hits += 1\n\t\t\tvalue = super(_LFUCache, self).__getitem__(key, *args, **kwds)\n\t\t\tself._bump_to_front(key)\n\t\t\treturn value\n\t\t\t\n\t\tdef __", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tif maxsize < 1:\n\t\traise ValueError('maxsize must be at least 1')\n\n\tcache = OrderedDict()\n\thits = 0\n\tmisses = 0\n\n\tdef wrapper(wrapped):\n\t\tnonlocal hits, misses\n\n\t\t @wraps(wrapped)\n\t\t def wrapped_func(*args, **kwargs):\n\t\t\t nonlocal cache, hits, misses\n\n\t\t\t key = _make_key(args, kwargs, typed)\n\t\t\t if key in cache:\n\t\t\t\t hits += 1\n\t\t\t\t value = cache[key]\n\t\t\t\t return value\n\t\t\t else:\n\t\t\t\t misses += 1\n\t\t\t\t value = wrapped(*args, **kwargs)\n\t\t\t\t if len(cache) >= maxsize:\n\t\t\t\t\t cache.popitem(last=False)\n\t\t\t\t cache[key] = value\n\t\t\t\t return value\n\n\t\t return wrapped_func\n\n\t return wrapper\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tfrom functools import lru_cache\n\t\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args if typed else tuple(args)\n\t\t\ttry:\n\t\t\t\tvalue = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = user_function(*args, **kwargs)\n\t\t", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = OrderedDict()\n\thits = [0] * maxsize\n\tmisses = [0] * maxsize\n\tfull = False\n\tlock = RLock()\n\n\tdef wrapper(key, *args, **kwds):\n\t\tresult = None\n\t\tif key not in cache:\n\t\t\tmisses[0] += 1\n\t\telse:\n\t\t\thits[0] += 1\n\t\t\tresult = cache[key]\n\t\t\tdel cache[key]\n\t\t\tcache[key] = result\n\t\t\tdel hits[0], misses[0]\n\t\t\tcache[key] = result\n\n\t\tif full:\n\t\t\tfor i in range(maxsize - 1, 0, -1):\n\t\t\t\thits[i], hits[i - 1] = hits[i - 1], hits[i]\n\t\t\t\tmisses[i], misses[i - 1] = misses[i - 1], misses[i]\n\t\t\t\tcache[i], cache[i - 1] = cache[i - 1], cache[i]\n\t\t\t\tif cache[i] == key:\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tcache[", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tmaxrepl = maxsize\n\trepl=[]\n\tcache={}\n\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not cache:\n\t\t\t\trepl=[]\n\t\t\t\tcache={}\n\t\t\t\tmaxrepl = maxsize\n\t\t\t\n\t\t\tif typed:\n\t\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\n\t\t\tif key in cache:\n\t\t\t\tvalue = cache[key]\n\t\t\t\trepl.remove(key)\n\t\t\t\trepl.insert(0, key)\n\t\t\t\treturn value\n\t\t\telse:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tif maxrepl == 0:\n\t\t\t\t\t# remove the least frequently used cache entry\n\t\t\t\t\trepl.pop()\n\t\t\t\t\tcache.pop(repl[-1])\n\t\t\t\t\tmaxrepl -= 1\n\t\t\t\telse:\n\t\t\t\t\tmaxrepl -= 1\n\t\t\t\trepl.insert(0, key)\n\t\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = _LFUCache(maxsize, typed)\n\n\tdef decorating_function(*args, **kwargs):\n\t\treturn cache[args, kwargs]\n\n\treturn decorating_function\n\n\nclass _LFUCache(dict):\n\t\"\"\"\n\tAn LFUCache is a specialization of the DictCache class that adds an\nordering based on frequency of use, allowing the eviction of the\nleast frequently used items first.\n\t\"\"\"\n\tdef __init__(self, maxsize=128, typed=False):\n\t\tsuper().__init__()\n\t\tself.maxsize = maxsize\n\t\tself.counter = collections.Counter()\n\t\tself.typed = typed\n\n\tdef __getitem__(self, key):\n\t\ttry:\n\t\t\tvalue = super().__getitem__(key)\n\t\t\tself.counter[key] += 1\n\t\t\treturn value\n\t\texcept KeyError:\n\t\t\traise KeyError('Key is not in the cache.')\n\n\tdef __setitem__(self, key, value):\n\t\tif len(self) >= self.maxsize:\n\t\t\tfor _ in range(self.maxsize):\n\t\t\t\tkey_to_evict, _ = self.counter.", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = _LFUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper_function(*args):\n\t\t\t# if self.cache_clear:\n\t\t\t# \tcache.clear()\n\t\t\t# else:\n\t\t\t# \tif self.cache_get:\n\t\t\t# \t\ttry:\n\t\t\t# \t\t\treturn cache[args]\n\t\t\t# \t\texcept KeyError:\n\t\t\t# \t\t\tpass\n\t\t\t# \tif self.cache_set:\n\t\t\t# \t\tresult = user_function(*args)\n\t\t\t# \t\tcache[args] = result\n\t\t\t# \t\treturn result\n\t\t\t# \telse:\n\t\t\t# \t\treturn user_function(*args)\n\n\t\t\t# if self.cache_clear:\n\t\t\t# \tself.cache.clear()\n\t\t\t# else:\n\t\t\t# \tif self.cache_get:\n\t\t\t# \t\ttry:\n\t\t\t# \t\t\treturn self.cache[args]\n\t\t\t# \t", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t# TODO: Implement LFU cache.\n\t# TODO: Add support for typed=True.\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tclass _Cache(dict):\n\t\tdef __init__(self, maxsize, typed):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.hits = 0\n\t\t\tself.misses = 0\n\t\t\tself.currsize = 0\n\t\t\tself.typed = typed\n\t\t\tself.mapping = {}\n\t\t\tself.queue = []\n\t\n\t\tdef __len__(self):\n\t\t\treturn self.currsize\n\t\n\t\tdef __getitem__(self, key):\n\t\t\tself.hits += 1\n\t\t\treturn dict.__getitem__(self, key)\n\t\n\t\tdef __setitem__(self, key, value):\n\t\t\tif self.currsize >= self.maxsize:\n\t\t\t\twhile len(self.queue) and self.queue[0][1] < self.currsize - self.maxsize:\n\t\t\t\t\tdel self.mapping[self.queue.pop(0)[0]]\n\t\t\t\t\tself.currsize -= 1\n\t\n\t\t\toldsize = self.currsize\n\t\t\tdict.__setitem__(self, key, value)\n\t\t\tself.currsize = oldsize + 1\n\t\n\t\t\tif key", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tdef decorator(user_function):\n\t\t\n\t\tcache = {}\n\t\thits = [0] * maxsize\n\t\tmisses = [0] * maxsize\n\t\tfull = False\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\t# UserSays:\n\t\t\t# I can't find any documentation on how this works, can someone explain it?\n\t\t\t# I've been using this decorator for years, and I've never seen this line before.\n\t\t\t# What does it do?\n\t\t\t\n\t\t\tif full and len(cache) == maxsize:\n\t\t\t\t# Remove the least frequently used cache entry\n\t\t\t\t# (which is guaranteed to be the oldest entry due to the\n\t\t\t\t# Least Frequently Used algorithm).\n\t\t\t\t# The following line is equivalent to:\n\t\t\t\t#     lru = hits.index(min(hits))\n\t\t\t\t# But since we only have 256 possible hits values, we can use\n\t\t\t\t# a linear scan to find the minimum (or the first hit value\n\t\t\t\t# found, which is the same since the hits are monotonically\n\t\t\t\t# increasing).\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tclass _LFUCache(dict):\n\t\tdef __init__(self, maxsize, typed, *args, **kwds):\n\t\t\tif maxsize < 0:\n\t\t\t\traise ValueError('maxsize must be non-negative')\n\t\t\tself.maxsize = maxsize\n\t\t\tself.typed = typed\n\t\t\tself.hits = self.misses = self.currsize = 0\n\t\t\tself.queue = list()\n\t\t\tself.mapping = dict()\n\t\t\tsuper(_LFUCache, self).__init__(*args, **kwds)\n\t\t\t\n\t\tdef __delitem__(self, key, *args, **kwds):\n\t\t\tsuper(_LFUCache, self).__delitem__(key, *args, **kwds)\n\t\t\tself._remove_from_queue(key)\n\t\t\t\n\t\tdef __getitem__(self, key, *args, **kwds):\n\t\t\tself.hits += 1\n\t\t\tvalue = super(_LFUCache, self).__getitem__(key, *args, **kwds)\n\t\t\tself._bump_to_front(key)\n\t\t\treturn value\n\t\t\t\n\t\tdef __", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tif maxsize < 1:\n\t\traise ValueError('maxsize must be at least 1')\n\n\tcache = OrderedDict()\n\thits = 0\n\tmisses = 0\n\n\tdef wrapper(wrapped):\n\t\tnonlocal hits, misses\n\n\t\t @wraps(wrapped)\n\t\t def wrapped_func(*args, **kwargs):\n\t\t\t nonlocal cache, hits, misses\n\n\t\t\t key = _make_key(args, kwargs, typed)\n\t\t\t if key in cache:\n\t\t\t\t hits += 1\n\t\t\t\t value = cache[key]\n\t\t\t\t return value\n\t\t\t else:\n\t\t\t\t misses += 1\n\t\t\t\t value = wrapped(*args, **kwargs)\n\t\t\t\t if len(cache) >= maxsize:\n\t\t\t\t\t cache.popitem(last=False)\n\t\t\t\t cache[key] = value\n\t\t\t\t return value\n\n\t\t return wrapped_func\n\n\t return wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\tfrom functools import lru_cache\n\t\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\t\t\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args if typed else tuple(args)\n\t\t\ttry:\n\t\t\t\tvalue = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = user_function(*args, **kwargs)\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = OrderedDict()\n\thits = [0] * maxsize\n\tmisses = [0] * maxsize\n\tfull = False\n\tlock = RLock()\n\n\tdef wrapper(key, *args, **kwds):\n\t\tresult = None\n\t\tif key not in cache:\n\t\t\tmisses[0] += 1\n\t\telse:\n\t\t\thits[0] += 1\n\t\t\tresult = cache[key]\n\t\t\tdel cache[key]\n\t\t\tcache[key] = result\n\t\t\tdel hits[0], misses[0]\n\t\t\tcache[key] = result\n\n\t\tif full:\n\t\t\tfor i in range(maxsize - 1, 0, -1):\n\t\t\t\thits[i], hits[i - 1] = hits[i - 1], hits[i]\n\t\t\t\tmisses[i], misses[i - 1] = misses[i - 1], misses[i]\n\t\t\t\tcache[i], cache[i - 1] = cache[i - 1], cache[i]\n\t\t\t\tif cache[i] == key:\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tcache[", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tmaxrepl = maxsize\n\trepl=[]\n\tcache={}\n\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not cache:\n\t\t\t\trepl=[]\n\t\t\t\tcache={}\n\t\t\t\tmaxrepl = maxsize\n\t\t\t\n\t\t\tif typed:\n\t\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\n\t\t\tif key in cache:\n\t\t\t\tvalue = cache[key]\n\t\t\t\trepl.remove(key)\n\t\t\t\trepl.insert(0, key)\n\t\t\t\treturn value\n\t\t\telse:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tif maxrepl == 0:\n\t\t\t\t\t# remove the least frequently used cache entry\n\t\t\t\t\trepl.pop()\n\t\t\t\t\tcache.pop(repl[-1])\n\t\t\t\t\tmaxrepl -= 1\n\t\t\t\telse:\n\t\t\t\t\tmaxrepl -= 1\n\t\t\t\trepl.insert(0, key)\n\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\traise NotImplementedError\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\treturn self.pop(self.keys()[0])\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey=self.keys()[0]\n\tvalue=self[key]\n\tdel self[key]\n\treturn key,value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\titem = self.keys[0], self.values[0]\n\tself.delete(item[0])\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\treturn self.pop(next(iter(self)))\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif not self.is_empty():\n\t\tkey = self._keys[0]\n\t\tvalue = self._values[0]\n\t\tdel self._keys[0]\n\t\tdel self._values[0]\n\t\treturn (key,value)\n\telse:\n\t\traise Empty('Deque is empty')\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif not self.is_empty():\n\t\tdata = self.head\n\t\tself.head = data.next\n\t\treturn data.key, data.val\n\telse:\n\t\traise Exception('List is empty')\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\treturn self.popitem()\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\titem = self.items[0]\n\tself.items.pop(0)\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif self.is_empty():\n\t\traise Exception()\n\telse:\n\t\treturn self.pop(self.queue[0])\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\treturn self.pop(self.keys()[0])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey=self.keys()[0]\n\tvalue=self[key]\n\tdel self[key]\n\treturn key,value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\titem = self.keys[0], self.values[0]\n\tself.delete(item[0])\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\treturn self.pop(next(iter(self)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif not self.is_empty():\n\t\tkey = self._keys[0]\n\t\tvalue = self._values[0]\n\t\tdel self._keys[0]\n\t\tdel self._values[0]\n\t\treturn (key,value)\n\telse:\n\t\traise Empty('Deque is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif not self.is_empty():\n\t\tdata = self.head\n\t\tself.head = data.next\n\t\treturn data.key, data.val\n\telse:\n\t\traise Exception('List is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\treturn self.popitem()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\titem = self.items[0]\n\tself.items.pop(0)\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif self.is_empty():\n\t\traise Exception()\n\telse:\n\t\treturn self.pop(self.queue[0])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif self.has_key(key):\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self.get(key)\n\telse:\n\t\tself[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif self.has_key(key):\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self.get(key)\n\telse:\n\t\tself[key] = default\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\tif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\treturn self.__dict__.get(key, default)\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self.__dict__[key]\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\treturn self.__dict__[key] if key in self.__dict__ else default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\treturn default\n\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self._dict:\n\t\treturn self._dict[key]\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self._data:\n\t\treturn self._data[key]\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\treturn default\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\treturn self.__dict__.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self.__dict__[key]\n\telse:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\treturn self.__dict__[key] if key in self.__dict__ else default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self._dict:\n\t\treturn self._dict[key]\n\telse:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self._data:\n\t\treturn self._data[key]\n\telse:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\treturn default\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef decorated(*args, **kwargs):\n\t\t\tif lock is None:\n\t\t\t\tlock = threading.Lock()\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\tcache = cache.get(key(*args, **kwargs))\n\t\t\t\texcept TypeError:\n\t\t\t\t\tcache = cache.get(key)\n\t\t\t\tif cache is not None:\n\t\t\t\t\treturn cache\n\t\t\t\telse:\n\t\t\t\t\tcache = f(*args, **kwargs)\n\t\t\t\t\tcache = cache if isinstance(cache, tuple) else (cache,)\n\t\t\t\t\tcache = list(map(lambda x: x if isinstance(x, str) else str(x), cache))\n\t\t\t\t\tcache = tuple(cache)\n\t\t\t\t\tcache = cache if len(cache) == 1 else cache[0]\n\t\t\t\t\tcache = cache if not isinstance(cache, tuple) else (cache,)\n\t\t\t\t\tcache = list(map(lambda x: x if isinstance(x, str) else str(x), cache))\n\t\t\t\t\t", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef callmethod(self, *args, **kwargs):\n\t\t\tcachekey = key(self, *args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tif cachekey in cache:\n\t\t\t\t\treturn cache[cachekey]\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[cachekey] = result\n\t\t\t\treturn result\n\t\tcallmethod.__name__ = func.__name__\n\t\tcallmethod.__doc__ = func.__doc__\n\t\tcallmethod.__dict__.update(func.__dict__)\n\t\treturn callmethod\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is None:\n\t\t\t\tlock = threading.Lock()\n\t\t\tkeyval = key(self, *args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[keyval]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[keyval] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tDecorates a function.\n\t\t\"\"\"\n\t\tdef wrap(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tCaches the result.\n\t\t\t\"\"\"\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.get(key(*args, **kwargs)) or f(*args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn cache.get(key(*args, **kwargs)) or f(*args, **kwargs)\n\t\twrap.cache = cache\n\t\twrap.key = key\n\t\twrap.lock = lock\n\t\treturn wrap\n\treturn decorate\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef inner(*args, **kwargs):\n\t\t\tdef get_key():\n\t\t\t\treturn key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tkey = get_key()\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tpass\n\t\t\t\treturn result\n\t\treturn inner\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\tnonlocal cache\n\t\t\tnonlocal key\n\n\t\t\tif cache is None:\n\t\t\t\tcache = {}\n\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\tif key not in cache:\n\t\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tif key not in cache:\n\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\t\treturn cache[key]\n\t\treturn inner\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tif not cache:\n\t\treturn\n\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache_key = key(*args, **kwargs)\n\t\t\tcached = cache.get(cache_key)\n\t\t\tif cached:\n\t\t\t\treturn cached\n\t\t\tvalue = func(*args, **kwargs)\n\t\t\twith lock(cache_key):\n\t\t\t\tcache.set(cache_key, value)\n\t\t\treturn value\n\t\treturn wrapper\n\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tif key not in cache:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tif key not in cache:\n\t\t\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\twrapper.uncached = func\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorate(func):\n\t\tkeyfunc = makekey(key)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = keyfunc(*args, **kwargs)\n\t\t\tvalue = cache.get(key)\n\t\t\tif value is None:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache.set(key, value, lock=lock)\n\t\t\treturn value\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\treturn wrapper\n\treturn decorate\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorate(f):\n\t\tdef wrap(self, *args, **kwargs):\n\t\t\ttry:\n\t\t\t\tcache = cache(self)\n\t\t\texcept AttributeError:\n\t\t\t\traise NotImplementedError('Caching requires a cache attribute')\n\t\t\tkey = key(self, args, kwargs)\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tr = f(self, *args, **kwargs)\n\t\t\t\tfinally:\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.release()\n\t\t\t\tcache[key] = r\n\t\t\t\treturn r\n\t\treturn wrap\n\treturn decorate\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef decorated(*args, **kwargs):\n\t\t\tif lock is None:\n\t\t\t\tlock = threading.Lock()\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\tcache = cache.get(key(*args, **kwargs))\n\t\t\t\texcept TypeError:\n\t\t\t\t\tcache = cache.get(key)\n\t\t\t\tif cache is not None:\n\t\t\t\t\treturn cache\n\t\t\t\telse:\n\t\t\t\t\tcache = f(*args, **kwargs)\n\t\t\t\t\tcache = cache if isinstance(cache, tuple) else (cache,)\n\t\t\t\t\tcache = list(map(lambda x: x if isinstance(x, str) else str(x), cache))\n\t\t\t\t\tcache = tuple(cache)\n\t\t\t\t\tcache = cache if len(cache) == 1 else cache[0]\n\t\t\t\t\tcache = cache if not isinstance(cache, tuple) else (cache,)\n\t\t\t\t\tcache = list(map(lambda x: x if isinstance(x, str) else str(x), cache))\n\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef callmethod(self, *args, **kwargs):\n\t\t\tcachekey = key(self, *args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tif cachekey in cache:\n\t\t\t\t\treturn cache[cachekey]\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[cachekey] = result\n\t\t\t\treturn result\n\t\tcallmethod.__name__ = func.__name__\n\t\tcallmethod.__doc__ = func.__doc__\n\t\tcallmethod.__dict__.update(func.__dict__)\n\t\treturn callmethod\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is None:\n\t\t\t\tlock = threading.Lock()\n\t\t\tkeyval = key(self, *args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[keyval]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[keyval] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tDecorates a function.\n\t\t\"\"\"\n\t\tdef wrap(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tCaches the result.\n\t\t\t\"\"\"\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.get(key(*args, **kwargs)) or f(*args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn cache.get(key(*args, **kwargs)) or f(*args, **kwargs)\n\t\twrap.cache = cache\n\t\twrap.key = key\n\t\twrap.lock = lock\n\t\treturn wrap\n\treturn decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef inner(*args, **kwargs):\n\t\t\tdef get_key():\n\t\t\t\treturn key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tkey = get_key()\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tpass\n\t\t\t\treturn result\n\t\treturn inner\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\tnonlocal cache\n\t\t\tnonlocal key\n\n\t\t\tif cache is None:\n\t\t\t\tcache = {}\n\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\tif key not in cache:\n\t\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tif key not in cache:\n\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\t\treturn cache[key]\n\t\treturn inner\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tif not cache:\n\t\treturn\n\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache_key = key(*args, **kwargs)\n\t\t\tcached = cache.get(cache_key)\n\t\t\tif cached:\n\t\t\t\treturn cached\n\t\t\tvalue = func(*args, **kwargs)\n\t\t\twith lock(cache_key):\n\t\t\t\tcache.set(cache_key, value)\n\t\t\treturn value\n\t\treturn wrapper\n\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tif key not in cache:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tif key not in cache:\n\t\t\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\twrapper.uncached = func\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorate(func):\n\t\tkeyfunc = makekey(key)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = keyfunc(*args, **kwargs)\n\t\t\tvalue = cache.get(key)\n\t\t\tif value is None:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache.set(key, value, lock=lock)\n\t\t\treturn value\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\treturn wrapper\n\treturn decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorate(f):\n\t\tdef wrap(self, *args, **kwargs):\n\t\t\ttry:\n\t\t\t\tcache = cache(self)\n\t\t\texcept AttributeError:\n\t\t\t\traise NotImplementedError('Caching requires a cache attribute')\n\t\t\tkey = key(self, args, kwargs)\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tr = f(self, *args, **kwargs)\n\t\t\t\tfinally:\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.release()\n\t\t\t\tcache[key] = r\n\t\t\t\treturn r\n\t\treturn wrap\n\treturn decorate\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif not isinstance(e, cls):\n\t\te = cls(e)\n\n\tpath = []\n\twhile isinstance(e, cls) and e.path and len(path) < max_path_level:\n\t\tpath.append(e.path[-1])\n\t\te = e.cause\n\t\tif e is None: break\n\tpath = '->'.join(path)\n\tif e is not None:\n\t\tpath += ' -> ' + str(e)\n\n\tindent = 0\n\tstack = []\n\twhile e is not None and len(stack) < max_level:\n\t\tstack.append(e)\n\t\te = e.cause\n\t\tindent += 1\n\t\tif e is None: break\n\n\tresult = ''\n\tfor i, e in enumerate(stack[::-1]):\n\t\tresult += ('\\n' + ' '*indent*4) + str(e)\n\t\tindent -= 1\n\t\tif i == 0:\n\t\t\tresult = ' -> ' + path + result\n\treturn result\n\nclass RpcError(Exception):\n\t\"\"\"\n\tRPC error\n\t\"\"\"\n\tdef __init__(self, msg='', code=0, cause=None):\n\t\t", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tfrom types import FrameType\n\n\tdef _excstr(exc, indent=0, max_level=30):\n\t\t\"\"\"\n\t\tFormat the exception as a string\n\t\t\"\"\"\n\t\tif max_level < 0:\n\t\t\treturn '<...>'\n\t\tindent += 1\n\t\ts = '  ' * indent\n\t\tif isinstance(exc, cls):\n\t\t\ts += str(exc)\n\t\telse:\n\t\t\ts += str(exc)\n\t\ts += '\\n'\n\t\tfor tb in exc.__traceback__.tb_next:\n\t\t\tif max_level > 0:\n\t\t\t\ts += '  ' * indent + '...\\n'\n\t\t\t\tmax_level -= 1\n\t\t\telse:\n\t\t\t\ts += '  ' * indent + '...\\n'\n\t\t\t\tbreak\n\t\t\tif tb.tb_frame is not None:\n\t\t\t\ts += sprintf_traceback_frame(tb)\n\t\treturn s\n\n\tdef sprintf_traceback_frame(tb):\n\t\t\"\"\"\n\t\tReturns a string representation of the traceback frame tb.\n\t\t\"\"\"\n\t\tif t", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif max_level <= 0:\n\t\treturn '...'\n\n\tif max_path_level < 0:\n\t\treturn '...'\n\n\tdef indent(level):\n\t\treturn ' ' * 4 * level\n\n\tdef format_dict(d):\n\t\treturn ', '.join(f'{k}: {v}' for k, v in d.items())\n\n\tdef format_dict_or_str(d):\n\t\tif isinstance(d, str):\n\t\t\treturn d\n\t\telse:\n\t\t\treturn format_dict(d)\n\n\tdef format_args(args):\n\t\tif len(args) == 0:\n\t\t\treturn ''\n\t\tif len(args) == 1:\n\t\t\treturn f' {args[0]}'\n\t\treturn ', ' + ', '.join(str(a) for a in args)\n\n\tdef format_traceback(tb):\n\t\tif max_path_level <= 0:\n\t\t\treturn '...'\n\n\t\tlines = []\n\t\tfor frame in tb:\n\t\t\tif not frame:\n\t\t\t\tcontinue\n\t\t\tfilename = frame.filename\n\t\t\tif not filename:\n\t\t\t\tfilename = '?'\n\t\t", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tlevel = 0\n\tpath = []\n\twhile e:\n\t\tif level > max_level:\n\t\t\treturn str(e)\n\t\tpath.append(e.name)\n\t\tif level > max_path_level:\n\t\t\te = e.cause\n\t\t\tcontinue\n\t\tif e.cause:\n\t\t\te = e.cause\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif e.info:\n\t\t\te = e.info\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif e.next:\n\t\t\te = e.next\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tbreak\n\tpath = path[::-1]\n\treturn '\\n'.join(path)\n\nclass SysException(Exception):\n\t\"\"\"\n\tSysException(msg, cause=None, info=None, next=None)\n\t\"\"\"\n\tdef __init__(self, msg, cause=None, info=None, next=None):\n\t\tException.__init__(self)\n\t\tself.msg = msg\n\t\tself.cause = cause\n\t\tself.info = info\n\t\tself.next = next\n\t\tself.name = self.__class__.__", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif not isinstance(e, cls):\n\t\treturn str(e)\n\tif e.level > max_level:\n\t\treturn \"Level too deep\"\n\tif e.path_level > max_path_level:\n\t\treturn \"Level too deep\"\n\treturn f\"{e.path}({e.lineno}): {e.message}: {e.file}\"\n\nclass Exc(Exception):\n\t\"\"\"\n\tBase exception\n\t\"\"\"\n\tdef __init__(self, message, lineno=None, path=None):\n\t\tself.message = message\n\t\tself.lineno = lineno\n\t\tself.path = path\n\t\tself.level = 1\n\t\tself.path_level = 0\n\t\tself.file = \"\"\n\n\tdef __str__(self):\n\t\treturn extostr(self)\n\nclass ExcLevel(Exc):\n\t\"\"\"\n\tA \"level\" exception\n\t\"\"\"\n\tdef __init__(self, message, lineno=None, path=None, level=1):\n\t\tsuper().__init__(message, lineno, path)\n\t\tself.level = level\n\nclass ExcPath(Exc):\n\t\"\"\"\n\tA \"path\" exception\n\t\"\"\"\n\tdef", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif max_level == 0:\n\t\treturn \"Max level reached\"\n\tif max_path_level == 0:\n\t\treturn \"Max path level reached\"\n\n\tmsg = str(e)\n\tif isinstance(e, Exception):\n\t\tmsg += \". \"\n\n\tif e.__traceback__ is not None:\n\t\ttb = e.__traceback__\n\t\ttb_list = []\n\t\twhile tb is not None:\n\t\t\ttb_list.append((tb.tb_lineno, tb.tb_frame.f_code.co_name, tb.tb_frame.f_code.co_filename))\n\t\t\ttb = tb.tb_next\n\t\tif len(tb_list) > max_path_level:\n\t\t\ttb_list = tb_list[:max_path_level]\n\t\t\tmsg += \"Exception Path: \\n\"\n\t\t\tfor i, (lineno, name, filename) in enumerate(tb_list):\n\t\t\t\tmsg += \"\\t\" + str(i+1) + \". \" + filename + \": \" + name + \"() at line \" + str(lineno) + \"\\n\"\n\t\t\tif len(tb_list) == max_path_level:\n\t\t", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\ts = []\n\tfor i in range(max_level):\n\t\ts.append(\"[%d] %s\" % (i, e))\n\t\te = e.__cause__\n\t\tif e is None:\n\t\t\tbreak\n\t\tif i == max_level - 1:\n\t\t\ts.append(\"[%d] %s\" % (max_level, \"Too many levels\"))\n\t\t\tbreak\n\ts = \"\\n\".join(s)\n\tfor i in range(max_path_level):\n\t\ts = s.replace(\"[%d] \" % i, '[' + (\".\" * (i + 1)) + \"] \")\n\treturn s\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport traceback\n\ttb_strs = []\n\ttb_nums = []\n\ttb_level = 0\n\ttb_path_level = 0\n\ttb_strs.append(str(e))\n\ttb_nums.append(0)\n\twhile e.__traceback__:\n\t\ttb_level += 1\n\t\ttb_nums.append(tb_level)\n\t\te = e.__traceback__\n\t\tif tb_level > max_level:\n\t\t\tbreak\n\t\tif len(tb_strs) >= max_level:\n\t\t\tbreak\n\t\tif tb_path_level > max_path_level:\n\t\t\ttb_strs.append('...')\n\t\t\tbreak\n\t\telse:\n\t\t\ttb_path_level += 1\n\t\ttb_strs.append(traceback.format_tb(e, 1)[0])\n\treturn '\\n'.join(tb_strs)\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif e.level > max_level:\n\t\te.msg = 'Exception Too Deep'\n\t\te.exc_class = 'Exception'\n\t\tif e.exc_class == 'Exception':\n\t\t\te.exc_class = 'Unknown'\n\t\treturn cls.fmt_str(e)\n\n\tif not e.exc_class:\n\t\te.exc_class = 'Unknown'\n\n\tif e.level > max_path_level:\n\t\te.path = ''\n\n\tif e.exc_class in cls.EXC_STR:\n\t\te.msg = cls.EXC_STR[e.exc_class]\n\telse:\n\t\te.msg = 'Unknown'\n\n\tif e.exc_class == 'SystemExit':\n\t\te.msg = '%s: %s' % (e.exc_class, e.exc_value)\n\n\tif e.exc_class != 'Exception':\n\t\te.msg = '%s: %s' % (e.exc_class, e.msg)\n\n\tif isinstance(e.exc_value, Exception):\n\t\te.msg = '%s: %s' % (e.exc_value.__", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tmsg = \"\"\n\t# get the message\n\tif e.message:\n\t\tmsg += e.message\n\t\tmsg += \"\\n\"\n\t# get the exception type\n\tmsg += \"Exception type: %s\" % e.__class__.__name__\n\tmsg += \"\\n\"\n\t# get the exception level\n\tmsg += \"Exception level: %s\" % e.level\n\tmsg += \"\\n\"\n\t# get the exception path\n\tmsg += \"Exception path: %s\" % e.path\n\tmsg += \"\\n\"\n\t# get the exception traceback\n\tmsg += \"Exception traceback:\\n\"\n\tmsg += \"\".join(traceback.format_exception(cls, e, e.__traceback__))\n\tmsg += \"\".join(traceback.format_exception(cls, e, e.__traceback__))\n\tif max_level >= e.level:\n\t\t# get the exception cause\n\t\tmsg += \"Exception cause:\\n\"\n\t\tmsg += extostr(e.cause, max_level, max_path_level)\n\t\tif max_path_level >= len(e.path):\n\t\t\t# get the exception path cause\n\t\t\tmsg += \"Exception path cause:\\n\"\n\t\t\tmsg += extostr(e.path_c"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif not isinstance(e, cls):\n\t\te = cls(e)\n\n\tpath = []\n\twhile isinstance(e, cls) and e.path and len(path) < max_path_level:\n\t\tpath.append(e.path[-1])\n\t\te = e.cause\n\t\tif e is None: break\n\tpath = '->'.join(path)\n\tif e is not None:\n\t\tpath += ' -> ' + str(e)\n\n\tindent = 0\n\tstack = []\n\twhile e is not None and len(stack) < max_level:\n\t\tstack.append(e)\n\t\te = e.cause\n\t\tindent += 1\n\t\tif e is None: break\n\n\tresult = ''\n\tfor i, e in enumerate(stack[::-1]):\n\t\tresult += ('\\n' + ' '*indent*4) + str(e)\n\t\tindent -= 1\n\t\tif i == 0:\n\t\t\tresult = ' -> ' + path + result\n\treturn result\n\nclass RpcError(Exception):\n\t\"\"\"\n\tRPC error\n\t\"\"\"\n\tdef __init__(self, msg='', code=0, cause=None):\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tfrom types import FrameType\n\n\tdef _excstr(exc, indent=0, max_level=30):\n\t\t\"\"\"\n\t\tFormat the exception as a string\n\t\t\"\"\"\n\t\tif max_level < 0:\n\t\t\treturn '<...>'\n\t\tindent += 1\n\t\ts = '  ' * indent\n\t\tif isinstance(exc, cls):\n\t\t\ts += str(exc)\n\t\telse:\n\t\t\ts += str(exc)\n\t\ts += '\\n'\n\t\tfor tb in exc.__traceback__.tb_next:\n\t\t\tif max_level > 0:\n\t\t\t\ts += '  ' * indent + '...\\n'\n\t\t\t\tmax_level -= 1\n\t\t\telse:\n\t\t\t\ts += '  ' * indent + '...\\n'\n\t\t\t\tbreak\n\t\t\tif tb.tb_frame is not None:\n\t\t\t\ts += sprintf_traceback_frame(tb)\n\t\treturn s\n\n\tdef sprintf_traceback_frame(tb):\n\t\t\"\"\"\n\t\tReturns a string representation of the traceback frame tb.\n\t\t\"\"\"\n\t\tif t", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif max_level <= 0:\n\t\treturn '...'\n\n\tif max_path_level < 0:\n\t\treturn '...'\n\n\tdef indent(level):\n\t\treturn ' ' * 4 * level\n\n\tdef format_dict(d):\n\t\treturn ', '.join(f'{k}: {v}' for k, v in d.items())\n\n\tdef format_dict_or_str(d):\n\t\tif isinstance(d, str):\n\t\t\treturn d\n\t\telse:\n\t\t\treturn format_dict(d)\n\n\tdef format_args(args):\n\t\tif len(args) == 0:\n\t\t\treturn ''\n\t\tif len(args) == 1:\n\t\t\treturn f' {args[0]}'\n\t\treturn ', ' + ', '.join(str(a) for a in args)\n\n\tdef format_traceback(tb):\n\t\tif max_path_level <= 0:\n\t\t\treturn '...'\n\n\t\tlines = []\n\t\tfor frame in tb:\n\t\t\tif not frame:\n\t\t\t\tcontinue\n\t\t\tfilename = frame.filename\n\t\t\tif not filename:\n\t\t\t\tfilename = '?'\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tlevel = 0\n\tpath = []\n\twhile e:\n\t\tif level > max_level:\n\t\t\treturn str(e)\n\t\tpath.append(e.name)\n\t\tif level > max_path_level:\n\t\t\te = e.cause\n\t\t\tcontinue\n\t\tif e.cause:\n\t\t\te = e.cause\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif e.info:\n\t\t\te = e.info\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif e.next:\n\t\t\te = e.next\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tbreak\n\tpath = path[::-1]\n\treturn '\\n'.join(path)\n\nclass SysException(Exception):\n\t\"\"\"\n\tSysException(msg, cause=None, info=None, next=None)\n\t\"\"\"\n\tdef __init__(self, msg, cause=None, info=None, next=None):\n\t\tException.__init__(self)\n\t\tself.msg = msg\n\t\tself.cause = cause\n\t\tself.info = info\n\t\tself.next = next\n\t\tself.name = self.__class__.__", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif not isinstance(e, cls):\n\t\treturn str(e)\n\tif e.level > max_level:\n\t\treturn \"Level too deep\"\n\tif e.path_level > max_path_level:\n\t\treturn \"Level too deep\"\n\treturn f\"{e.path}({e.lineno}): {e.message}: {e.file}\"\n\nclass Exc(Exception):\n\t\"\"\"\n\tBase exception\n\t\"\"\"\n\tdef __init__(self, message, lineno=None, path=None):\n\t\tself.message = message\n\t\tself.lineno = lineno\n\t\tself.path = path\n\t\tself.level = 1\n\t\tself.path_level = 0\n\t\tself.file = \"\"\n\n\tdef __str__(self):\n\t\treturn extostr(self)\n\nclass ExcLevel(Exc):\n\t\"\"\"\n\tA \"level\" exception\n\t\"\"\"\n\tdef __init__(self, message, lineno=None, path=None, level=1):\n\t\tsuper().__init__(message, lineno, path)\n\t\tself.level = level\n\nclass ExcPath(Exc):\n\t\"\"\"\n\tA \"path\" exception\n\t\"\"\"\n\tdef", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif max_level == 0:\n\t\treturn \"Max level reached\"\n\tif max_path_level == 0:\n\t\treturn \"Max path level reached\"\n\n\tmsg = str(e)\n\tif isinstance(e, Exception):\n\t\tmsg += \". \"\n\n\tif e.__traceback__ is not None:\n\t\ttb = e.__traceback__\n\t\ttb_list = []\n\t\twhile tb is not None:\n\t\t\ttb_list.append((tb.tb_lineno, tb.tb_frame.f_code.co_name, tb.tb_frame.f_code.co_filename))\n\t\t\ttb = tb.tb_next\n\t\tif len(tb_list) > max_path_level:\n\t\t\ttb_list = tb_list[:max_path_level]\n\t\t\tmsg += \"Exception Path: \\n\"\n\t\t\tfor i, (lineno, name, filename) in enumerate(tb_list):\n\t\t\t\tmsg += \"\\t\" + str(i+1) + \". \" + filename + \": \" + name + \"() at line \" + str(lineno) + \"\\n\"\n\t\t\tif len(tb_list) == max_path_level:\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\ts = []\n\tfor i in range(max_level):\n\t\ts.append(\"[%d] %s\" % (i, e))\n\t\te = e.__cause__\n\t\tif e is None:\n\t\t\tbreak\n\t\tif i == max_level - 1:\n\t\t\ts.append(\"[%d] %s\" % (max_level, \"Too many levels\"))\n\t\t\tbreak\n\ts = \"\\n\".join(s)\n\tfor i in range(max_path_level):\n\t\ts = s.replace(\"[%d] \" % i, '[' + (\".\" * (i + 1)) + \"] \")\n\treturn s\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport traceback\n\ttb_strs = []\n\ttb_nums = []\n\ttb_level = 0\n\ttb_path_level = 0\n\ttb_strs.append(str(e))\n\ttb_nums.append(0)\n\twhile e.__traceback__:\n\t\ttb_level += 1\n\t\ttb_nums.append(tb_level)\n\t\te = e.__traceback__\n\t\tif tb_level > max_level:\n\t\t\tbreak\n\t\tif len(tb_strs) >= max_level:\n\t\t\tbreak\n\t\tif tb_path_level > max_path_level:\n\t\t\ttb_strs.append('...')\n\t\t\tbreak\n\t\telse:\n\t\t\ttb_path_level += 1\n\t\ttb_strs.append(traceback.format_tb(e, 1)[0])\n\treturn '\\n'.join(tb_strs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tif e.level > max_level:\n\t\te.msg = 'Exception Too Deep'\n\t\te.exc_class = 'Exception'\n\t\tif e.exc_class == 'Exception':\n\t\t\te.exc_class = 'Unknown'\n\t\treturn cls.fmt_str(e)\n\n\tif not e.exc_class:\n\t\te.exc_class = 'Unknown'\n\n\tif e.level > max_path_level:\n\t\te.path = ''\n\n\tif e.exc_class in cls.EXC_STR:\n\t\te.msg = cls.EXC_STR[e.exc_class]\n\telse:\n\t\te.msg = 'Unknown'\n\n\tif e.exc_class == 'SystemExit':\n\t\te.msg = '%s: %s' % (e.exc_class, e.exc_value)\n\n\tif e.exc_class != 'Exception':\n\t\te.msg = '%s: %s' % (e.exc_class, e.msg)\n\n\tif isinstance(e.exc_value, Exception):\n\t\te.msg = '%s: %s' % (e.exc_value.__", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tmsg = \"\"\n\t# get the message\n\tif e.message:\n\t\tmsg += e.message\n\t\tmsg += \"\\n\"\n\t# get the exception type\n\tmsg += \"Exception type: %s\" % e.__class__.__name__\n\tmsg += \"\\n\"\n\t# get the exception level\n\tmsg += \"Exception level: %s\" % e.level\n\tmsg += \"\\n\"\n\t# get the exception path\n\tmsg += \"Exception path: %s\" % e.path\n\tmsg += \"\\n\"\n\t# get the exception traceback\n\tmsg += \"Exception traceback:\\n\"\n\tmsg += \"\".join(traceback.format_exception(cls, e, e.__traceback__))\n\tmsg += \"\".join(traceback.format_exception(cls, e, e.__traceback__))\n\tif max_level >= e.level:\n\t\t# get the exception cause\n\t\tmsg += \"Exception cause:\\n\"\n\t\tmsg += extostr(e.cause, max_level, max_path_level)\n\t\tif max_path_level >= len(e.path):\n\t\t\t# get the exception path cause\n\t\t\tmsg += \"Exception path cause:\\n\"\n\t\t\tmsg += extostr(e.path_c", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'a'\n\tif overwrite:\n\t\tmode = 'w'\n\ttry:\n\t\tif encoding:\n\t\t\twith open(file_name, mode, encoding=encoding) as f:\n\t\t\t\tf.write(text_buffer)\n\t\telse:\n\t\t\twith open(file_name, mode) as f:\n\t\t\t\tf.write(text_buffer)\n\texcept Exception as e:\n\t\tprint(\"Could not write to file: \" + file_name + \" \" + str(e))\n\t\treturn False\n\treturn True\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t# Open the file\n\tif overwrite:\n\t\tf = codecs.open(file_name, 'w', encoding)\n\telse:\n\t\tf = codecs.open(file_name, 'a', encoding)\n\n\t# Write the text\n\tf.write(text_buffer)\n\n\t# Close the file\n\tf.close()\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t# Check if the file exists\n\tif os.path.isfile(file_name) and not overwrite:\n\t\tprint(f'File {file_name} already exists, writing to a new file')\n\t\tfile_name = file_name + str(time.time())\n\n\t# Open the file\n\twith open(file_name, 'a', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\twrite_file = open(file_name, 'w', encoding=encoding)\n\t\twrite_file.write(text_buffer)\n\t\twrite_file.close()\n\telse:\n\t\tif overwrite:\n\t\t\twrite_file = open(file_name, 'w', encoding=encoding)\n\t\t\twrite_file.write(text_buffer)\n\t\t\twrite_file.close()\n\t\telse:\n\t\t\tappend_file = open(file_name, 'a', encoding=encoding)\n\t\t\tappend_file.write(text_buffer)\n\t\t\tappend_file.close()\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif not overwrite and os.path.exists(file_name):\n\t\treturn\n\n\twith io.open(file_name, 'w', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif not file_name:\n\t\treturn\n\tif not overwrite:\n\t\tif os.path.exists(file_name):\n\t\t\treturn\n\twith codecs.open(file_name, \"w\", encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\n# -----------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'wb'\n\tif overwrite:\n\t\tmode = 'wb'\n\telse:\n\t\tmode = 'ab'\n\twith open(file_name, mode=mode, encoding=encoding) as f:\n\t\tf.write(text_buffer.encode(encoding))\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif file_name is None:\n\t\treturn\n\ttry:\n\t\twith open(file_name, \"a\", encoding=encoding) as f:\n\t\t\tif not overwrite:\n\t\t\t\tf.write(text_buffer)\n\texcept IOError:\n\t\tprint(\"Error while writing text to file\")\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'a'\n\tif overwrite:\n\t\tmode = 'w'\n\ttry:\n\t\tif encoding:\n\t\t\twith open(file_name, mode, encoding=encoding) as f:\n\t\t\t\tf.write(text_buffer)\n\t\telse:\n\t\t\twith open(file_name, mode) as f:\n\t\t\t\tf.write(text_buffer)\n\texcept Exception as e:\n\t\tprint(\"Could not write to file: \" + file_name + \" \" + str(e))\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t# Open the file\n\tif overwrite:\n\t\tf = codecs.open(file_name, 'w', encoding)\n\telse:\n\t\tf = codecs.open(file_name, 'a', encoding)\n\n\t# Write the text\n\tf.write(text_buffer)\n\n\t# Close the file\n\tf.close()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t# Check if the file exists\n\tif os.path.isfile(file_name) and not overwrite:\n\t\tprint(f'File {file_name} already exists, writing to a new file')\n\t\tfile_name = file_name + str(time.time())\n\n\t# Open the file\n\twith open(file_name, 'a', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\twrite_file = open(file_name, 'w', encoding=encoding)\n\t\twrite_file.write(text_buffer)\n\t\twrite_file.close()\n\telse:\n\t\tif overwrite:\n\t\t\twrite_file = open(file_name, 'w', encoding=encoding)\n\t\t\twrite_file.write(text_buffer)\n\t\t\twrite_file.close()\n\t\telse:\n\t\t\tappend_file = open(file_name, 'a', encoding=encoding)\n\t\t\tappend_file.write(text_buffer)\n\t\t\tappend_file.close()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif not overwrite and os.path.exists(file_name):\n\t\treturn\n\n\twith io.open(file_name, 'w', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif not file_name:\n\t\treturn\n\tif not overwrite:\n\t\tif os.path.exists(file_name):\n\t\t\treturn\n\twith codecs.open(file_name, \"w\", encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\n# -----------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'wb'\n\tif overwrite:\n\t\tmode = 'wb'\n\telse:\n\t\tmode = 'ab'\n\twith open(file_name, mode=mode, encoding=encoding) as f:\n\t\tf.write(text_buffer.encode(encoding))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tif file_name is None:\n\t\treturn\n\ttry:\n\t\twith open(file_name, \"a\", encoding=encoding) as f:\n\t\t\tif not overwrite:\n\t\t\t\tf.write(text_buffer)\n\texcept IOError:\n\t\tprint(\"Error while writing text to file\")\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\t# Open the file\n\twith open(file_name, mode='r', encoding=encoding) as f:\n\t\t# Read the file\n\t\tfile_text = f.read()\n\t\t# Return the file_text\n\t\treturn file_text\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttextbuffer = gtk.TextBuffer()\n\tfile = open(file_name, \"r\")\n\ttextbuffer.set_text(file.read())\n\tfile.close()\n\treturn textbuffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\ttext = f.read()\n\t\t\treturn text\n\texcept Exception as e:\n\t\tprint(\"Error: \" + str(e))\n\t\treturn \"\"\n\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile = open(file_name, 'r', encoding=encoding)\n\ttbuffer = gtk.TextBuffer(file.read())\n\tfile.close()\n\treturn tbuffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tf = open(file_name, 'r', encoding=encoding)\n\tcontent = f.read()\n\tf.close()\n\treturn content\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as file:\n\t\treturn file.read()\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile = open(file_name, \"r\", encoding=encoding)\n\tbuffer = file.read()\n\tfile.close()\n\treturn buffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile = open(file_name, encoding=encoding)\n\ttext = file.read()\n\tfile.close()\n\treturn text\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttext = f.read()\n\t\treturn text\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\t# Open the file\n\twith open(file_name, mode='r', encoding=encoding) as f:\n\t\t# Read the file\n\t\tfile_text = f.read()\n\t\t# Return the file_text\n\t\treturn file_text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttextbuffer = gtk.TextBuffer()\n\tfile = open(file_name, \"r\")\n\ttextbuffer.set_text(file.read())\n\tfile.close()\n\treturn textbuffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\ttext = f.read()\n\t\t\treturn text\n\texcept Exception as e:\n\t\tprint(\"Error: \" + str(e))\n\t\treturn \"\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile = open(file_name, 'r', encoding=encoding)\n\ttbuffer = gtk.TextBuffer(file.read())\n\tfile.close()\n\treturn tbuffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tf = open(file_name, 'r', encoding=encoding)\n\tcontent = f.read()\n\tf.close()\n\treturn content\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as file:\n\t\treturn file.read()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile = open(file_name, \"r\", encoding=encoding)\n\tbuffer = file.read()\n\tfile.close()\n\treturn buffer\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile = open(file_name, encoding=encoding)\n\ttext = file.read()\n\tfile.close()\n\treturn text\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttext = f.read()\n\t\treturn text\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile = open(file_name)\n\t\tfile.close()\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\treturn False\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile = open(file_name)\n\t\tfile.close()\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\troot = logging.getLogger()\n\tif root.handlers:\n\t\tfor handler in root.handlers:\n\t\t\troot.removeHandler(handler)\n\tcls._logger = None\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tfor handler in cls.logger.handlers:\n\t\tcls.logger.removeHandler(handler)\n\tcls.logger = None\n\nclass Logger:\n\t\"\"\"\n\tLogging class\n\t\"\"\"\n\tlogger = None\n\n\tdef __init__(self, log_file: str, debug: bool = True):\n\t\t\"\"\"\n\t\tInitialize the logger\n\t\t\"\"\"\n\t\tif self.logger is not None:\n\t\t\tself._reset_logging(self)\n\t\tself.logger = logging.getLogger()\n\t\tself.logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\tfh = logging.FileHandler(log_file)\n\t\tfh.setFormatter(formatter)\n\t\tsh = logging.StreamHandler()\n\t\tsh.setFormatter(formatter)\n\t\tself.logger.addHandler(fh)\n\t\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tfor handler in cls.logger.handlers:\n\t\tcls.logger.removeHandler(handler)\n\tcls.logger.handlers = []\n\tcls.logger.setLevel(logging.DEBUG)\n\t\ndef _setup_logging():\n\t\"\"\"\n\tSetup the logging system\n\t\"\"\"\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s - %(message)s')\n\thandler = logging.StreamHandler(sys.stderr)\n\thandler.setFormatter(formatter)\n\thandler.setLevel(logging.DEBUG)\n\t\n\t# add the handlers to the logger\n\tlogger.addHandler(handler)\n\tlogger.setLevel(logging.DEBUG)\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t# Remove all previously configured handlers\n\tfor handler in cls.get_handlers():\n\t\tcls.remove_handler(handler)\n\n\t# Remove all filters\n\tfor handler in cls.get_handlers():\n\t\thandler.filters = []\n\n\t# Remove all previously configured formatters\n\tfor handler in cls.get_handlers():\n\t\thandler.setFormatter(None)\n\n\t# Remove all previously configured loggers\n\tfor logger in cls.get_loggers():\n\t\tcls.remove_logger(logger)\n\n\t# Remove all previously configured loggers\n\tfor name in cls.get_loggers():\n\t\tcls.remove_logger(name)\n\n\t# Disable propagate\n\tcls.propagate = False\n\n\t# Remove all previously configured root handlers\n\tfor handler in cls.get_handlers(cls.root):\n\t\tcls.remove_handler(handler)\n\n\t# Remove all previously configured root handlers\n\tfor name in cls.get_handlers(cls.root):\n\t\tcls.remove_handler(name)\n\n\t# Remove all previously configured root formatters\n\tfor handler in cls.get_handlers(cls.root):\n\t\thandler.setFormatter(None)\n\n\t# Remove all previously configured root format", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tif cls.logger:\n\t\tcls.logger.handlers = []\n\n\t\tfor logger in logging.Logger.manager.loggerDict.values():\n\t\t\tif isinstance(logger, logging.Logger):\n\t\t\t\tlogger.handlers = []\n\n\t\t\t\tfor handler in logger.handlers:\n\t\t\t\t\tlogger.removeHandler(handler)\n\n\t\t\t\t\thandler.close()\n\n\t\tlogging.shutdown()\n\n\t\tcls.logger = None\n\n\tdef reset_logging():\n\t\t_reset_logging(cls)\n\n\t\tcls.logger = logging.getLogger(\"py-log-tracker\")\n\n\t\tcls.logger.setLevel(logging.INFO)\n\n\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t\thandler = logging.StreamHandler()\n\t\thandler.setLevel(logging.INFO)\n\t\thandler.setFormatter(formatter)\n\n\t\tcls.logger.addHandler(handler)\n\n\t\tcls.logger.info(\"Logging system reset\")\n\n\t\tcls.logger.info(\"py-log-tracker initialized\")\n\n\t\treset_logging = cls.reset", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.shutdown()\n\tlogging.basicConfig(level=logging.DEBUG,\n\t                    format='%(asctime)s %(levelname)-8s %(message)s',\n\t                    datefmt='%Y-%m-%d %H:%M:%S',\n\t                    filename='logs.txt',\n\t                    filemode='a')\n\tcls.logger = logging.getLogger('my_logger')\n\tcls.logger.setLevel(logging.DEBUG)\n\nclass Test(object):\n\n\t@classmethod\n\tdef setup_class(cls):\n\t\t\"\"\"\n\t\tRun once before all tests\n\t\t\"\"\"\n\t\t_reset_logging(cls)\n\n\t@classmethod\n\tdef teardown_class(cls):\n\t\t\"\"\"\n\t\tRun once after all tests\n\t\t\"\"\"\n\t\t_reset_logging(cls)\n\n\tdef setup_method(self):\n\t\t\"\"\"\n\t\tRun before each test\n\t\t\"\"\"\n\t\t_reset_logging(self)\n\n\tdef teardown_method(self):\n\t\t\"\"\"\n\t\tRun after each test\n\t\t\"\"\"\n\t\t_reset_logging(self)\n\n\tdef test_1(self):\n\t\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger.remove()\n\tcls.logger.add(cls.LOG_FILE, format=cls.LOGGING_FORMAT, level=cls.LOGGING_LEVEL)\n\tcls.logger.add(cls.LOG_STREAM, format=cls.LOGGING_FORMAT, level=cls.LOGGING_LEVEL)\n\n\tcls.logger.info('Logging system reset')\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogging.config.fileConfig(os.path.join(os.path.dirname(__file__), 'logging.ini'))\n\tcls.logger = logging.getLogger(__name__)\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging, logging.config\n\n\t# Remove all handlers associated with loggers\n\tfor logger in logging.Logger.manager.loggerDict.values():\n\t\tfor handler in logger.handlers[:]:\n\t\t\tlogger.removeHandler(handler)\n\n\t# Remove all previous loggers\n\tfor logger in logging.Logger.manager.loggerDict.copy().values():\n\t\tif not isinstance(logger, logging.PlaceHolder):\n\t\t\tlogging.Logger.manager.loggerDict.pop(logger.name, None)\n\n\t# Reconfigure logging\n\tcls._configure_logging()\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogging.shutdown()\n\tlogging.basicConfig(level=logging.WARNING)\n\tlogging.getLogger().setLevel(logging.WARNING)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\troot = logging.getLogger()\n\tif root.handlers:\n\t\tfor handler in root.handlers:\n\t\t\troot.removeHandler(handler)\n\tcls._logger = None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tfor handler in cls.logger.handlers:\n\t\tcls.logger.removeHandler(handler)\n\tcls.logger = None\n\nclass Logger:\n\t\"\"\"\n\tLogging class\n\t\"\"\"\n\tlogger = None\n\n\tdef __init__(self, log_file: str, debug: bool = True):\n\t\t\"\"\"\n\t\tInitialize the logger\n\t\t\"\"\"\n\t\tif self.logger is not None:\n\t\t\tself._reset_logging(self)\n\t\tself.logger = logging.getLogger()\n\t\tself.logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\tfh = logging.FileHandler(log_file)\n\t\tfh.setFormatter(formatter)\n\t\tsh = logging.StreamHandler()\n\t\tsh.setFormatter(formatter)\n\t\tself.logger.addHandler(fh)\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tfor handler in cls.logger.handlers:\n\t\tcls.logger.removeHandler(handler)\n\tcls.logger.handlers = []\n\tcls.logger.setLevel(logging.DEBUG)\n\t\ndef _setup_logging():\n\t\"\"\"\n\tSetup the logging system\n\t\"\"\"\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s - %(message)s')\n\thandler = logging.StreamHandler(sys.stderr)\n\thandler.setFormatter(formatter)\n\thandler.setLevel(logging.DEBUG)\n\t\n\t# add the handlers to the logger\n\tlogger.addHandler(handler)\n\tlogger.setLevel(logging.DEBUG)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t# Remove all previously configured handlers\n\tfor handler in cls.get_handlers():\n\t\tcls.remove_handler(handler)\n\n\t# Remove all filters\n\tfor handler in cls.get_handlers():\n\t\thandler.filters = []\n\n\t# Remove all previously configured formatters\n\tfor handler in cls.get_handlers():\n\t\thandler.setFormatter(None)\n\n\t# Remove all previously configured loggers\n\tfor logger in cls.get_loggers():\n\t\tcls.remove_logger(logger)\n\n\t# Remove all previously configured loggers\n\tfor name in cls.get_loggers():\n\t\tcls.remove_logger(name)\n\n\t# Disable propagate\n\tcls.propagate = False\n\n\t# Remove all previously configured root handlers\n\tfor handler in cls.get_handlers(cls.root):\n\t\tcls.remove_handler(handler)\n\n\t# Remove all previously configured root handlers\n\tfor name in cls.get_handlers(cls.root):\n\t\tcls.remove_handler(name)\n\n\t# Remove all previously configured root formatters\n\tfor handler in cls.get_handlers(cls.root):\n\t\thandler.setFormatter(None)\n\n\t# Remove all previously configured root format", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tif cls.logger:\n\t\tcls.logger.handlers = []\n\n\t\tfor logger in logging.Logger.manager.loggerDict.values():\n\t\t\tif isinstance(logger, logging.Logger):\n\t\t\t\tlogger.handlers = []\n\n\t\t\t\tfor handler in logger.handlers:\n\t\t\t\t\tlogger.removeHandler(handler)\n\n\t\t\t\t\thandler.close()\n\n\t\tlogging.shutdown()\n\n\t\tcls.logger = None\n\n\tdef reset_logging():\n\t\t_reset_logging(cls)\n\n\t\tcls.logger = logging.getLogger(\"py-log-tracker\")\n\n\t\tcls.logger.setLevel(logging.INFO)\n\n\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t\thandler = logging.StreamHandler()\n\t\thandler.setLevel(logging.INFO)\n\t\thandler.setFormatter(formatter)\n\n\t\tcls.logger.addHandler(handler)\n\n\t\tcls.logger.info(\"Logging system reset\")\n\n\t\tcls.logger.info(\"py-log-tracker initialized\")\n\n\t\treset_logging = cls.reset", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.shutdown()\n\tlogging.basicConfig(level=logging.DEBUG,\n\t                    format='%(asctime)s %(levelname)-8s %(message)s',\n\t                    datefmt='%Y-%m-%d %H:%M:%S',\n\t                    filename='logs.txt',\n\t                    filemode='a')\n\tcls.logger = logging.getLogger('my_logger')\n\tcls.logger.setLevel(logging.DEBUG)\n\nclass Test(object):\n\n\t@classmethod\n\tdef setup_class(cls):\n\t\t\"\"\"\n\t\tRun once before all tests\n\t\t\"\"\"\n\t\t_reset_logging(cls)\n\n\t@classmethod\n\tdef teardown_class(cls):\n\t\t\"\"\"\n\t\tRun once after all tests\n\t\t\"\"\"\n\t\t_reset_logging(cls)\n\n\tdef setup_method(self):\n\t\t\"\"\"\n\t\tRun before each test\n\t\t\"\"\"\n\t\t_reset_logging(self)\n\n\tdef teardown_method(self):\n\t\t\"\"\"\n\t\tRun after each test\n\t\t\"\"\"\n\t\t_reset_logging(self)\n\n\tdef test_1(self):\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger.remove()\n\tcls.logger.add(cls.LOG_FILE, format=cls.LOGGING_FORMAT, level=cls.LOGGING_LEVEL)\n\tcls.logger.add(cls.LOG_STREAM, format=cls.LOGGING_FORMAT, level=cls.LOGGING_LEVEL)\n\n\tcls.logger.info('Logging system reset')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogging.config.fileConfig(os.path.join(os.path.dirname(__file__), 'logging.ini'))\n\tcls.logger = logging.getLogger(__name__)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging, logging.config\n\n\t# Remove all handlers associated with loggers\n\tfor logger in logging.Logger.manager.loggerDict.values():\n\t\tfor handler in logger.handlers[:]:\n\t\t\tlogger.removeHandler(handler)\n\n\t# Remove all previous loggers\n\tfor logger in logging.Logger.manager.loggerDict.copy().values():\n\t\tif not isinstance(logger, logging.PlaceHolder):\n\t\t\tlogging.Logger.manager.loggerDict.pop(logger.name, None)\n\n\t# Reconfigure logging\n\tcls._configure_logging()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogging.shutdown()\n\tlogging.basicConfig(level=logging.WARNING)\n\tlogging.getLogger().setLevel(logging.WARNING)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'Fallback'\n\n# This is the same as in test_py_fallback.py\n\nclass Test_Fallback_Module(unittest.TestCase):\n\n\tdef test_version(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.PYQT_VERSION_STR)\n\n\tdef test_Qt_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.Qt)\n\n\tdef test_QtGui_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtGui)\n\n\tdef test_QtCore_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtCore)\n\n\tdef test_QtNetwork_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtNetwork)\n\n\tdef test_QtXml_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtXml)\n\n\tdef test_QtSql_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtSql)\n\n\tdef test_QtSv", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'C'\n\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.__class__.__name__[:-3]\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn _getTargetClass()\n\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.Py\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn Tkinter\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\"This method must be overridden in subclasses\")\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.__class__.__name__.split('Py')[0].split('Fallback')[0]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'Fallback'\n\n# This is the same as in test_py_fallback.py\n\nclass Test_Fallback_Module(unittest.TestCase):\n\n\tdef test_version(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.PYQT_VERSION_STR)\n\n\tdef test_Qt_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.Qt)\n\n\tdef test_QtGui_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtGui)\n\n\tdef test_QtCore_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtCore)\n\n\tdef test_QtNetwork_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtNetwork)\n\n\tdef test_QtXml_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtXml)\n\n\tdef test_QtSql_constants(self):\n\t\timport PyQt4\n\t\tself.assertTrue(PyQt4.QtSql)\n\n\tdef test_QtSv", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'C'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.__class__.__name__[:-3]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn _getTargetClass()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.Py\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn Tkinter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\"This method must be overridden in subclasses\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.__class__.__name__.split('Py')[0].split('Fallback')[0]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\ti = 0\n\twhile i < len(orderings):\n\t\tif i > 0 and orderings[i] == orderings[i - 1]:\n\t\t\tdel orderings[i]\n\t\telse:\n\t\t\ti += 1\n\treturn orderings\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in result:\n\t\t\t\tresult.append(element)\n\treturn result\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult += ordering\n\treturn list(set(result))\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tif isinstance(orderings, (list, tuple)):\n\t\treturn _legacy_mergeOrderings(orderings[0]) + _legacy_mergeOrderings(orderings[1:])\n\telse:\n\t\treturn [orderings]\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn list(set().union(*orderings))\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t# This is essentially a \"smart\" version of OrderedDict\n\tmerged = []\n\tfor ordering in orderings:\n\t\tmerged.extend(ordering)\n\treturn list(set(merged))\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tif not orderings:\n\t\traise ValueError('No orderings provided.')\n\n\tresult = []\n\tfor order in orderings:\n\t\tif order:\n\t\t\tresult.extend(set(order))\n\n\treturn result\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn sorted(set(chain(*orderings)))\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn list(set(sum(orderings, [])))\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn list(set(itertools.chain.from_iterable(orderings)))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\ti = 0\n\twhile i < len(orderings):\n\t\tif i > 0 and orderings[i] == orderings[i - 1]:\n\t\t\tdel orderings[i]\n\t\telse:\n\t\t\ti += 1\n\treturn orderings\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in result:\n\t\t\t\tresult.append(element)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult += ordering\n\treturn list(set(result))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tif isinstance(orderings, (list, tuple)):\n\t\treturn _legacy_mergeOrderings(orderings[0]) + _legacy_mergeOrderings(orderings[1:])\n\telse:\n\t\treturn [orderings]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn list(set().union(*orderings))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t# This is essentially a \"smart\" version of OrderedDict\n\tmerged = []\n\tfor ordering in orderings:\n\t\tmerged.extend(ordering)\n\treturn list(set(merged))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tif not orderings:\n\t\traise ValueError('No orderings provided.')\n\n\tresult = []\n\tfor order in orderings:\n\t\tif order:\n\t\t\tresult.extend(set(order))\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn sorted(set(chain(*orderings)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn list(set(sum(orderings, [])))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\treturn list(set(itertools.chain.from_iterable(orderings)))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IProvides.implementedBy(object):\n\t\treturn object.providedBy\n\treturn None\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfor interface in interfaces.providedBy(object):\n\t\tif IObjectDerivedFrom.providedBy(interface):\n\t\t\tif IObjectDerivedFrom.providedBy(object) and IObjectDerivedFrom.directlyProvidedBy(object).__contains__(interface):\n\t\t\t\treturn IObjectDerivedFrom.directlyProvidedBy(object)[interface]\n\t\t\telse:\n\t\t\t\treturn True\n\t\telse:\n\t\t\treturn interface\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IInterfaceDerivedFrom.__provides__(IInterfaceDerivedFrom, object):\n\t\treturn object.__provides__()\n\telse:\n\t\treturn None\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IObjectSpecification.providedBy(object):\n\t\treturn object.getObjectSpecification()\n\telse:\n\t\treturn ObjectSpecification(object)\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object.providedBy(None)\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn _get_adapter(object, _providedBy_interfaces, None)\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IObjectDerivedFromProvidedBy.providedBy(object):\n\t\treturn object.getDerivedFrom()\n\telse:\n\t\treturn object.getObject()\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object._providedBy.__provides__\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object.__provides__()\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IObjectSpecification.providedBy(object):\n\t\treturn object.provides\n\telse:\n\t\treturn directlyProvidedBy.bases(object.__class__)\n\n\ndirectlyProvidedBy.bases = classmethod(bases)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IProvides.implementedBy(object):\n\t\treturn object.providedBy\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfor interface in interfaces.providedBy(object):\n\t\tif IObjectDerivedFrom.providedBy(interface):\n\t\t\tif IObjectDerivedFrom.providedBy(object) and IObjectDerivedFrom.directlyProvidedBy(object).__contains__(interface):\n\t\t\t\treturn IObjectDerivedFrom.directlyProvidedBy(object)[interface]\n\t\t\telse:\n\t\t\t\treturn True\n\t\telse:\n\t\t\treturn interface\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IInterfaceDerivedFrom.__provides__(IInterfaceDerivedFrom, object):\n\t\treturn object.__provides__()\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IObjectSpecification.providedBy(object):\n\t\treturn object.getObjectSpecification()\n\telse:\n\t\treturn ObjectSpecification(object)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object.providedBy(None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn _get_adapter(object, _providedBy_interfaces, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IObjectDerivedFromProvidedBy.providedBy(object):\n\t\treturn object.getDerivedFrom()\n\telse:\n\t\treturn object.getObject()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object._providedBy.__provides__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object.__provides__()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif IObjectSpecification.providedBy(object):\n\t\treturn object.provides\n\telse:\n\t\treturn directlyProvidedBy.bases(object.__class__)\n\n\ndirectlyProvidedBy.bases = classmethod(bases)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not any(issubclass(c, b) for b in classes):\n\t\t\tbases.append(c)\n\treturn bases\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tfor s in classes:\n\t\t\tif s in c.subclasses and s != c:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tminimal.append(c)\n\treturn minimal\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\tminimalBases = []\n\tfor base in classes:\n\t\tif len(base.subclasses) == 0:\n\t\t\tminimalBases.append(base)\n\treturn minimalBases\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not hasSubclasses(c):\n\t\t\tbases.append(c)\n\treturn bases\n\t\ndef hasSubclasses(c):\n\t\"\"\"\n\tReturns True if the class has subclasses.\n\t\"\"\"\n\tfor s in c.subclasses:\n\t\tif not s in c.subclasses:\n\t\t\treturn True\n\treturn False\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimalBases = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tminimalBases.append(c)\n\treturn minimalBases\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimalBaseClasses = []\n\tfor c in classes:\n\t\tif not c.hasSubclasses():\n\t\t\tminimalBaseClasses.append(c)\n\treturn minimalBaseClasses\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tclasses = classes.copy()\n\tfor class_ in classes:\n\t\tfor subclass in classes[class_]['subclasses']:\n\t\t\tclasses[class_]['subclasses'].remove(subclass)\n\t\t\tclasses[subclass]['superclasses'].remove(class_)\n\treturn classes\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimalClasses = []\n\tfor cl in classes:\n\t\tif not cl.subclasses:\n\t\t\tminimalClasses.append(cl)\n\treturn minimalClasses\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tif c.get_name() not in bases:\n\t\t\t\tbases.append(c.get_name())\n\treturn bases\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tminimal.append(c)\n\treturn minimal\n"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not any(issubclass(c, b) for b in classes):\n\t\t\tbases.append(c)\n\treturn bases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tfor s in classes:\n\t\t\tif s in c.subclasses and s != c:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tminimal.append(c)\n\treturn minimal\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\tminimalBases = []\n\tfor base in classes:\n\t\tif len(base.subclasses) == 0:\n\t\t\tminimalBases.append(base)\n\treturn minimalBases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not hasSubclasses(c):\n\t\t\tbases.append(c)\n\treturn bases\n\t\ndef hasSubclasses(c):\n\t\"\"\"\n\tReturns True if the class has subclasses.\n\t\"\"\"\n\tfor s in c.subclasses:\n\t\tif not s in c.subclasses:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimalBases = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tminimalBases.append(c)\n\treturn minimalBases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimalBaseClasses = []\n\tfor c in classes:\n\t\tif not c.hasSubclasses():\n\t\t\tminimalBaseClasses.append(c)\n\treturn minimalBaseClasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tclasses = classes.copy()\n\tfor class_ in classes:\n\t\tfor subclass in classes[class_]['subclasses']:\n\t\t\tclasses[class_]['subclasses'].remove(subclass)\n\t\t\tclasses[subclass]['superclasses'].remove(class_)\n\treturn classes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimalClasses = []\n\tfor cl in classes:\n\t\tif not cl.subclasses:\n\t\t\tminimalClasses.append(cl)\n\treturn minimalClasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tbases = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tif c.get_name() not in bases:\n\t\t\t\tbases.append(c.get_name())\n\treturn bases\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tminimal.append(c)\n\treturn minimal\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\tresult = []\n\t\tfor attribute in self.__dict__:\n\t\t\tif not attribute.startswith('_'):\n\t\t\t\tresult.append((attribute, getattr(self, attribute).__doc__))\n\t\treturn result\n\telse:\n\t\tself.__dict__[self.current].__doc__\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t# TODO: Add description\n\tif all:\n\t\treturn list(self._attrs.keys())\n\telse:\n\t\treturn self._attrs.keys()\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn self.attr(all=True)\n\telse:\n\t\treturn self.attr(all=False)\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\n\tif all:\n\t\treturn self.namesAndDescriptions()\n\t\n\treturn [(self.name(), self.description())]\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn [(n,v.description) for n,v in list(self.__dict__.items()) if isinstance(v, Attribute)]\n\telse:\n\t\tfor n,v in list(self.__dict__.items()):\n\t\t\tif isinstance(v, Attribute):\n\t\t\t\treturn (n, v.description)\n\n\traise AttributeError('No attribute found')\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\treturn '{} - {}'.format(self.name, self.description)\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn [{'name': self.name, 'description': self.description}]\n\treturn {'name': self.name, 'description': self.description}\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\tnamesAndDescriptions = []\n\t\tfor name in self.attributeNames():\n\t\t\tnamesAndDescriptions.append((name, self.attributeDescription(name)))\n\t\treturn namesAndDescriptions\n\telse:\n\t\treturn self.name(), self.description()\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn [{'name': name, 'description': self.attrs[name].description} for name in list(self.attrs.keys())]\n\telse:\n\t\treturn {'name': self.name, 'description': self.description}\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\tkeys = list(self.keys)\n\telse:\n\t\tkeys = [self.current]\n\n\treturn [(k, self.keys[k]) for k in keys]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\tresult = []\n\t\tfor attribute in self.__dict__:\n\t\t\tif not attribute.startswith('_'):\n\t\t\t\tresult.append((attribute, getattr(self, attribute).__doc__))\n\t\treturn result\n\telse:\n\t\tself.__dict__[self.current].__doc__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t# TODO: Add description\n\tif all:\n\t\treturn list(self._attrs.keys())\n\telse:\n\t\treturn self._attrs.keys()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn self.attr(all=True)\n\telse:\n\t\treturn self.attr(all=False)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\n\tif all:\n\t\treturn self.namesAndDescriptions()\n\t\n\treturn [(self.name(), self.description())]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn [(n,v.description) for n,v in list(self.__dict__.items()) if isinstance(v, Attribute)]\n\telse:\n\t\tfor n,v in list(self.__dict__.items()):\n\t\t\tif isinstance(v, Attribute):\n\t\t\t\treturn (n, v.description)\n\n\traise AttributeError('No attribute found')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\treturn '{} - {}'.format(self.name, self.description)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn [{'name': self.name, 'description': self.description}]\n\treturn {'name': self.name, 'description': self.description}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\tnamesAndDescriptions = []\n\t\tfor name in self.attributeNames():\n\t\t\tnamesAndDescriptions.append((name, self.attributeDescription(name)))\n\t\treturn namesAndDescriptions\n\telse:\n\t\treturn self.name(), self.description()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\treturn [{'name': name, 'description': self.attrs[name].description} for name in list(self.attrs.keys())]\n\telse:\n\t\treturn {'name': self.name, 'description': self.description}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tif all:\n\t\tkeys = list(self.keys)\n\telse:\n\t\tkeys = [self.current]\n\n\treturn [(k, self.keys[k]) for k in keys]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn [\n\t\t\t'name', 'id', 'status', 'created', 'updated', 'deleted',\n\t\t\t'owner', 'private_key', 'public_key', 'private_key_id',\n\t\t\t'public_key_id', 'key_fingerprint', 'key_material', 'data',\n\t\t\t'metadata', 'fingerprint', 'size', 'url', 'title', 'domain',\n\t\t\t'format', 'mime_type', 'content_type', 'content_encoding',\n\t\t\t'content_disposition', 'filename', 'language', 'location',\n\t\t\t'latitude', 'longitude', 'altitude', 'accuracy', 'author',\n\t\t\t'rights', 'description', 'license', 'tags', 'extended', 'raw',\n\t\t\t'image', 'thumbnail', 'icon', 'small_icon', 'medium_icon',\n\t\t\t'large_icon', 'full', 'tiny', 'small', 'medium', 'large',\n\t\t\t'original', 'poster', 'backdrop', 'poster_original',\n\t\t\t'backdrop_original', 'poster_thumbnail', 'backdrop_thumbnail',\n\t\t\t'poster_small', '", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.__class__.__table__.columns.keys()\n\telse:\n\t\treturn [f.key for f in self.__table__.columns if not f.foreign_keys]\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.names_with_base\n\telse:\n\t\treturn self.names\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn [x for x in self.__dict__.keys() if not x.startswith(\"_\")]\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.all_attr_names\n\telse:\n\t\treturn self.attr_names\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn list(self.__dict__.keys())\n\telse:\n\t\treturn list(self._fields)\n\n\n@property\ndef fields(self):\n\t\"\"\"\n\tReturn the fields of current class.\n\t\"\"\"\n\treturn self._fields\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\treturn self._names(self.attrs, all)\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\tif all:\n\t\treturn [self.names[i] for i in range(len(self.names))]\n\telse:\n\t\treturn self.names[self.current:]\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = [n for n in dir(self) if not n.startswith(\"__\") and not n.startswith(\"_\") and not n.startswith(\"_\")]\n\tif not all:\n\t\tnames.remove(\"names\")\n\treturn names\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn [name for name in dir(self) if not name.startswith('_')]\n\telse:\n\t\treturn [name for name in dir(self) if not name.startswith('_') and not name.startswith('__')]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn [\n\t\t\t'name', 'id', 'status', 'created', 'updated', 'deleted',\n\t\t\t'owner', 'private_key', 'public_key', 'private_key_id',\n\t\t\t'public_key_id', 'key_fingerprint', 'key_material', 'data',\n\t\t\t'metadata', 'fingerprint', 'size', 'url', 'title', 'domain',\n\t\t\t'format', 'mime_type', 'content_type', 'content_encoding',\n\t\t\t'content_disposition', 'filename', 'language', 'location',\n\t\t\t'latitude', 'longitude', 'altitude', 'accuracy', 'author',\n\t\t\t'rights', 'description', 'license', 'tags', 'extended', 'raw',\n\t\t\t'image', 'thumbnail', 'icon', 'small_icon', 'medium_icon',\n\t\t\t'large_icon', 'full', 'tiny', 'small', 'medium', 'large',\n\t\t\t'original', 'poster', 'backdrop', 'poster_original',\n\t\t\t'backdrop_original', 'poster_thumbnail', 'backdrop_thumbnail',\n\t\t\t'poster_small', '", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.__class__.__table__.columns.keys()\n\telse:\n\t\treturn [f.key for f in self.__table__.columns if not f.foreign_keys]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.names_with_base\n\telse:\n\t\treturn self.names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn [x for x in self.__dict__.keys() if not x.startswith(\"_\")]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn self.all_attr_names\n\telse:\n\t\treturn self.attr_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn list(self.__dict__.keys())\n\telse:\n\t\treturn list(self._fields)\n\n\n@property\ndef fields(self):\n\t\"\"\"\n\tReturn the fields of current class.\n\t\"\"\"\n\treturn self._fields\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\treturn self._names(self.attrs, all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\tif all:\n\t\treturn [self.names[i] for i in range(len(self.names))]\n\telse:\n\t\treturn self.names[self.current:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = [n for n in dir(self) if not n.startswith(\"__\") and not n.startswith(\"_\") and not n.startswith(\"_\")]\n\tif not all:\n\t\tnames.remove(\"names\")\n\treturn names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tif all:\n\t\treturn [name for name in dir(self) if not name.startswith('_')]\n\telse:\n\t\treturn [name for name in dir(self) if not name.startswith('_') and not name.startswith('__')]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tret = []\n\tfor i in sequence:\n\t\tif isinstance(i, tuple):\n\t\t\tret.append(i)\n\t\telif isinstance(i, Declaration):\n\t\t\tret.append(i)\n\t\telse:\n\t\t\tret.append((i,))\n\tsequence = ret\n\n\tif output is None:\n\t\toutput = []\n\tif isinstance(output, Declaration):\n\t\toutput = [output]\n\tif not isinstance(output, list):\n\t\traise TypeError(\"output must be a list\")\n\n\tfor i in sequence:\n\t\tif isinstance(i, tuple):\n\t\t\tif len(i) == 3:\n\t\t\t\tif not isinstance(i[0], str):\n\t\t\t\t\traise TypeError(\"first argument of a tuple must be a string\")\n\t\t\t\tif not isinstance(i[1], Interface):\n\t\t\t\t\traise TypeError(\"second argument of a tuple must be an Interface\")\n\t\t\t\tif not isinstance(i[2], Declaration):\n\t\t\t\t\traise TypeError(\"third argument of a tuple must be a Declaration\")\n\t\t\t\toutput.append((i[0], i[1], i[2]))\n\t\t\telse:\n\t\t\t", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not isinstance(sequence, (list, tuple)):\n\t\tsequence = (sequence,)\n\n\tif output is None:\n\t\toutput = []\n\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, (tuple, list)):\n\t\t\t_normalizeargs(item, output)\n\t\telse:\n\t\t\toutput.append(item)\n\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Declaration):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, tuple):\n\t\treturn list(sequence)\n\n\tif isinstance(sequence, collections.Sequence):\n\t\treturn list(sequence)\n\n\treturn [sequence]\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence.args\n\tif not output:\n\t\toutput = []\n\tfor object in sequence:\n\t\tif isinstance(object, Declaration):\n\t\t\tfor argument in object.args:\n\t\t\t\toutput.append(argument)\n\t\telif isinstance(object, (tuple, list)):\n\t\t\tfor item in object:\n\t\t\t\toutput.append(item)\n\t\telif isinstance(object, Interface):\n\t\t\toutput.append(object)\n\treturn output\n\n\nclass Declaration(object):\n\t\"\"\"\n\tDeclaration of an interface.\n\n\tA declaration contains one or more interfaces, each with a name and a\n\tset of arguments.\n\t\"\"\"\n\n\tdef __init__(self, name, *args, **kwargs):\n\t\t\"\"\"\n\t\tCreate a Declaration.\n\n\t\t:param name: Name of the declaration\n\t\t:type name: str\n\t\t:param args: List of interfaces and arguments\n\t\t:type args: list\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.args = _normalizeargs(args)\n\t\tself.kwargs = kwargs\n\n\tdef __str__(self):\n\t\treturn self.name\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, (tuple, list)):\n\t\treturn tuple(map(_normalizeargs, (sequence,)))\n\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence\n\n\tif isinstance(sequence, tuple):\n\t\treturn Declaration(*sequence)\n\n\tif isinstance(sequence, str):\n\t\tif output is None:\n\t\t\traise TypeError(\"Can't normalize string without output\")\n\t\treturn interface(output, sequence)\n\n\tif isinstance(sequence, Interface):\n\t\treturn sequence\n\n\traise TypeError(f\"Argument must be a Declaration, Interface, \"\n\t\t\t\t\tf\"or string, not {type(sequence).__qualname__}\")\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not sequence:\n\t\treturn []\n\n\tif isinstance(sequence, Declaration):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, tuple):\n\t\treturn list(sequence)\n\n\tif isinstance(sequence, Implements):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, Interface):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, type(None)):\n\t\treturn []\n\n\tif isinstance(sequence, str):\n\t\tif ',' in sequence:\n\t\t\treturn [Interface(i.strip()) for i in sequence.split(',')]\n\t\telse:\n\t\t\treturn [Interface(sequence)]\n\n\tif isinstance(sequence, list):\n\t\treturn sequence\n\n\traise TypeError('Unhandled sequence type %s' % type(sequence))\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, ABCDeclaration):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, tuple):\n\t\tfor item in sequence:\n\t\t\t_normalizeargs(item, output)\n\telse:\n\t\toutput.append(sequence)\n\treturn output\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Declaration):\n\t\treturn [sequence]\n\tif isinstance(sequence, tuple):\n\t\treturn list(sequence)\n\tif isinstance(sequence, list):\n\t\tif len(sequence)==0:\n\t\t\traise ValueError('Empty declaration list')\n\t\tif len(sequence)==1:\n\t\t\telem = sequence[0]\n\t\t\tif isinstance(elem, Declaration):\n\t\t\t\treturn [elem]\n\t\t\tif isinstance(elem, tuple):\n\t\t\t\treturn list(elem)\n\t\t\tif isinstance(elem, list):\n\t\t\t\tif len(elem)==0:\n\t\t\t\t\traise ValueError('Empty declaration list')\n\t\t\t\tif len(elem)==1:\n\t\t\t\t\telem = elem[0]\n\t\t\t\t\tif isinstance(elem, Declaration):\n\t\t\t\t\t\treturn [elem]\n\t\t\t\t\tif isinstance(elem, tuple):\n\t\t\t\t\t\treturn list(elem)\n\t\t\t\treturn list(elem)\n\t\treturn list(sequence)\n\tif isinstance(sequence, Declaration.Abstract):\n\t\treturn [sequence]\n\tif isinstance(sequence, Declaration.Implements):", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = []\n\tfor i, arg in enumerate(sequence):\n\t\tif isinstance(arg, Declaration):\n\t\t\toutput.append(arg)\n\t\telif isinstance(arg, tuple):\n\t\t\toutput.append(arg)\n\t\telse:\n\t\t\toutput.append((arg,))\n\treturn tuple(output)\n\n\nclass Declaration(object):\n\t\"\"\"\n\tDeclaration base class\n\t\"\"\"\n\tdef __init__(self, name, *args, **kwargs):\n\t\tself.name = name\n\t\tself.args = _normalizeargs(args)\n\t\tself.kwargs = kwargs\n\n\tdef __repr__(self):\n\t\treturn '<Declaration %s>' % (self.name,)\n\n\tdef __str__(self):\n\t\treturn 'Declaration %s' % (self.name,)\n\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, Declaration):\n\t\t\treturn False\n\t\tif self.name != other.name:\n\t\t\treturn False\n\t\tif self.args != other.args:\n\t\t\treturn False\n\t\treturn True\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\n\tif isinstance(sequence, str):\n\t\tsequence = [sequence]\n\n\tfor i in sequence:\n\t\tif isinstance(i, tuple):\n\t\t\tfor j in i:\n\t\t\t\t_normalizeargs(j, output)\n\t\telif isinstance(i, str):\n\t\t\toutput.append(i)\n\t\telse:\n\t\t\tfor j in i.declarations():\n\t\t\t\t_normalizeargs(j, output)\n\treturn output\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tret = []\n\tfor i in sequence:\n\t\tif isinstance(i, tuple):\n\t\t\tret.append(i)\n\t\telif isinstance(i, Declaration):\n\t\t\tret.append(i)\n\t\telse:\n\t\t\tret.append((i,))\n\tsequence = ret\n\n\tif output is None:\n\t\toutput = []\n\tif isinstance(output, Declaration):\n\t\toutput = [output]\n\tif not isinstance(output, list):\n\t\traise TypeError(\"output must be a list\")\n\n\tfor i in sequence:\n\t\tif isinstance(i, tuple):\n\t\t\tif len(i) == 3:\n\t\t\t\tif not isinstance(i[0], str):\n\t\t\t\t\traise TypeError(\"first argument of a tuple must be a string\")\n\t\t\t\tif not isinstance(i[1], Interface):\n\t\t\t\t\traise TypeError(\"second argument of a tuple must be an Interface\")\n\t\t\t\tif not isinstance(i[2], Declaration):\n\t\t\t\t\traise TypeError(\"third argument of a tuple must be a Declaration\")\n\t\t\t\toutput.append((i[0], i[1], i[2]))\n\t\t\telse:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not isinstance(sequence, (list, tuple)):\n\t\tsequence = (sequence,)\n\n\tif output is None:\n\t\toutput = []\n\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, (tuple, list)):\n\t\t\t_normalizeargs(item, output)\n\t\telse:\n\t\t\toutput.append(item)\n\n\treturn output\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Declaration):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, tuple):\n\t\treturn list(sequence)\n\n\tif isinstance(sequence, collections.Sequence):\n\t\treturn list(sequence)\n\n\treturn [sequence]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence.args\n\tif not output:\n\t\toutput = []\n\tfor object in sequence:\n\t\tif isinstance(object, Declaration):\n\t\t\tfor argument in object.args:\n\t\t\t\toutput.append(argument)\n\t\telif isinstance(object, (tuple, list)):\n\t\t\tfor item in object:\n\t\t\t\toutput.append(item)\n\t\telif isinstance(object, Interface):\n\t\t\toutput.append(object)\n\treturn output\n\n\nclass Declaration(object):\n\t\"\"\"\n\tDeclaration of an interface.\n\n\tA declaration contains one or more interfaces, each with a name and a\n\tset of arguments.\n\t\"\"\"\n\n\tdef __init__(self, name, *args, **kwargs):\n\t\t\"\"\"\n\t\tCreate a Declaration.\n\n\t\t:param name: Name of the declaration\n\t\t:type name: str\n\t\t:param args: List of interfaces and arguments\n\t\t:type args: list\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.args = _normalizeargs(args)\n\t\tself.kwargs = kwargs\n\n\tdef __str__(self):\n\t\treturn self.name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, (tuple, list)):\n\t\treturn tuple(map(_normalizeargs, (sequence,)))\n\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence\n\n\tif isinstance(sequence, tuple):\n\t\treturn Declaration(*sequence)\n\n\tif isinstance(sequence, str):\n\t\tif output is None:\n\t\t\traise TypeError(\"Can't normalize string without output\")\n\t\treturn interface(output, sequence)\n\n\tif isinstance(sequence, Interface):\n\t\treturn sequence\n\n\traise TypeError(f\"Argument must be a Declaration, Interface, \"\n\t\t\t\t\tf\"or string, not {type(sequence).__qualname__}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not sequence:\n\t\treturn []\n\n\tif isinstance(sequence, Declaration):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, tuple):\n\t\treturn list(sequence)\n\n\tif isinstance(sequence, Implements):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, Interface):\n\t\treturn [sequence]\n\n\tif isinstance(sequence, type(None)):\n\t\treturn []\n\n\tif isinstance(sequence, str):\n\t\tif ',' in sequence:\n\t\t\treturn [Interface(i.strip()) for i in sequence.split(',')]\n\t\telse:\n\t\t\treturn [Interface(sequence)]\n\n\tif isinstance(sequence, list):\n\t\treturn sequence\n\n\traise TypeError('Unhandled sequence type %s' % type(sequence))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, ABCDeclaration):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, tuple):\n\t\tfor item in sequence:\n\t\t\t_normalizeargs(item, output)\n\telse:\n\t\toutput.append(sequence)\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Declaration):\n\t\treturn [sequence]\n\tif isinstance(sequence, tuple):\n\t\treturn list(sequence)\n\tif isinstance(sequence, list):\n\t\tif len(sequence)==0:\n\t\t\traise ValueError('Empty declaration list')\n\t\tif len(sequence)==1:\n\t\t\telem = sequence[0]\n\t\t\tif isinstance(elem, Declaration):\n\t\t\t\treturn [elem]\n\t\t\tif isinstance(elem, tuple):\n\t\t\t\treturn list(elem)\n\t\t\tif isinstance(elem, list):\n\t\t\t\tif len(elem)==0:\n\t\t\t\t\traise ValueError('Empty declaration list')\n\t\t\t\tif len(elem)==1:\n\t\t\t\t\telem = elem[0]\n\t\t\t\t\tif isinstance(elem, Declaration):\n\t\t\t\t\t\treturn [elem]\n\t\t\t\t\tif isinstance(elem, tuple):\n\t\t\t\t\t\treturn list(elem)\n\t\t\t\treturn list(elem)\n\t\treturn list(sequence)\n\tif isinstance(sequence, Declaration.Abstract):\n\t\treturn [sequence]\n\tif isinstance(sequence, Declaration.Implements):", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = []\n\tfor i, arg in enumerate(sequence):\n\t\tif isinstance(arg, Declaration):\n\t\t\toutput.append(arg)\n\t\telif isinstance(arg, tuple):\n\t\t\toutput.append(arg)\n\t\telse:\n\t\t\toutput.append((arg,))\n\treturn tuple(output)\n\n\nclass Declaration(object):\n\t\"\"\"\n\tDeclaration base class\n\t\"\"\"\n\tdef __init__(self, name, *args, **kwargs):\n\t\tself.name = name\n\t\tself.args = _normalizeargs(args)\n\t\tself.kwargs = kwargs\n\n\tdef __repr__(self):\n\t\treturn '<Declaration %s>' % (self.name,)\n\n\tdef __str__(self):\n\t\treturn 'Declaration %s' % (self.name,)\n\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, Declaration):\n\t\t\treturn False\n\t\tif self.name != other.name:\n\t\t\treturn False\n\t\tif self.args != other.args:\n\t\t\treturn False\n\t\treturn True\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\n\tif isinstance(sequence, str):\n\t\tsequence = [sequence]\n\n\tfor i in sequence:\n\t\tif isinstance(i, tuple):\n\t\t\tfor j in i:\n\t\t\t\t_normalizeargs(j, output)\n\t\telif isinstance(i, str):\n\t\t\toutput.append(i)\n\t\telse:\n\t\t\tfor j in i.declarations():\n\t\t\t\t_normalizeargs(j, output)\n\treturn output\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\ntry:\n\t_c_optimizations = _c_optimizations_available()\nexcept ImportError:\n\t_c_optimizations = False\n\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .optimizations import c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n\nc_optimizations = _c_optimizations_available()\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\t\t\ndef _c_optimizations_available_and_loaded():\n\t\"\"\"\n\tIf available and loaded, return the C optimization module.\n\t\"\"\"\n\ttry:\n\t\tfrom . import _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\t\t\ndef _c_optimizations_available_but_not_loaded():\n\t\"\"\"\n\tIf available but not loaded, return True\n\t\"\"\"\n\ttry:\n\t\tfrom . import _c_optimizations\n\texcept ImportError:\n\t\treturn True\n\t\t\ndef _use_c_optimizations(func, args):\n\t\"\"\"\n\tDetermine if the C optimization modules should be used.\n\t\"\"\"\n\tif _c_optimizations_available_and_loaded():\n\t\treturn True\n\telif _c_optimizations_available_but_not_loaded():\n\t\tfrom . import _c_optimizations\n\t\t_c_optimizations.load_c_optimizations()\n\t\treturn True\n\telse:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .optimizations import c_optimizations\n\t\treturn c_optimizations\n\texcept:\n\t\treturn False\n\n\nclass Optimizations:\n\t\"\"\"\n\tClass to hold the optimizations for the classifier.\n\t\"\"\"\n\n\tdef __init__(self, optimizations):\n\t\t\"\"\"\n\t\tInitialize the optimizations.\n\n\t\t:param optimizations: A list of optimizations to use.\n\t\t\"\"\"\n\t\tif _c_optimizations_available():\n\t\t\tself.optimizations = c_optimizations.get(optimizations)\n\t\telse:\n\t\t\tself.optimizations = optimizations\n\n\tdef _optimize_data_and_labels(self, data, labels, data_copy_mode):\n\t\t\"\"\"\n\t\tOptimize the data and labels.\n\n\t\t:param data: The data to optimize.\n\t\t:param labels: The labels to optimize.\n\t\t:param data_copy_mode: The mode to use when copying data.\n\t\t:return: The optimized data and labels.\n\t\t\"\"\"\n\t\tif self.optimizations is not None:\n\t\t\treturn self.optimizations.optimize_data_and_labels(data, labels, data_copy_mode)\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .c_optimizations import C_optimizations\n\t\treturn C_optimizations\n\texcept ImportError:\n\t\treturn False\n\n\nC_optimizations = _c_optimizations_available()\n\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport C_optimizations\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn C_optimizations\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\treturn __import__('optimizations', {}, {}, ['optimizations'])\n\texcept:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .optimizations import _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\n\nc_optimizations = _c_optimizations_available()\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\ntry:\n\t_c_optimizations = _c_optimizations_available()\nexcept ImportError:\n\t_c_optimizations = False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .optimizations import c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n\nc_optimizations = _c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom . import _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\t\t\ndef _c_optimizations_available_and_loaded():\n\t\"\"\"\n\tIf available and loaded, return the C optimization module.\n\t\"\"\"\n\ttry:\n\t\tfrom . import _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\t\t\ndef _c_optimizations_available_but_not_loaded():\n\t\"\"\"\n\tIf available but not loaded, return True\n\t\"\"\"\n\ttry:\n\t\tfrom . import _c_optimizations\n\texcept ImportError:\n\t\treturn True\n\t\t\ndef _use_c_optimizations(func, args):\n\t\"\"\"\n\tDetermine if the C optimization modules should be used.\n\t\"\"\"\n\tif _c_optimizations_available_and_loaded():\n\t\treturn True\n\telif _c_optimizations_available_but_not_loaded():\n\t\tfrom . import _c_optimizations\n\t\t_c_optimizations.load_c_optimizations()\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .optimizations import c_optimizations\n\t\treturn c_optimizations\n\texcept:\n\t\treturn False\n\n\nclass Optimizations:\n\t\"\"\"\n\tClass to hold the optimizations for the classifier.\n\t\"\"\"\n\n\tdef __init__(self, optimizations):\n\t\t\"\"\"\n\t\tInitialize the optimizations.\n\n\t\t:param optimizations: A list of optimizations to use.\n\t\t\"\"\"\n\t\tif _c_optimizations_available():\n\t\t\tself.optimizations = c_optimizations.get(optimizations)\n\t\telse:\n\t\t\tself.optimizations = optimizations\n\n\tdef _optimize_data_and_labels(self, data, labels, data_copy_mode):\n\t\t\"\"\"\n\t\tOptimize the data and labels.\n\n\t\t:param data: The data to optimize.\n\t\t:param labels: The labels to optimize.\n\t\t:param data_copy_mode: The mode to use when copying data.\n\t\t:return: The optimized data and labels.\n\t\t\"\"\"\n\t\tif self.optimizations is not None:\n\t\t\treturn self.optimizations.optimize_data_and_labels(data, labels, data_copy_mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .c_optimizations import C_optimizations\n\t\treturn C_optimizations\n\texcept ImportError:\n\t\treturn False\n\n\nC_optimizations = _c_optimizations_available()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport C_optimizations\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn C_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\treturn __import__('optimizations', {}, {}, ['optimizations'])\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom .optimizations import _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\n\nc_optimizations = _c_optimizations_available()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn False\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\tif not PYNINI_AVAILABLE:\n\t\treturn False\n\tif \"PYNINI_USE_C\" not in os.environ:\n\t\treturn True\n\tif os.environ[\"PYNINI_USE_C\"].lower() == \"true\":\n\t\treturn True\n\treturn False\n\n\nif _should_attempt_c_optimizations():\n\tfrom . import c_optimizations\n\n\ttry:\n\t\tfrom . import c_optimizations\n\texcept ImportError:\n\t\tpass\n\n\n# 2. Define the main functions.\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\tglobal _attempt_c_optimizations, _has_c_optimizations\n\tif _attempt_c_optimizations is None:\n\t\t_attempt_c_optimizations = True\n\t\ttry:\n\t\t\timport c_optimizations\n\t\t\t_has_c_optimizations = True\n\t\t\t_attempt_c_optimizations = False\n\t\texcept ImportError:\n\t\t\t_has_c_optimizations = False\n\treturn _attempt_c_optimizations\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\t# If we've failed to import the C module, we don't use the C optimizations.\n\t\timport pydevd_cython_wrapper\n\texcept ImportError:\n\t\treturn False\n\t\n\treturn pydevd_cython_wrapper.USE_CYTHON\n\n\nif _should_attempt_c_optimizations():\n\tfrom _pydevd_bundle.pydevd_cython_wrapper import trace_dispatch, trace_dispatch_exception, add_command, \\\n\t\tget_next_command, process_internal_command, replace_sys_settrace_func, \\\n\t\treplace_sys_settrace_func_callable, get_current_thread_id, get_current_thread_id_as_str, \\\n\t\tget_current_thread_id_as_int, get_current_thread_id_as_negative, get_thread_id, \\\n\t\tget_thread_id_as_int, get_thread_id_as_negative, get_thread_id_as_str, \\\n\t\tget_thread_local_data, set_thread_local_data, set_additional_thread_info, \\\n\t\tget_additional_thread_info, get_thread_local_data_", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn False\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t# This is necessary because when we compile the C extension, we use\n\t# the CPython version number, so that we can compile against the\n\t# Python version that was used to build CPython. If we don't do this,\n\t# then we run into problems when trying to load the C extension after\n\t# the CPython version has been updated.\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\tif not PYNINI_AVAILABLE:\n\t\treturn False\n\tif \"PYNINI_USE_C\" not in os.environ:\n\t\treturn True\n\tif os.environ[\"PYNINI_USE_C\"].lower() == \"true\":\n\t\treturn True\n\treturn False\n\n\nif _should_attempt_c_optimizations():\n\tfrom . import c_optimizations\n\n\ttry:\n\t\tfrom . import c_optimizations\n\texcept ImportError:\n\t\tpass\n\n\n# 2. Define the main functions.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\tglobal _attempt_c_optimizations, _has_c_optimizations\n\tif _attempt_c_optimizations is None:\n\t\t_attempt_c_optimizations = True\n\t\ttry:\n\t\t\timport c_optimizations\n\t\t\t_has_c_optimizations = True\n\t\t\t_attempt_c_optimizations = False\n\t\texcept ImportError:\n\t\t\t_has_c_optimizations = False\n\treturn _attempt_c_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\t# If we've failed to import the C module, we don't use the C optimizations.\n\t\timport pydevd_cython_wrapper\n\texcept ImportError:\n\t\treturn False\n\t\n\treturn pydevd_cython_wrapper.USE_CYTHON\n\n\nif _should_attempt_c_optimizations():\n\tfrom _pydevd_bundle.pydevd_cython_wrapper import trace_dispatch, trace_dispatch_exception, add_command, \\\n\t\tget_next_command, process_internal_command, replace_sys_settrace_func, \\\n\t\treplace_sys_settrace_func_callable, get_current_thread_id, get_current_thread_id_as_str, \\\n\t\tget_current_thread_id_as_int, get_current_thread_id_as_negative, get_thread_id, \\\n\t\tget_thread_id_as_int, get_thread_id_as_negative, get_thread_id_as_str, \\\n\t\tget_thread_local_data, set_thread_local_data, set_additional_thread_info, \\\n\t\tget_additional_thread_info, get_thread_local_data_", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t# This is necessary because when we compile the C extension, we use\n\t# the CPython version number, so that we can compile against the\n\t# Python version that was used to build CPython. If we don't do this,\n\t# then we run into problems when trying to load the C extension after\n\t# the CPython version has been updated.\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn (os.environ.get(\"PURE_PYTHON\") != None and os.environ.get(\"PURE_PYTHON\") != \"0\")\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.environ.get(\"PURE_PYTHON\", \"0\") != \"0\"\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\ttry:\n\t\treturn int(os.environ['PURE_PYTHON']) == 0\n\texcept:\n\t\treturn False\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn \"PURE_PYTHON\" in os.environ and os.environ[\"PURE_PYTHON\"].lower() not in (\"0\", \"false\", \"no\", \"f\", \"n\")\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.getenv(\"PURE_PYTHON\") != None and os.getenv(\"PURE_PYTHON\") != \"0\"\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn (os.environ.get(\"PURE_PYTHON\") != None and os.environ.get(\"PURE_PYTHON\") != \"0\")\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.getenv('PURE_PYTHON') and (os.getenv('PURE_PYTHON') != '0') and (os.getenv('PURE_PYTHON') != 'false')\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON', 0) != 0\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn (os.environ.get(\"PURE_PYTHON\") != None) and (os.environ.get(\"PURE_PYTHON\") != \"0\")\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.environ.get(\"PURE_PYTHON\", None) not in (None, \"0\")\n\n\nclass CCompiler(object):\n\t\"\"\"\n\tCCompiler class\n\t\"\"\"\n\n\tclass CC:\n\t\t\"\"\"\n\t\tClass CC\n\t\t\"\"\"\n\n\t\tdef __init__(self, type):\n\t\t\t\"\"\"\n\t\t\tConstructor\n\t\t\t\"\"\"\n\t\t\tself.type = type\n\n\t\tdef __eq__(self, other):\n\t\t\treturn self.type == other.type\n\n\t\tdef __hash__(self):\n\t\t\treturn hash(self.type)\n\n\t\tdef __str__(self):\n\t\t\treturn self.type\n\n\tclass CCFlag:\n\t\t\"\"\"\n\t\tClass CCFlag\n\t\t\"\"\"\n\n\t\tdef __init__(self, flag):\n\t\t\t\"\"\"\n\t\t\tConstructor\n\t\t\t\"\"\"\n\t\t\tself.flag = flag\n\n\t\tdef __eq__(self, other):\n\t\t\treturn self.flag == other.flag\n\n\t\tdef __hash__(self):\n\t\t\treturn hash(self.flag)\n\n\t\tdef __str__(self):\n\t\t\treturn self.flag\n\n\tclass CCO"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn (os.environ.get(\"PURE_PYTHON\") != None and os.environ.get(\"PURE_PYTHON\") != \"0\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.environ.get(\"PURE_PYTHON\", \"0\") != \"0\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\ttry:\n\t\treturn int(os.environ['PURE_PYTHON']) == 0\n\texcept:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn \"PURE_PYTHON\" in os.environ and os.environ[\"PURE_PYTHON\"].lower() not in (\"0\", \"false\", \"no\", \"f\", \"n\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.getenv(\"PURE_PYTHON\") != None and os.getenv(\"PURE_PYTHON\") != \"0\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn (os.environ.get(\"PURE_PYTHON\") != None and os.environ.get(\"PURE_PYTHON\") != \"0\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.getenv('PURE_PYTHON') and (os.getenv('PURE_PYTHON') != '0') and (os.getenv('PURE_PYTHON') != 'false')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON', 0) != 0\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn (os.environ.get(\"PURE_PYTHON\") != None) and (os.environ.get(\"PURE_PYTHON\") != \"0\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\treturn os.environ.get(\"PURE_PYTHON\", None) not in (None, \"0\")\n\n\nclass CCompiler(object):\n\t\"\"\"\n\tCCompiler class\n\t\"\"\"\n\n\tclass CC:\n\t\t\"\"\"\n\t\tClass CC\n\t\t\"\"\"\n\n\t\tdef __init__(self, type):\n\t\t\t\"\"\"\n\t\t\tConstructor\n\t\t\t\"\"\"\n\t\t\tself.type = type\n\n\t\tdef __eq__(self, other):\n\t\t\treturn self.type == other.type\n\n\t\tdef __hash__(self):\n\t\t\treturn hash(self.type)\n\n\t\tdef __str__(self):\n\t\t\treturn self.type\n\n\tclass CCFlag:\n\t\t\"\"\"\n\t\tClass CCFlag\n\t\t\"\"\"\n\n\t\tdef __init__(self, flag):\n\t\t\t\"\"\"\n\t\t\tConstructor\n\t\t\t\"\"\"\n\t\t\tself.flag = flag\n\n\t\tdef __eq__(self, other):\n\t\t\treturn self.flag == other.flag\n\n\t\tdef __hash__(self):\n\t\t\treturn hash(self.flag)\n\n\t\tdef __str__(self):\n\t\t\treturn self.flag\n\n\tclass CCO", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn int(os.environ.get('C_OPTIMIZATIONS', '0'))\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn False\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\tfrom distutils.util import get_platform\n\tfrom distutils.version import LooseVersion\n\timport sys\n\tfrom sysconfig import get_config_var\n\tfrom . import _minimal_ext_modules\n\n\tif sys.platform == 'darwin':\n\t\t# Make sure that we're on Yosemite or higher.\n\t\t# This is needed because of https://github.com/mathiasbynens/CSS.crashcourse/issues/3\n\t\tversion = os.uname()[2].split('.')\n\t\tif len(version) >= 2 and LooseVersion(version[0] + '.' + version[1]) < LooseVersion('14.0'):\n\t\t\treturn False\n\n\tif sys.platform == 'win32':\n\t\t# Make sure that we're on Windows 7 or higher.\n\t\t# This is needed because of https://github.com/mathiasbynens/CSS.crashcourse/issues/4\n\t\tversion = get_platform().split('-')\n\t\tif len(version) >= 2 and LooseVersion(version[1]) < LooseVersion('6.1'):\n\t\t\treturn False\n\n\t# Make sure that we're not on a 32-bit platform.\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\tfrom distutils.sysconfig import get_config_var\n\t\t_optimization_flags = get_config_var('OPT')\n\t\tif _optimization_flags:\n\t\t\tif '-O' in _optimization_flags:\n\t\t\t\treturn True\n\t\t\telif '-O0' in _optimization_flags:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\t# No optimization flags set, we can't tell\n\t\t\treturn True\n\texcept:\n\t\t# Failed to get the optimization flags, we can't tell\n\t\treturn True\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t# The C optimizations are needed if the CPython version is\n\t# less than 2.7.12.\n\tif sys.version_info[0] < 2 or \\\n\t   sys.version_info[0:2] == (2, 7) and sys.version_info[2] < 12:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn False\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn int(os.environ.get('C_OPTIMIZATIONS', '0'))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\tfrom distutils.util import get_platform\n\tfrom distutils.version import LooseVersion\n\timport sys\n\tfrom sysconfig import get_config_var\n\tfrom . import _minimal_ext_modules\n\n\tif sys.platform == 'darwin':\n\t\t# Make sure that we're on Yosemite or higher.\n\t\t# This is needed because of https://github.com/mathiasbynens/CSS.crashcourse/issues/3\n\t\tversion = os.uname()[2].split('.')\n\t\tif len(version) >= 2 and LooseVersion(version[0] + '.' + version[1]) < LooseVersion('14.0'):\n\t\t\treturn False\n\n\tif sys.platform == 'win32':\n\t\t# Make sure that we're on Windows 7 or higher.\n\t\t# This is needed because of https://github.com/mathiasbynens/CSS.crashcourse/issues/4\n\t\tversion = get_platform().split('-')\n\t\tif len(version) >= 2 and LooseVersion(version[1]) < LooseVersion('6.1'):\n\t\t\treturn False\n\n\t# Make sure that we're not on a 32-bit platform.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\tfrom distutils.sysconfig import get_config_var\n\t\t_optimization_flags = get_config_var('OPT')\n\t\tif _optimization_flags:\n\t\t\tif '-O' in _optimization_flags:\n\t\t\t\treturn True\n\t\t\telif '-O0' in _optimization_flags:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\t# No optimization flags set, we can't tell\n\t\t\treturn True\n\texcept:\n\t\t# Failed to get the optimization flags, we can't tell\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t# The C optimizations are needed if the CPython version is\n\t# less than 2.7.12.\n\tif sys.version_info[0] < 2 or \\\n\t   sys.version_info[0:2] == (2, 7) and sys.version_info[2] < 12:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tif self.bins is None:\n\t\tself.bins = self.make_bins()\n\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.__dict__ = {}\n\tself.bins = self.make_bins()\n\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n\tself.bins_cache = {}\n\tself.bins_cache_size = 0\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself._context = {}\n\tself.bins = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.make_bins()\n\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.__dict__ = {}\n\tself.bins = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tself.bins.clear()\n\tself.bins = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = dict()\n\tself.bins = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tif self.use_bins:\n\t\tself.bins = self.make_bins()\n\telse:\n\t\tself.bins = self.initial_value\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tif self.bins is None:\n\t\tself.bins = self.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.__dict__ = {}\n\tself.bins = self.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n\tself.bins_cache = {}\n\tself.bins_cache_size = 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself._context = {}\n\tself.bins = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.__dict__ = {}\n\tself.bins = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tself.bins.clear()\n\tself.bins = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = dict()\n\tself.bins = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tif self.use_bins:\n\t\tself.bins = self.make_bins()\n\telse:\n\t\tself.bins = self.initial_value\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\n\t\t\"to_csv is deprecated since Lena 0.5.2. \"\n\t\t\"Iterables are converted to tables.\",\n\t\tDeprec", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\"to_csv() is deprecated. Use ToCSV instead.\",\n\t\t\t\t  DeprecationWarning)\n\tif header is not None:\n\t\tyield header\n\tfor point in self:\n\t\tyield separator.join([str(value) for value in point])\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"to_csv is not used, iterables are converted to tables.\",\n\t\tDeprecationWarning)\n\treturn self.", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tmsg = \"to_csv is not used. Iterables are converted to tables.\"\n\traise DeprecationWarning(msg)\n\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\n\t\t\"to_csv is deprecated since Lena 0.5. \"\n\t\t\"To convert iterables to tables, use lena.output.ToCSV.\",\n\t\tDeprecationWarning,\n\t\tstacklevel=2\n\t)\n\tfrom lena.structures import Table\n\tfrom lena.core import Sequence\n\n\tif header is not None:\n\t\tyield str(header)\n\tfor point in self:\n\t\tyield Table.from_iterable(point, separator).__str__()\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\"to_csv is deprecated in Lena 0.5.\", DeprecationWarning)\n\n\tif header:\n\t\tyield header\n\n\tfor point in self:\n\t\tyield separator.join(str(value) for value in point)\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"to_csv is deprecated in Lena 0.5. \"\n\t\t\"Use lena.output.ToCSV.\",\n\t\tDeprecationWarning,\n\t\tstacklevel=2,\n\t)\n\n\tif header is not None:\n\t\tyield header\n\n\tfor point in self:\n\t\tyield separator.join(map(str, point))\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn self\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.core import Sequence\n\tfrom lena.structures import Table\n\tfrom lena.structures.table import Table\n\tfrom lena.structures.point import Point\n\n\tdef _to_csv(sequence):\n\t\tif not isinstance(sequence, Sequence):\n\t\t\traise TypeError(\"'sequence' is not a sequence\")\n\n\t\tif not isinstance(sequence, Table):\n\t\t\treturn sequence\n\n\t\tif header is not None:\n\t\t\tyield header\n\t\t\tyield \"\"\n\n\t\tfor row in sequence:\n\t\t\tif not isinstance(row, (Tuple, list, Sequence)):\n\t\t\t\traise TypeError(\"'sequence' contains not a sequence\")\n\n\t\t\tif not isinstance(row[0], Point):\n\t\t\t\traise TypeError(\"'sequence' contains not a point\")\n\n\t\t\tfor column in row:\n\t\t\t\tif not isinstance(column, (int, float, complex)):\n\t\t\t\t\traise TypeError(\"'sequence' contains not a number\")\n\n\t\t\tvalues = [str(column) for column in row]\n\t\t\tyield separator.join(values)\n\n\t\t\tyield \"\"\n\n\treturn _to_csv\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"to_csv() is deprecated. For multidimensional data, \"\n\t\t\"use to_table() from lena.output.CSV.\",\n\t\tDeprecationWarning\n\t)\n\treturn CSV(separator=separator, header=header)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\n\t\t\"to_csv is deprecated since Lena 0.5.2. \"\n\t\t\"Iterables are converted to tables.\",\n\t\tDeprec", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\"to_csv() is deprecated. Use ToCSV instead.\",\n\t\t\t\t  DeprecationWarning)\n\tif header is not None:\n\t\tyield header\n\tfor point in self:\n\t\tyield separator.join([str(value) for value in point])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"to_csv is not used, iterables are converted to tables.\",\n\t\tDeprecationWarning)\n\treturn self.", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tmsg = \"to_csv is not used. Iterables are converted to tables.\"\n\traise DeprecationWarning(msg)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\n\t\t\"to_csv is deprecated since Lena 0.5. \"\n\t\t\"To convert iterables to tables, use lena.output.ToCSV.\",\n\t\tDeprecationWarning,\n\t\tstacklevel=2\n\t)\n\tfrom lena.structures import Table\n\tfrom lena.core import Sequence\n\n\tif header is not None:\n\t\tyield str(header)\n\tfor point in self:\n\t\tyield Table.from_iterable(point, separator).__str__()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\"to_csv is deprecated in Lena 0.5.\", DeprecationWarning)\n\n\tif header:\n\t\tyield header\n\n\tfor point in self:\n\t\tyield separator.join(str(value) for value in point)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"to_csv is deprecated in Lena 0.5. \"\n\t\t\"Use lena.output.ToCSV.\",\n\t\tDeprecationWarning,\n\t\tstacklevel=2,\n\t)\n\n\tif header is not None:\n\t\tyield header\n\n\tfor point in self:\n\t\tyield separator.join(map(str, point))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.core import Sequence\n\tfrom lena.structures import Table\n\tfrom lena.structures.table import Table\n\tfrom lena.structures.point import Point\n\n\tdef _to_csv(sequence):\n\t\tif not isinstance(sequence, Sequence):\n\t\t\traise TypeError(\"'sequence' is not a sequence\")\n\n\t\tif not isinstance(sequence, Table):\n\t\t\treturn sequence\n\n\t\tif header is not None:\n\t\t\tyield header\n\t\t\tyield \"\"\n\n\t\tfor row in sequence:\n\t\t\tif not isinstance(row, (Tuple, list, Sequence)):\n\t\t\t\traise TypeError(\"'sequence' contains not a sequence\")\n\n\t\t\tif not isinstance(row[0], Point):\n\t\t\t\traise TypeError(\"'sequence' contains not a point\")\n\n\t\t\tfor column in row:\n\t\t\t\tif not isinstance(column, (int, float, complex)):\n\t\t\t\t\traise TypeError(\"'sequence' contains not a number\")\n\n\t\t\tvalues = [str(column) for column in row]\n\t\t\tyield separator.join(values)\n\n\t\t\tyield \"\"\n\n\treturn _to_csv\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"to_csv() is deprecated. For multidimensional data, \"\n\t\t\"use to_table() from lena.output.CSV.\",\n\t\tDeprecationWarning\n\t)\n\treturn CSV(separator=separator, header=header)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_indices = []\n\tfor i in range(len(self.coords)):\n\t\tif self.coords[i].name == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tif coord_name is None:\n\t\treturn None\n\telif coord_name in self.coord_names:\n\t\treturn list(self.coord_indices[self.coord_names.index(coord_name)])\n\telse:\n\t\treturn None\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor i, coord in enumerate(self.coords):\n\t\tif coord == coord_name:\n\t\t\tindices.append(i)\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tind = []\n\tfor i,j in enumerate(self.coords):\n\t\tif j[0] == coord_name:\n\t\t\tind.append(i)\n\treturn ind\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_indices = []\n\tfor i, coord in enumerate(self.coords):\n\t\tif coord == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tif coord_name not in self.coords:\n\t\traise ValueError(f\"Coord {coord_name} not in {self.coords}\")\n\n\tif self.err_coords is None:\n\t\treturn np.zeros(0, dtype=int)\n\n\terr_inds = []\n\tfor err_name in self.err_coords:\n\t\tif coord_name in err_name:\n\t\t\terr_inds.append(self.err_coords[err_name])\n\n\tif len(err_inds) == 1:\n\t\treturn err_inds[0]\n\telif len(err_inds) == 0:\n\t\treturn np.zeros(0, dtype=int)\n\telse:\n\t\traise ValueError(f\"Multiple error coords found for coord {coord_name}, \"\n\t\t\t\t\t\tf\"are you sure this is what you wanted?\")\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t# get error indices\n\terr_inds = np.where(~np.isfinite(self.errors[coord_name]))[0]\n\n\t# get error values\n\terr_vals = self.errors[coord_name][err_inds]\n\n\t# get error values\n\terr_vals_abs = np.abs(err_vals)\n\n\t# get error values\n\terr_vals_rel = err_vals_abs / np.abs(self.errors[coord_name])\n\n\t# get error values\n\terr_inds_rel = np.where(err_vals_rel > 0.01)[0]\n\n\t# get error values\n\terr_inds_abs = np.where(err_vals_abs > 1)[0]\n\n\t# get error values\n\terr_inds_rel_abs = np.intersect1d(err_inds_rel, err_inds_abs)\n\n\t# get error values\n\terr_inds_rel_abs_ind = np.where(err_inds_rel_abs == err_inds)[0]\n\n\t# get error values\n\terr_inds_rel_abs_ind_rel = err_inds_rel_abs[err_inds_rel_abs_ind]\n\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_inds = np.where(np.isnan(self.x[coord_name]))[0]\n\treturn err_inds\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\treturn list(range(len(self.coords[coord_name])))\n\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_indices = []\n\tfor i, (k, v) in enumerate(self.coords.items()):\n\t\tif k == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_indices = []\n\tfor i in range(len(self.coords)):\n\t\tif self.coords[i].name == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tif coord_name is None:\n\t\treturn None\n\telif coord_name in self.coord_names:\n\t\treturn list(self.coord_indices[self.coord_names.index(coord_name)])\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor i, coord in enumerate(self.coords):\n\t\tif coord == coord_name:\n\t\t\tindices.append(i)\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tind = []\n\tfor i,j in enumerate(self.coords):\n\t\tif j[0] == coord_name:\n\t\t\tind.append(i)\n\treturn ind\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_indices = []\n\tfor i, coord in enumerate(self.coords):\n\t\tif coord == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tif coord_name not in self.coords:\n\t\traise ValueError(f\"Coord {coord_name} not in {self.coords}\")\n\n\tif self.err_coords is None:\n\t\treturn np.zeros(0, dtype=int)\n\n\terr_inds = []\n\tfor err_name in self.err_coords:\n\t\tif coord_name in err_name:\n\t\t\terr_inds.append(self.err_coords[err_name])\n\n\tif len(err_inds) == 1:\n\t\treturn err_inds[0]\n\telif len(err_inds) == 0:\n\t\treturn np.zeros(0, dtype=int)\n\telse:\n\t\traise ValueError(f\"Multiple error coords found for coord {coord_name}, \"\n\t\t\t\t\t\tf\"are you sure this is what you wanted?\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t# get error indices\n\terr_inds = np.where(~np.isfinite(self.errors[coord_name]))[0]\n\n\t# get error values\n\terr_vals = self.errors[coord_name][err_inds]\n\n\t# get error values\n\terr_vals_abs = np.abs(err_vals)\n\n\t# get error values\n\terr_vals_rel = err_vals_abs / np.abs(self.errors[coord_name])\n\n\t# get error values\n\terr_inds_rel = np.where(err_vals_rel > 0.01)[0]\n\n\t# get error values\n\terr_inds_abs = np.where(err_vals_abs > 1)[0]\n\n\t# get error values\n\terr_inds_rel_abs = np.intersect1d(err_inds_rel, err_inds_abs)\n\n\t# get error values\n\terr_inds_rel_abs_ind = np.where(err_inds_rel_abs == err_inds)[0]\n\n\t# get error values\n\terr_inds_rel_abs_ind_rel = err_inds_rel_abs[err_inds_rel_abs_ind]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_inds = np.where(np.isnan(self.x[coord_name]))[0]\n\treturn err_inds\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\treturn list(range(len(self.coords[coord_name])))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terr_indices = []\n\tfor i, (k, v) in enumerate(self.coords.items()):\n\t\tif k == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tfor i in range(len(self.errors)):\n\t\te = self.errors[i]\n\t\tcontext.error[e.name] = {\"index\": i}\n\n\t\t# if we are plotting a function, it is enough\n\t\t# to have one error\n\t\tif e.name == \"error_E_low\" \\\n\t\t\tand self.name == \"function\" and len(self.errors) == 1:\n\t\t\tbreak\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif not self.error_names:\n\t\treturn\n\tif not context:\n\t\treturn\n\tif not context.error:\n\t\tcontext.error = {}\n\tfor p in self.error_names:\n\t\tif p not in context.value:\n\t\t\tcontinue\n\t\tif p not in context.error:\n\t\t\tcontext.error[p] = {}\n\t\tcontext.error[p][\"index\"] = self.error_names.index(p)\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.error_E_high is not None:\n\t\tcontext.error[\"x_high\"] = {\"index\": self.index}\n\tif self.error_E_low is not None:\n\t\tcontext.error[\"x_low\"] = {\"index\": self.index}\n\tif self.error_t_high is not None:\n\t\tcontext.error[\"y_high\"] = {\"index\": self.index}\n\tif self.error_t_low is not None:\n\t\tcontext.error[\"y_low\"] = {\"index\": self.index}\n\tif self.error_value_high is not None:\n\t\tcontext.error[\"z_high\"] = {\"index\": self.index}\n\tif self.error_value_low is not None:\n\t\tcontext.error[\"z_low\"] = {\"index\": self.index}\n\tif self.error_x_high is not None:\n\t\tcontext.error[\"x_high\"] = {\"index\": self.index}\n\tif self.error_x_low is not None:\n\t\tcontext.error[\"x_low\"] = {\"index\": self.index}\n\tif self.error_y_high is not None:\n\t\tcontext.error[\"y_high\"] = {\"index\": self.index}\n\tif self.", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself._update_context_geometry(context)\n\tself._update_context_fields(context)\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.error:\n\t\tcontext.error = context.error.copy()\n\t\tfor name in self.error:\n\t\t\tcontext.error[name] = {}\n\t\t\tfor error in self.error[name]:\n\t\t\t\tcontext.error[name][error] = dict(index=self.error[name][error])\n\n\tif self.fields:\n\t\tcontext.value = context.value.copy()\n\t\tfor field in self.fields:\n\t\t\tcontext.value[field] = self[field]\n\n\tif self.fields_error:\n\t\tcontext.value = context.value.copy()\n\t\tfor field_error in self.fields_error:\n\t\t\tcontext.value[field_error] = self[field_error]\n\n\t\tcontext.error = context.error.copy()\n\t\tfor field in self.fields:\n\t\t\tcontext.error[field] = {}\n\t\t\tfor error in self.fields_error[field]:\n\t\t\t\tcontext.error[field][error] = dict(index=self.fields_error[field][error])\n\n\t\t\t\t# TODO: add error to the value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t_update_context_generic(self, context)\n\n\tif self.error is not None:\n\t\tfor i, error in enumerate(self.error):\n\t\t\tcontext.error[error.name] = {\n\t\t\t\t\"index\": i\n\t\t\t}\n\t\t\tcontext.error[error.name].update(error.context)\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.has_errors:\n\t\tcontext.error = context.error or {}\n\t\tfor i, err in enumerate(self.errors):\n\t\t\tcontext.error[err] = {\"index\": i}\n\tif self.has_values:\n\t\tcontext.value = context.value or {}\n\t\tfor i, val in enumerate(self.values):\n\t\t\tif isinstance(val, basestring):\n\t\t\t\tval = {\"name\": val}\n\t\t\tcontext.value[val[\"name\"]] = {\"index\": i}\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext.error = context.error or {}\n\tcontext.error_names = context.error_names or {}\n\tif self.error is not None:\n\t\tfor name in self.error:\n\t\t\tif name in context.error_names:\n\t\t\t\ti = context.error_names[name]\n\t\t\telse:\n\t\t\t\ti = len(context.error_names)\n\t\t\t\tcontext.error_names[name] = i\n\t\t\tif name not in context.error:\n\t\t\t\tcontext.error[name] = {}\n\t\t\tcontext.error[name][self.name] = i\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif len(context.error) == 0:\n\t\treturn\n\n\tcontext.error = [context.error[0]]\n\tfor i in range(1, len(context.error)):\n\t\tk = context.error[i].keys()[0]\n\t\tif k not in context.error[0]:\n\t\t\tcontext.error[0][k] = {}\n\t\tcontext.error[0][k] = {**context.error[0][k], **context.error[i]}\n\n\t# Update a value of an error, if it exists.\n\tdef update_error(err, err_name):\n\t\tif err_name not in context.value:\n\t\t\treturn\n\t\tif not isinstance(context.value[err_name], dict):\n\t\t\treturn\n\t\tif not isinstance(context.value[err_name][err], dict):\n\t\t\treturn\n\t\tcontext.value[err_name][err] = {**context.value[err_name][err], **context.error[i]}\n\n\tfor err in context.error[0]:\n\t\tupdate_error(err, \"error_x\")\n\t\tupdate_error(err, \"error_y\")\n\t\tupdate_error(err, \"error_z\")\n\n\t", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif \"error\" not in context:\n\t\tcontext[\"error\"] = {}\n\tfor i, value in enumerate(self._values):\n\t\tif isinstance(value, Field):\n\t\t\tif value.name not in context[\"value\"]:\n\t\t\t\tcontext[\"value\"][value.name] = []\n\t\t\tcontext[\"value\"][value.name].append(value.value)\n\t\t\tif value.error is not None:\n\t\t\t\terror_name = value.name.split(\"_\")[-1]\n\t\t\t\tif error_name not in context[\"error\"]:\n\t\t\t\t\tcontext[\"error\"][error_name] = {}\n\t\t\t\tcontext[\"error\"][error_name][\"index\"] = i\n\n\tfor value in self._values:\n\t\tif isinstance(value, Field):\n\t\t\tif value.error is not None:\n\t\t\t\terror_name = value.name.split(\"_\")[-1]\n\t\t\t\tif error_name in context[\"error\"]:\n\t\t\t\t\tcontext[\"error\"][error_name][\"error\"] = value.error\n\n\tfor i, value in enumerate(self._values):\n\t\tif isinstance(value, Field):\n\t\t\tif value.error is not"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tfor i in range(len(self.errors)):\n\t\te = self.errors[i]\n\t\tcontext.error[e.name] = {\"index\": i}\n\n\t\t# if we are plotting a function, it is enough\n\t\t# to have one error\n\t\tif e.name == \"error_E_low\" \\\n\t\t\tand self.name == \"function\" and len(self.errors) == 1:\n\t\t\tbreak\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif not self.error_names:\n\t\treturn\n\tif not context:\n\t\treturn\n\tif not context.error:\n\t\tcontext.error = {}\n\tfor p in self.error_names:\n\t\tif p not in context.value:\n\t\t\tcontinue\n\t\tif p not in context.error:\n\t\t\tcontext.error[p] = {}\n\t\tcontext.error[p][\"index\"] = self.error_names.index(p)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.error_E_high is not None:\n\t\tcontext.error[\"x_high\"] = {\"index\": self.index}\n\tif self.error_E_low is not None:\n\t\tcontext.error[\"x_low\"] = {\"index\": self.index}\n\tif self.error_t_high is not None:\n\t\tcontext.error[\"y_high\"] = {\"index\": self.index}\n\tif self.error_t_low is not None:\n\t\tcontext.error[\"y_low\"] = {\"index\": self.index}\n\tif self.error_value_high is not None:\n\t\tcontext.error[\"z_high\"] = {\"index\": self.index}\n\tif self.error_value_low is not None:\n\t\tcontext.error[\"z_low\"] = {\"index\": self.index}\n\tif self.error_x_high is not None:\n\t\tcontext.error[\"x_high\"] = {\"index\": self.index}\n\tif self.error_x_low is not None:\n\t\tcontext.error[\"x_low\"] = {\"index\": self.index}\n\tif self.error_y_high is not None:\n\t\tcontext.error[\"y_high\"] = {\"index\": self.index}\n\tif self.", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself._update_context_geometry(context)\n\tself._update_context_fields(context)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.error:\n\t\tcontext.error = context.error.copy()\n\t\tfor name in self.error:\n\t\t\tcontext.error[name] = {}\n\t\t\tfor error in self.error[name]:\n\t\t\t\tcontext.error[name][error] = dict(index=self.error[name][error])\n\n\tif self.fields:\n\t\tcontext.value = context.value.copy()\n\t\tfor field in self.fields:\n\t\t\tcontext.value[field] = self[field]\n\n\tif self.fields_error:\n\t\tcontext.value = context.value.copy()\n\t\tfor field_error in self.fields_error:\n\t\t\tcontext.value[field_error] = self[field_error]\n\n\t\tcontext.error = context.error.copy()\n\t\tfor field in self.fields:\n\t\t\tcontext.error[field] = {}\n\t\t\tfor error in self.fields_error[field]:\n\t\t\t\tcontext.error[field][error] = dict(index=self.fields_error[field][error])\n\n\t\t\t\t# TODO: add error to the value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t_update_context_generic(self, context)\n\n\tif self.error is not None:\n\t\tfor i, error in enumerate(self.error):\n\t\t\tcontext.error[error.name] = {\n\t\t\t\t\"index\": i\n\t\t\t}\n\t\t\tcontext.error[error.name].update(error.context)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.has_errors:\n\t\tcontext.error = context.error or {}\n\t\tfor i, err in enumerate(self.errors):\n\t\t\tcontext.error[err] = {\"index\": i}\n\tif self.has_values:\n\t\tcontext.value = context.value or {}\n\t\tfor i, val in enumerate(self.values):\n\t\t\tif isinstance(val, basestring):\n\t\t\t\tval = {\"name\": val}\n\t\t\tcontext.value[val[\"name\"]] = {\"index\": i}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext.error = context.error or {}\n\tcontext.error_names = context.error_names or {}\n\tif self.error is not None:\n\t\tfor name in self.error:\n\t\t\tif name in context.error_names:\n\t\t\t\ti = context.error_names[name]\n\t\t\telse:\n\t\t\t\ti = len(context.error_names)\n\t\t\t\tcontext.error_names[name] = i\n\t\t\tif name not in context.error:\n\t\t\t\tcontext.error[name] = {}\n\t\t\tcontext.error[name][self.name] = i\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif len(context.error) == 0:\n\t\treturn\n\n\tcontext.error = [context.error[0]]\n\tfor i in range(1, len(context.error)):\n\t\tk = context.error[i].keys()[0]\n\t\tif k not in context.error[0]:\n\t\t\tcontext.error[0][k] = {}\n\t\tcontext.error[0][k] = {**context.error[0][k], **context.error[i]}\n\n\t# Update a value of an error, if it exists.\n\tdef update_error(err, err_name):\n\t\tif err_name not in context.value:\n\t\t\treturn\n\t\tif not isinstance(context.value[err_name], dict):\n\t\t\treturn\n\t\tif not isinstance(context.value[err_name][err], dict):\n\t\t\treturn\n\t\tcontext.value[err_name][err] = {**context.value[err_name][err], **context.error[i]}\n\n\tfor err in context.error[0]:\n\t\tupdate_error(err, \"error_x\")\n\t\tupdate_error(err, \"error_y\")\n\t\tupdate_error(err, \"error_z\")\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif \"error\" not in context:\n\t\tcontext[\"error\"] = {}\n\tfor i, value in enumerate(self._values):\n\t\tif isinstance(value, Field):\n\t\t\tif value.name not in context[\"value\"]:\n\t\t\t\tcontext[\"value\"][value.name] = []\n\t\t\tcontext[\"value\"][value.name].append(value.value)\n\t\t\tif value.error is not None:\n\t\t\t\terror_name = value.name.split(\"_\")[-1]\n\t\t\t\tif error_name not in context[\"error\"]:\n\t\t\t\t\tcontext[\"error\"][error_name] = {}\n\t\t\t\tcontext[\"error\"][error_name][\"index\"] = i\n\n\tfor value in self._values:\n\t\tif isinstance(value, Field):\n\t\t\tif value.error is not None:\n\t\t\t\terror_name = value.name.split(\"_\")[-1]\n\t\t\t\tif error_name in context[\"error\"]:\n\t\t\t\t\tcontext[\"error\"][error_name][\"error\"] = value.error\n\n\tfor i, value in enumerate(self._values):\n\t\tif isinstance(value, Field):\n\t\t\tif value.error is not", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.trapz(bins, edges)\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.sum(np.diff(edges) * bins)\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.trapz(bins, edges)\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0.0\n\n\tfor bin, edge in zip(bins, edges):\n\t\tarea += bin * (edge[1] - edge[0])\n\n\treturn area\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0.0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.sum(bins*np.diff(edges))\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.trapz(bins, edges)\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.trapz(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.sum(np.diff(edges) * bins)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.trapz(bins, edges)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0.0\n\n\tfor bin, edge in zip(bins, edges):\n\t\tarea += bin * (edge[1] - edge[0])\n\n\treturn area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0.0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.sum(bins*np.diff(edges))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\treturn np.trapz(bins, edges)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif len(seq) != 3:\n\t\treturn False\n\ttry:\n\t\tfill_request_seq = FillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif not isinstance(seq, list):\n\t\treturn False, None\n\n\tif len(seq) < 2:\n\t\treturn False, None\n\n\tif not isinstance(seq[0], int):\n\t\treturn False, None\n\tif not isinstance(seq[1], bool):\n\t\treturn False, None\n\n\treturn True, seq[0], seq[1]\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif not isinstance(seq, tuple):\n\t\treturn False\n\tif len(seq) != 3:\n\t\treturn False\n\tif not is_fill_request_type(seq[0]):\n\t\treturn False\n\tif not is_id(seq[1]):\n\t\treturn False\n\tif not is_id(seq[2]):\n\t\treturn False\n\treturn True\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif len(seq) != 3:\n\t\treturn False\n\tif not isinstance(seq[0], list):\n\t\treturn False\n\tif not isinstance(seq[1], bool):\n\t\treturn False\n\tif not isinstance(seq[2], int):\n\t\treturn False\n\treturn True\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\treturn is_fill_request_seq_or_not(seq, True)\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq.from_seq(seq)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept Exception:\n\t\treturn False\n\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif len(seq) != 3:\n\t\treturn False\n\ttry:\n\t\tfill_request_seq = FillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif not isinstance(seq, list):\n\t\treturn False, None\n\n\tif len(seq) < 2:\n\t\treturn False, None\n\n\tif not isinstance(seq[0], int):\n\t\treturn False, None\n\tif not isinstance(seq[1], bool):\n\t\treturn False, None\n\n\treturn True, seq[0], seq[1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif not isinstance(seq, tuple):\n\t\treturn False\n\tif len(seq) != 3:\n\t\treturn False\n\tif not is_fill_request_type(seq[0]):\n\t\treturn False\n\tif not is_id(seq[1]):\n\t\treturn False\n\tif not is_id(seq[2]):\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tif len(seq) != 3:\n\t\treturn False\n\tif not isinstance(seq[0], list):\n\t\treturn False\n\tif not isinstance(seq[1], bool):\n\t\treturn False\n\tif not isinstance(seq[2], int):\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\treturn is_fill_request_seq_or_not(seq, True)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq.from_seq(seq)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept Exception:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\nclass BaseFillRequest:\n\t\"\"\"\n\tThe base class for all fill request classes.\n\t\"\"\"\n\tdef __init__(self, fill, request):\n\t\tif not is_fill_request_el(self):\n\t\t\traise TypeError('The subclass of BaseFillRequest must have the fill and request attributes.')\n\t\tself.fill = fill\n\t\tself.request = request\n\n\nclass FillRequest(BaseFillRequest):\n\t\"\"\"\n\tThe fill request class.\n\t\"\"\"\n\tdef __init__(self, fill, request):\n\t\tsuper().__init__(fill, request)\n\n\tdef __str__(self):\n\t\treturn f'FillRequest {self.fill} {self.request}'\n\n\tdef as_dict(self):\n\t\t\"\"\"\n\t\tConvert the fill request to a dictionary.\n\t\t\"\"\"\n\t\treturn {'fill': self.fill, 'request': self.request}\n\n\nclass FillRequests:\n\t\"\"\"\n\tThe collection of fill requests.\n\t\"\"\"\n\tdef __init__(self, fill_requests):\n\t\tself.fill_requests = []\n\t\tfor fill_request in fill_requests:\n\t\t\t", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\t\tobj.request()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\nclass BaseFillRequest:\n\t\"\"\"\n\tThe base class for all fill request classes.\n\t\"\"\"\n\tdef __init__(self, fill, request):\n\t\tif not is_fill_request_el(self):\n\t\t\traise TypeError('The subclass of BaseFillRequest must have the fill and request attributes.')\n\t\tself.fill = fill\n\t\tself.request = request\n\n\nclass FillRequest(BaseFillRequest):\n\t\"\"\"\n\tThe fill request class.\n\t\"\"\"\n\tdef __init__(self, fill, request):\n\t\tsuper().__init__(fill, request)\n\n\tdef __str__(self):\n\t\treturn f'FillRequest {self.fill} {self.request}'\n\n\tdef as_dict(self):\n\t\t\"\"\"\n\t\tConvert the fill request to a dictionary.\n\t\t\"\"\"\n\t\treturn {'fill': self.fill, 'request': self.request}\n\n\nclass FillRequests:\n\t\"\"\"\n\tThe collection of fill requests.\n\t\"\"\"\n\tdef __init__(self, fill_requests):\n\t\tself.fill_requests = []\n\t\tfor fill_request in fill_requests:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\t\tobj.request()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, \"run\")\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, \"run\")\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, \"run\")\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, \"run\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, \"run\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, \"run\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tif hasattr(obj, 'fill'):\n\t\treturn True\n\treturn False\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tif not hasattr(obj, 'fill_el'):\n\t\treturn False\n\tif not hasattr(obj, 'compute_el'):\n\t\treturn False\n\treturn True\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tif hasattr(obj, 'fill'):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tif not hasattr(obj, 'fill_el'):\n\t\treturn False\n\tif not hasattr(obj, 'compute_el'):\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn {}\n\telse:\n\t\tres = {}\n\t\tfor key in d1.keys():\n\t\t\tif key not in d2.keys():\n\t\t\t\tres[key] = d1[key]\n\t\t\telif type(d1[key]) == dict and type(d2[key]) == dict:\n\t\t\t\tres[key] = difference(d", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn d1\n\telse:\n\t\td3 = {}\n\t\tfor k, v in d1.items():\n\t\t\tif k in d2:\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\t\td3[k] = difference(v, d2[k], level=level-1)\n\t\t\t\t\telse:\n\t\t\t\t\t\td3[k] = d2[k]\n\t\t\t\telse:\n\t\t\t\t\tif not isinstance(d2[k], list):\n\t\t\t\t\t\td3[k] = d2[k]\n\t\t\telse:\n\t\t\t\td3[k] = v\n\treturn d3\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tif level == 0:\n\t\treturn d3\n\tif level == -1:\n\t\tlevel = len(d1)\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\td3[k] = d1[k]\n\t\telif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n\t\t\td3[k] = difference(d1[k], d2[k], level-1)\n\treturn d3\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn d1\n\telse:\n\t\treturn dict([(i,difference(d1[i], d2[i], level-1)) for i in d1 if i not in d2])\n\t\t#return {k: difference(v, d2[k], level-1) for k,v in d1.items() if k not in d2}\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn dict()\n\td3 = dict()\n\tfor k, v in d1.items():\n\t\tif k in d2:\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\td3[k] = difference(v, d2[k], level - 1)\n\t\t\t\telse:\n\t\t\t\t\td3[k] = v\n\t\t\telse:\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\td3[k] = v\n\t\t\t\telse:\n\t\t\t\t\tif v != d2[k]:\n\t\t\t\t\t\td3[k] = v\n\t\telse:\n\t\t\td3[k] = v\n\treturn d3\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn {}\n\td = {}\n\tfor key in d1:\n\t\tif key not in d2:\n\t\t\td[key] = d1[key]\n\t\telif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n\t\t\td[key] = difference(d1[key], d2[key], level=level - 1)\n\treturn d\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\n\titems1 = d1.items()\n\titems2 = d2.items()\n\tif level == -1:\n\t\tlevel = len(items1)\n\n\tfor item in items1:\n\t\tif item not in items2:\n\t\t\tdiff[item] = d1[item]\n\t\telif type(d1[item]) == type({}):\n\t\t\tdif = difference(d1[item], d2[item], level-1)\n\t\t\tif len(dif) > 0:\n\t\t\t\tdiff[item] = dif\n\n\treturn diff\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tfor k,v in d1.iteritems():\n\t\tif k in d2:\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\tdiff = difference(v, d2[k], level+1)\n\t\t\t\t\tif diff:\n\t\t\t\t\t\td3[k] = diff\n\t\t\t\telif not d2[k] == v:\n\t\t\t\t\td3[k] = v\n\t\t\telif v != d2[k]:\n\t\t\t\td3[k] = v\n\treturn d3\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn {}\n\td = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\td[key] = value\n\t\telif isinstance(value, dict) and isinstance(d2[key], dict):\n\t\t\td[key] = difference(value, d2[key], level - 1)\n\treturn d\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2 or d2[key] != value:\n\t\t\tdiff[key] = value\n\treturn diff\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn {}\n\telse:\n\t\tres = {}\n\t\tfor key in d1.keys():\n\t\t\tif key not in d2.keys():\n\t\t\t\tres[key] = d1[key]\n\t\t\telif type(d1[key]) == dict and type(d2[key]) == dict:\n\t\t\t\tres[key] = difference(d", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn d1\n\telse:\n\t\td3 = {}\n\t\tfor k, v in d1.items():\n\t\t\tif k in d2:\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\t\td3[k] = difference(v, d2[k], level=level-1)\n\t\t\t\t\telse:\n\t\t\t\t\t\td3[k] = d2[k]\n\t\t\t\telse:\n\t\t\t\t\tif not isinstance(d2[k], list):\n\t\t\t\t\t\td3[k] = d2[k]\n\t\t\telse:\n\t\t\t\td3[k] = v\n\treturn d3\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tif level == 0:\n\t\treturn d3\n\tif level == -1:\n\t\tlevel = len(d1)\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\td3[k] = d1[k]\n\t\telif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n\t\t\td3[k] = difference(d1[k], d2[k], level-1)\n\treturn d3\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn d1\n\telse:\n\t\treturn dict([(i,difference(d1[i], d2[i], level-1)) for i in d1 if i not in d2])\n\t\t#return {k: difference(v, d2[k], level-1) for k,v in d1.items() if k not in d2}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn dict()\n\td3 = dict()\n\tfor k, v in d1.items():\n\t\tif k in d2:\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\td3[k] = difference(v, d2[k], level - 1)\n\t\t\t\telse:\n\t\t\t\t\td3[k] = v\n\t\t\telse:\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\td3[k] = v\n\t\t\t\telse:\n\t\t\t\t\tif v != d2[k]:\n\t\t\t\t\t\td3[k] = v\n\t\telse:\n\t\t\td3[k] = v\n\treturn d3\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn {}\n\td = {}\n\tfor key in d1:\n\t\tif key not in d2:\n\t\t\td[key] = d1[key]\n\t\telif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n\t\t\td[key] = difference(d1[key], d2[key], level=level - 1)\n\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\n\titems1 = d1.items()\n\titems2 = d2.items()\n\tif level == -1:\n\t\tlevel = len(items1)\n\n\tfor item in items1:\n\t\tif item not in items2:\n\t\t\tdiff[item] = d1[item]\n\t\telif type(d1[item]) == type({}):\n\t\t\tdif = difference(d1[item], d2[item], level-1)\n\t\t\tif len(dif) > 0:\n\t\t\t\tdiff[item] = dif\n\n\treturn diff\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td3 = {}\n\tfor k,v in d1.iteritems():\n\t\tif k in d2:\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\tdiff = difference(v, d2[k], level+1)\n\t\t\t\t\tif diff:\n\t\t\t\t\t\td3[k] = diff\n\t\t\t\telif not d2[k] == v:\n\t\t\t\t\td3[k] = v\n\t\t\telif v != d2[k]:\n\t\t\t\td3[k] = v\n\treturn d3\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tif level == 0:\n\t\treturn {}\n\td = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\td[key] = value\n\t\telif isinstance(value, dict) and isinstance(d2[key], dict):\n\t\t\td[key] = difference(value, d2[key], level - 1)\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key, value in d1.items():\n\t\tif key not in d2 or d2[key] != value:\n\t\t\tdiff[key] = value\n\treturn diff\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.coord_type is None:\n\t\traise TypeError(\"Histogram coordinate type is unknown.\")\n\telif self.coord_type is int:\n\t\tif not isinstance(coord, int):\n\t\t\traise TypeError(\"Histogram coordinate must be an integer.\")\n\telif self.coord_type is tuple:\n\t\tif not isinstance(coord, tuple):\n\t\t\traise TypeError(\"Histogram coordinate must be a tuple.\")\n\t\tif len(coord) != len(self.coord_edges):\n\t\t\traise ValueError(\"Histogram coordinate tuple has wrong number of coordinates.\")\n\t\tfor (i, c) in enumerate(coord):\n\t\t\tif not isinstance(c, int):\n\t\t\t\traise TypeError(\"Histogram coordinate must be an integer tuple.\")\n\t\t\tif c < self.coord_edges[i][0] or c >= self.coord_edges[i][-1]:\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(\"Histogram coordinate tuple is out of range.\")\n\n\tif not isinstance(weight, (int, float)):\n\t\traise TypeError(\"Histogram weight must be a number.\")\n\tif weight < 0:\n\t\traise ValueError(\"", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self._histogram:\n\t\tif self._coord_type == 'normalized':\n\t\t\tcoord = self._coord_transform(coord)\n\t\tself._histogram[coord] += weight\n\t\tself._update()\n\t\treturn self\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor i in range(self.dim):\n\t\tif coord[i] < self.min[i] or coord[i] > self.max[i]:\n\t\t\tbreak\n\telse:\n\t\tself.hist[coord] += weight\n\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.get(coord) is None:\n\t\tself.set(coord, weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._check_coord(coord)\n\tself._data[coord] += weight\n\treturn self\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t# TODO: this doesn't seem to be what the docstring says it does\n\t# TODO: this should be able to fill histogram with weights\n\tif self.ndim > 1:\n\t\traise NotImplementedError\n\tif isinstance(coord, tuple):\n\t\tcoord = coord[0]\n\tidx = int(round(coord))\n\tif idx < 0 or idx >= self.n:\n\t\treturn\n\tself.hist[idx] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.edges[0] <= coord < self.edges[-1]:\n\t\tself._fill(coord, weight)\ndef fill_n(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram with the given *coords* and *weights*.\n\nAll coordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor coord, weight in zip(coords, weights):\n\t\tself.fill(coord, weight)\ndef fill_array(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram with the given *coords* and *weights*.\n\nAll coordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor coord, weight in zip(coords, weights):\n\t\tself.fill(coord, weight)\ndef fill_coords(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram with the given *coords* and *weights*.\n\nAll coordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor coord, weight in zip(coords, weights):\n\t\tself.fill(coord, weight)\ndef fill_xy(self, x, y, weights=None):\n\t\"\"\"\n\tFill histogram with the given *x*, *y* and", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.add(coord, weight)\n\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif not self.is_filled:\n\t\traise ValueError(\"Histogram must be filled first.\")\n\n\tif self.is_uniform:\n\t\tif not self.is_weighted:\n\t\t\tself._data[coord] += weight\n\t\telse:\n\t\t\tself._data[coord] += weight * self._bins[coord]\n\telse:\n\t\tif not self.is_weighted:\n\t\t\tself._data[coord] += weight\n\t\telse:\n\t\t\tself._data[coord] += weight * self._bins[coord]\n\n\t\tself._count += weight\n\n\treturn self\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.coord_outside(coord):\n\t\treturn\n\tif self.coord_outside(coord, self.edges):\n\t\treturn\n\tself[coord] += weight\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.coord_type is None:\n\t\traise TypeError(\"Histogram coordinate type is unknown.\")\n\telif self.coord_type is int:\n\t\tif not isinstance(coord, int):\n\t\t\traise TypeError(\"Histogram coordinate must be an integer.\")\n\telif self.coord_type is tuple:\n\t\tif not isinstance(coord, tuple):\n\t\t\traise TypeError(\"Histogram coordinate must be a tuple.\")\n\t\tif len(coord) != len(self.coord_edges):\n\t\t\traise ValueError(\"Histogram coordinate tuple has wrong number of coordinates.\")\n\t\tfor (i, c) in enumerate(coord):\n\t\t\tif not isinstance(c, int):\n\t\t\t\traise TypeError(\"Histogram coordinate must be an integer tuple.\")\n\t\t\tif c < self.coord_edges[i][0] or c >= self.coord_edges[i][-1]:\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(\"Histogram coordinate tuple is out of range.\")\n\n\tif not isinstance(weight, (int, float)):\n\t\traise TypeError(\"Histogram weight must be a number.\")\n\tif weight < 0:\n\t\traise ValueError(\"", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self._histogram:\n\t\tif self._coord_type == 'normalized':\n\t\t\tcoord = self._coord_transform(coord)\n\t\tself._histogram[coord] += weight\n\t\tself._update()\n\t\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor i in range(self.dim):\n\t\tif coord[i] < self.min[i] or coord[i] > self.max[i]:\n\t\t\tbreak\n\telse:\n\t\tself.hist[coord] += weight\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.get(coord) is None:\n\t\tself.set(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._check_coord(coord)\n\tself._data[coord] += weight\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t# TODO: this doesn't seem to be what the docstring says it does\n\t# TODO: this should be able to fill histogram with weights\n\tif self.ndim > 1:\n\t\traise NotImplementedError\n\tif isinstance(coord, tuple):\n\t\tcoord = coord[0]\n\tidx = int(round(coord))\n\tif idx < 0 or idx >= self.n:\n\t\treturn\n\tself.hist[idx] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.edges[0] <= coord < self.edges[-1]:\n\t\tself._fill(coord, weight)\ndef fill_n(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram with the given *coords* and *weights*.\n\nAll coordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor coord, weight in zip(coords, weights):\n\t\tself.fill(coord, weight)\ndef fill_array(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram with the given *coords* and *weights*.\n\nAll coordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor coord, weight in zip(coords, weights):\n\t\tself.fill(coord, weight)\ndef fill_coords(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram with the given *coords* and *weights*.\n\nAll coordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfor coord, weight in zip(coords, weights):\n\t\tself.fill(coord, weight)\ndef fill_xy(self, x, y, weights=None):\n\t\"\"\"\n\tFill histogram with the given *x*, *y* and", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.add(coord, weight)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif not self.is_filled:\n\t\traise ValueError(\"Histogram must be filled first.\")\n\n\tif self.is_uniform:\n\t\tif not self.is_weighted:\n\t\t\tself._data[coord] += weight\n\t\telse:\n\t\t\tself._data[coord] += weight * self._bins[coord]\n\telse:\n\t\tif not self.is_weighted:\n\t\t\tself._data[coord] += weight\n\t\telse:\n\t\t\tself._data[coord] += weight * self._bins[coord]\n\n\t\tself._count += weight\n\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self.coord_outside(coord):\n\t\treturn\n\tif self.coord_outside(coord, self.edges):\n\t\treturn\n\tself[coord] += weight\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise InvalidLabels(labels)\n\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\n\treturn labels\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError('labels should be a dict.')\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\treturn True\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"Labels must be a dictionary with string keys and values.\")\n\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\traise TypeError(\"Label keys must be strings.\")\n\t\tvalidate_key(key)\n\n\t\tif not isinstance(value, str):\n\t\t\traise TypeError(\"Label values must be strings.\")\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabel_dict = {}\n\tfor key, value in labels.items():\n\t\tlabel_dict[key] = validate_key(key)\n\t\tvalidate_value(value)\n\treturn label_dict\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels must be dict\")\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels should be a dictionary\")\n\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkeys, values = [], []\n\tfor key, value in labels.items():\n\t\tkeys.append(_validate_key(key))\n\t\tvalues.append(_validate_value(value))\n\treturn keys, values\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise ValueError('labels should be a dict')\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise InvalidLabels(labels)\n\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\n\treturn labels\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError('labels should be a dict.')\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"Labels must be a dictionary with string keys and values.\")\n\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\traise TypeError(\"Label keys must be strings.\")\n\t\tvalidate_key(key)\n\n\t\tif not isinstance(value, str):\n\t\t\traise TypeError(\"Label values must be strings.\")\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tlabel_dict = {}\n\tfor key, value in labels.items():\n\t\tlabel_dict[key] = validate_key(key)\n\t\tvalidate_value(value)\n\treturn label_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels must be dict\")\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise TypeError(\"labels should be a dictionary\")\n\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkeys, values = [], []\n\tfor key, value in labels.items():\n\t\tkeys.append(_validate_key(key))\n\t\tvalues.append(_validate_value(value))\n\treturn keys, values\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise ValueError('labels should be a dict')\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t# The resources are named with the following pattern:\n\t# <namespace>/<name>\n\t# where:\n\t# namespace is composed by:\n\t# * a name of up to 253 characters\n\t# * separated by a slash\n\t# * composed by lower case alphanumeric characters, '-', '_' or '.'\n\t# name is composed by:\n\t# * a name of up to 253 characters\n\t# * composed by lower case alphanumeric characters, '-', '_' or '.'\n\t# the regex is defined by the following expression:\n\t# (a-z0-9-_.){1,253}/(a-z0-9-_.){1,253}\n\treturn (r\"(a-z0-9-_.){1,253}/(a-z0-9-_.){1,253}\")\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn [\n\t\tre.compile(r\"^[a-z0-9-_]*$\"),\n\t\tre.compile(r\"^[a-z0-9-_]+(\\.[a-z0-9-_]+)*$\"),\n\t\tre.compile(r\"^[a-z0-9-_]+(\\.[a-z0-9-_]+)*$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"cluster_name\": \"^[a-z0-9][a-z0-9\\-\\.]{0,255}[a-z0-9]$\",\n\t\t\"namespace_name\": \"^[a-z0-9][a-z0-9\\-\\.]{0,255}[a-z0-9]$\"\n\t}\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"namespace\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"ingress\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"service\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"statefulset\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"deployment\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"job\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"pod\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"replicaset\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {'resource_name': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-]*[a-z0-9])?$',\n\t\t\t'resource_name_all': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-\\.]{0,61}[a-z0-9])?$',\n\t\t\t'resource_name_short': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-]{0,28}[a-z0-9])?$',\n\t\t\t'resource_name_long': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-]{0,63}[a-z0-9])?$'}\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9]([-a-z0-9]*[a-z0-9])?))*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9]([-a-z0-9]*[a-z0-9])?))*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9]([-a-z0-9]*[a-z0-9])?))*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"cluster\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"project\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"namespace\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"secret\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"serviceaccount\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"node\": \"^[a-z][a-z0-9-]{0,59}$\"\n\t}\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"namespace\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"cluster\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"node\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"service\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"deployment\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"ingress\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"nodeport\": r\"^[a-z0-9.-]{1,63}$\"\n\t}\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn (\n\t\t'^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$',\n\t\t'^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$',\n\t\t'^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$'\n\t)\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"cluster\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"node\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"namespace\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"storageClass\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"resourceQuota\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"role\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"roleBinding\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"serviceAccount\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"service\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"ingress\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"deployment\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"daemonset\": r\"^[a-z][a-z0-9-]*$\","], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t# The resources are named with the following pattern:\n\t# <namespace>/<name>\n\t# where:\n\t# namespace is composed by:\n\t# * a name of up to 253 characters\n\t# * separated by a slash\n\t# * composed by lower case alphanumeric characters, '-', '_' or '.'\n\t# name is composed by:\n\t# * a name of up to 253 characters\n\t# * composed by lower case alphanumeric characters, '-', '_' or '.'\n\t# the regex is defined by the following expression:\n\t# (a-z0-9-_.){1,253}/(a-z0-9-_.){1,253}\n\treturn (r\"(a-z0-9-_.){1,253}/(a-z0-9-_.){1,253}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn [\n\t\tre.compile(r\"^[a-z0-9-_]*$\"),\n\t\tre.compile(r\"^[a-z0-9-_]+(\\.[a-z0-9-_]+)*$\"),\n\t\tre.compile(r\"^[a-z0-9-_]+(\\.[a-z0-9-_]+)*$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?$\"),\n\t\tre.compile(r\"^([a-z0-9-_]+(\\.[a-z0-9-_]+)*)?", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"cluster_name\": \"^[a-z0-9][a-z0-9\\-\\.]{0,255}[a-z0-9]$\",\n\t\t\"namespace_name\": \"^[a-z0-9][a-z0-9\\-\\.]{0,255}[a-z0-9]$\"\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"namespace\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"ingress\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"service\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"statefulset\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"deployment\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"job\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"pod\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"replicaset\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {'resource_name': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-]*[a-z0-9])?$',\n\t\t\t'resource_name_all': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-\\.]{0,61}[a-z0-9])?$',\n\t\t\t'resource_name_short': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-]{0,28}[a-z0-9])?$',\n\t\t\t'resource_name_long': r'^(?!.*--)[a-z0-9](?:[a-z0-9\\-]{0,63}[a-z0-9])?$'}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9]([-a-z0-9]*[a-z0-9])?))*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9]([-a-z0-9]*[a-z0-9])?))*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9]([-a-z0-9]*[a-z0-9])?))*$\": \"FQDN\",\n\t\t\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.([a-z0-9", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"cluster\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"project\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"namespace\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"secret\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"serviceaccount\": \"^[a-z][a-z0-9-]{0,59}$\",\n\t\t\"node\": \"^[a-z][a-z0-9-]{0,59}$\"\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"namespace\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"cluster\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"node\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"service\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"deployment\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"ingress\": r\"^[a-z0-9.-]{1,63}$\",\n\t\t\"nodeport\": r\"^[a-z0-9.-]{1,63}$\"\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn (\n\t\t'^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$',\n\t\t'^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$',\n\t\t'^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$'\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\treturn {\n\t\t\"cluster\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"node\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"namespace\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"storageClass\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"resourceQuota\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"role\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"roleBinding\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"serviceAccount\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"service\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"ingress\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"deployment\": r\"^[a-z][a-z0-9-]*$\",\n\t\t\"daemonset\": r\"^[a-z][a-z0-9-]*$\",", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\treturn value if value is None or value.strip() == '' or value.strip() == 'null' else re.match(r\"^[-+]?[0-9]*\\.?[0-9]+$\", value)\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\timport logging\n\n\tif type(value) is str:\n\t\treturn value\n\telse:\n\t\tlogging.error(\"Unsupported type %s for value %s.\" % (type(value), value))\n\t\treturn None\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\treturn bool(REGEX[value.value.lower()].match(value.value))\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t# Check if the value is a valid number.\n\tif not value.isdigit():\n\t\traise ValueError(\"Value must be an integer.\")\n\n\t# Check if the value is in the proper range.\n\tif int(value) < 0 or int(value) > 999:\n\t\traise ValueError(\"Value must be between 0 and 999.\")\n\n\treturn True\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tif value is None:\n\t\treturn False\n\tif isinstance(value, str):\n\t\treturn False\n\treturn True\n\nclass Parameter:\n\t\"\"\"\n\tParameter, as defined in the REST API.\n\t\"\"\"\n\n\tdef __init__(self, name, value, regex, optional):\n\t\tself.name = name\n\t\tself.value = value\n\t\tself.regex = regex\n\t\tself.optional = optional\n\n\tdef validate(self):\n\t\t\"\"\"\n\t\tValidate the value against the regular expression.\n\t\t\"\"\"\n\t\treturn validate_value(self.value)\n\n\tdef __str__(self):\n\t\treturn 'Name: {0}, Value: {1}, Regex: {2}, Optional: {3}'.format(self.name, self.value, self.regex, self.optional)\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t# TODO: Implement this function\n\tif (re.match(FLOAT_REGEX, value)):\n\t\treturn float(value)\n\telif (re.match(INT_REGEX, value)):\n\t\treturn int(value)\n\telif (re.match(BOOL_REGEX, value)):\n\t\treturn bool(value)\n\telif (re.match(STRING_REGEX, value)):\n\t\treturn str(value)\n\telse:\n\t\treturn None\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tif value is None:\n\t\treturn\n\telif value.strip() == '':\n\t\treturn\n\telse:\n\t\tif not re.match(r'^[0-9]*$', value):\n\t\t\traise ValidationError(\n\t\t\t\t\"Invalid value : '{0}'\".format(value),\n\t\t\t\tcode='invalid'\n\t\t\t)\n\n\nclass IntegerField(forms.IntegerField):\n\t\"\"\"\n\tField for validating and casting integer values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass NumberField(forms.CharField):\n\t\"\"\"\n\tField for validating and casting real values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass DateField(forms.DateField):\n\t\"\"\"\n\tField for validating and casting date values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass TimeField(forms.TimeField):\n\t\"\"\"\n\tField for validating and casting time values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass DateTimeField(forms.DateTimeField):\n\t\"\"\"\n\tField for validating and casting datetime values.\n\t\"\"\"\n\tdefault_validators = [validate_", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tvalue = value.strip()\n\tif not value:\n\t\traise ValidationError(\"Value is not set.\")\n\n\tif not re.match(r\"^[a-zA-Z0-9_-]+$\", value):\n\t\traise ValidationError(\"Value must only contain alphanumeric characters, dashes and underscores.\")\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tif not isinstance(value, basestring):\n\t\traise ValueError('ValueTypeError: value must be a string.')\n\tif value == '':\n\t\treturn False\n\tif not re.match('^[0-9]*$', value):\n\t\traise ValueError('ValueTypeError: value must be a valid number.')\n\telse:\n\t\treturn True\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tvalid_value = False\n\tfor regex, _ in REGEXES:\n\t\tif re.match(regex, value):\n\t\t\tvalid_value = True\n\t\t\tbreak\n\tif not valid_value:\n\t\traise ValidationError(\"'%s' is not a valid value\" % value)\n\treturn value\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\treturn value if value is None or value.strip() == '' or value.strip() == 'null' else re.match(r\"^[-+]?[0-9]*\\.?[0-9]+$\", value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\timport logging\n\n\tif type(value) is str:\n\t\treturn value\n\telse:\n\t\tlogging.error(\"Unsupported type %s for value %s.\" % (type(value), value))\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\treturn bool(REGEX[value.value.lower()].match(value.value))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t# Check if the value is a valid number.\n\tif not value.isdigit():\n\t\traise ValueError(\"Value must be an integer.\")\n\n\t# Check if the value is in the proper range.\n\tif int(value) < 0 or int(value) > 999:\n\t\traise ValueError(\"Value must be between 0 and 999.\")\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tif value is None:\n\t\treturn False\n\tif isinstance(value, str):\n\t\treturn False\n\treturn True\n\nclass Parameter:\n\t\"\"\"\n\tParameter, as defined in the REST API.\n\t\"\"\"\n\n\tdef __init__(self, name, value, regex, optional):\n\t\tself.name = name\n\t\tself.value = value\n\t\tself.regex = regex\n\t\tself.optional = optional\n\n\tdef validate(self):\n\t\t\"\"\"\n\t\tValidate the value against the regular expression.\n\t\t\"\"\"\n\t\treturn validate_value(self.value)\n\n\tdef __str__(self):\n\t\treturn 'Name: {0}, Value: {1}, Regex: {2}, Optional: {3}'.format(self.name, self.value, self.regex, self.optional)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t# TODO: Implement this function\n\tif (re.match(FLOAT_REGEX, value)):\n\t\treturn float(value)\n\telif (re.match(INT_REGEX, value)):\n\t\treturn int(value)\n\telif (re.match(BOOL_REGEX, value)):\n\t\treturn bool(value)\n\telif (re.match(STRING_REGEX, value)):\n\t\treturn str(value)\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tif value is None:\n\t\treturn\n\telif value.strip() == '':\n\t\treturn\n\telse:\n\t\tif not re.match(r'^[0-9]*$', value):\n\t\t\traise ValidationError(\n\t\t\t\t\"Invalid value : '{0}'\".format(value),\n\t\t\t\tcode='invalid'\n\t\t\t)\n\n\nclass IntegerField(forms.IntegerField):\n\t\"\"\"\n\tField for validating and casting integer values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass NumberField(forms.CharField):\n\t\"\"\"\n\tField for validating and casting real values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass DateField(forms.DateField):\n\t\"\"\"\n\tField for validating and casting date values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass TimeField(forms.TimeField):\n\t\"\"\"\n\tField for validating and casting time values.\n\t\"\"\"\n\tdefault_validators = [validate_value]\n\n\nclass DateTimeField(forms.DateTimeField):\n\t\"\"\"\n\tField for validating and casting datetime values.\n\t\"\"\"\n\tdefault_validators = [validate_", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tvalue = value.strip()\n\tif not value:\n\t\traise ValidationError(\"Value is not set.\")\n\n\tif not re.match(r\"^[a-zA-Z0-9_-]+$\", value):\n\t\traise ValidationError(\"Value must only contain alphanumeric characters, dashes and underscores.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tif not isinstance(value, basestring):\n\t\traise ValueError('ValueTypeError: value must be a string.')\n\tif value == '':\n\t\treturn False\n\tif not re.match('^[0-9]*$', value):\n\t\traise ValueError('ValueTypeError: value must be a valid number.')\n\telse:\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tvalid_value = False\n\tfor regex, _ in REGEXES:\n\t\tif re.match(regex, value):\n\t\t\tvalid_value = True\n\t\t\tbreak\n\tif not valid_value:\n\t\traise ValidationError(\"'%s' is not a valid value\" % value)\n\treturn value\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tfor (pattern, message) in patterns:\n\t\tif re.match(pattern, key):\n\t\t\treturn True\n\treturn False\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key == \"\":\n\t\treturn False\n\treturn re.match(r\"^[a-z0-9_-]{1,50}$\", key)\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif not re.match(key_regex, key):\n\t\traise KeyError(\"Key must match regular expression: {}\".format(key_regex))\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key.isdigit():\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key.strip() == '':\n\t\tprint('Key cannot be blank.')\n\t\treturn False\n\n\treturn True\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tkey_regex = {\n\t\t'name': r'^[a-zA-Z]+$',\n\t\t'email': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n\t\t'password': r'^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z])[0-9a-zA-Z]{8,}$',\n\t\t'phone': r'^[0-9\\+\\-\\(\\)\\ ]+$',\n\t\t'url': r'^[a-zA-Z0-9\\/\\:\\@\\.\\_\\-\\+\\?\\&\\=\\;\\$\\%\\#\\*\\(\\)\\~\\,\\#\\&\\!\\_\\-]+$',\n\t\t'ip': r'^(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key is None or len(key) == 0:\n\t\traise ValueError(\"Key should not be None or empty.\")\n\tif not re.match(r\"^[a-zA-Z0-9_-]+$\", key):\n\t\traise ValueError(\"Key should match the regular expression: r'^[a-zA-Z0-9_-]+$'.\")\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif re.match(key_regex, key):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\treturn re.match(r'^(?!.*\\.\\.)(?!.*\\.$)[^\\W][\\w.\\-]*$', key)\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif not re.match(VALID_KEY_REGEX, key):\n\t\traise ValueError(\"Key '{}' does not match the pattern\".format(key))\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tfor (pattern, message) in patterns:\n\t\tif re.match(pattern, key):\n\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key == \"\":\n\t\treturn False\n\treturn re.match(r\"^[a-z0-9_-]{1,50}$\", key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif not re.match(key_regex, key):\n\t\traise KeyError(\"Key must match regular expression: {}\".format(key_regex))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key.isdigit():\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key.strip() == '':\n\t\tprint('Key cannot be blank.')\n\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tkey_regex = {\n\t\t'name': r'^[a-zA-Z]+$',\n\t\t'email': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n\t\t'password': r'^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z])[0-9a-zA-Z]{8,}$',\n\t\t'phone': r'^[0-9\\+\\-\\(\\)\\ ]+$',\n\t\t'url': r'^[a-zA-Z0-9\\/\\:\\@\\.\\_\\-\\+\\?\\&\\=\\;\\$\\%\\#\\*\\(\\)\\~\\,\\#\\&\\!\\_\\-]+$',\n\t\t'ip': r'^(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif key is None or len(key) == 0:\n\t\traise ValueError(\"Key should not be None or empty.\")\n\tif not re.match(r\"^[a-zA-Z0-9_-]+$\", key):\n\t\traise ValueError(\"Key should match the regular expression: r'^[a-zA-Z0-9_-]+$'.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif re.match(key_regex, key):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\treturn re.match(r'^(?!.*\\.\\.)(?!.*\\.$)[^\\W][\\w.\\-]*$', key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tif not re.match(VALID_KEY_REGEX, key):\n\t\traise ValueError(\"Key '{}' does not match the pattern\".format(key))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\telif isinstance(value, list):\n\t\t\tif not first_level:\n\t\t\t\tobserver_schema_dict[key] = []\n\t\t\telse:\n\t\t\t\tobserver_schema_dict[key] = value\n\t\telse:\n\t\t\tobserver_schema_dict[key] = value\n\treturn observer_schema_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tif not manifest_dict:\n\t\treturn {}\n\telse:\n\t\tnew_dict = {}\n\t\tfor k, v in manifest_dict.items():\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif first_level:\n\t\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v, first_level=False)\n\t\t\t\telse:\n\t\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v, first_level=True)\n\t\t\telif isinstance(v, list):\n\t\t\t\tnew_dict[k] = []\n\t\t\telse:\n\t\t\t\tnew_dict[k] = None\n\t\treturn new_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif first_level:\n\t\tnew_dict[\"type\"] = manifest_dict[\"type\"]\n\t\tnew_dict[\"description\"] = manifest_dict[\"description\"]\n\t\tnew_dict[\"default\"] = manifest_dict[\"default\"]\n\t\tnew_dict[\"required\"] = manifest_dict[\"required\"]\n\t\tnew_dict[\"properties\"] = {}\n\t\tfor key, value in manifest_dict[\"properties\"].items():\n\t\t\tnew_dict[\"properties\"][key] = generate_default_observer_schema_dict(value, first_level=False)\n\telse:\n\t\tnew_dict[\"type\"] = manifest_dict[\"type\"]\n\t\tnew_dict[\"description\"] = manifest_dict[\"description\"]\n\t\tnew_dict[\"default\"] = manifest_dict[\"default\"]\n\t\tnew_dict[\"required\"] = manifest_dict[\"required\"]\n\t\tnew_dict[\"properties\"] = {}\n\t\tfor key, value in manifest_dict[\"properties\"].items():\n\t\t\tnew_dict[\"properties\"][key] = generate_default_observer_schema_dict(value, first_level=False)\n\treturn new_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif first_level:\n\t\tnew_dict[\"type\"] = \"object\"\n\t\tnew_dict[\"required\"] = []\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value[0], first_level=False)\n\t\telse:\n\t\t\tnew_dict[key] = {}\n\t\t\tnew_dict[key][\"type\"] = str(type(value))\n\treturn new_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\telif isinstance(value, list):\n\t\t\tdefault_observer_schema_dict[key] = []\n\t\telif isinstance(value, str):\n\t\t\tdefault_observer_schema_dict[key] = \"\"\n\t\telse:\n\t\t\tdefault_observer_schema_dict[key] = value\n\treturn default_observer_schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tif \"properties\" in manifest_dict and manifest_dict[\"properties\"]:\n\t\tfor key, value in manifest_dict[\"properties\"].items():\n\t\t\tif \"type\" in value and value[\"type\"] in [\"object\", \"array\"]:\n\t\t\t\tif value[\"type\"] == \"array\":\n\t\t\t\t\tdefault_observer_schema_dict[key] = []\n\t\t\t\telse:\n\t\t\t\t\tdefault_observer_schema_dict[key] = {}\n\t\t\telse:\n\t\t\t\tif value[\"type\"] == \"boolean\":\n\t\t\t\t\tdefault_observer_schema_dict[key] = False\n\t\t\t\telif value[\"type\"] == \"integer\":\n\t\t\t\t\tdefault_observer_schema_dict[key] = 0\n\t\t\t\telse:\n\t\t\t\t\tdefault_observer_schema_dict[key] = \"\"\n\telse:\n\t\tif first_level:\n\t\t\tdefault_observer_schema_dict = {\n\t\t\t\t\"meta\": {\n\t\t\t\t\t\"id\": \"\",\n\t\t\t\t\t\"name\": \"\",\n\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\"tags\": []", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t# print(\"manifest_dict is {}\".format(manifest_dict))\n\tnew_dict = {}\n\tfor k, v in manifest_dict.items():\n\t\tif isinstance(v, dict):\n\t\t\tif first_level:\n\t\t\t\tv[\"type\"] = \"object\"\n\t\t\t\tv[\"properties\"] = generate_default_observer_schema_dict(v, first_level=False)\n\t\t\t\tnew_dict[k] = v\n\t\t\telse:\n\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v)\n\t\telif isinstance(v, list):\n\t\t\tif first_level:\n\t\t\t\tv[\"type\"] = \"array\"\n\t\t\t\tv[\"items\"] = generate_default_observer_schema_dict(v[0], first_level=False)\n\t\t\t\tnew_dict[k] = v\n\t\t\telse:\n\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v[0])\n\t\telse:\n\t\t\tnew_dict[k] = v\n\treturn new_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tif not first_level:\n\t\tfor key in manifest_dict.keys():\n\t\t\tif type(manifest_dict[key]) == dict:\n\t\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=False)\n\t\t\telse:\n\t\t\t\tdefault_observer_schema_dict[key] = manifest_dict[key]\n\telse:\n\t\tfor key in manifest_dict.keys():\n\t\t\tif type(manifest_dict[key]) == dict:\n\t\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=True)\n\t\t\telse:\n\t\t\t\tdefault_observer_schema_dict[key] = manifest_dict[key]\n\treturn default_observer_schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) == dict:\n\t\t\t# The value is a dictionary.\n\t\t\tif value.get('type', None) == 'object':\n\t\t\t\t# value['type'] is 'object'\n\t\t\t\tif key not in observer_schema_dict:\n\t\t\t\t\tobserver_schema_dict[key] = {}\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, False)\n\t\t\telif value.get('type', None) == 'array':\n\t\t\t\t# value['type'] is 'array'\n\t\t\t\tif key not in observer_schema_dict:\n\t\t\t\t\tobserver_schema_dict[key] = []\n\t\t\t\tif first_level:\n\t\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value['items'], False)\n\t\t\t\telse:\n\t\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value['items'], True)\n\t\t\telse:\n\t\t\t\t# value['type'] is not", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tif first_level:\n\t\tnew_dict = {}\n\telse:\n\t\tnew_dict = {'type' : 'object'}\n\t\tnew_dict['properties'] = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) == dict:\n\t\t\tnew_dict['properties'][key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) == list:\n\t\t\tnew_dict['properties'][key] = generate_default_observer_schema_dict(value[0])\n\t\telse:\n\t\t\tnew_dict['properties'][key] = {'type' : type(value).__name__}\n\tif first_level:\n\t\treturn new_dict\n\telse:\n\t\treturn {'type' : 'object', 'properties' : new_dict}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\telif isinstance(value, list):\n\t\t\tif not first_level:\n\t\t\t\tobserver_schema_dict[key] = []\n\t\t\telse:\n\t\t\t\tobserver_schema_dict[key] = value\n\t\telse:\n\t\t\tobserver_schema_dict[key] = value\n\treturn observer_schema_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tif not manifest_dict:\n\t\treturn {}\n\telse:\n\t\tnew_dict = {}\n\t\tfor k, v in manifest_dict.items():\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif first_level:\n\t\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v, first_level=False)\n\t\t\t\telse:\n\t\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v, first_level=True)\n\t\t\telif isinstance(v, list):\n\t\t\t\tnew_dict[k] = []\n\t\t\telse:\n\t\t\t\tnew_dict[k] = None\n\t\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif first_level:\n\t\tnew_dict[\"type\"] = manifest_dict[\"type\"]\n\t\tnew_dict[\"description\"] = manifest_dict[\"description\"]\n\t\tnew_dict[\"default\"] = manifest_dict[\"default\"]\n\t\tnew_dict[\"required\"] = manifest_dict[\"required\"]\n\t\tnew_dict[\"properties\"] = {}\n\t\tfor key, value in manifest_dict[\"properties\"].items():\n\t\t\tnew_dict[\"properties\"][key] = generate_default_observer_schema_dict(value, first_level=False)\n\telse:\n\t\tnew_dict[\"type\"] = manifest_dict[\"type\"]\n\t\tnew_dict[\"description\"] = manifest_dict[\"description\"]\n\t\tnew_dict[\"default\"] = manifest_dict[\"default\"]\n\t\tnew_dict[\"required\"] = manifest_dict[\"required\"]\n\t\tnew_dict[\"properties\"] = {}\n\t\tfor key, value in manifest_dict[\"properties\"].items():\n\t\t\tnew_dict[\"properties\"][key] = generate_default_observer_schema_dict(value, first_level=False)\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tif first_level:\n\t\tnew_dict[\"type\"] = \"object\"\n\t\tnew_dict[\"required\"] = []\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value[0], first_level=False)\n\t\telse:\n\t\t\tnew_dict[key] = {}\n\t\t\tnew_dict[key][\"type\"] = str(type(value))\n\treturn new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\telif isinstance(value, list):\n\t\t\tdefault_observer_schema_dict[key] = []\n\t\telif isinstance(value, str):\n\t\t\tdefault_observer_schema_dict[key] = \"\"\n\t\telse:\n\t\t\tdefault_observer_schema_dict[key] = value\n\treturn default_observer_schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tif \"properties\" in manifest_dict and manifest_dict[\"properties\"]:\n\t\tfor key, value in manifest_dict[\"properties\"].items():\n\t\t\tif \"type\" in value and value[\"type\"] in [\"object\", \"array\"]:\n\t\t\t\tif value[\"type\"] == \"array\":\n\t\t\t\t\tdefault_observer_schema_dict[key] = []\n\t\t\t\telse:\n\t\t\t\t\tdefault_observer_schema_dict[key] = {}\n\t\t\telse:\n\t\t\t\tif value[\"type\"] == \"boolean\":\n\t\t\t\t\tdefault_observer_schema_dict[key] = False\n\t\t\t\telif value[\"type\"] == \"integer\":\n\t\t\t\t\tdefault_observer_schema_dict[key] = 0\n\t\t\t\telse:\n\t\t\t\t\tdefault_observer_schema_dict[key] = \"\"\n\telse:\n\t\tif first_level:\n\t\t\tdefault_observer_schema_dict = {\n\t\t\t\t\"meta\": {\n\t\t\t\t\t\"id\": \"\",\n\t\t\t\t\t\"name\": \"\",\n\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\"tags\": []", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t# print(\"manifest_dict is {}\".format(manifest_dict))\n\tnew_dict = {}\n\tfor k, v in manifest_dict.items():\n\t\tif isinstance(v, dict):\n\t\t\tif first_level:\n\t\t\t\tv[\"type\"] = \"object\"\n\t\t\t\tv[\"properties\"] = generate_default_observer_schema_dict(v, first_level=False)\n\t\t\t\tnew_dict[k] = v\n\t\t\telse:\n\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v)\n\t\telif isinstance(v, list):\n\t\t\tif first_level:\n\t\t\t\tv[\"type\"] = \"array\"\n\t\t\t\tv[\"items\"] = generate_default_observer_schema_dict(v[0], first_level=False)\n\t\t\t\tnew_dict[k] = v\n\t\t\telse:\n\t\t\t\tnew_dict[k] = generate_default_observer_schema_dict(v[0])\n\t\telse:\n\t\t\tnew_dict[k] = v\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tdefault_observer_schema_dict = {}\n\tif not first_level:\n\t\tfor key in manifest_dict.keys():\n\t\t\tif type(manifest_dict[key]) == dict:\n\t\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=False)\n\t\t\telse:\n\t\t\t\tdefault_observer_schema_dict[key] = manifest_dict[key]\n\telse:\n\t\tfor key in manifest_dict.keys():\n\t\t\tif type(manifest_dict[key]) == dict:\n\t\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=True)\n\t\t\telse:\n\t\t\t\tdefault_observer_schema_dict[key] = manifest_dict[key]\n\treturn default_observer_schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) == dict:\n\t\t\t# The value is a dictionary.\n\t\t\tif value.get('type', None) == 'object':\n\t\t\t\t# value['type'] is 'object'\n\t\t\t\tif key not in observer_schema_dict:\n\t\t\t\t\tobserver_schema_dict[key] = {}\n\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value, False)\n\t\t\telif value.get('type', None) == 'array':\n\t\t\t\t# value['type'] is 'array'\n\t\t\t\tif key not in observer_schema_dict:\n\t\t\t\t\tobserver_schema_dict[key] = []\n\t\t\t\tif first_level:\n\t\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value['items'], False)\n\t\t\t\telse:\n\t\t\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value['items'], True)\n\t\t\telse:\n\t\t\t\t# value['type'] is not", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tif first_level:\n\t\tnew_dict = {}\n\telse:\n\t\tnew_dict = {'type' : 'object'}\n\t\tnew_dict['properties'] = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) == dict:\n\t\t\tnew_dict['properties'][key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) == list:\n\t\t\tnew_dict['properties'][key] = generate_default_observer_schema_dict(value[0])\n\t\telse:\n\t\t\tnew_dict['properties'][key] = {'type' : type(value).__name__}\n\tif first_level:\n\t\treturn new_dict\n\telse:\n\t\treturn {'type' : 'object', 'properties' : new_dict}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor obj in response:\n\t\tif \"_observed\" not in obj:\n\t\t\tobj[\"_observed\"] = {}\n\t\telse:\n\t\t\tif \"_observed\" not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[\"_observed\"] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[\"_observed\"] = last_applied_manifest[\n\t\t\t\t\t\"_observed\"\n\t\t\t\t]\n\t\t\t\tfor k in last_applied_manifest[\"_observed\"]:\n\t\t\t\t\tif k not in obj[\"_observed\"]:\n\t\t\t\t\t\tobj[\"_observed\"][k] = last_applied_manifest[\"_observed\"][\n\t\t\t\t\t\t\tk\n\t\t\t\t\t\t]\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(\n\t\t\t\t\t\t\tlast_applied_manifest[\"_observed\"][k], dict\n\t\t\t\t\t\t) and isinstance(obj[\"_observed\"][k], dict):\n\t\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field[\"type\"] == \"dict\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field[\"name\"]], field[\"fields\"], response\n\t\t\t)\n\t\telif field[\"type\"] == \"list\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field[\"name\"]], field[\"fields\"], response\n\t\t\t)\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key in observer_schema:\n\t\tif key in response:\n\t\t\tif isinstance(response[key], dict):\n\t\t\t\t# if the response is a dict, then we need to go deeper\n\t\t\t\t# and check if the key is already in the last_applied_manifest\n\t\t\t\tif key in last_applied_manifest:\n\t\t\t\t\t# if the key is already in the last_applied_manifest,\n\t\t\t\t\t# then we need to check if the value is a dict\n\t\t\t\t\tif isinstance(last_applied_manifest[key], dict):\n\t\t\t\t\t\t# if the value is a dict, then we need to go deeper\n\t\t\t\t\t\t# and check if the key is already in the response\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[key],\n\t\t\t\t\t\t\tobserver_schema[key],\n\t\t\t\t\t\t\tresponse[key],\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# if the value is not a dict, then we need to initialize\n\t\t\t\t\t\t# the dict with the value of", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tfield_name = field[\"fieldPath\"]\n\t\tif field_name in response[-1]:\n\t\t\tlast_applied_manifest[-1][field_name] = response[-1][field_name]\n\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\tlast_applied_manifest, field[\"children\"], response[-1][field_name]\n\t\t)\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif len(observer_schema) == 0:\n\t\treturn\n\tfor field_name in observer_schema[0]:\n\t\tif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = {}\n\t\tif len(observer_schema[0][field_name]) == 0:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[1],\n\t\t\t\tresponse[0][field_name],\n\t\t\t)\n\t\telse:\n\t\t\tif field_name not in response[0]:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[1],\n\t\t\t\tresponse[0][field_name],\n\t\t\t)\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tassert len(last_applied_manifest) == len(observer_schema)\n\tassert len(response) == len(observer_schema)\n\tfor i in range(len(observer_schema)):\n\t\tfield_schema = observer_schema[i]\n\t\tfield_name = field_schema['name']\n\t\tfield_value = response[i]\n\t\tif 'fields' in field_schema:\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tfield_schema['fields'],\n\t\t\t\tfield_value\n\t\t\t)\n\t\telif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = field_value\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor i, item in enumerate(last_applied_manifest):\n\t\tif isinstance(item, list):\n\t\t\tif isinstance(observer_schema[i], list):\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[i], observer_schema[i], response[i]\n\t\t\t\t)\n\t\t\telif isinstance(observer_schema[i], dict):\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[i], observer_schema[i], response[i]\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\traise Exception(\n\t\t\t\t\t\"Unrecognized type in observer_schema: {}\".format(\n\t\t\t\t\t\ttype(observer_schema[i])\n\t\t\t\t\t)\n\t\t\t\t)\n\t\telif isinstance(item, dict):\n\t\t\tif isinstance(observer_schema[i], list):\n\t\t\t\traise Exception(\n\t\t\t\t\t\"Unrecognized type in observer_schema: {}\".format(\n\t\t\t\t\t\ttype(observer_schema[i])\n\t\t\t", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field[\"type\"] == \"list\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = []\n\t\t\tfor item in response:\n\t\t\t\tif item[\"name\"] not in last_applied_manifest[field[\"name\"]]:\n\t\t\t\t\tlast_applied_manifest[field[\"name\"]].append(item[\"name\"])\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field[\"name\"]][-1],\n\t\t\t\t\t\tfield[\"schema\"],\n\t\t\t\t\t\titem[\"value\"],\n\t\t\t\t\t)\n\t\telif field[\"type\"] == \"dict\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field[\"name\"]],\n\t\t\t\tfield[\"schema\"],\n\t\t\t\tresponse,\n\t\t\t)\n\t\telif field[\"type\"] ==", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif len(observer_schema) == 0:\n\t\treturn\n\tobserver_schema_field_name = observer_schema[0]\n\tobserver_schema_field_type = observer_schema[1]\n\n\tif observer_schema_field_name not in last_applied_manifest:\n\t\tlast_applied_manifest[observer_schema_field_name] = None\n\n\tif response[0] == 'observer':\n\t\tif observer_schema_field_name not in response[1]:\n\t\t\treturn\n\t\tresponse_field_value = response[1][observer_schema_field_name]\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[observer_schema_field_name],\n\t\t\tobserver_schema_field_type,\n\t\t\t[observer_schema_field_name, response_field_value])\n\telse:\n\t\tif observer_schema_field_name not in response[1]:\n\t\t\treturn\n\t\tresponse_field_value = response[1][observer_schema_field_name]\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[observer_schema", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor i in range(len(response)):\n\t\tif isinstance(response[i], list):\n\t\t\tif len(last_applied_manifest[i]) == len(response[i]):\n\t\t\t\tfor j in range(len(response[i])):\n\t\t\t\t\tif last_applied_manifest[i][j] == {}:\n\t\t\t\t\t\tlast_applied_manifest[i][j] = {}\n\t\t\t\t\tif response[i][j][\"kind\"]:\n\t\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[i][j],\n\t\t\t\t\t\t\tobserver_schema[i][j],\n\t\t\t\t\t\t\tresponse[i][j],\n\t\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tfor j in range(len(last_applied_manifest[i])):\n\t\t\t\t\tif last_applied_manifest[i][j] == {}:\n\t\t\t\t\t\tlast_applied_manifest[i][j] = {}\n\t\t\t\tfor j in range(len(response[i])):\n\t\t\t\t\tif response[i][j]"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor obj in response:\n\t\tif \"_observed\" not in obj:\n\t\t\tobj[\"_observed\"] = {}\n\t\telse:\n\t\t\tif \"_observed\" not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[\"_observed\"] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[\"_observed\"] = last_applied_manifest[\n\t\t\t\t\t\"_observed\"\n\t\t\t\t]\n\t\t\t\tfor k in last_applied_manifest[\"_observed\"]:\n\t\t\t\t\tif k not in obj[\"_observed\"]:\n\t\t\t\t\t\tobj[\"_observed\"][k] = last_applied_manifest[\"_observed\"][\n\t\t\t\t\t\t\tk\n\t\t\t\t\t\t]\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(\n\t\t\t\t\t\t\tlast_applied_manifest[\"_observed\"][k], dict\n\t\t\t\t\t\t) and isinstance(obj[\"_observed\"][k], dict):\n\t\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field[\"type\"] == \"dict\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field[\"name\"]], field[\"fields\"], response\n\t\t\t)\n\t\telif field[\"type\"] == \"list\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field[\"name\"]], field[\"fields\"], response\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key in observer_schema:\n\t\tif key in response:\n\t\t\tif isinstance(response[key], dict):\n\t\t\t\t# if the response is a dict, then we need to go deeper\n\t\t\t\t# and check if the key is already in the last_applied_manifest\n\t\t\t\tif key in last_applied_manifest:\n\t\t\t\t\t# if the key is already in the last_applied_manifest,\n\t\t\t\t\t# then we need to check if the value is a dict\n\t\t\t\t\tif isinstance(last_applied_manifest[key], dict):\n\t\t\t\t\t\t# if the value is a dict, then we need to go deeper\n\t\t\t\t\t\t# and check if the key is already in the response\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[key],\n\t\t\t\t\t\t\tobserver_schema[key],\n\t\t\t\t\t\t\tresponse[key],\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# if the value is not a dict, then we need to initialize\n\t\t\t\t\t\t# the dict with the value of", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tfield_name = field[\"fieldPath\"]\n\t\tif field_name in response[-1]:\n\t\t\tlast_applied_manifest[-1][field_name] = response[-1][field_name]\n\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\tlast_applied_manifest, field[\"children\"], response[-1][field_name]\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif len(observer_schema) == 0:\n\t\treturn\n\tfor field_name in observer_schema[0]:\n\t\tif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = {}\n\t\tif len(observer_schema[0][field_name]) == 0:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[1],\n\t\t\t\tresponse[0][field_name],\n\t\t\t)\n\t\telse:\n\t\t\tif field_name not in response[0]:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[1],\n\t\t\t\tresponse[0][field_name],\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tassert len(last_applied_manifest) == len(observer_schema)\n\tassert len(response) == len(observer_schema)\n\tfor i in range(len(observer_schema)):\n\t\tfield_schema = observer_schema[i]\n\t\tfield_name = field_schema['name']\n\t\tfield_value = response[i]\n\t\tif 'fields' in field_schema:\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tfield_schema['fields'],\n\t\t\t\tfield_value\n\t\t\t)\n\t\telif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = field_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor i, item in enumerate(last_applied_manifest):\n\t\tif isinstance(item, list):\n\t\t\tif isinstance(observer_schema[i], list):\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[i], observer_schema[i], response[i]\n\t\t\t\t)\n\t\t\telif isinstance(observer_schema[i], dict):\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[i], observer_schema[i], response[i]\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\traise Exception(\n\t\t\t\t\t\"Unrecognized type in observer_schema: {}\".format(\n\t\t\t\t\t\ttype(observer_schema[i])\n\t\t\t\t\t)\n\t\t\t\t)\n\t\telif isinstance(item, dict):\n\t\t\tif isinstance(observer_schema[i], list):\n\t\t\t\traise Exception(\n\t\t\t\t\t\"Unrecognized type in observer_schema: {}\".format(\n\t\t\t\t\t\ttype(observer_schema[i])\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field[\"type\"] == \"list\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = []\n\t\t\tfor item in response:\n\t\t\t\tif item[\"name\"] not in last_applied_manifest[field[\"name\"]]:\n\t\t\t\t\tlast_applied_manifest[field[\"name\"]].append(item[\"name\"])\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field[\"name\"]][-1],\n\t\t\t\t\t\tfield[\"schema\"],\n\t\t\t\t\t\titem[\"value\"],\n\t\t\t\t\t)\n\t\telif field[\"type\"] == \"dict\":\n\t\t\tif field[\"name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field[\"name\"]] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field[\"name\"]],\n\t\t\t\tfield[\"schema\"],\n\t\t\t\tresponse,\n\t\t\t)\n\t\telif field[\"type\"] ==", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif len(observer_schema) == 0:\n\t\treturn\n\tobserver_schema_field_name = observer_schema[0]\n\tobserver_schema_field_type = observer_schema[1]\n\n\tif observer_schema_field_name not in last_applied_manifest:\n\t\tlast_applied_manifest[observer_schema_field_name] = None\n\n\tif response[0] == 'observer':\n\t\tif observer_schema_field_name not in response[1]:\n\t\t\treturn\n\t\tresponse_field_value = response[1][observer_schema_field_name]\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[observer_schema_field_name],\n\t\t\tobserver_schema_field_type,\n\t\t\t[observer_schema_field_name, response_field_value])\n\telse:\n\t\tif observer_schema_field_name not in response[1]:\n\t\t\treturn\n\t\tresponse_field_value = response[1][observer_schema_field_name]\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[observer_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor i in range(len(response)):\n\t\tif isinstance(response[i], list):\n\t\t\tif len(last_applied_manifest[i]) == len(response[i]):\n\t\t\t\tfor j in range(len(response[i])):\n\t\t\t\t\tif last_applied_manifest[i][j] == {}:\n\t\t\t\t\t\tlast_applied_manifest[i][j] = {}\n\t\t\t\t\tif response[i][j][\"kind\"]:\n\t\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[i][j],\n\t\t\t\t\t\t\tobserver_schema[i][j],\n\t\t\t\t\t\t\tresponse[i][j],\n\t\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tfor j in range(len(last_applied_manifest[i])):\n\t\t\t\t\tif last_applied_manifest[i][j] == {}:\n\t\t\t\t\t\tlast_applied_manifest[i][j] = {}\n\t\t\t\tfor j in range(len(response[i])):\n\t\t\t\t\tif response[i][j]", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field in observer_schema[\"observed_fields\"]:\n\t\tif observed_field not in response:\n\t\t\traise KeyError(\n\t\t\t\tf\"observed field {observed_field} is not present in the response\"\n\t\t\t)\n\n\t\tif \"observer_schema\" in response[observed_field]:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tresponse[observed_field][\"observer_schema\"],\n\t\t\t\tresponse[observed_field],\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# For each field in the observer_schema, check if it is present in\n\t# the response. If so, update last_applied_manifest with the value\n\tfor field_name in observer_schema:\n\t\t# Check if the field is contained in the response\n\t\tif field_name in response:\n\t\t\t# Update the value of the field in last_applied_manifest\n\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\t\t# If the field is a list, recursively call this function\n\t\t\tif observer_schema[field_name]['type'] == 'list':\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest, field_name,\n\t\t\t\t\tresponse[field_name]\n\t\t\t\t)\n\t\t\t# If the field is a dict, recursively call this function\n\t\t\telif observer_schema[field_name]['type'] == 'dict':\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tobserver_schema[field_name]['items'],\n\t\t\t\t\tresponse[field_", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\t\tif isinstance(observer_schema[field], dict):\n\t\t\tif field in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\tresponse[field],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\tif isinstance(observer_schema[field], list):\n\t\t\tif field in response:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\tresponse[field],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field] = []\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field == \"observed\":\n\t\t\tif \"observed\" not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[\"observed\"] = {}\n\t\t\tfor observed_field in observer_schema[field]:\n\t\t\t\tif isinstance(observer_schema[field][observed_field], dict):\n\t\t\t\t\tif observed_field not in last_applied_manifest[\"observed\"]:\n\t\t\t\t\t\tlast_applied_manifest[\"observed\"][observed_field] = {}\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[\"observed\"][observed_field],\n\t\t\t\t\t\tobserver_schema[field][observed_field],\n\t\t\t\t\t\tresponse,\n\t\t\t\t\t)\n\t\t\t\telif isinstance(observer_schema[field][observed_field], list):\n\t\t\t\t\tif observed_field not in last_applied_manifest[\"observed\"]:\n\t\t\t\t\t\tlast_applied_manifest[\"observed\"][observed_field]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key, value in observer_schema.items():\n\t\tif key == \"last_observed_value\":\n\t\t\tlast_applied_manifest[key] = response[key]\n\t\telif key == \"observed_fields\":\n\t\t\tfor field in value:\n\t\t\t\tif field not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tvalue[field],\n\t\t\t\t\tresponse[key][field],\n\t\t\t\t)\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"The field {} is not present in a Kubernetes response\".format(\n\t\t\t\t\tkey\n\t\t\t\t)\n\t\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in response:\n\t\t\traise KeyError(\n\t\t\t\tf\"{field} not found in the Kubernetes response\"\n\t\t\t)\n\t\t# Recursive call\n\t\tif isinstance(observer_schema[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\t\t# If the field is a list, we need to update the manifest list\n\t\telif isinstance(observer_schema[field], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\t\t# Otherwise, we can just update the field value\n\t\telse:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field in response[\"observed_fields\"]:\n\t\tif observed_field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[observed_field] = {}\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[observed_field],\n\t\t\tobserver_schema[observed_field],\n\t\t\tresponse[\"observed_fields\"][observed_field],\n\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\t\tif field == \"spec\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field],\n\t\t\t)\n\t\telif field == \"status\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field],\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field in response:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\telif field in last_applied_manifest:\n\t\t\t# For fields that are not present in the Kubernetes response, we\n\t\t\t# keep the last_applied_manifest value\n\t\t\tpass\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"Observed field {} is not present in the Kubernetes response\".\n\t\t\t\tformat(field)\n\t\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field_name in observer_schema.keys():\n\t\tfield_schema = observer_schema[field_name]\n\t\tfield_value = response.get(field_name, None)\n\t\tif field_schema[\"type\"] == \"object\":\n\t\t\tif field_value is not None:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name], field_schema, field_value\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\telif field_schema[\"type\"] == \"list\":\n\t\t\tif field_value is not None:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name], field"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field in observer_schema[\"observed_fields\"]:\n\t\tif observed_field not in response:\n\t\t\traise KeyError(\n\t\t\t\tf\"observed field {observed_field} is not present in the response\"\n\t\t\t)\n\n\t\tif \"observer_schema\" in response[observed_field]:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tresponse[observed_field][\"observer_schema\"],\n\t\t\t\tresponse[observed_field],\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# For each field in the observer_schema, check if it is present in\n\t# the response. If so, update last_applied_manifest with the value\n\tfor field_name in observer_schema:\n\t\t# Check if the field is contained in the response\n\t\tif field_name in response:\n\t\t\t# Update the value of the field in last_applied_manifest\n\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\t\t# If the field is a list, recursively call this function\n\t\t\tif observer_schema[field_name]['type'] == 'list':\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest, field_name,\n\t\t\t\t\tresponse[field_name]\n\t\t\t\t)\n\t\t\t# If the field is a dict, recursively call this function\n\t\t\telif observer_schema[field_name]['type'] == 'dict':\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tobserver_schema[field_name]['items'],\n\t\t\t\t\tresponse[field_", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\t\tif isinstance(observer_schema[field], dict):\n\t\t\tif field in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\tresponse[field],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\tif isinstance(observer_schema[field], list):\n\t\t\tif field in response:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\tresponse[field],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field] = []\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field == \"observed\":\n\t\t\tif \"observed\" not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[\"observed\"] = {}\n\t\t\tfor observed_field in observer_schema[field]:\n\t\t\t\tif isinstance(observer_schema[field][observed_field], dict):\n\t\t\t\t\tif observed_field not in last_applied_manifest[\"observed\"]:\n\t\t\t\t\t\tlast_applied_manifest[\"observed\"][observed_field] = {}\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[\"observed\"][observed_field],\n\t\t\t\t\t\tobserver_schema[field][observed_field],\n\t\t\t\t\t\tresponse,\n\t\t\t\t\t)\n\t\t\t\telif isinstance(observer_schema[field][observed_field], list):\n\t\t\t\t\tif observed_field not in last_applied_manifest[\"observed\"]:\n\t\t\t\t\t\tlast_applied_manifest[\"observed\"][observed_field]", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key, value in observer_schema.items():\n\t\tif key == \"last_observed_value\":\n\t\t\tlast_applied_manifest[key] = response[key]\n\t\telif key == \"observed_fields\":\n\t\t\tfor field in value:\n\t\t\t\tif field not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tvalue[field],\n\t\t\t\t\tresponse[key][field],\n\t\t\t\t)\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"The field {} is not present in a Kubernetes response\".format(\n\t\t\t\t\tkey\n\t\t\t\t)\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in response:\n\t\t\traise KeyError(\n\t\t\t\tf\"{field} not found in the Kubernetes response\"\n\t\t\t)\n\t\t# Recursive call\n\t\tif isinstance(observer_schema[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\t\t# If the field is a list, we need to update the manifest list\n\t\telif isinstance(observer_schema[field], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\t\t# Otherwise, we can just update the field value\n\t\telse:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field in response[\"observed_fields\"]:\n\t\tif observed_field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[observed_field] = {}\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[observed_field],\n\t\t\tobserver_schema[observed_field],\n\t\t\tresponse[\"observed_fields\"][observed_field],\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\t\tif field == \"spec\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field],\n\t\t\t)\n\t\telif field == \"status\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field],\n\t\t\t\tobserver_schema[field],\n\t\t\t\tresponse[field],\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field in response:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\telif field in last_applied_manifest:\n\t\t\t# For fields that are not present in the Kubernetes response, we\n\t\t\t# keep the last_applied_manifest value\n\t\t\tpass\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"Observed field {} is not present in the Kubernetes response\".\n\t\t\t\tformat(field)\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field_name in observer_schema.keys():\n\t\tfield_schema = observer_schema[field_name]\n\t\tfield_value = response.get(field_name, None)\n\t\tif field_schema[\"type\"] == \"object\":\n\t\t\tif field_value is not None:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name], field_schema, field_value\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\telif field_schema[\"type\"] == \"list\":\n\t\t\tif field_value is not None:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name], field", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource in app[\"spec\"][\"manifest\"]:\n\t\tif \"observerSchema\" not in resource:\n\t\t\tresource[\"observerSchema\"] = {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {},\n\t\t\t\t\"required\": []\n\t\t\t}\n\n\t\t\tfor status in resource[\"status\"]:\n\t\t\t\tif \"observerSchema\" not in status:\n\t\t\t\t\tstatus[\"observerSchema\"] = {\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {},\n\t\t\t\t\t\t\"required\": []\n\t\t\t\t\t}\n\n\t\t\t\t\tfor condition in status[\"conditions\"]:\n\t\t\t\t\t\tif \"observerSchema\" not in condition:\n\t\t\t\t\t\t\tcondition[\"observerSchema\"] = {\n\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\"properties\": {},\n\t\t\t\t\t\t\t\t\"required\": []\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tfor field in condition[\"fields\"]:\n\t\t\t\t\t\t\t\tif \"observerSchema\" not in field:\n\t\t\t\t\t\t\t\t\tfield", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource in app.spec.manifest:\n\t\tif resource.kind in [\n\t\t\tKind.STATEFUL_SET,\n\t\t\tKind.DEPLOYMENT,\n\t\t\tKind.REPLICA_SET,\n\t\t\tKind.DAEMON_SET,\n\t\t\tKind.JOB,\n\t\t\tKind.CRON_JOB,\n\t\t]:\n\t\t\tresource.observer_schema = generate_stateful_set_observer_schema()\n\t\telif resource.kind in [\n\t\t\tKind.SERVICE,\n\t\t\tKind.INGRESS,\n\t\t\tKind.SECRET,\n\t\t\tKind.PERSISTENT_VOLUME,\n\t\t\tKind.PERSISTENT_VOLUME_CLAIM,\n\t\t\tKind.POD_DISRUPTION_BUDGET,\n\t\t\tKind.NETWORK_POLICY,\n\t\t\tKind.POD,\n\t\t\tKind.SERVICE_ACCOUNT,\n\t\t\tKind.ROLE,\n\t\t\tKind.ROLE_BINDING,\n\t\t\tKind.CLUSTER_ROLE,\n\t\t\tKind.CLUSTER_ROLE_BINDING,\n\t\t\tKind.", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource in app.spec.manifest:\n\t\tif resource.spec.observer_schema is None:\n\t\t\tresource.spec.observer_schema = generate_default_observer_schema_for_resource(resource.spec.type)\n\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource_type in app.spec.manifest.keys():\n\t\tif not app.spec.observer_schema.get(resource_type):\n\t\t\tapp.spec.observer_schema[resource_type] = generate_default_observer_schema_for(resource_type)\n\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tif not hasattr(app, \"observer_schema_map\"):\n\t\tapp.observer_schema_map = {}\n\n\tdefault_observer_schema = {\n\t\t\"kind\": \"Observe\",\n\t\t\"spec\": {\n\t\t\t\"manifest\": {}\n\t\t},\n\t\t\"status\": {\n\t\t\t\"status\": \"Pending\",\n\t\t\t\"observedGeneration\": 0\n\t\t}\n\t}\n\n\tfor k8s_resource in app.k8s_resources:\n\t\tif k8s_resource.name not in app.observer_schema_map:\n\t\t\tapp.observer_schema_map[k8s_resource.name] = default_observer_schema\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {}\n\tfor resource_type in app[\"spec\"][\"manifest\"].keys():\n\t\tobserver_schema[resource_type] = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"spec\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"manifest\": {\n\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\tresource_type: {\n\t\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\t\t\t\t\"phase\": {\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"enum\": [\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Running\",\n\t\t\t\t\t\t\t\t\t\t\t", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource_type in app.spec.manifest.keys():\n\t\tobserver_schema = app.spec.observer_schema.get(resource_type, None)\n\t\tif not observer_schema:\n\t\t\tapp.spec.observer_schema[resource_type] = {}\n\n\treturn app\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tif not app.spec.manifest:\n\t\treturn\n\t\n\tobserver_schemas = {}\n\t\n\tfor k8s_kind in app.spec.manifest:\n\t\tif k8s_kind not in app.spec.observer_schema:\n\t\t\tobserver_schemas[k8s_kind] = {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\"description\": \"The status of the Kubernetes resource\",\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\"ready\": {\n\t\t\t\t\t\t\t\t\"description\": \"Whether this resource is ready\",\n\t\t\t\t\t\t\t\t\"type\": \"boolean\"\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\n\tapp.spec.observer_schema.update(observer_schemas)\n\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tresource_spec = app.spec.manifest.get('k8s-resource-spec')\n\tif resource_spec is None:\n\t\treturn\n\n\tfor resource in resource_spec:\n\t\tobserver_schema = resource_spec[resource].get('observer-schema')\n\t\tif observer_schema is None:\n\t\t\tresource_observer_schema = generate_observer_schema(resource)\n\t\t\tresource_spec[resource]['observer-schema'] = resource_observer_schema\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {}\n\tfor resource in app['spec']['manifest']:\n\t\tif 'observerSchema' not in app['spec']['manifest'][resource]:\n\t\t\tobserver_schema[resource] = {\n\t\t\t\t'type': 'object',\n\t\t\t\t'properties': {\n\t\t\t\t\t'status': {\n\t\t\t\t\t\t'type': 'object',\n\t\t\t\t\t\t'properties': {\n\t\t\t\t\t\t\t'replicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'readyReplicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'updatedReplicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'availableReplicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'unavailableReplicas': {\n\t\t\t\t\t\t\t\t'type"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource in app[\"spec\"][\"manifest\"]:\n\t\tif \"observerSchema\" not in resource:\n\t\t\tresource[\"observerSchema\"] = {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {},\n\t\t\t\t\"required\": []\n\t\t\t}\n\n\t\t\tfor status in resource[\"status\"]:\n\t\t\t\tif \"observerSchema\" not in status:\n\t\t\t\t\tstatus[\"observerSchema\"] = {\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {},\n\t\t\t\t\t\t\"required\": []\n\t\t\t\t\t}\n\n\t\t\t\t\tfor condition in status[\"conditions\"]:\n\t\t\t\t\t\tif \"observerSchema\" not in condition:\n\t\t\t\t\t\t\tcondition[\"observerSchema\"] = {\n\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\"properties\": {},\n\t\t\t\t\t\t\t\t\"required\": []\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tfor field in condition[\"fields\"]:\n\t\t\t\t\t\t\t\tif \"observerSchema\" not in field:\n\t\t\t\t\t\t\t\t\tfield", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource in app.spec.manifest:\n\t\tif resource.kind in [\n\t\t\tKind.STATEFUL_SET,\n\t\t\tKind.DEPLOYMENT,\n\t\t\tKind.REPLICA_SET,\n\t\t\tKind.DAEMON_SET,\n\t\t\tKind.JOB,\n\t\t\tKind.CRON_JOB,\n\t\t]:\n\t\t\tresource.observer_schema = generate_stateful_set_observer_schema()\n\t\telif resource.kind in [\n\t\t\tKind.SERVICE,\n\t\t\tKind.INGRESS,\n\t\t\tKind.SECRET,\n\t\t\tKind.PERSISTENT_VOLUME,\n\t\t\tKind.PERSISTENT_VOLUME_CLAIM,\n\t\t\tKind.POD_DISRUPTION_BUDGET,\n\t\t\tKind.NETWORK_POLICY,\n\t\t\tKind.POD,\n\t\t\tKind.SERVICE_ACCOUNT,\n\t\t\tKind.ROLE,\n\t\t\tKind.ROLE_BINDING,\n\t\t\tKind.CLUSTER_ROLE,\n\t\t\tKind.CLUSTER_ROLE_BINDING,\n\t\t\tKind.", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource in app.spec.manifest:\n\t\tif resource.spec.observer_schema is None:\n\t\t\tresource.spec.observer_schema = generate_default_observer_schema_for_resource(resource.spec.type)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource_type in app.spec.manifest.keys():\n\t\tif not app.spec.observer_schema.get(resource_type):\n\t\t\tapp.spec.observer_schema[resource_type] = generate_default_observer_schema_for(resource_type)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tif not hasattr(app, \"observer_schema_map\"):\n\t\tapp.observer_schema_map = {}\n\n\tdefault_observer_schema = {\n\t\t\"kind\": \"Observe\",\n\t\t\"spec\": {\n\t\t\t\"manifest\": {}\n\t\t},\n\t\t\"status\": {\n\t\t\t\"status\": \"Pending\",\n\t\t\t\"observedGeneration\": 0\n\t\t}\n\t}\n\n\tfor k8s_resource in app.k8s_resources:\n\t\tif k8s_resource.name not in app.observer_schema_map:\n\t\t\tapp.observer_schema_map[k8s_resource.name] = default_observer_schema\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {}\n\tfor resource_type in app[\"spec\"][\"manifest\"].keys():\n\t\tobserver_schema[resource_type] = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"spec\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"manifest\": {\n\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\tresource_type: {\n\t\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\t\t\t\t\"phase\": {\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"enum\": [\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Running\",\n\t\t\t\t\t\t\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfor resource_type in app.spec.manifest.keys():\n\t\tobserver_schema = app.spec.observer_schema.get(resource_type, None)\n\t\tif not observer_schema:\n\t\t\tapp.spec.observer_schema[resource_type] = {}\n\n\treturn app\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tif not app.spec.manifest:\n\t\treturn\n\t\n\tobserver_schemas = {}\n\t\n\tfor k8s_kind in app.spec.manifest:\n\t\tif k8s_kind not in app.spec.observer_schema:\n\t\t\tobserver_schemas[k8s_kind] = {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\"description\": \"The status of the Kubernetes resource\",\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\"ready\": {\n\t\t\t\t\t\t\t\t\"description\": \"Whether this resource is ready\",\n\t\t\t\t\t\t\t\t\"type\": \"boolean\"\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\n\tapp.spec.observer_schema.update(observer_schemas)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tresource_spec = app.spec.manifest.get('k8s-resource-spec')\n\tif resource_spec is None:\n\t\treturn\n\n\tfor resource in resource_spec:\n\t\tobserver_schema = resource_spec[resource].get('observer-schema')\n\t\tif observer_schema is None:\n\t\t\tresource_observer_schema = generate_observer_schema(resource)\n\t\t\tresource_spec[resource]['observer-schema'] = resource_observer_schema\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {}\n\tfor resource in app['spec']['manifest']:\n\t\tif 'observerSchema' not in app['spec']['manifest'][resource]:\n\t\t\tobserver_schema[resource] = {\n\t\t\t\t'type': 'object',\n\t\t\t\t'properties': {\n\t\t\t\t\t'status': {\n\t\t\t\t\t\t'type': 'object',\n\t\t\t\t\t\t'properties': {\n\t\t\t\t\t\t\t'replicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'readyReplicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'updatedReplicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'availableReplicas': {\n\t\t\t\t\t\t\t\t'type': 'integer'\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t'unavailableReplicas': {\n\t\t\t\t\t\t\t\t'type", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass SQLConverter(ABC):\n\t\"\"\"\n\tSQLConverter is an abstract class which converts SQL queries and parameters\n\tfor a specific database engine.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.sql_format = None\n\n\t@abstractmethod\n\tdef convert(\n\t\tself, sql: AnyStr, params: Union[Dict[Union[str, int], Any], Sequence[Any]]\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._converter.convert\n\t\t\"\"\"\n\t\tpass\n\n\tdef _build_where(self, where: List[Tuple[AnyStr, Any]]) -> AnyStr:\n\t\t\"\"\"\n\t\tBuild where part of SQL query\n\t\t\"\"\"\n\t\treturn \" WHERE \" + \" AND \".join(\n\t\t\t\"%s %s ?\" % (field, self.sql_format.get_operator(operator))\n\t\t\tfor field, operator in where\n\t\t)\n\n\tdef _build_order(self, order: List[Tuple[AnyStr, Any]]) -> AnyStr:\n\t\t\"\"\"\n\t\tBuild", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tif not sql:\n\t\treturn \"\", params\n\n\tif isinstance(params, dict):\n\t\tparams = [params]\n\n\treturn self._converter.convert(sql, params)\n\n# ----------------------------------------------------------------------------------------------------------------------\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass Converter(abc.ABC):\n\t@abc.abstractmethod\n\tdef convert(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\nclass ConverterFactory(abc.ABC):\n\t@abc.abstractmethod\n\tdef create(self, dialect: str) -> Converter:\n\t\traise NotImplementedError()\n\n\nclass DefaultConverterFactory(ConverterFactory):\n\tdef create(self, dialect: str) -> Converter:\n\t\tif dialect == 'mysql':\n\t\t\treturn MySqlConverter()\n\t\telif dialect == 'postgres':\n\t\t\treturn PostgreSqlConverter()\n\t\telse:\n\t\t\traise NotImplementedError('Unknown dialect {0}'.format(dialect))\n\n\nclass MySqlConverter(Converter):\n\tdef convert(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql=sql, params=params)\n\n\nclass SQLAlchemyConverter(Converter):\n\t\"\"\"\n\tConverter implementation for SQLAlchemy\n\t\"\"\"\n\tdef __init__(self, dialect: str, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself._dialect = dialect\n\n\tdef convert(\n\t\t\tself,\n\t\t\tsql: AnyStr,\n\t\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._dialect.compile\n\t\t:param sql: sql to convert\n\t\t:param params: params to convert\n\t\t:return: Tuple of converted sql and params\n\t\t\"\"\"\n\t\treturn self._dialect.compile(sql=sql, params=params)\n\n\nclass SQLLiteConverter(SQLAlchemyConverter):\n\t\"\"\"\n\tConverter implementation for SQLLite\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(\"sqlite\", *args, **kwargs)\n\n\nclass", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass Converter:\n\t\"\"\"\n\tConverter class.\n\t\"\"\"\n\n\tdef __init__(self, dialect: AnyStr, db_type: AnyStr, **kwargs: Any):\n\t\t\"\"\"\n\t\tConstructor.\n\n\t\t:param dialect: SqlAlchemy dialect name\n\t\t:param db_type: Database type (mysql, postgres, etc.)\n\t\t:param kwargs: Any additional parameters\n\t\t\"\"\"\n\t\tself._dialect = dialect\n\t\tself._db_type = db_type\n\t\tself._kwargs = kwargs\n\t\tself._converter = ConverterBase(dialect, db_type, **kwargs)\n\n\tdef convert(\n\t\tself, sql: AnyStr, params: Union[Dict[Union[str, int], Any], Sequence[Any]]\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._converter.convert\n\n\t\t:param sql: sql\n\t\t:param params: sql parameters\n\t\t:return: sql, sql parameters\n\t\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tif params is None:\n\t\tparams = {}\n\n\treturn self._converter.convert(sql, params)\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass Postgres(Database):\n\t\"\"\"\n\tPostgres database interface\n\t\"\"\"\n\t_converter = PostgresConverter()\n\t_dialect = 'postgres'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself._converter = PostgresConverter()\n\t\tsuper().__init__(*args, **kwargs)\n\n\nclass MSSQL(Database):\n\t\"\"\"\n\tMSSQL database interface\n\t\"\"\"\n\t_converter = MSSQLConverter()\n\t_dialect = 'mssql'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself._converter = MSSQLConverter()\n\t\tsuper().__init__(*args, **kwargs)\n\n\nclass Oracle(Database):\n\t\"\"\"\n\tOracle database interface\n\t\"\"\"\n\t_converter = OracleConverter()\n\t_dialect = 'oracle'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself._converter = OracleConverter()\n\t\tsuper().__init__(*args, **kwargs)\n\n\nclass MySQL(Database):\n\t\"\"\"\n\tMySQL database interface\n\t\"\"\"\n\t_converter = MySQLConverter()\n\t_dia", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tif self._converter is None:\n\t\treturn sql, params\n\telse:\n\t\treturn self._converter.convert(sql, params)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass SQLConverter(ABC):\n\t\"\"\"\n\tSQLConverter is an abstract class which converts SQL queries and parameters\n\tfor a specific database engine.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.sql_format = None\n\n\t@abstractmethod\n\tdef convert(\n\t\tself, sql: AnyStr, params: Union[Dict[Union[str, int], Any], Sequence[Any]]\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._converter.convert\n\t\t\"\"\"\n\t\tpass\n\n\tdef _build_where(self, where: List[Tuple[AnyStr, Any]]) -> AnyStr:\n\t\t\"\"\"\n\t\tBuild where part of SQL query\n\t\t\"\"\"\n\t\treturn \" WHERE \" + \" AND \".join(\n\t\t\t\"%s %s ?\" % (field, self.sql_format.get_operator(operator))\n\t\t\tfor field, operator in where\n\t\t)\n\n\tdef _build_order(self, order: List[Tuple[AnyStr, Any]]) -> AnyStr:\n\t\t\"\"\"\n\t\tBuild", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tif not sql:\n\t\treturn \"\", params\n\n\tif isinstance(params, dict):\n\t\tparams = [params]\n\n\treturn self._converter.convert(sql, params)\n\n# ----------------------------------------------------------------------------------------------------------------------\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass Converter(abc.ABC):\n\t@abc.abstractmethod\n\tdef convert(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\nclass ConverterFactory(abc.ABC):\n\t@abc.abstractmethod\n\tdef create(self, dialect: str) -> Converter:\n\t\traise NotImplementedError()\n\n\nclass DefaultConverterFactory(ConverterFactory):\n\tdef create(self, dialect: str) -> Converter:\n\t\tif dialect == 'mysql':\n\t\t\treturn MySqlConverter()\n\t\telif dialect == 'postgres':\n\t\t\treturn PostgreSqlConverter()\n\t\telse:\n\t\t\traise NotImplementedError('Unknown dialect {0}'.format(dialect))\n\n\nclass MySqlConverter(Converter):\n\tdef convert(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql=sql, params=params)\n\n\nclass SQLAlchemyConverter(Converter):\n\t\"\"\"\n\tConverter implementation for SQLAlchemy\n\t\"\"\"\n\tdef __init__(self, dialect: str, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself._dialect = dialect\n\n\tdef convert(\n\t\t\tself,\n\t\t\tsql: AnyStr,\n\t\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._dialect.compile\n\t\t:param sql: sql to convert\n\t\t:param params: params to convert\n\t\t:return: Tuple of converted sql and params\n\t\t\"\"\"\n\t\treturn self._dialect.compile(sql=sql, params=params)\n\n\nclass SQLLiteConverter(SQLAlchemyConverter):\n\t\"\"\"\n\tConverter implementation for SQLLite\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(\"sqlite\", *args, **kwargs)\n\n\nclass", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass Converter:\n\t\"\"\"\n\tConverter class.\n\t\"\"\"\n\n\tdef __init__(self, dialect: AnyStr, db_type: AnyStr, **kwargs: Any):\n\t\t\"\"\"\n\t\tConstructor.\n\n\t\t:param dialect: SqlAlchemy dialect name\n\t\t:param db_type: Database type (mysql, postgres, etc.)\n\t\t:param kwargs: Any additional parameters\n\t\t\"\"\"\n\t\tself._dialect = dialect\n\t\tself._db_type = db_type\n\t\tself._kwargs = kwargs\n\t\tself._converter = ConverterBase(dialect, db_type, **kwargs)\n\n\tdef convert(\n\t\tself, sql: AnyStr, params: Union[Dict[Union[str, int], Any], Sequence[Any]]\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._converter.convert\n\n\t\t:param sql: sql\n\t\t:param params: sql parameters\n\t\t:return: sql, sql parameters\n\t\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tif params is None:\n\t\tparams = {}\n\n\treturn self._converter.convert(sql, params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\treturn self._converter.convert(sql, params)\n\n\nclass Postgres(Database):\n\t\"\"\"\n\tPostgres database interface\n\t\"\"\"\n\t_converter = PostgresConverter()\n\t_dialect = 'postgres'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself._converter = PostgresConverter()\n\t\tsuper().__init__(*args, **kwargs)\n\n\nclass MSSQL(Database):\n\t\"\"\"\n\tMSSQL database interface\n\t\"\"\"\n\t_converter = MSSQLConverter()\n\t_dialect = 'mssql'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself._converter = MSSQLConverter()\n\t\tsuper().__init__(*args, **kwargs)\n\n\nclass Oracle(Database):\n\t\"\"\"\n\tOracle database interface\n\t\"\"\"\n\t_converter = OracleConverter()\n\t_dialect = 'oracle'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself._converter = OracleConverter()\n\t\tsuper().__init__(*args, **kwargs)\n\n\nclass MySQL(Database):\n\t\"\"\"\n\tMySQL database interface\n\t\"\"\"\n\t_converter = MySQLConverter()\n\t_dia", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tif self._converter is None:\n\t\treturn sql, params\n\telse:\n\t\treturn self._converter.convert(sql, params)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tif len(many_params) == 0:\n\t\treturn sql, many_params\n\tif not isinstance(many_params[0], (list, tuple, dict)):\n\t\traise TypeError(\"many_params must be an array of dictionaries\")\n\tif isinstance(many_params[0], list) and isinstance(many_params[0][0], (list, tuple)):\n\t\traise TypeError(\"Can't handle nested list/tuple tuples\")\n\tif isinstance(many_params[0], list):\n\t\tmany_params = [dict(zip(many_params[0], x)) for x in many_params]\n\telse:\n\t\tmany_params = [dict(zip(many_params[0], x)) for x in zip(*many_params)]\n\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tif self._converter.many and self._converter.many_str:\n\t\treturn self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tif not isinstance(many_params, (list, tuple)):\n\t\traise ValueError('many_params must be a sequence')\n\tif not many_params:\n\t\traise ValueError('many_params must be a non-empty sequence')\n\n\tif isinstance(many_params[0], dict):\n\t\tconverted_many_params = [\n\t\t\tself._converter.convert_many(sql, params)\n\t\t\tfor params in many_params\n\t\t]\n\telse:\n\t\tconverted_many_params = [\n\t\t\tself._converter.convert(sql, params)\n\t\t\tfor params in many_params\n\t\t]\n\n\treturn sql, converted_many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params, self._quote_char)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tif len(many_params) == 0:\n\t\treturn sql, many_params\n\tif not isinstance(many_params[0], (list, tuple, dict)):\n\t\traise TypeError(\"many_params must be an array of dictionaries\")\n\tif isinstance(many_params[0], list) and isinstance(many_params[0][0], (list, tuple)):\n\t\traise TypeError(\"Can't handle nested list/tuple tuples\")\n\tif isinstance(many_params[0], list):\n\t\tmany_params = [dict(zip(many_params[0], x)) for x in many_params]\n\telse:\n\t\tmany_params = [dict(zip(many_params[0], x)) for x in zip(*many_params)]\n\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tif self._converter.many and self._converter.many_str:\n\t\treturn self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tif not isinstance(many_params, (list, tuple)):\n\t\traise ValueError('many_params must be a sequence')\n\tif not many_params:\n\t\traise ValueError('many_params must be a non-empty sequence')\n\n\tif isinstance(many_params[0], dict):\n\t\tconverted_many_params = [\n\t\t\tself._converter.convert_many(sql, params)\n\t\t\tfor params in many_params\n\t\t]\n\telse:\n\t\tconverted_many_params = [\n\t\t\tself._converter.convert(sql, params)\n\t\t\tfor params in many_params\n\t\t]\n\n\treturn sql, converted_many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params, self._quote_char)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == '/':\n\t\treturn True\n\telse:\n\t\treturn self.exists(path)\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == self.root or path.startswith(self.root + \"/\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif not path:\n\t\treturn True\n\tif os.path.exists(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn self._is_ocfl_object(path)\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\treturn path in self.root or path == ''\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\treturn (path == self.root or path == self.root + '/')\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\treturn True\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif not self.exists(path):\n\t\treturn False\n\telif path == '/':\n\t\treturn True\n\telif self.is_dir(path):\n\t\treturn self.exists(path+'/fcr-ocfl-1.1.json')\n\telse:\n\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == \"/\":\n\t\treturn True\n\ttry:\n\t\tif not self.fs.exists(path):\n\t\t\traise FileNotFoundError\n\t\twith self.fs.open(path + \"/\" + OCFL_VALID_FILE, \"r\") as f:\n\t\t\tif f.readline() == OCFL_VALID_FILE_CONTENTS:\n\t\t\t\treturn True\n\texcept FileNotFoundError:\n\t\treturn False\n\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif not self.is_ocfl_object(path):\n\t\tself.logger.error(\"Path is not OCFL: {}\".format(path))\n\t\treturn False\n\n\troot = self.get_ocfl_root(path)\n\tif not self.validate_root(root):\n\t\tself.logger.error(\"Invalid root: {}\".format(path))\n\t\treturn False\n\n\tif self.validate_dir(root, \"inventory\"):\n\t\tself.logger.error(\"Invalid inventory dir: {}\".format(path))\n\t\treturn False\n\n\tif self.validate_dir(root, \"fixity\"):\n\t\tself.logger.error(\"Invalid fixity dir: {}\".format(path))\n\t\treturn False\n\n\tif self.validate_dir(root, \"content\"):\n\t\tself.logger.error(\"Invalid content dir: {}\".format(path))\n\t\treturn False\n\n\tfor subdir in self.list_dir(root):\n\t\tif subdir in [\"inventory\", \"fixity\", \"content\"]:\n\t\t\tcontinue\n\n\t\tif not self.validate_dir(root, subdir):\n\t\t\tself.logger.error(\"Invalid subdir: {}\".format(path))\n\t\t\treturn False\n\n\t", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == self.root:\n\t\treturn True\n\telif path.startswith(self.root):\n\t\treturn self.pyfs.exists(path)\n\telse:\n\t\treturn False\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == '/':\n\t\treturn True\n\telse:\n\t\treturn self.exists(path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == self.root or path.startswith(self.root + \"/\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif not path:\n\t\treturn True\n\tif os.path.exists(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn self._is_ocfl_object(path)\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\treturn path in self.root or path == ''\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\treturn (path == self.root or path == self.root + '/')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif not self.exists(path):\n\t\treturn False\n\telif path == '/':\n\t\treturn True\n\telif self.is_dir(path):\n\t\treturn self.exists(path+'/fcr-ocfl-1.1.json')\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == \"/\":\n\t\treturn True\n\ttry:\n\t\tif not self.fs.exists(path):\n\t\t\traise FileNotFoundError\n\t\twith self.fs.open(path + \"/\" + OCFL_VALID_FILE, \"r\") as f:\n\t\t\tif f.readline() == OCFL_VALID_FILE_CONTENTS:\n\t\t\t\treturn True\n\texcept FileNotFoundError:\n\t\treturn False\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif not self.is_ocfl_object(path):\n\t\tself.logger.error(\"Path is not OCFL: {}\".format(path))\n\t\treturn False\n\n\troot = self.get_ocfl_root(path)\n\tif not self.validate_root(root):\n\t\tself.logger.error(\"Invalid root: {}\".format(path))\n\t\treturn False\n\n\tif self.validate_dir(root, \"inventory\"):\n\t\tself.logger.error(\"Invalid inventory dir: {}\".format(path))\n\t\treturn False\n\n\tif self.validate_dir(root, \"fixity\"):\n\t\tself.logger.error(\"Invalid fixity dir: {}\".format(path))\n\t\treturn False\n\n\tif self.validate_dir(root, \"content\"):\n\t\tself.logger.error(\"Invalid content dir: {}\".format(path))\n\t\treturn False\n\n\tfor subdir in self.list_dir(root):\n\t\tif subdir in [\"inventory\", \"fixity\", \"content\"]:\n\t\t\tcontinue\n\n\t\tif not self.validate_dir(root, subdir):\n\t\t\tself.logger.error(\"Invalid subdir: {}\".format(path))\n\t\t\treturn False\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tif path == self.root:\n\t\treturn True\n\telif path.startswith(self.root):\n\t\treturn self.pyfs.exists(path)\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif self.messages:\n\t\treturn ''.join([prefix + str(x) for x in sorted(self.messages)])\n\telse:\n\t\treturn ''\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstr_list = []\n\tfor each_msg in sorted(self.messages):\n\t\tstr_list.append(prefix + each_msg)\n\t# str_list is a list of string, return '\\n'.join(str_list)\n\treturn '\\n'.join(str_list)\n\n\n# \u7b80\u5355\u6613\u61c2\u7684\u6d4b\u8bd5\nif __name__ == '__main__':\n\tmsgs = [\n\t'Hi',\n\t'Hello',\n\t'Hi Hello'\n\t]\n\n\t# \u521b\u5efa\u4e00\u4e2aMessageLogger\u7684\u5bf9\u8c61\n\tmsglog = MessageLogger()\n\t# \u6dfb\u52a0msg\u5230msglog.message\u4e2d\n\tfor each_msg in msgs:\n\t\tmsglog.add(each_msg)\n\n\t# \u6253\u5370msglog.message\u4e2d\u7684\u5185\u5bb9\n\tprint(msglog.status())\n\n\t# \u6e05\u7a7amsglog.messages\n\tmsglog.clear()\n\n\t# \u518d\u6b21\u6253\u5370msglog.message\u4e2d\u7684\u5185\u5bb9\uff0c\u5e94\u8be5\u662f\u7a7a\u7684\n\tprint(msglog.status())\n\n\t# \u518d\u6b21\u6dfb\u52a0msg\u5230msglog.message\u4e2d\n\tfor each_msg in msgs:\n\t\tmsglog.add(each_msg)\n\n\t# \u6253", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif self.messages:\n\t\treturn prefix + ''.join(str(msg) for msg in self.messages)\n\telse:\n\t\treturn ''\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = []\n\tfor message in self.messages:\n\t\tresult.append(prefix + message)\n\treturn '\\n'.join(result)\n\n\t\ndef status_dict(self, prefix=''):\n\t\"\"\"\n\tReturn a dict with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = {}\n\tfor message in self.messages:\n\t\tresult[prefix + message] = 1\n\treturn result\n\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif not self.messages:\n\t\treturn ''\n\tret = ''\n\tfor x in self.messages:\n\t\tret += prefix + str(x) + '\\n'\n\treturn ret\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\treturn '\\n'.join(map(lambda x: prefix + x, self.messages))\n\n\nclass Task(object):\n\t\"\"\"\n\tA task is a set of messages to be sent to a server. A task is used to control how many messages are sent at a time.\n\t\"\"\"\n\tdef __init__(self, messages, **kwargs):\n\t\t\"\"\"\n\t\tInitialize a task.\n\n\t\t:param messages: The messages to be sent.\n\t\t:type messages: list of str.\n\t\t:param kwargs: Other parameters.\n\t\t\"\"\"\n\t\tself.messages = messages\n\t\tself.kwargs = kwargs\n\t\tself.send_counter = 0\n\t\tself.sent_counter = 0\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturn the iterator for the messages of this task.\n\n\t\t:return: The iterator.\n\t\t:rtype: list iterator.\n\t\t\"\"\"\n\t\treturn iter(self.messages)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn the number of messages in this task.\n\n\t\t:return: The number of messages.\n\t\t:rtype: int.\n\t\t\"\"\"\n\t\treturn len(self.messages)\n\n\tdef", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\treturn '\\n'.join([prefix + str(elem) for elem in sorted(self.messages)])\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tmessage_str = prefix + \"status_str\\n\"\n\tfor message in self.status_str_sorted():\n\t\tmessage_str += prefix + message + '\\n'\n\treturn message_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult_str = ''\n\tif hasattr(self, 'messages'):\n\t\tfor message in self.messages:\n\t\t\tif hasattr(message, 'status_str'):\n\t\t\t\tresult_str += message.status_str(prefix)\n\t\t\telse:\n\t\t\t\tresult_str += str(message)\n\treturn result_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif self.messages is None:\n\t\treturn ''\n\telse:\n\t\treturn '\\n'.join([prefix + x for x in self.messages])\n\n\nclass Logging(object):\n\t\"\"\"\n\tA class for logging.\n\t\"\"\"\n\tdef __init__(self, messages=None):\n\t\t\"\"\"\n\t\tmessages should be a list of strings.\n\t\t\"\"\"\n\t\tself.messages = messages or []\n\t\n\tdef __repr__(self):\n\t\treturn self.status_str()\n\t\n\tdef add(self, msg):\n\t\t\"\"\"\n\t\tAdd a message to self.messages.\n\t\t\"\"\"\n\t\tself.messages.append(msg)\n\t\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear self.messages.\n\t\t\"\"\"\n\t\tself.messages = []\n\t\n\tdef status(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\tif self.messages is None:\n\t\t\treturn ''\n\t\telse:\n\t\t\treturn '\\n'.join(self.messages)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif self.messages:\n\t\treturn ''.join([prefix + str(x) for x in sorted(self.messages)])\n\telse:\n\t\treturn ''\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstr_list = []\n\tfor each_msg in sorted(self.messages):\n\t\tstr_list.append(prefix + each_msg)\n\t# str_list is a list of string, return '\\n'.join(str_list)\n\treturn '\\n'.join(str_list)\n\n\n# \u7b80\u5355\u6613\u61c2\u7684\u6d4b\u8bd5\nif __name__ == '__main__':\n\tmsgs = [\n\t'Hi',\n\t'Hello',\n\t'Hi Hello'\n\t]\n\n\t# \u521b\u5efa\u4e00\u4e2aMessageLogger\u7684\u5bf9\u8c61\n\tmsglog = MessageLogger()\n\t# \u6dfb\u52a0msg\u5230msglog.message\u4e2d\n\tfor each_msg in msgs:\n\t\tmsglog.add(each_msg)\n\n\t# \u6253\u5370msglog.message\u4e2d\u7684\u5185\u5bb9\n\tprint(msglog.status())\n\n\t# \u6e05\u7a7amsglog.messages\n\tmsglog.clear()\n\n\t# \u518d\u6b21\u6253\u5370msglog.message\u4e2d\u7684\u5185\u5bb9\uff0c\u5e94\u8be5\u662f\u7a7a\u7684\n\tprint(msglog.status())\n\n\t# \u518d\u6b21\u6dfb\u52a0msg\u5230msglog.message\u4e2d\n\tfor each_msg in msgs:\n\t\tmsglog.add(each_msg)\n\n\t# \u6253", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif self.messages:\n\t\treturn prefix + ''.join(str(msg) for msg in self.messages)\n\telse:\n\t\treturn ''\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = []\n\tfor message in self.messages:\n\t\tresult.append(prefix + message)\n\treturn '\\n'.join(result)\n\n\t\ndef status_dict(self, prefix=''):\n\t\"\"\"\n\tReturn a dict with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = {}\n\tfor message in self.messages:\n\t\tresult[prefix + message] = 1\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif not self.messages:\n\t\treturn ''\n\tret = ''\n\tfor x in self.messages:\n\t\tret += prefix + str(x) + '\\n'\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\treturn '\\n'.join(map(lambda x: prefix + x, self.messages))\n\n\nclass Task(object):\n\t\"\"\"\n\tA task is a set of messages to be sent to a server. A task is used to control how many messages are sent at a time.\n\t\"\"\"\n\tdef __init__(self, messages, **kwargs):\n\t\t\"\"\"\n\t\tInitialize a task.\n\n\t\t:param messages: The messages to be sent.\n\t\t:type messages: list of str.\n\t\t:param kwargs: Other parameters.\n\t\t\"\"\"\n\t\tself.messages = messages\n\t\tself.kwargs = kwargs\n\t\tself.send_counter = 0\n\t\tself.sent_counter = 0\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturn the iterator for the messages of this task.\n\n\t\t:return: The iterator.\n\t\t:rtype: list iterator.\n\t\t\"\"\"\n\t\treturn iter(self.messages)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn the number of messages in this task.\n\n\t\t:return: The number of messages.\n\t\t:rtype: int.\n\t\t\"\"\"\n\t\treturn len(self.messages)\n\n\tdef", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\treturn '\\n'.join([prefix + str(elem) for elem in sorted(self.messages)])\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tmessage_str = prefix + \"status_str\\n\"\n\tfor message in self.status_str_sorted():\n\t\tmessage_str += prefix + message + '\\n'\n\treturn message_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult_str = ''\n\tif hasattr(self, 'messages'):\n\t\tfor message in self.messages:\n\t\t\tif hasattr(message, 'status_str'):\n\t\t\t\tresult_str += message.status_str(prefix)\n\t\t\telse:\n\t\t\t\tresult_str += str(message)\n\treturn result_str\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tif self.messages is None:\n\t\treturn ''\n\telse:\n\t\treturn '\\n'.join([prefix + x for x in self.messages])\n\n\nclass Logging(object):\n\t\"\"\"\n\tA class for logging.\n\t\"\"\"\n\tdef __init__(self, messages=None):\n\t\t\"\"\"\n\t\tmessages should be a list of strings.\n\t\t\"\"\"\n\t\tself.messages = messages or []\n\t\n\tdef __repr__(self):\n\t\treturn self.status_str()\n\t\n\tdef add(self, msg):\n\t\t\"\"\"\n\t\tAdd a message to self.messages.\n\t\t\"\"\"\n\t\tself.messages.append(msg)\n\t\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear self.messages.\n\t\t\"\"\"\n\t\tself.messages = []\n\t\n\tdef status(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\tif self.messages is None:\n\t\t\treturn ''\n\t\telse:\n\t\t\treturn '\\n'.join(self.messages)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.log.status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str())\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.log.status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str())\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn self.log.status_str(prefix=prefix)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.log.status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tif self.log.status_str:\n\t\treturn prefix + self.log.status_str\n\telse:\n\t\treturn prefix + 'No message available.'\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tlog_str = self.log.status_str()\n\tif log_str:\n\t\tlog_str = ' ' + log_str\n\telse:\n\t\tlog_str = ''\n\treturn '%s%s%s%s' % (prefix, self.status, log_str, self.name)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.log.status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.log.status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn self.log.status_str(prefix=prefix)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.log.status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\treturn '%s%s' % (prefix, self.log.status_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tif self.log.status_str:\n\t\treturn prefix + self.log.status_str\n\telse:\n\t\treturn prefix + 'No message available.'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tlog_str = self.log.status_str()\n\tif log_str:\n\t\tlog_str = ' ' + log_str\n\telse:\n\t\tlog_str = ''\n\treturn '%s%s%s%s' % (prefix, self.status, log_str, self.name)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t# TODO: This is an example of a method - replace this comment with an explanation of what it does\n\tpass\n\n\t\ndef get_type(self):\n\t\"\"\"\n\tReturns the type of the inventory\n\t\"\"\"\n\tif self.type is None:\n\t\treturn self.spec_version\n\n\treturn self.type\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif extract_spec_version and inventory.get(\"type\") and inventory.get(\"type\") in self.valid_spec_versions:\n\t\tself.spec_version = inventory.get(\"type\")\n\n\tself.validate_structure(inventory)\n\tself.validate_content(inventory)\n\tself.validate_metadata(inventory)\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif extract_spec_version:\n\t\tif self._validate_extract_version(inventory):\n\t\t\tif self.spec_version == '2.1':\n\t\t\t\tself._validate_spec_version_2_1(inventory)\n\t\t\telif self.spec_version == '2.0':\n\t\t\t\tself._validate_spec_version_2_0(inventory)\n\t\t\telse:\n\t\t\t\traise ValueError(\"%s is not a valid specification version\" % self.spec_version)\n\t\telse:\n\t\t\tif self.spec_version == '2.1':\n\t\t\t\tself._validate_spec_version_2_1(inventory)\n\t\t\telif self.spec_version == '2.0':\n\t\t\t\tself._validate_spec_version_2_0(inventory)\n\t\t\telse:\n\t\t\t\traise ValueError(\"%s is not a valid specification version\" % self.spec_version)\n\telse:\n\t\tif self.spec_version == '2.1':\n\t\t\tself._validate_spec_version_2_1(inventory)\n\t\telif self.spec_version == '2.0':\n\t\t\tself._validate_spec_version_2_0(", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t# 1.1\n\tif not isinstance(inventory, dict) or not self.is_valid_inventory(inventory):\n\t\treturn False\n\t# 1.2\n\tif extract_spec_version:\n\t\ttype_value = inventory.get('type', None)\n\t\tif not type_value:\n\t\t\tinventory_spec_version = self.spec_version\n\t\telse:\n\t\t\tinventory_spec_version = self.get_spec_version(type_value)\n\t\tif not inventory_spec_version:\n\t\t\treturn False\n\t\tself.spec_version = inventory_spec_version\n\t# 1.3\n\tif not self.is_valid_spec_version():\n\t\treturn False\n\t# 1.4\n\tif not self.is_valid_id(inventory.get('id', None)):\n\t\treturn False\n\t# 1.5\n\tif not self.is_valid_name(inventory.get('name', None)):\n\t\treturn False\n\t# 1.6\n\tif not self.is_valid_description(inventory.get('description', None)):\n\t\treturn False\n\t# 1.7\n\tif not self.is_valid_toplevel_", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t# (D)\n\tif extract_spec_version:\n\t\tif \"type\" in inventory:\n\t\t\tif inventory[\"type\"] in self.spec_versions:\n\t\t\t\tself.spec_version = self.spec_versions[inventory[\"type\"]]\n\t\t\telse:\n\t\t\t\traise ValueError(\"{} is not a valid type value for inventory.\".format(inventory[\"type\"]))\n\t# Start of validation\n\tfor item in inventory:\n\t\tif item == \"type\":\n\t\t\tpass\n\t\telif item == \"id\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{} is not a string type.\".format(item))\n\t\telif item == \"name\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{} is not a string type.\".format(item))\n\t\telif item == \"description\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{} is not a string type.\".format(item))\n\t\telif item == \"version\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{}", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tself.logger.info(\"Validating inventory\")\n\tself.logger.debug(\"Inventory: %s\", inventory)\n\tif extract_spec_version:\n\t\tif \"type\" in inventory:\n\t\t\tself.spec_version = self.get_spec_version(inventory[\"type\"])\n\t\telse:\n\t\t\tself.spec_version = self.get_spec_version(self.spec_version)\n\t# Check that the inventory has a valid type\n\tif \"type\" not in inventory:\n\t\tself.logger.error(\"Inventory missing required field: type\")\n\t\tself.validation_errors.append(\"Inventory missing required field: type\")\n\telif not self.is_valid_type(inventory[\"type\"]):\n\t\tself.logger.error(\"Inventory type not supported: %s\", inventory[\"type\"])\n\t\tself.validation_errors.append(\"Inventory type not supported: \" + inventory[\"type\"])\n\telse:\n\t\tself.logger.info(\"Found inventory type: %s\", inventory[\"type\"])\n\t# Check that the inventory has a valid id\n\tif \"id\" not in inventory:\n\t\tself.logger.error(\"Inventory missing required field: id\")\n\t\tself.validation_errors.append", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif extract_spec_version:\n\t\tself.spec_version = inventory.get_spec_version()\n\t\tif self.spec_version is None:\n\t\t\traise ValidationException(\"Inventory missing 'type' attribute and no default version set.\")\n\t\tif self.spec_version not in self.spec_versions.keys():\n\t\t\traise ValidationException(\"Inventory 'type' attribute '{}' is not a valid version.\".format(self.spec_version))\n\t\tif self.spec_version not in self.spec_version_supported:\n\t\t\traise ValidationException(\"Inventory 'type' attribute '{}' is not yet supported.\".format(self.spec_version))\n\tif not isinstance(inventory, Inventory):\n\t\traise ValidationException(\"Inventory is not of type Inventory.\")\n\tif not isinstance(inventory.id, str):\n\t\traise ValidationException(\"Inventory 'id' attribute is not of type string.\")\n\tif not isinstance(inventory.name, str):\n\t\traise ValidationException(\"Inventory 'name' attribute is not of type string.\")\n\tif not isinstance(inventory.description, str):\n\t\traise ValidationException(\"Inventory 'description' attribute is not of type string.\")\n\tif not isinstance", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\treturn self._validate_inventory(inventory, extract_spec_version)\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif inventory is None:\n\t\traise ValueError('inventory is None')\n\tspec_version = None\n\tif extract_spec_version:\n\t\tspec_version = self.extract_spec_version(inventory)\n\t\tif not spec_version:\n\t\t\tspec_version = self.spec_version\n\tif not spec_version:\n\t\traise ValueError('spec_version is None')\n\tif not self.is_valid_spec_version(spec_version):\n\t\traise ValueError('spec_version is not valid')\n\tinventory_hash = hashlib.sha1(json.dumps(inventory, sort_keys=True).encode('utf-8')).hexdigest()\n\tinventory_hash_list = [inventory_hash, spec_version]\n\tfor x in self.spec_version_hash_list:\n\t\tif x in inventory_hash_list:\n\t\t\traise ValueError('inventory_hash_list is not valid')\n\tif not self.is_valid_inventory(inventory):\n\t\traise ValueError('inventory is not valid')\n\tif not self.is_valid_inventory_hash(inventory_hash):\n\t\traise ValueError('inventory_hash is not valid')\n\tif not self.", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\traise NotImplementedError\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t# TODO: This is an example of a method - replace this comment with an explanation of what it does\n\tpass\n\n\t\ndef get_type(self):\n\t\"\"\"\n\tReturns the type of the inventory\n\t\"\"\"\n\tif self.type is None:\n\t\treturn self.spec_version\n\n\treturn self.type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif extract_spec_version and inventory.get(\"type\") and inventory.get(\"type\") in self.valid_spec_versions:\n\t\tself.spec_version = inventory.get(\"type\")\n\n\tself.validate_structure(inventory)\n\tself.validate_content(inventory)\n\tself.validate_metadata(inventory)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif extract_spec_version:\n\t\tif self._validate_extract_version(inventory):\n\t\t\tif self.spec_version == '2.1':\n\t\t\t\tself._validate_spec_version_2_1(inventory)\n\t\t\telif self.spec_version == '2.0':\n\t\t\t\tself._validate_spec_version_2_0(inventory)\n\t\t\telse:\n\t\t\t\traise ValueError(\"%s is not a valid specification version\" % self.spec_version)\n\t\telse:\n\t\t\tif self.spec_version == '2.1':\n\t\t\t\tself._validate_spec_version_2_1(inventory)\n\t\t\telif self.spec_version == '2.0':\n\t\t\t\tself._validate_spec_version_2_0(inventory)\n\t\t\telse:\n\t\t\t\traise ValueError(\"%s is not a valid specification version\" % self.spec_version)\n\telse:\n\t\tif self.spec_version == '2.1':\n\t\t\tself._validate_spec_version_2_1(inventory)\n\t\telif self.spec_version == '2.0':\n\t\t\tself._validate_spec_version_2_0(", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t# 1.1\n\tif not isinstance(inventory, dict) or not self.is_valid_inventory(inventory):\n\t\treturn False\n\t# 1.2\n\tif extract_spec_version:\n\t\ttype_value = inventory.get('type', None)\n\t\tif not type_value:\n\t\t\tinventory_spec_version = self.spec_version\n\t\telse:\n\t\t\tinventory_spec_version = self.get_spec_version(type_value)\n\t\tif not inventory_spec_version:\n\t\t\treturn False\n\t\tself.spec_version = inventory_spec_version\n\t# 1.3\n\tif not self.is_valid_spec_version():\n\t\treturn False\n\t# 1.4\n\tif not self.is_valid_id(inventory.get('id', None)):\n\t\treturn False\n\t# 1.5\n\tif not self.is_valid_name(inventory.get('name', None)):\n\t\treturn False\n\t# 1.6\n\tif not self.is_valid_description(inventory.get('description', None)):\n\t\treturn False\n\t# 1.7\n\tif not self.is_valid_toplevel_", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t# (D)\n\tif extract_spec_version:\n\t\tif \"type\" in inventory:\n\t\t\tif inventory[\"type\"] in self.spec_versions:\n\t\t\t\tself.spec_version = self.spec_versions[inventory[\"type\"]]\n\t\t\telse:\n\t\t\t\traise ValueError(\"{} is not a valid type value for inventory.\".format(inventory[\"type\"]))\n\t# Start of validation\n\tfor item in inventory:\n\t\tif item == \"type\":\n\t\t\tpass\n\t\telif item == \"id\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{} is not a string type.\".format(item))\n\t\telif item == \"name\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{} is not a string type.\".format(item))\n\t\telif item == \"description\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{} is not a string type.\".format(item))\n\t\telif item == \"version\":\n\t\t\tif not isinstance(inventory[item], str):\n\t\t\t\traise ValueError(\"{}", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tself.logger.info(\"Validating inventory\")\n\tself.logger.debug(\"Inventory: %s\", inventory)\n\tif extract_spec_version:\n\t\tif \"type\" in inventory:\n\t\t\tself.spec_version = self.get_spec_version(inventory[\"type\"])\n\t\telse:\n\t\t\tself.spec_version = self.get_spec_version(self.spec_version)\n\t# Check that the inventory has a valid type\n\tif \"type\" not in inventory:\n\t\tself.logger.error(\"Inventory missing required field: type\")\n\t\tself.validation_errors.append(\"Inventory missing required field: type\")\n\telif not self.is_valid_type(inventory[\"type\"]):\n\t\tself.logger.error(\"Inventory type not supported: %s\", inventory[\"type\"])\n\t\tself.validation_errors.append(\"Inventory type not supported: \" + inventory[\"type\"])\n\telse:\n\t\tself.logger.info(\"Found inventory type: %s\", inventory[\"type\"])\n\t# Check that the inventory has a valid id\n\tif \"id\" not in inventory:\n\t\tself.logger.error(\"Inventory missing required field: id\")\n\t\tself.validation_errors.append", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif extract_spec_version:\n\t\tself.spec_version = inventory.get_spec_version()\n\t\tif self.spec_version is None:\n\t\t\traise ValidationException(\"Inventory missing 'type' attribute and no default version set.\")\n\t\tif self.spec_version not in self.spec_versions.keys():\n\t\t\traise ValidationException(\"Inventory 'type' attribute '{}' is not a valid version.\".format(self.spec_version))\n\t\tif self.spec_version not in self.spec_version_supported:\n\t\t\traise ValidationException(\"Inventory 'type' attribute '{}' is not yet supported.\".format(self.spec_version))\n\tif not isinstance(inventory, Inventory):\n\t\traise ValidationException(\"Inventory is not of type Inventory.\")\n\tif not isinstance(inventory.id, str):\n\t\traise ValidationException(\"Inventory 'id' attribute is not of type string.\")\n\tif not isinstance(inventory.name, str):\n\t\traise ValidationException(\"Inventory 'name' attribute is not of type string.\")\n\tif not isinstance(inventory.description, str):\n\t\traise ValidationException(\"Inventory 'description' attribute is not of type string.\")\n\tif not isinstance", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\treturn self._validate_inventory(inventory, extract_spec_version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tif inventory is None:\n\t\traise ValueError('inventory is None')\n\tspec_version = None\n\tif extract_spec_version:\n\t\tspec_version = self.extract_spec_version(inventory)\n\t\tif not spec_version:\n\t\t\tspec_version = self.spec_version\n\tif not spec_version:\n\t\traise ValueError('spec_version is None')\n\tif not self.is_valid_spec_version(spec_version):\n\t\traise ValueError('spec_version is not valid')\n\tinventory_hash = hashlib.sha1(json.dumps(inventory, sort_keys=True).encode('utf-8')).hexdigest()\n\tinventory_hash_list = [inventory_hash, spec_version]\n\tfor x in self.spec_version_hash_list:\n\t\tif x in inventory_hash_list:\n\t\t\traise ValueError('inventory_hash_list is not valid')\n\tif not self.is_valid_inventory(inventory):\n\t\traise ValueError('inventory is not valid')\n\tif not self.is_valid_inventory_hash(inventory_hash):\n\t\traise ValueError('inventory_hash is not valid')\n\tif not self.", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_needed = {}\n\tfor manifest_file in manifest_files:\n\t\tfor digest in manifest_file.digests:\n\t\t\tdigest_type = digest.type\n\t\t\tif digest_type not in digests_needed:\n\t\t\t\tdigests_needed[digest_type] = set()\n\t\t\tdigests_needed[digest_type].add(digest.hash)\n\n\tfor digest_type in digests_needed:\n\t\tfor digest in digests_needed[digest_type]:\n\t\t\tif digest not in digests_used[digest_type]:\n\t\t\t\treturn self.error(\"Missing digest for %s: %s\" % (digest_type, digest))\n\n\treturn True\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor file in manifest_files:\n\t\tif file.is_image_manifest:\n\t\t\tfor child in file.children:\n\t\t\t\tif child.digest is not None and child.digest not in digests_used:\n\t\t\t\t\tself.error(\"Digest %s is not used in this image\" % child.digest)\n\t\t\t\tif child.digest is not None and child.digest in digests_used and not child.is_used_in_image:\n\t\t\t\t\tself.error(\"Digest %s is used in this image, but not in the manifest\" % child.digest)\n\t\telse:\n\t\t\tif file.digest is not None and file.digest not in digests_used:\n\t\t\t\tself.error(\"Digest %s is not used in this image\" % file.digest)\n\t\t\tif file.digest is not None and file.digest in digests_used and not file.is_used_in_image:\n\t\t\t\tself.error(\"Digest %s is used in this image, but not in the manifest\" % file.digest)\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tprint(\"Checking digests in \" + manifest_file.name)\n\t\tfh = open(manifest_file.name, \"r\")\n\t\tdata = fh.read()\n\t\tfh.close()\n\t\tfor digest in digests_used:\n\t\t\tif not self.check_digest_present_and_used(data, digest):\n\t\t\t\tself.error(\"Digest '{}' is not present in {}\".format(digest, manifest_file.name))\n\t\t\t\treturn False\n\t\t\tif not self.check_digest_present_and_used(data, digest + \".sha256\"):\n\t\t\t\tself.error(\"Digest '{}' is not present in {}\".format(digest + \".sha256\", manifest_file.name))\n\t\t\t\treturn False\n\treturn True\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif not digest in manifest_files:\n\t\t\tself.error(\"Digest \" + digest + \" is needed by the manifest, but not used.\")\n\t\telif not manifest_files[digest]:\n\t\t\tself.error(\"Digest \" + digest + \" is used in the manifest, but not present.\")\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif digest not in manifest_files:\n\t\t\treturn self.error(\"Missing digest: \" + digest)\n\n\tfor file in manifest_files:\n\t\tif not digests_used.get(file):\n\t\t\treturn self.error(\"Unused digest: \" + file)\n\nclass ManifestChecker(object):\n\t\"\"\"\n\tChecks a manifest file.\n\t\"\"\"\n\tdef __init__(self, file, manifest):\n\t\tself.file = file\n\t\tself.manifest = manifest\n\n\tdef print_error(self, message):\n\t\tprint(\"Error: \" + self.file + \": \" + message)\n\n\tdef error(self, message):\n\t\tself.print_error(message)\n\t\treturn False\n\n\tdef check_file_exists(self, file):\n\t\tif not os.path.exists(file):\n\t\t\treturn self.error(\"File does not exist: \" + file)\n\t\treturn True\n\n\tdef check_file_is_not_empty(self, file):\n\t\tfile_size = os.path.getsize(file)\n\t\tif not file_size:\n\t\t\treturn self.error(\"File is empty: \" +", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_files = [os.path.basename(manifest_file) for manifest_file in manifest_files]\n\tfor digest_used in digests_used:\n\t\tif digest_used not in manifest_files:\n\t\t\tself.error(\"Digest used in manifest but not present: \" + digest_used)\n\n\tfor manifest_file in manifest_files:\n\t\tif not manifest_file in digests_used:\n\t\t\tself.error(\"Digest present in manifest but not used: \" + manifest_file)\n\n\treturn True\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = set()\n\tfor manifest_file in manifest_files:\n\t\tfor digest in self.get_digests_from_manifest(manifest_file):\n\t\t\tdigests_present.add(digest)\n\n\tfor digest in digests_used:\n\t\tif digest not in digests_present:\n\t\t\tself.error(\"Digest {0} in use, but not present in any manifest\".format(digest))\n\n\treturn True\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_file.digests:\n\t\t\t\tself.error(\"digest %s missing from manifest file %s\" % (digest, manifest_file.path))\n\t\t\telse:\n\t\t\t\tif not manifest_file.digests[digest]:\n\t\t\t\t\tself.error(\"digest %s not used in manifest file %s\" % (digest, manifest_file.path))\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif digest not in manifest_files:\n\t\t\tself.error(\"Digest %s is not present in manifest\" % digest)\n\t\t\treturn\n\t\tif not manifest_files[digest].digest_used:\n\t\t\tself.error(\"Digest %s is present in manifest but is not used\" % digest)\n\t\t\treturn\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest_file in manifest_files:\n\t\tif digest_file.digest not in digests_used:\n\t\t\tself.error(\"Digest %s in manifest.xml is not used\" % digest_file.digest)\n\t\t\treturn False\n\t\tif digest_file.digest in self.digests_present:\n\t\t\tself.digests_present[digest_file.digest].append(digest_file)\n\t\telse:\n\t\t\tself.digests_present[digest_file.digest] = [digest_file]\n\treturn True\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_needed = {}\n\tfor manifest_file in manifest_files:\n\t\tfor digest in manifest_file.digests:\n\t\t\tdigest_type = digest.type\n\t\t\tif digest_type not in digests_needed:\n\t\t\t\tdigests_needed[digest_type] = set()\n\t\t\tdigests_needed[digest_type].add(digest.hash)\n\n\tfor digest_type in digests_needed:\n\t\tfor digest in digests_needed[digest_type]:\n\t\t\tif digest not in digests_used[digest_type]:\n\t\t\t\treturn self.error(\"Missing digest for %s: %s\" % (digest_type, digest))\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor file in manifest_files:\n\t\tif file.is_image_manifest:\n\t\t\tfor child in file.children:\n\t\t\t\tif child.digest is not None and child.digest not in digests_used:\n\t\t\t\t\tself.error(\"Digest %s is not used in this image\" % child.digest)\n\t\t\t\tif child.digest is not None and child.digest in digests_used and not child.is_used_in_image:\n\t\t\t\t\tself.error(\"Digest %s is used in this image, but not in the manifest\" % child.digest)\n\t\telse:\n\t\t\tif file.digest is not None and file.digest not in digests_used:\n\t\t\t\tself.error(\"Digest %s is not used in this image\" % file.digest)\n\t\t\tif file.digest is not None and file.digest in digests_used and not file.is_used_in_image:\n\t\t\t\tself.error(\"Digest %s is used in this image, but not in the manifest\" % file.digest)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tprint(\"Checking digests in \" + manifest_file.name)\n\t\tfh = open(manifest_file.name, \"r\")\n\t\tdata = fh.read()\n\t\tfh.close()\n\t\tfor digest in digests_used:\n\t\t\tif not self.check_digest_present_and_used(data, digest):\n\t\t\t\tself.error(\"Digest '{}' is not present in {}\".format(digest, manifest_file.name))\n\t\t\t\treturn False\n\t\t\tif not self.check_digest_present_and_used(data, digest + \".sha256\"):\n\t\t\t\tself.error(\"Digest '{}' is not present in {}\".format(digest + \".sha256\", manifest_file.name))\n\t\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif not digest in manifest_files:\n\t\t\tself.error(\"Digest \" + digest + \" is needed by the manifest, but not used.\")\n\t\telif not manifest_files[digest]:\n\t\t\tself.error(\"Digest \" + digest + \" is used in the manifest, but not present.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif digest not in manifest_files:\n\t\t\treturn self.error(\"Missing digest: \" + digest)\n\n\tfor file in manifest_files:\n\t\tif not digests_used.get(file):\n\t\t\treturn self.error(\"Unused digest: \" + file)\n\nclass ManifestChecker(object):\n\t\"\"\"\n\tChecks a manifest file.\n\t\"\"\"\n\tdef __init__(self, file, manifest):\n\t\tself.file = file\n\t\tself.manifest = manifest\n\n\tdef print_error(self, message):\n\t\tprint(\"Error: \" + self.file + \": \" + message)\n\n\tdef error(self, message):\n\t\tself.print_error(message)\n\t\treturn False\n\n\tdef check_file_exists(self, file):\n\t\tif not os.path.exists(file):\n\t\t\treturn self.error(\"File does not exist: \" + file)\n\t\treturn True\n\n\tdef check_file_is_not_empty(self, file):\n\t\tfile_size = os.path.getsize(file)\n\t\tif not file_size:\n\t\t\treturn self.error(\"File is empty: \" +", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_files = [os.path.basename(manifest_file) for manifest_file in manifest_files]\n\tfor digest_used in digests_used:\n\t\tif digest_used not in manifest_files:\n\t\t\tself.error(\"Digest used in manifest but not present: \" + digest_used)\n\n\tfor manifest_file in manifest_files:\n\t\tif not manifest_file in digests_used:\n\t\t\tself.error(\"Digest present in manifest but not used: \" + manifest_file)\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = set()\n\tfor manifest_file in manifest_files:\n\t\tfor digest in self.get_digests_from_manifest(manifest_file):\n\t\t\tdigests_present.add(digest)\n\n\tfor digest in digests_used:\n\t\tif digest not in digests_present:\n\t\t\tself.error(\"Digest {0} in use, but not present in any manifest\".format(digest))\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_file.digests:\n\t\t\t\tself.error(\"digest %s missing from manifest file %s\" % (digest, manifest_file.path))\n\t\t\telse:\n\t\t\t\tif not manifest_file.digests[digest]:\n\t\t\t\t\tself.error(\"digest %s not used in manifest file %s\" % (digest, manifest_file.path))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif digest not in manifest_files:\n\t\t\tself.error(\"Digest %s is not present in manifest\" % digest)\n\t\t\treturn\n\t\tif not manifest_files[digest].digest_used:\n\t\t\tself.error(\"Digest %s is present in manifest but is not used\" % digest)\n\t\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tfor digest_file in manifest_files:\n\t\tif digest_file.digest not in digests_used:\n\t\t\tself.error(\"Digest %s in manifest.xml is not used\" % digest_file.digest)\n\t\t\treturn False\n\t\tif digest_file.digest in self.digests_present:\n\t\t\tself.digests_present[digest_file.digest].append(digest_file)\n\t\telse:\n\t\t\tself.digests_present[digest_file.digest] = [digest_file]\n\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif len(self.inventory) != len(prior.inventory):\n\t\tself.error(\"Mismatched inventory counts: {} vs {}\".format(len(self.inventory), len(prior.inventory)))\n\t\treturn\n\tif len(self.inventory) == 0:\n\t\treturn\n\tif len(self.inventory[0]) != len(prior.inventory[0]):\n\t\tself.error(\"Mismatched inventory columns: {} vs {}\".format(len(self.inventory[0]), len(prior.inventory[0])))\n\t\treturn\n\tfor ii in range(len(self.inventory)):\n\t\tfor jj in range(len(self.inventory[ii])):\n\t\t\tif self.inventory[ii][jj] != prior.inventory[ii][jj]:\n\t\t\t\tself.error(\"Mismatched inventory values: {} vs {}\".format(self.inventory[ii][jj], prior.inventory[ii][jj]))\n\t\t\t\treturn\n\n#\n#\tValidate the inventory.\n#\ndef validate(self):\n\t#\n\t#\tCheck the version.\n\t#\n\tif not self.version:\n\t\tself.error", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif not type(prior) is InventoryValidator:\n\t\tself.error(\"Error: Prior inventory is not an InventoryValidator object.\")\n\t\treturn\n\tif not prior.inventory.has_same_dimensions(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different dimensions from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_measurements(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different measurements from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_units(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different units from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_location(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different location from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_time(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different time from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_data_source(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different data source from current invent", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tself.prior_version = prior\n\tself.prior_version.check_inventory_integrity()\n\tif not self.check_prior_version_integrity(prior):\n\t\tself.error(\"Prior version is not a valid prior version\")\n\t\treturn 0\n\treturn 1\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.version() != self.version() - 1:\n\t\traise Exception(\"Incorrect prior version for validation.\")\n\tif prior.fingerprint() != self.fingerprint():\n\t\traise Exception(\"Incorrect prior fingerprint for validation.\")\n\tif prior.count() != self.count():\n\t\traise Exception(\"Incorrect prior count for validation.\")\n\tif prior.count_distributed() != self.count_distributed():\n\t\traise Exception(\"Incorrect prior distributed count for validation.\")\n\tif prior.count_remaining() != self.count_remaining():\n\t\traise Exception(\"Incorrect prior remaining count for validation.\")\n\tif prior.count_remaining() != self.count_remaining():\n\t\traise Exception(\"Incorrect prior remaining count for validation.\")\n\tif prior.count_assigned() != self.count_assigned():\n\t\traise Exception(\"Incorrect prior assigned count for validation.\")\n\tif prior.count_in_stock() != self.count_in_stock():\n\t\traise Exception(\"Incorrect prior in stock count for validation.\")\n\tif prior.count_on_order() != self.count_on_order():\n\t\traise Exception(\"Incorrect prior on order count for validation.\")\n\tif prior.count_on_order_backorder() != self.count_on", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.mol_count != self.mol_count:\n\t\tself.error(\"InventoryValidator.validate_as_prior_version: The number of molecules in this inventory is not equal to the number of molecules in the prior inventory.\")\n\t\treturn\n\tif prior.mol_count != len(self.inventory):\n\t\tself.error(\"InventoryValidator.validate_as_prior_version: The number of molecules in the prior inventory is not equal to the number of molecules in this inventory.\")\n\t\treturn\n\tfor mol in prior.inventory:\n\t\tif mol not in self.inventory:\n\t\t\tself.error(\"InventoryValidator.validate_as_prior_version: The prior inventory contains a molecule that is not in this inventory.\")\n\t\t\treturn\n\t\tif self.inventory[mol].count != prior.inventory[mol].count:\n\t\t\tself.error(\"InventoryValidator.validate_as_prior_version: The prior inventory contains a different number of copies of a molecule than this inventory.\")\n\t\t\treturn\n\t\tif self.inventory[mol].cost != prior.inventory[mol].cost:\n\t\t\tself.error(\"InventoryValidator.validate", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif self.inv_type != prior.inv_type:\n\t\tself.error(\"Prior inventory type does not match current inventory type.\")\n\n\tif self.inv_type == \"part\":\n\t\tif self.part_name != prior.part_name:\n\t\t\tself.error(\"Part names do not match.\")\n\n\t\tif self.revision != prior.revision:\n\t\t\tself.error(\"Part revisions do not match.\")\n\n\t\tif self.manufacturer != prior.manufacturer:\n\t\t\tself.error(\"Part manufacturers do not match.\")\n\n\t\tif self.part_number != prior.part_number:\n\t\t\tself.error(\"Part numbers do not match.\")\n\n\t\tif self.price != prior.price:\n\t\t\tself.error(\"Part prices do not match.\")\n\n\t\tif self.quantity != prior.quantity:\n\t\t\tself.error(\"Part quantities do not match.\")\n\n\t\tif self.notes != prior.notes:\n\t\t\tself.error(\"Part notes do not match.\")\n\n\t\tif self.date_code != prior.date_code:\n\t\t\tself.error(\"Part date codes do not match.\")\n\n\t\tif self.part_class != prior.part_class:", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif self.version != prior.version:\n\t\tself.error(u\"%s cannot be validated against a prior version %s\" % (self.version, prior.version))\n\t\treturn\n\n\t# Validate that the prior inventory has the same set of tables\n\t# (with the same columns) as the current inventory.\n\tfor table in self.tables:\n\t\tif table not in prior.tables:\n\t\t\tself.error(u\"%s prior inventory lacks expected table %s\" % (prior.version, table))\n\t\t\treturn\n\t\telse:\n\t\t\tif self.tables[table] != prior.tables[table]:\n\t\t\t\tself.error(u\"%s prior inventory table %s has different columns than expected\" % (prior.version, table))\n\t\t\t\treturn\n\n\t# Validate that the prior inventory has the same set of views\n\t# (with the same columns) as the current inventory.\n\tfor view in self.views:\n\t\tif view not in prior.views:\n\t\t\tself.error(u\"%s prior inventory lacks expected view %s\" % (prior.version, view))\n\t\t\treturn\n\t\telse:\n\t\t\tif self.views[view]", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.inventory_version != self.inventory_version - 1: \n\t\tself.error(\"The prior inventory version is not one less than the current inventory version.\")\n\tif prior.number_of_items != self.number_of_items:\n\t\tself.error(\"The prior inventory has a different number of items than the current inventory.\")\n\tif prior.number_of_categories != self.number_of_categories:\n\t\tself.error(\"The prior inventory has a different number of categories than the current inventory.\")\n\tif prior.number_of_prices != self.number_of_prices:\n\t\tself.error(\"The prior inventory has a different number of prices than the current inventory.\")\n\tif prior.number_of_locations != self.number_of_locations:\n\t\tself.error(\"The prior inventory has a different number of locations than the current inventory.\")\n\tif prior.number_of_suppliers != self.number_of_suppliers:\n\t\tself.error(\"The prior inventory has a different number of suppliers than the current inventory.\")\n\t\n\t# Check that all categories have the same name and description\n\tfor category_index in range(self.number_of_categories):\n\t\tif prior.categories[category", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.version != self.version - 1:\n\t\tself.error(\"Prior version is not one less than current version.\")\n\t\treturn\n\tif prior.version != self.version:\n\t\tself.error(\"Prior version is not one less than current version.\")\n\t\treturn\n\tif len(prior.inventory_list) != len(self.inventory_list):\n\t\tself.error(\"Number of inventory items does not match prior version.\")\n\t\treturn\n\tif len(prior.inventory_list) != len(self.inventory_list):\n\t\tself.error(\"Number of inventory items does not match prior version.\")\n\t\treturn\n\tfor i in range(len(self.inventory_list)):\n\t\tif self.inventory_list[i].id != prior.inventory_list[i].id:\n\t\t\tself.error(\"Inventory item IDs do not match prior version.\")\n\t\t\treturn\n\t\tif self.inventory_list[i].item != prior.inventory_list[i].item:\n\t\t\tself.error(\"Item names do not match prior version.\")\n\t\t\treturn\n\t\tif self.inventory_list[i].quantity != prior.inventory_list[i].quantity:\n\t\t\tself", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif self.version != prior.version:\n\t\tself.error(\"Attempting to validate an inventory with version '{}' against prior inventory of version '{}'.\".format(self.version, prior.version))\n\tif self.num_items != prior.num_items:\n\t\tself.error(\"Attempting to validate an inventory with '{}' items against prior inventory with '{}' items.\".format(self.num_items, prior.num_items))\n\n\t#Check that all items are present in the prior inventory.\n\tfor item in self.items:\n\t\tif item not in prior.items:\n\t\t\tself.error(\"Item '{}' not present in prior inventory.\".format(item))\n\t\telse:\n\t\t\tprior_item = prior.items[item]\n\t\t\tif prior_item.num_instances != self.items[item].num_instances:\n\t\t\t\tself.error(\"Item '{}' has '{}' instances in current inventory but '{}' instances in prior inventory.\".format(item, self.items[item].num_instances, prior_item.num_instances))\n\n\t\t\tif prior_item.num_instances != self.items[item].num_instances:\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif len(self.inventory) != len(prior.inventory):\n\t\tself.error(\"Mismatched inventory counts: {} vs {}\".format(len(self.inventory), len(prior.inventory)))\n\t\treturn\n\tif len(self.inventory) == 0:\n\t\treturn\n\tif len(self.inventory[0]) != len(prior.inventory[0]):\n\t\tself.error(\"Mismatched inventory columns: {} vs {}\".format(len(self.inventory[0]), len(prior.inventory[0])))\n\t\treturn\n\tfor ii in range(len(self.inventory)):\n\t\tfor jj in range(len(self.inventory[ii])):\n\t\t\tif self.inventory[ii][jj] != prior.inventory[ii][jj]:\n\t\t\t\tself.error(\"Mismatched inventory values: {} vs {}\".format(self.inventory[ii][jj], prior.inventory[ii][jj]))\n\t\t\t\treturn\n\n#\n#\tValidate the inventory.\n#\ndef validate(self):\n\t#\n\t#\tCheck the version.\n\t#\n\tif not self.version:\n\t\tself.error", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif not type(prior) is InventoryValidator:\n\t\tself.error(\"Error: Prior inventory is not an InventoryValidator object.\")\n\t\treturn\n\tif not prior.inventory.has_same_dimensions(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different dimensions from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_measurements(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different measurements from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_units(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different units from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_location(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different location from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_time(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different time from current inventory.\")\n\t\treturn\n\tif not prior.inventory.has_same_data_source(self.inventory):\n\t\tself.error(\"Error: Prior inventory has different data source from current invent", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tself.prior_version = prior\n\tself.prior_version.check_inventory_integrity()\n\tif not self.check_prior_version_integrity(prior):\n\t\tself.error(\"Prior version is not a valid prior version\")\n\t\treturn 0\n\treturn 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.version() != self.version() - 1:\n\t\traise Exception(\"Incorrect prior version for validation.\")\n\tif prior.fingerprint() != self.fingerprint():\n\t\traise Exception(\"Incorrect prior fingerprint for validation.\")\n\tif prior.count() != self.count():\n\t\traise Exception(\"Incorrect prior count for validation.\")\n\tif prior.count_distributed() != self.count_distributed():\n\t\traise Exception(\"Incorrect prior distributed count for validation.\")\n\tif prior.count_remaining() != self.count_remaining():\n\t\traise Exception(\"Incorrect prior remaining count for validation.\")\n\tif prior.count_remaining() != self.count_remaining():\n\t\traise Exception(\"Incorrect prior remaining count for validation.\")\n\tif prior.count_assigned() != self.count_assigned():\n\t\traise Exception(\"Incorrect prior assigned count for validation.\")\n\tif prior.count_in_stock() != self.count_in_stock():\n\t\traise Exception(\"Incorrect prior in stock count for validation.\")\n\tif prior.count_on_order() != self.count_on_order():\n\t\traise Exception(\"Incorrect prior on order count for validation.\")\n\tif prior.count_on_order_backorder() != self.count_on", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.mol_count != self.mol_count:\n\t\tself.error(\"InventoryValidator.validate_as_prior_version: The number of molecules in this inventory is not equal to the number of molecules in the prior inventory.\")\n\t\treturn\n\tif prior.mol_count != len(self.inventory):\n\t\tself.error(\"InventoryValidator.validate_as_prior_version: The number of molecules in the prior inventory is not equal to the number of molecules in this inventory.\")\n\t\treturn\n\tfor mol in prior.inventory:\n\t\tif mol not in self.inventory:\n\t\t\tself.error(\"InventoryValidator.validate_as_prior_version: The prior inventory contains a molecule that is not in this inventory.\")\n\t\t\treturn\n\t\tif self.inventory[mol].count != prior.inventory[mol].count:\n\t\t\tself.error(\"InventoryValidator.validate_as_prior_version: The prior inventory contains a different number of copies of a molecule than this inventory.\")\n\t\t\treturn\n\t\tif self.inventory[mol].cost != prior.inventory[mol].cost:\n\t\t\tself.error(\"InventoryValidator.validate", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif self.inv_type != prior.inv_type:\n\t\tself.error(\"Prior inventory type does not match current inventory type.\")\n\n\tif self.inv_type == \"part\":\n\t\tif self.part_name != prior.part_name:\n\t\t\tself.error(\"Part names do not match.\")\n\n\t\tif self.revision != prior.revision:\n\t\t\tself.error(\"Part revisions do not match.\")\n\n\t\tif self.manufacturer != prior.manufacturer:\n\t\t\tself.error(\"Part manufacturers do not match.\")\n\n\t\tif self.part_number != prior.part_number:\n\t\t\tself.error(\"Part numbers do not match.\")\n\n\t\tif self.price != prior.price:\n\t\t\tself.error(\"Part prices do not match.\")\n\n\t\tif self.quantity != prior.quantity:\n\t\t\tself.error(\"Part quantities do not match.\")\n\n\t\tif self.notes != prior.notes:\n\t\t\tself.error(\"Part notes do not match.\")\n\n\t\tif self.date_code != prior.date_code:\n\t\t\tself.error(\"Part date codes do not match.\")\n\n\t\tif self.part_class != prior.part_class:", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif self.version != prior.version:\n\t\tself.error(u\"%s cannot be validated against a prior version %s\" % (self.version, prior.version))\n\t\treturn\n\n\t# Validate that the prior inventory has the same set of tables\n\t# (with the same columns) as the current inventory.\n\tfor table in self.tables:\n\t\tif table not in prior.tables:\n\t\t\tself.error(u\"%s prior inventory lacks expected table %s\" % (prior.version, table))\n\t\t\treturn\n\t\telse:\n\t\t\tif self.tables[table] != prior.tables[table]:\n\t\t\t\tself.error(u\"%s prior inventory table %s has different columns than expected\" % (prior.version, table))\n\t\t\t\treturn\n\n\t# Validate that the prior inventory has the same set of views\n\t# (with the same columns) as the current inventory.\n\tfor view in self.views:\n\t\tif view not in prior.views:\n\t\t\tself.error(u\"%s prior inventory lacks expected view %s\" % (prior.version, view))\n\t\t\treturn\n\t\telse:\n\t\t\tif self.views[view]", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.inventory_version != self.inventory_version - 1: \n\t\tself.error(\"The prior inventory version is not one less than the current inventory version.\")\n\tif prior.number_of_items != self.number_of_items:\n\t\tself.error(\"The prior inventory has a different number of items than the current inventory.\")\n\tif prior.number_of_categories != self.number_of_categories:\n\t\tself.error(\"The prior inventory has a different number of categories than the current inventory.\")\n\tif prior.number_of_prices != self.number_of_prices:\n\t\tself.error(\"The prior inventory has a different number of prices than the current inventory.\")\n\tif prior.number_of_locations != self.number_of_locations:\n\t\tself.error(\"The prior inventory has a different number of locations than the current inventory.\")\n\tif prior.number_of_suppliers != self.number_of_suppliers:\n\t\tself.error(\"The prior inventory has a different number of suppliers than the current inventory.\")\n\t\n\t# Check that all categories have the same name and description\n\tfor category_index in range(self.number_of_categories):\n\t\tif prior.categories[category", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif prior.version != self.version - 1:\n\t\tself.error(\"Prior version is not one less than current version.\")\n\t\treturn\n\tif prior.version != self.version:\n\t\tself.error(\"Prior version is not one less than current version.\")\n\t\treturn\n\tif len(prior.inventory_list) != len(self.inventory_list):\n\t\tself.error(\"Number of inventory items does not match prior version.\")\n\t\treturn\n\tif len(prior.inventory_list) != len(self.inventory_list):\n\t\tself.error(\"Number of inventory items does not match prior version.\")\n\t\treturn\n\tfor i in range(len(self.inventory_list)):\n\t\tif self.inventory_list[i].id != prior.inventory_list[i].id:\n\t\t\tself.error(\"Inventory item IDs do not match prior version.\")\n\t\t\treturn\n\t\tif self.inventory_list[i].item != prior.inventory_list[i].item:\n\t\t\tself.error(\"Item names do not match prior version.\")\n\t\t\treturn\n\t\tif self.inventory_list[i].quantity != prior.inventory_list[i].quantity:\n\t\t\tself", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tif self.version != prior.version:\n\t\tself.error(\"Attempting to validate an inventory with version '{}' against prior inventory of version '{}'.\".format(self.version, prior.version))\n\tif self.num_items != prior.num_items:\n\t\tself.error(\"Attempting to validate an inventory with '{}' items against prior inventory with '{}' items.\".format(self.num_items, prior.num_items))\n\n\t#Check that all items are present in the prior inventory.\n\tfor item in self.items:\n\t\tif item not in prior.items:\n\t\t\tself.error(\"Item '{}' not present in prior inventory.\".format(item))\n\t\telse:\n\t\t\tprior_item = prior.items[item]\n\t\t\tif prior_item.num_instances != self.items[item].num_instances:\n\t\t\t\tself.error(\"Item '{}' has '{}' instances in current inventory but '{}' instances in prior inventory.\".format(item, self.items[item].num_instances, prior_item.num_instances))\n\n\t\t\tif prior_item.num_instances != self.items[item].num_instances:\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state_file_path in inventory.list_states(version):\n\t\tlogical_path_map[state_file_path] = state_file_path\n\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor group_name in inventory.keys():\n\t\tif version is not None:\n\t\t\tif group_name.startswith(version):\n\t\t\t\tfor state in inventory[group_name]:\n\t\t\t\t\tlogical_path_map[state] = inventory[group_name][state]\n\t\telse:\n\t\t\tfor state in inventory[group_name]:\n\t\t\t\tlogical_path_map[state] = inventory[group_name][state]\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state_item in inventory.get_states():\n\t\tlogical_path_map[state_item.get_logical_path()] = state_item.get_physical_path(version)\n\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\t# find the states in the inventory\n\tfor path in inventory.get_files_in_category(version, 'states'):\n\t\t# get the state name\n\t\tstate_name = os.path.splitext(os.path.basename(path))[0]\n\t\t# add the state to the logical path map\n\t\tlogical_path_map[state_name] = path\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tfiles = {}\n\tfor state in inventory:\n\t\tfiles[state] = inventory[state]['files'][version]\n\treturn files\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor state in inventory:\n\t\tpath_map[state] = os.path.join(\"./\", state, \"main.py\")\n\treturn path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor state in inventory:\n\t\tpath_map[state] = os.path.join(version, inventory[state])\n\n\treturn path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state in inventory.list_state_dirs(version):\n\t\tfilename = state.find('state').attrib['name']\n\t\tif '.' in filename:\n\t\t\tlogical_path_map[filename.split('.')[0]] = state\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tfiles = {}\n\n\tfor group, hosts in inventory.iteritems():\n\t\tif hosts:\n\t\t\tfor host in hosts:\n\t\t\t\tfiles[host] = \"roles/{}/{}/{}/{}/tasks/\".format(group, host, version, host)\n\n\treturn files\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tif version == \"0.1\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.2\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.3\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.4\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.5\":\n\t\tstate_file_paths = {}\n\t\tfor state in"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state_file_path in inventory.list_states(version):\n\t\tlogical_path_map[state_file_path] = state_file_path\n\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor group_name in inventory.keys():\n\t\tif version is not None:\n\t\t\tif group_name.startswith(version):\n\t\t\t\tfor state in inventory[group_name]:\n\t\t\t\t\tlogical_path_map[state] = inventory[group_name][state]\n\t\telse:\n\t\t\tfor state in inventory[group_name]:\n\t\t\t\tlogical_path_map[state] = inventory[group_name][state]\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state_item in inventory.get_states():\n\t\tlogical_path_map[state_item.get_logical_path()] = state_item.get_physical_path(version)\n\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\t# find the states in the inventory\n\tfor path in inventory.get_files_in_category(version, 'states'):\n\t\t# get the state name\n\t\tstate_name = os.path.splitext(os.path.basename(path))[0]\n\t\t# add the state to the logical path map\n\t\tlogical_path_map[state_name] = path\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tfiles = {}\n\tfor state in inventory:\n\t\tfiles[state] = inventory[state]['files'][version]\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor state in inventory:\n\t\tpath_map[state] = os.path.join(\"./\", state, \"main.py\")\n\treturn path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor state in inventory:\n\t\tpath_map[state] = os.path.join(version, inventory[state])\n\n\treturn path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state in inventory.list_state_dirs(version):\n\t\tfilename = state.find('state').attrib['name']\n\t\tif '.' in filename:\n\t\t\tlogical_path_map[filename.split('.')[0]] = state\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tfiles = {}\n\n\tfor group, hosts in inventory.iteritems():\n\t\tif hosts:\n\t\t\tfor host in hosts:\n\t\t\t\tfiles[host] = \"roles/{}/{}/{}/{}/tasks/\".format(group, host, version, host)\n\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tif version == \"0.1\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.2\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.3\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.4\":\n\t\tstate_file_paths = {}\n\t\tfor state in inventory.get_states():\n\t\t\tstate_file_paths.update({state.get_name():state.get_file_path()})\n\t\treturn state_file_paths\n\tif version == \"0.5\":\n\t\tstate_file_paths = {}\n\t\tfor state in", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t# Check that fixity block is not empty\n\tif not fixity:\n\t\treturn\n\n\t# Check that fixity block is of the form:\n\t# fixity:\n\t#   <path>:\n\t#     checksum: <checksum>\n\t#   <path>:\n\t#     checksum: <checksum>\n\t#   ...\n\t# where <path> is the path of the file relative to the dataset root and <checksum> is the checksum of the file.\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"fixity block must be a dictionary\")\n\t\treturn\n\n\t# Check that all keys of the fixity block are paths\n\tfor key in fixity:\n\t\tif not isinstance(key, str):\n\t\t\tself.error(\"all keys in the fixity block must be paths\")\n\t\t\treturn\n\n\t# Check that all values of the fixity block are dictionaries with a checksum field\n\tfor value in fixity.values():\n\t\tif not isinstance(value, dict):\n\t\t\tself.error(\"all values in the fixity block must be dictionaries\")\n\t\t\treturn\n\t\tif \"checksum\" not in value:\n\t\t\tself.error(\"all values in the fixity block must have a", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif not isinstance(fixity['fixity'], list):\n\t\tself.error(\"The fixity block is not a list of fixity entries\")\n\n\tfor f in manifest_files:\n\t\tif f not in fixity['fixity']:\n\t\t\tself.error(\"File \" + f + \" is not listed in the fixity block\")\n\n\tfor f in fixity['fixity']:\n\t\tif not isinstance(f, dict):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is not a dictionary\")\n\t\tif 'type' not in f or not isinstance(f['type'], str):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is missing type\")\n\t\tif 'algorithm' not in f or not isinstance(f['algorithm'], str):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is missing algorithm\")\n\t\tif 'value' not in f or not isinstance(f['value'], str):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is missing value\")\n\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t# Make sure that all files in the manifest are referenced in the fixity block\n\tfor manifest_file in manifest_files:\n\t\tif manifest_file not in fixity:\n\t\t\tself.error(\"File %s is not referenced in the fixity block.\" % manifest_file)\n\n\t# Make sure that the fixity block references only files in the manifest\n\tfor fixity_file in fixity:\n\t\tif fixity_file not in manifest_files:\n\t\t\tself.error(\"File %s is not in the manifest.\" % fixity_file)\n\n\t# Make sure that the fixity block references only files in the manifest\n\tfor fixity_file in fixity:\n\t\tif not validate_fixity_structure(fixity[fixity_file]):\n\t\t\tself.error(\"Fixity entry for file %s does not contain a UUID.\" % fixity_file)\n\n\t# Make sure that the fixity block references only files in the manifest\n\tfor fixity_file in fixity:\n\t\tif not validate_fixity_structure(fixity[fixity_file]):\n\t\t\tself.error(\"Fixity entry for file %s does not contain a UUID.\" % fixity_file)\n\n\t# Make sure that the fixity block references only files in", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif not fixity:\n\t\treturn\n\ttry:\n\t\tif 'checksum' in fixity:\n\t\t\tfor checksum in fixity['checksum']:\n\t\t\t\tif not manifest_files.get(checksum['file']):\n\t\t\t\t\tself.error('The checksum file {} is not in the manifest'.format(checksum['file']))\n\t\t\t\tif checksum['algorithm'] not in ['md5', 'sha1', 'sha256', 'sha512']:\n\t\t\t\t\tself.error('The checksum algorithm {} is not supported.'.format(checksum['algorithm']))\n\t\t\t\tif 'value' not in checksum:\n\t\t\t\t\tself.error('Checksums must have a value.')\n\t\tif 'checksum-type' in fixity:\n\t\t\tself.error('The checksum-type field is no longer supported, please use the checksums field.')\n\t\tif 'checksum-type-format' in fixity:\n\t\t\tself.error('The checksum-type-format field is no longer supported, please use the checksums field.')\n\t\tif 'checksum-type-value' in fixity:\n\t\t\tself.error('The checksum-type-value", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\tif not isinstance(fixity, dict):\n\t\t\tself.error(\"Fixity block must be a dictionary\")\n\t\t\treturn\n\t\tif \"fixity\" not in fixity:\n\t\t\tself.error(\"Fixity block must contain a 'fixity' key\")\n\t\t\treturn\n\t\tif not isinstance(fixity[\"fixity\"], dict):\n\t\t\tself.error(\"Fixity dictionary must be a dictionary\")\n\t\t\treturn\n\t\tfor file_name in fixity[\"fixity\"]:\n\t\t\tif file_name not in manifest_files:\n\t\t\t\tself.error(\"File {} not referenced in manifest\".format(file_name))\n\t\t\t\treturn\n\t\t\tif not isinstance(fixity[\"fixity\"][file_name], dict):\n\t\t\t\tself.error(\"Fixity dictionary for {} must be a dictionary\".format(file_name))\n\t\t\t\treturn\n\t\t\tfor algorithm in fixity[\"fixity\"][file_name]:\n\t\t\t\tif algorithm not in self.fixity_algorithms:\n\t\t\t\t\tself.error(\"{} is not a valid fixity algorithm\".format(algorithm))\n\t\t\t\t\treturn\n\t\t\t\tif not isinstance(", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif not self.validate_structure(fixity, 'Fixity'):\n\t\treturn\n\tfor file in manifest_files:\n\t\tif file.file_id not in fixity:\n\t\t\tself.error('File %s is not in fixity' % file.file_id)\n\t\telse:\n\t\t\tfor alg, value in fixity[file.file_id].items():\n\t\t\t\tif alg not in file.algorithms:\n\t\t\t\t\tself.error('Algorithm %s is not supported for %s (file %s)' % (alg, file.file_id, file.file_path))\n\t\t\t\t\tcontinue\n\t\t\t\tif value != file.fixity[alg]:\n\t\t\t\t\tself.error('Fixity check failed for %s (file %s)' % (file.file_id, file.file_path))\n\t\t\t\t\tcontinue\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif fixity is None:\n\t\treturn\n\t\n\tif type(fixity) != dict:\n\t\tself.error(\"fixity\", \"fixity block is not a dictionary\")\n\t\n\tif \"files\" not in fixity:\n\t\tself.error(\"fixity\", \"fixity block is missing files key\")\n\t\n\tif type(fixity[\"files\"]) != list:\n\t\tself.error(\"fixity\", \"files key is not a list\")\n\t\n\tfor f in fixity[\"files\"]:\n\t\tif type(f) != str:\n\t\t\tself.error(\"fixity\", \"file path is not a string\")\n\t\t\t\n\t\tif f not in manifest_files:\n\t\t\tself.error(\"fixity\", \"file path \" + f + \" is not listed in the manifest\")\n\t\t\t\n\tif \"checksums\" not in fixity:\n\t\tself.error(\"fixity\", \"fixity block is missing checksums key\")\n\t\t\n\tif type(fixity[\"checksums\"]) != list:\n\t\tself.error(\"fixity\", \"checksums key is not a list\")\n\t\t\n\tfor c in fixity[\"checksums\"]:\n\t\tif type(c) != dict:\n\t\t\t", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"fixity must be a dictionary\")\n\t\treturn \n\t\n\tfor key, value in fixity.iteritems():\n\t\tif not isinstance(value, dict):\n\t\t\tself.error(\"fixity must be a dictionary\")\n\t\t\treturn \n\t\n\tfor key, value in manifest_files.iteritems():\n\t\tif not key in fixity:\n\t\t\tself.error(\"fixity must contain an entry for each file in manifest\")\n\t\t\treturn \n\t\t\t\n\t\tfor key2, value2 in value.iteritems():\n\t\t\tif not key2 in fixity[key]:\n\t\t\t\tself.error(\"fixity must contain an entry for each file in manifest\")\n\t\t\t\treturn \n\t\t\t\t\n\t\t\tif not isinstance(fixity[key][key2], dict):\n\t\t\t\tself.error(\"fixity entry must be a dictionary\")\n\t\t\t\treturn \n\t\n\treturn \n\t\n\t\ndef validate_checksum(self, checksum, fixity):\n\t\"\"\"\n\tValidate checksum block in inventory. Check if checksums are defined for all files listed in the fixity block. Return error()", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif fixity != None:\n\t\tif type(fixity) == list:\n\t\t\tfor f in fixity:\n\t\t\t\tif f == None:\n\t\t\t\t\tself.error(\"fixity\", \"fixity can't contain None\")\n\t\t\t\tif type(f) != dict:\n\t\t\t\t\tself.error(\"fixity\", \"fixity must be a list of dicts\")\n\t\t\t\tif \"file\" not in f.keys():\n\t\t\t\t\tself.error(\"fixity\", \"fixity must have a file key\")\n\t\t\t\tif type(f[\"file\"]) != str:\n\t\t\t\t\tself.error(\"fixity\", \"fixity file must be a string\")\n\t\t\t\tif f[\"file\"] not in manifest_files:\n\t\t\t\t\tself.error(\"fixity\", \"fixity file must exist in manifest\")\n\t\t\t\tif \"algorithm\" in f.keys() and f[\"algorithm\"] not in [\"MD5\", \"SHA-1\", \"SHA-256\", \"SHA-512\"]:\n\t\t\t\t\tself.error(\"fixity\", \"fixity algorithm must be one of MD5, SHA-1, SHA-256, SHA-512\")\n\t\t", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t# Validate that the fixity block contains the correct keys\n\t# Validate that the fixity block contains the correct keys\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"Fixity block should be a dict\")\n\t\treturn\n\tif not set(fixity.keys()).issubset(set([\"checksum\", \"algorithm\", \"file\"])):\n\t\tself.error(\"Fixity block should contain 'checksum', 'algorithm', and 'file' keys\")\n\t\treturn\n\tif not isinstance(fixity[\"checksum\"], str):\n\t\tself.error(\"'checksum' value in fixity block should be a string\")\n\t\treturn\n\tif not isinstance(fixity[\"algorithm\"], str):\n\t\tself.error(\"'algorithm' value in fixity block should be a string\")\n\t\treturn\n\tif not isinstance(fixity[\"file\"], list):\n\t\tself.error(\"'file' value in fixity block should be a list of strings\")\n\t\treturn\n\tfor filename in fixity[\"file\"]:\n\t\tif not isinstance(filename, str):\n\t\t\tself.error(\"'file' value in fixity block should be a list of strings\")\n\t\t\treturn\n\t\tif not filename in manifest_files:\n\t\t\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t# Check that fixity block is not empty\n\tif not fixity:\n\t\treturn\n\n\t# Check that fixity block is of the form:\n\t# fixity:\n\t#   <path>:\n\t#     checksum: <checksum>\n\t#   <path>:\n\t#     checksum: <checksum>\n\t#   ...\n\t# where <path> is the path of the file relative to the dataset root and <checksum> is the checksum of the file.\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"fixity block must be a dictionary\")\n\t\treturn\n\n\t# Check that all keys of the fixity block are paths\n\tfor key in fixity:\n\t\tif not isinstance(key, str):\n\t\t\tself.error(\"all keys in the fixity block must be paths\")\n\t\t\treturn\n\n\t# Check that all values of the fixity block are dictionaries with a checksum field\n\tfor value in fixity.values():\n\t\tif not isinstance(value, dict):\n\t\t\tself.error(\"all values in the fixity block must be dictionaries\")\n\t\t\treturn\n\t\tif \"checksum\" not in value:\n\t\t\tself.error(\"all values in the fixity block must have a", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif not isinstance(fixity['fixity'], list):\n\t\tself.error(\"The fixity block is not a list of fixity entries\")\n\n\tfor f in manifest_files:\n\t\tif f not in fixity['fixity']:\n\t\t\tself.error(\"File \" + f + \" is not listed in the fixity block\")\n\n\tfor f in fixity['fixity']:\n\t\tif not isinstance(f, dict):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is not a dictionary\")\n\t\tif 'type' not in f or not isinstance(f['type'], str):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is missing type\")\n\t\tif 'algorithm' not in f or not isinstance(f['algorithm'], str):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is missing algorithm\")\n\t\tif 'value' not in f or not isinstance(f['value'], str):\n\t\t\tself.error(\"Fixity entry for \" + f + \" is missing value\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t# Make sure that all files in the manifest are referenced in the fixity block\n\tfor manifest_file in manifest_files:\n\t\tif manifest_file not in fixity:\n\t\t\tself.error(\"File %s is not referenced in the fixity block.\" % manifest_file)\n\n\t# Make sure that the fixity block references only files in the manifest\n\tfor fixity_file in fixity:\n\t\tif fixity_file not in manifest_files:\n\t\t\tself.error(\"File %s is not in the manifest.\" % fixity_file)\n\n\t# Make sure that the fixity block references only files in the manifest\n\tfor fixity_file in fixity:\n\t\tif not validate_fixity_structure(fixity[fixity_file]):\n\t\t\tself.error(\"Fixity entry for file %s does not contain a UUID.\" % fixity_file)\n\n\t# Make sure that the fixity block references only files in the manifest\n\tfor fixity_file in fixity:\n\t\tif not validate_fixity_structure(fixity[fixity_file]):\n\t\t\tself.error(\"Fixity entry for file %s does not contain a UUID.\" % fixity_file)\n\n\t# Make sure that the fixity block references only files in", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif not fixity:\n\t\treturn\n\ttry:\n\t\tif 'checksum' in fixity:\n\t\t\tfor checksum in fixity['checksum']:\n\t\t\t\tif not manifest_files.get(checksum['file']):\n\t\t\t\t\tself.error('The checksum file {} is not in the manifest'.format(checksum['file']))\n\t\t\t\tif checksum['algorithm'] not in ['md5', 'sha1', 'sha256', 'sha512']:\n\t\t\t\t\tself.error('The checksum algorithm {} is not supported.'.format(checksum['algorithm']))\n\t\t\t\tif 'value' not in checksum:\n\t\t\t\t\tself.error('Checksums must have a value.')\n\t\tif 'checksum-type' in fixity:\n\t\t\tself.error('The checksum-type field is no longer supported, please use the checksums field.')\n\t\tif 'checksum-type-format' in fixity:\n\t\t\tself.error('The checksum-type-format field is no longer supported, please use the checksums field.')\n\t\tif 'checksum-type-value' in fixity:\n\t\t\tself.error('The checksum-type-value", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\tif not isinstance(fixity, dict):\n\t\t\tself.error(\"Fixity block must be a dictionary\")\n\t\t\treturn\n\t\tif \"fixity\" not in fixity:\n\t\t\tself.error(\"Fixity block must contain a 'fixity' key\")\n\t\t\treturn\n\t\tif not isinstance(fixity[\"fixity\"], dict):\n\t\t\tself.error(\"Fixity dictionary must be a dictionary\")\n\t\t\treturn\n\t\tfor file_name in fixity[\"fixity\"]:\n\t\t\tif file_name not in manifest_files:\n\t\t\t\tself.error(\"File {} not referenced in manifest\".format(file_name))\n\t\t\t\treturn\n\t\t\tif not isinstance(fixity[\"fixity\"][file_name], dict):\n\t\t\t\tself.error(\"Fixity dictionary for {} must be a dictionary\".format(file_name))\n\t\t\t\treturn\n\t\t\tfor algorithm in fixity[\"fixity\"][file_name]:\n\t\t\t\tif algorithm not in self.fixity_algorithms:\n\t\t\t\t\tself.error(\"{} is not a valid fixity algorithm\".format(algorithm))\n\t\t\t\t\treturn\n\t\t\t\tif not isinstance(", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif not self.validate_structure(fixity, 'Fixity'):\n\t\treturn\n\tfor file in manifest_files:\n\t\tif file.file_id not in fixity:\n\t\t\tself.error('File %s is not in fixity' % file.file_id)\n\t\telse:\n\t\t\tfor alg, value in fixity[file.file_id].items():\n\t\t\t\tif alg not in file.algorithms:\n\t\t\t\t\tself.error('Algorithm %s is not supported for %s (file %s)' % (alg, file.file_id, file.file_path))\n\t\t\t\t\tcontinue\n\t\t\t\tif value != file.fixity[alg]:\n\t\t\t\t\tself.error('Fixity check failed for %s (file %s)' % (file.file_id, file.file_path))\n\t\t\t\t\tcontinue\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif fixity is None:\n\t\treturn\n\t\n\tif type(fixity) != dict:\n\t\tself.error(\"fixity\", \"fixity block is not a dictionary\")\n\t\n\tif \"files\" not in fixity:\n\t\tself.error(\"fixity\", \"fixity block is missing files key\")\n\t\n\tif type(fixity[\"files\"]) != list:\n\t\tself.error(\"fixity\", \"files key is not a list\")\n\t\n\tfor f in fixity[\"files\"]:\n\t\tif type(f) != str:\n\t\t\tself.error(\"fixity\", \"file path is not a string\")\n\t\t\t\n\t\tif f not in manifest_files:\n\t\t\tself.error(\"fixity\", \"file path \" + f + \" is not listed in the manifest\")\n\t\t\t\n\tif \"checksums\" not in fixity:\n\t\tself.error(\"fixity\", \"fixity block is missing checksums key\")\n\t\t\n\tif type(fixity[\"checksums\"]) != list:\n\t\tself.error(\"fixity\", \"checksums key is not a list\")\n\t\t\n\tfor c in fixity[\"checksums\"]:\n\t\tif type(c) != dict:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"fixity must be a dictionary\")\n\t\treturn \n\t\n\tfor key, value in fixity.iteritems():\n\t\tif not isinstance(value, dict):\n\t\t\tself.error(\"fixity must be a dictionary\")\n\t\t\treturn \n\t\n\tfor key, value in manifest_files.iteritems():\n\t\tif not key in fixity:\n\t\t\tself.error(\"fixity must contain an entry for each file in manifest\")\n\t\t\treturn \n\t\t\t\n\t\tfor key2, value2 in value.iteritems():\n\t\t\tif not key2 in fixity[key]:\n\t\t\t\tself.error(\"fixity must contain an entry for each file in manifest\")\n\t\t\t\treturn \n\t\t\t\t\n\t\t\tif not isinstance(fixity[key][key2], dict):\n\t\t\t\tself.error(\"fixity entry must be a dictionary\")\n\t\t\t\treturn \n\t\n\treturn \n\t\n\t\ndef validate_checksum(self, checksum, fixity):\n\t\"\"\"\n\tValidate checksum block in inventory. Check if checksums are defined for all files listed in the fixity block. Return error()", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tif fixity != None:\n\t\tif type(fixity) == list:\n\t\t\tfor f in fixity:\n\t\t\t\tif f == None:\n\t\t\t\t\tself.error(\"fixity\", \"fixity can't contain None\")\n\t\t\t\tif type(f) != dict:\n\t\t\t\t\tself.error(\"fixity\", \"fixity must be a list of dicts\")\n\t\t\t\tif \"file\" not in f.keys():\n\t\t\t\t\tself.error(\"fixity\", \"fixity must have a file key\")\n\t\t\t\tif type(f[\"file\"]) != str:\n\t\t\t\t\tself.error(\"fixity\", \"fixity file must be a string\")\n\t\t\t\tif f[\"file\"] not in manifest_files:\n\t\t\t\t\tself.error(\"fixity\", \"fixity file must exist in manifest\")\n\t\t\t\tif \"algorithm\" in f.keys() and f[\"algorithm\"] not in [\"MD5\", \"SHA-1\", \"SHA-256\", \"SHA-512\"]:\n\t\t\t\t\tself.error(\"fixity\", \"fixity algorithm must be one of MD5, SHA-1, SHA-256, SHA-512\")\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t# Validate that the fixity block contains the correct keys\n\t# Validate that the fixity block contains the correct keys\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"Fixity block should be a dict\")\n\t\treturn\n\tif not set(fixity.keys()).issubset(set([\"checksum\", \"algorithm\", \"file\"])):\n\t\tself.error(\"Fixity block should contain 'checksum', 'algorithm', and 'file' keys\")\n\t\treturn\n\tif not isinstance(fixity[\"checksum\"], str):\n\t\tself.error(\"'checksum' value in fixity block should be a string\")\n\t\treturn\n\tif not isinstance(fixity[\"algorithm\"], str):\n\t\tself.error(\"'algorithm' value in fixity block should be a string\")\n\t\treturn\n\tif not isinstance(fixity[\"file\"], list):\n\t\tself.error(\"'file' value in fixity block should be a list of strings\")\n\t\treturn\n\tfor filename in fixity[\"file\"]:\n\t\tif not isinstance(filename, str):\n\t\t\tself.error(\"'file' value in fixity block should be a list of strings\")\n\t\t\treturn\n\t\tif not filename in manifest_files:\n\t\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "_id": "62b463153879012d19481498", "code": ["def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t# 1. Check if path is a directory.\n\t# 2. If it is, return the files and folders in the directory.\n\t# 3. If it is not, return the path.\n\tif os.path.isdir(path):\n\t\tfiles = []\n\t\tfor f in os.listdir(path):\n\t\t\tfiles.append(f)\n\t\treturn files\n\telse:\n\t\treturn path\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tresult = []\n\tfor x in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, x)):\n\t\t\tresult.append(x)\n\treturn result\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn glob.glob(path + \"/*\")\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tfiles.append(file)\n\treturn files\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t# 1. Check if path is a directory.\n\t# 2. If it is, return the files and folders in the directory.\n\t# 3. If it is not, return the path.\n\tif os.path.isdir(path):\n\t\tfiles = []\n\t\tfor f in os.listdir(path):\n\t\t\tfiles.append(f)\n\t\treturn files\n\telse:\n\t\treturn path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tresult = []\n\tfor x in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, x)):\n\t\t\tresult.append(x)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn glob.glob(path + \"/*\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\treturn os.listdir(path)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tfiles.append(file)\n\treturn files\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor f in files:\n\t\tif f not in xmls:\n\t\t\tdata.setdefault(f, []).append(source + f)\n\treturn data\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tfiles_by_xml = {}\n\tfor xml in xmls:\n\t\tif xml not in files_by_xml:\n\t\t\tfiles_by_xml[xml] = []\n\t\tfor file in files:\n\t\t\tif xml in file:\n\t\t\t\tfiles_by_xml[xml].append(file)\n\treturn files_by_xml\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\n\tfor xml in xmls:\n\t\tresult[xml] = []\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif file.endswith(xml):\n\t\t\t\tresult[xml].append(file)\n\t\t\t\tbreak\n\n\treturn result\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\txml_files = [f for f in files if xml in f]\n\t\tdata[xml] = xml_files\n\treturn data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tdata[xml] = [f for f in files if f.startswith(xml)]\n\treturn data\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor filename in xmls:\n\t\tresult[filename] = []\n\tfor file in files:\n\t\tfilename = os.path.splitext(file)[0] + \".xml\"\n\t\tif filename in xmls:\n\t\t\tresult[filename].append(file)\n\treturn result\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t# make sure we have at least one file and one xml\n\tif len(files) == 0 and len(xmls) == 0:\n\t\traise ValueError(\"At least one xml and one file must be provided.\")\n\n\t# make sure we have at least one file\n\tif len(files) == 0:\n\t\traise ValueError(\"At least one file must be provided.\")\n\n\t# make sure we have at least one xml\n\tif len(xmls) == 0:\n\t\traise ValueError(\"At least one xml must be provided.\")\n\n\t# create a map from xml to files\n\txml_to_files = {}\n\tfor xml in xmls:\n\t\tif xml in xml_to_files:\n\t\t\txml_to_files[xml].append(files[0])\n\t\telse:\n\t\t\txml_to_files[xml] = [files[0]]\n\n\t# create a map from xml to files\n\tfiles_to_xmls = {}\n\tfor file in files:\n\t\tif file in files_to_xmls:\n\t\t\tfiles_to_xmls[file].append(xmls[0])\n\t\telse:\n\t\t\tfiles_to_xmls[file] = [xmls[0]]\n\n\t# group files", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor xml in xmls:\n\t\tresult[xml] = [f for f in files if f.endswith(xml)]\n\treturn result\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor xml in xmls:\n\t\tresult[xml['filename']] = []\n\n\tfor file in files:\n\t\tresult[file['xml_id']].append(file)\n\n\treturn result\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tdata[xml] = []\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif xml in file:\n\t\t\t\tdata[xml].append(file)\n\treturn data\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor f in files:\n\t\tif f not in xmls:\n\t\t\tdata.setdefault(f, []).append(source + f)\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tfiles_by_xml = {}\n\tfor xml in xmls:\n\t\tif xml not in files_by_xml:\n\t\t\tfiles_by_xml[xml] = []\n\t\tfor file in files:\n\t\t\tif xml in file:\n\t\t\t\tfiles_by_xml[xml].append(file)\n\treturn files_by_xml\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\n\tfor xml in xmls:\n\t\tresult[xml] = []\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif file.endswith(xml):\n\t\t\t\tresult[xml].append(file)\n\t\t\t\tbreak\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\txml_files = [f for f in files if xml in f]\n\t\tdata[xml] = xml_files\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tdata[xml] = [f for f in files if f.startswith(xml)]\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor filename in xmls:\n\t\tresult[filename] = []\n\tfor file in files:\n\t\tfilename = os.path.splitext(file)[0] + \".xml\"\n\t\tif filename in xmls:\n\t\t\tresult[filename].append(file)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t# make sure we have at least one file and one xml\n\tif len(files) == 0 and len(xmls) == 0:\n\t\traise ValueError(\"At least one xml and one file must be provided.\")\n\n\t# make sure we have at least one file\n\tif len(files) == 0:\n\t\traise ValueError(\"At least one file must be provided.\")\n\n\t# make sure we have at least one xml\n\tif len(xmls) == 0:\n\t\traise ValueError(\"At least one xml must be provided.\")\n\n\t# create a map from xml to files\n\txml_to_files = {}\n\tfor xml in xmls:\n\t\tif xml in xml_to_files:\n\t\t\txml_to_files[xml].append(files[0])\n\t\telse:\n\t\t\txml_to_files[xml] = [files[0]]\n\n\t# create a map from xml to files\n\tfiles_to_xmls = {}\n\tfor file in files:\n\t\tif file in files_to_xmls:\n\t\t\tfiles_to_xmls[file].append(xmls[0])\n\t\telse:\n\t\t\tfiles_to_xmls[file] = [xmls[0]]\n\n\t# group files", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor xml in xmls:\n\t\tresult[xml] = [f for f in files if f.endswith(xml)]\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\tfor xml in xmls:\n\t\tresult[xml['filename']] = []\n\n\tfor file in files:\n\t\tresult[file['xml_id']].append(file)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {}\n\tfor xml in xmls:\n\t\tdata[xml] = []\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif xml in file:\n\t\t\t\tdata[xml].append(file)\n\treturn data\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn file_path.endswith(prefix + \"-\") or file_path.endswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbase = os.path.basename(file_path)\n\tif prefix + \"-\" in base or prefix + \".\" in base:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t# if the file path is a directory, return false\n\tif not os.path.isfile(file_path):\n\t\treturn False\n\t# if the file path exists and is a file, return true if the basename of the path startswith the given prefix and \"-\" or the given prefix and \".\"\n\t# otherwise, return false\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tif file_path is None:\n\t\treturn False\n\tfilename = os.path.basename(file_path)\n\tif filename is None:\n\t\treturn False\n\tif filename.startswith(prefix + \"-\") or filename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + '-') or basename.startswith(prefix + '.')\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn file_path.split(\"/\")[-1].startswith(prefix + \"-\") or file_path.split(\"/\")[-1].startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn file_path.startswith(prefix) or file_path.startswith(prefix + \"-\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn file_path.endswith(prefix + \"-\") or file_path.endswith(prefix + \".\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbase = os.path.basename(file_path)\n\tif prefix + \"-\" in base or prefix + \".\" in base:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t# if the file path is a directory, return false\n\tif not os.path.isfile(file_path):\n\t\treturn False\n\t# if the file path exists and is a file, return true if the basename of the path startswith the given prefix and \"-\" or the given prefix and \".\"\n\t# otherwise, return false\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tif file_path is None:\n\t\treturn False\n\tfilename = os.path.basename(file_path)\n\tif filename is None:\n\t\treturn False\n\tif filename.startswith(prefix + \"-\") or filename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + '-') or basename.startswith(prefix + '.')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn file_path.split(\"/\")[-1].startswith(prefix + \"-\") or file_path.split(\"/\")[-1].startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn file_path.startswith(prefix) or file_path.startswith(prefix + \"-\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [file for file in files if file.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [file for file in files if file.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatched_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatched_files.append(file)\n\treturn matched_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tres = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tres.append(f)\n\treturn res\n\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [file for file in files if file.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [file for file in files if file.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatched_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatched_files.append(file)\n\treturn matched_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tres = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tres.append(f)\n\treturn res\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfile_groups = _group_files_by_xml_filename(folder)\n\tif file_groups.keys():\n\t\t_explore_file_groups(file_groups)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint(\"Exploring folder: \" + folder)\n\tfiles = _get_files(folder)\n\tfiles = _group_files_by_xml_filename(files)\n\tif len(files) == 0:\n\t\tprint(\"No files found\")\n\telse:\n\t\tfor group in files:\n\t\t\tprint(\"Group: \" + str(group))\n\t\t\tfor f in files[group]:\n\t\t\t\tprint(\"\\t\" + f)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tif not os.path.isdir(folder):\n\t\tprint('The given path does not exist.')\n\t\treturn\n\tprint('Exploring ' + folder)\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tif filename.endswith('.xml'):\n\t\t\t\t_group_files_by_xml_filename(folder, root, filename)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = os.listdir(folder)\n\tfiles.sort()\n\t\n\tgroups = []\n\tfor filename in files:\n\t\tgroups.append(filename[:-4] + '.xml')\n\t\n\tgrouped_files = _group_files_by_xml_filename(folder, files)\n\tfor group in grouped_files:\n\t\tgroup.sort()\n\t\n\treturn grouped_files\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor file in files:\n\t\t\tif file.endswith(\".xml\"):\n\t\t\t\tfiles.append(os.path.join(root, file))\n\treturn _group_files_by_xml_filename(files)\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(folder)\n\texplored_files = []\n\tfor file_group in files:\n\t\t# get the most recent file in the group\n\t\tlatest_file = max(file_group, key=os.path.getctime)\n\t\tdirectory = os.path.dirname(latest_file)\n\t\tfilename = os.path.basename(latest_file)\n\t\t# remove the .xml extension from the filename\n\t\tfilename_without_extension = filename.split(\".\")[0]\n\t\t# convert the filename to a datetime object\n\t\tdatetime_obj = datetime.datetime.strptime(filename_without_extension, \"%Y%m%d-%H%M%S\")\n\t\texplored_files.append((directory, filename_without_extension, datetime_obj))\n\treturn explored_files\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = os.listdir(folder)\n\tfiles = [os.path.join(folder, f) for f in files]\n\tsorted_files = _group_files_by_xml_filename(files)\n\tfor group in sorted_files:\n\t\tgroup_folder = os.path.join(folder, group)\n\t\tif not os.path.isdir(group_folder):\n\t\t\tos.makedirs(group_folder)\n\t\tfor f in sorted_files[group]:\n\t\t\tos.rename(f, os.path.join(group_folder, os.path.basename(f)))\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroup = {}\n\tfor file_name in os.listdir(folder):\n\t\tif os.path.isdir(os.path.join(folder, file_name)):\n\t\t\tgroup.update(_explore_folder(os.path.join(folder, file_name)))\n\t\t\tcontinue\n\t\tfile_path = os.path.join(folder, file_name)\n\t\tif not file_name.endswith('.xml'):\n\t\t\tcontinue\n\t\tgroup_name = _group_files_by_xml_filename(file_path)\n\t\tif group_name not in group:\n\t\t\tgroup[group_name] = []\n\t\tgroup[group_name].append(file_path)\n\treturn group\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(folder)\n\tfor filename, files in files.items():\n\t\t_explore_file(folder, filename, files)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = list()\n\tfor file in os.listdir(folder):\n\t\tfiles.append(os.path.join(folder, file))\n\treturn _group_files_by_xml_filename(files)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfile_groups = _group_files_by_xml_filename(folder)\n\tif file_groups.keys():\n\t\t_explore_file_groups(file_groups)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint(\"Exploring folder: \" + folder)\n\tfiles = _get_files(folder)\n\tfiles = _group_files_by_xml_filename(files)\n\tif len(files) == 0:\n\t\tprint(\"No files found\")\n\telse:\n\t\tfor group in files:\n\t\t\tprint(\"Group: \" + str(group))\n\t\t\tfor f in files[group]:\n\t\t\t\tprint(\"\\t\" + f)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tif not os.path.isdir(folder):\n\t\tprint('The given path does not exist.')\n\t\treturn\n\tprint('Exploring ' + folder)\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tif filename.endswith('.xml'):\n\t\t\t\t_group_files_by_xml_filename(folder, root, filename)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = os.listdir(folder)\n\tfiles.sort()\n\t\n\tgroups = []\n\tfor filename in files:\n\t\tgroups.append(filename[:-4] + '.xml')\n\t\n\tgrouped_files = _group_files_by_xml_filename(folder, files)\n\tfor group in grouped_files:\n\t\tgroup.sort()\n\t\n\treturn grouped_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(folder)\n\texplored_files = []\n\tfor file_group in files:\n\t\t# get the most recent file in the group\n\t\tlatest_file = max(file_group, key=os.path.getctime)\n\t\tdirectory = os.path.dirname(latest_file)\n\t\tfilename = os.path.basename(latest_file)\n\t\t# remove the .xml extension from the filename\n\t\tfilename_without_extension = filename.split(\".\")[0]\n\t\t# convert the filename to a datetime object\n\t\tdatetime_obj = datetime.datetime.strptime(filename_without_extension, \"%Y%m%d-%H%M%S\")\n\t\texplored_files.append((directory, filename_without_extension, datetime_obj))\n\treturn explored_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = os.listdir(folder)\n\tfiles = [os.path.join(folder, f) for f in files]\n\tsorted_files = _group_files_by_xml_filename(files)\n\tfor group in sorted_files:\n\t\tgroup_folder = os.path.join(folder, group)\n\t\tif not os.path.isdir(group_folder):\n\t\t\tos.makedirs(group_folder)\n\t\tfor f in sorted_files[group]:\n\t\t\tos.rename(f, os.path.join(group_folder, os.path.basename(f)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroup = {}\n\tfor file_name in os.listdir(folder):\n\t\tif os.path.isdir(os.path.join(folder, file_name)):\n\t\t\tgroup.update(_explore_folder(os.path.join(folder, file_name)))\n\t\t\tcontinue\n\t\tfile_path = os.path.join(folder, file_name)\n\t\tif not file_name.endswith('.xml'):\n\t\t\tcontinue\n\t\tgroup_name = _group_files_by_xml_filename(file_path)\n\t\tif group_name not in group:\n\t\t\tgroup[group_name] = []\n\t\tgroup[group_name].append(file_path)\n\treturn group\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(folder)\n\tfor filename, files in files.items():\n\t\t_explore_file(folder, filename, files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = list()\n\tfor file in os.listdir(folder):\n\t\tfiles.append(os.path.join(folder, file))\n\treturn _group_files_by_xml_filename(files)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\t# get component id\n\tcomponent_id = prefix.rsplit('/', 1)[1]\n\t\n\t# get file type\n\tfile_type = file_path.rsplit('.', 1)[1]\n\t\n\tif file_type != 'pdf':\n\t\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': file_type, 'file_path_full': os.path.join(prefix, file_path)}\n\telse:\n\t\treturn {'component_id': component_id, 'file_path': file_path, 'file_path_full': os.path.join(prefix, file_path)}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t# Check if the file name starts with the given prefix\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\t# Check if the file type is xml\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\t# Check if the file type is pdf\n\tif file_path.endswith(\".pdf\"):\n\t\treturn { \"component_id\" : file_path.replace(\".pdf\", \"\") }\n\t# Check if the file type is not pdf\n\treturn { \"component_id\" : file_path.replace(\".pdf\", \"\"), \"file_path\" : file_path }\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif file_path and file_path.startswith(prefix):\n\t\tname = os.path.basename(file_path)\n\t\tif name.startswith('.'):\n\t\t\tname = name[1:]\n\t\tif name.endswith('.xml'):\n\t\t\treturn None\n\t\tftype = name.split('.')[1]\n\t\tif ftype == 'pdf':\n\t\t\treturn dict(component_id=name[:-4], file_path=file_path)\n\t\telse:\n\t\t\treturn dict(component_id=name[:-4], file_path=file_path, ftype=ftype)\n\telse:\n\t\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\tif file_path.endswith(\".pdf\"):\n\t\treturn {\"component_id\": file_path[len(prefix):-4], \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": file_path[len(prefix):], \"file_path\": file_path, \"ftype\": \"other\"}\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tfile_name = file_path[len(prefix):]\n\tif file_name.startswith(\".\"):\n\t\treturn None\n\tif file_name.endswith(\".xml\"):\n\t\treturn None\n\tif file_name.endswith(\".pdf\"):\n\t\treturn {\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": \"pdf\",\n\t\t}\n\telse:\n\t\treturn {\n\t\t\t\"file_path\": file_path,\n\t\t}\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tfile_name = os.path.basename(file_path)\n\t\n\tif not file_name.startswith(prefix):\n\t\treturn None\n\t\n\tftype = file_name.split('.')[-1]\n\t\n\tif ftype == 'pdf':\n\t\treturn { prefix: file_path }\n\t\n\treturn { prefix: file_path, 'ftype': ftype, 'file_path': file_path }\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_path = file_path.strip()\n\tfile_name = os.path.basename(file_path)\n\tfile_name_ext = os.path.splitext(file_name)[-1]\n\tif file_name_ext == '.pdf' or file_name_ext == '.PDF':\n\t\treturn None\n\tif file_name_ext != '.xml':\n\t\treturn {\n\t\t\t'component_id': os.path.basename(file_path).split('.')[0],\n\t\t\t'file_path': file_path\n\t\t}\n\telement_tree = etree.parse(file_path)\n\tfor element in element_tree.iter():\n\t\tif element.tag == 'component_id':\n\t\t\tcomponent_id = element.text\n\t\t\tif component_id.startswith(prefix):\n\t\t\t\treturn {\n\t\t\t\t\t'component_id': component_id,\n\t\t\t\t\t'file_path': file_path\n\t\t\t\t}\n\t\t\treturn None\n\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif file_path.startswith(prefix) and not file_path.endswith(\".xml\"):\n\t\tfile_path = file_path.replace(prefix, \"\", 1)\n\t\tfile_path = file_path.strip(\"/\")\n\t\tif file_path.startswith(\".\"):\n\t\t\tfile_path = file_path[1:]\n\t\treturn {\n\t\t\t\"component_id\": file_path.split(\"/\")[0],\n\t\t\t\"file_path\": file_path\n\t\t}\n\telse:\n\t\treturn None\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tfile_type = file_path[len(prefix):].split(\".\")[-1]\n\tif file_type == \"pdf\":\n\t\treturn {\n\t\t\t\"component_id\": file_path[len(prefix):-len(\".pdf\")],\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": file_type\n\t\t}\n\telse:\n\t\treturn {\n\t\t\t\"component_id\": file_path[len(prefix):-len(\".txt\")],\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": file_type\n\t\t}\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tfile_path = file_path.strip()\n\t\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\t\n\tfile_path = file_path[len(prefix):]\n\t\n\tif not file_path.startswith('/'):\n\t\tfile_path = '/' + file_path\n\t\n\tif file_path.startswith('/pdf'):\n\t\treturn {'component_id': file_path.split('/')[1], 'file_path': file_path}\n\telif file_path.startswith('//'):\n\t\treturn {'component_id': file_path.split('/')[1], 'file_path': file_path, 'ftype': 'pdf'}\n\telse:\n\t\treturn {'component_id': file_path.split('/')[1], 'file_path': file_path, 'ftype': 'html'}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\t# get component id\n\tcomponent_id = prefix.rsplit('/', 1)[1]\n\t\n\t# get file type\n\tfile_type = file_path.rsplit('.', 1)[1]\n\t\n\tif file_type != 'pdf':\n\t\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': file_type, 'file_path_full': os.path.join(prefix, file_path)}\n\telse:\n\t\treturn {'component_id': component_id, 'file_path': file_path, 'file_path_full': os.path.join(prefix, file_path)}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t# Check if the file name starts with the given prefix\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\t# Check if the file type is xml\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\t# Check if the file type is pdf\n\tif file_path.endswith(\".pdf\"):\n\t\treturn { \"component_id\" : file_path.replace(\".pdf\", \"\") }\n\t# Check if the file type is not pdf\n\treturn { \"component_id\" : file_path.replace(\".pdf\", \"\"), \"file_path\" : file_path }\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif file_path and file_path.startswith(prefix):\n\t\tname = os.path.basename(file_path)\n\t\tif name.startswith('.'):\n\t\t\tname = name[1:]\n\t\tif name.endswith('.xml'):\n\t\t\treturn None\n\t\tftype = name.split('.')[1]\n\t\tif ftype == 'pdf':\n\t\t\treturn dict(component_id=name[:-4], file_path=file_path)\n\t\telse:\n\t\t\treturn dict(component_id=name[:-4], file_path=file_path, ftype=ftype)\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\tif file_path.endswith(\".pdf\"):\n\t\treturn {\"component_id\": file_path[len(prefix):-4], \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": file_path[len(prefix):], \"file_path\": file_path, \"ftype\": \"other\"}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tfile_name = file_path[len(prefix):]\n\tif file_name.startswith(\".\"):\n\t\treturn None\n\tif file_name.endswith(\".xml\"):\n\t\treturn None\n\tif file_name.endswith(\".pdf\"):\n\t\treturn {\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": \"pdf\",\n\t\t}\n\telse:\n\t\treturn {\n\t\t\t\"file_path\": file_path,\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tfile_name = os.path.basename(file_path)\n\t\n\tif not file_name.startswith(prefix):\n\t\treturn None\n\t\n\tftype = file_name.split('.')[-1]\n\t\n\tif ftype == 'pdf':\n\t\treturn { prefix: file_path }\n\t\n\treturn { prefix: file_path, 'ftype': ftype, 'file_path': file_path }\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_path = file_path.strip()\n\tfile_name = os.path.basename(file_path)\n\tfile_name_ext = os.path.splitext(file_name)[-1]\n\tif file_name_ext == '.pdf' or file_name_ext == '.PDF':\n\t\treturn None\n\tif file_name_ext != '.xml':\n\t\treturn {\n\t\t\t'component_id': os.path.basename(file_path).split('.')[0],\n\t\t\t'file_path': file_path\n\t\t}\n\telement_tree = etree.parse(file_path)\n\tfor element in element_tree.iter():\n\t\tif element.tag == 'component_id':\n\t\t\tcomponent_id = element.text\n\t\t\tif component_id.startswith(prefix):\n\t\t\t\treturn {\n\t\t\t\t\t'component_id': component_id,\n\t\t\t\t\t'file_path': file_path\n\t\t\t\t}\n\t\t\treturn None\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif file_path.startswith(prefix) and not file_path.endswith(\".xml\"):\n\t\tfile_path = file_path.replace(prefix, \"\", 1)\n\t\tfile_path = file_path.strip(\"/\")\n\t\tif file_path.startswith(\".\"):\n\t\t\tfile_path = file_path[1:]\n\t\treturn {\n\t\t\t\"component_id\": file_path.split(\"/\")[0],\n\t\t\t\"file_path\": file_path\n\t\t}\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tfile_type = file_path[len(prefix):].split(\".\")[-1]\n\tif file_type == \"pdf\":\n\t\treturn {\n\t\t\t\"component_id\": file_path[len(prefix):-len(\".pdf\")],\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": file_type\n\t\t}\n\telse:\n\t\treturn {\n\t\t\t\"component_id\": file_path[len(prefix):-len(\".txt\")],\n\t\t\t\"file_path\": file_path,\n\t\t\t\"ftype\": file_type\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tfile_path = file_path.strip()\n\t\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\t\n\tfile_path = file_path[len(prefix):]\n\t\n\tif not file_path.startswith('/'):\n\t\tfile_path = '/' + file_path\n\t\n\tif file_path.startswith('/pdf'):\n\t\treturn {'component_id': file_path.split('/')[1], 'file_path': file_path}\n\telif file_path.startswith('//'):\n\t\treturn {'component_id': file_path.split('/')[1], 'file_path': file_path, 'ftype': 'pdf'}\n\telse:\n\t\treturn {'component_id': file_path.split('/')[1], 'file_path': file_path, 'ftype': 'html'}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tif lang in self._renditions:\n\t\traise ValueError(f\"{lang} is already assigned a filepath.\")\n\telse:\n\t\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tif lang in self._renditions:\n\t\tself._renditions[lang].append(file_path)\n\telse:\n\t\tself._renditions[lang] = [file_path]\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n\treturn self\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tif lang in self._renditions:\n\t\traise ValueError(f\"{lang} is already assigned a filepath.\")\n\telse:\n\t\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tif lang in self._renditions:\n\t\tself._renditions[lang].append(file_path)\n\telse:\n\t\tself._renditions[lang] = [file_path]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n\treturn self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif not basename in self._assets:\n\t\tself._assets[basename] = file_path\n\t\treturn True\n\telse:\n\t\treturn False\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\treturn\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif basename in self._assets.keys():\n\t\tself._assets[basename] = file_path\n\telse:\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif basename in self._assets:\n\t\traise KeyError(f\"Key {basename} already exists in the assets dictionary\")\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif basename in self._assets:\n\t\tprint(\"[WARNING] : %s already exists in assets.\" % basename)\n\telse:\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\treturn file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif not basename in self._assets:\n\t\tself._assets[basename] = file_path\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\treturn\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif basename in self._assets.keys():\n\t\tself._assets[basename] = file_path\n\telse:\n\t\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif basename in self._assets:\n\t\traise KeyError(f\"Key {basename} already exists in the assets dictionary\")\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tif basename in self._assets:\n\t\tprint(\"[WARNING] : %s already exists in assets.\" % basename)\n\telse:\n\t\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\treturn file_path\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(_explore_zip(zip_path))\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zip_path)\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t# Extract the zip file to a temporary folder.\n\ttmpdir = tempfile.mkdtemp()\n\tzip_ref = zipfile.ZipFile(zip_path, 'r')\n\tzip_ref.extractall(tmpdir)\n\tzip_ref.close()\n\n\t# Group the extracted files by xml filename.\n\txml_files = {}\n\tfor root, dirs, files in os.walk(tmpdir):\n\t\tfor file in files:\n\t\t\tif file.endswith('.xml'):\n\t\t\t\txml_files[file] = _group_files_by_xml_filename(os.path.join(root, file))\n\n\t# Clean up.\n\tshutil.rmtree(tmpdir)\n\treturn xml_files\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgrouped_files = _group_files_by_xml_filename(zip_path)\n\treturn grouped_files\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t# 1. Get a list of all files in the zip.\n\tfile_list = []\n\twith zipfile.ZipFile(zip_path, \"r\") as z:\n\t\tfor f in z.namelist():\n\t\t\tfile_list.append(f)\n\n\t# 2. Group the files by xml filename.\n\treturn _group_files_by_xml_filename(file_list)\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zipfile.ZipFile(zip_path))\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgrouped_files = _group_files_by_xml_filename(zip_path)\n\treturn grouped_files\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as f:\n\t\tfor name in f.namelist():\n\t\t\tfiles.append(name)\n\n\tgroups = _group_files_by_xml_filename(files)\n\n\treturn groups\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zip_path)\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zip_path)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(_explore_zip(zip_path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zip_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t# Extract the zip file to a temporary folder.\n\ttmpdir = tempfile.mkdtemp()\n\tzip_ref = zipfile.ZipFile(zip_path, 'r')\n\tzip_ref.extractall(tmpdir)\n\tzip_ref.close()\n\n\t# Group the extracted files by xml filename.\n\txml_files = {}\n\tfor root, dirs, files in os.walk(tmpdir):\n\t\tfor file in files:\n\t\t\tif file.endswith('.xml'):\n\t\t\t\txml_files[file] = _group_files_by_xml_filename(os.path.join(root, file))\n\n\t# Clean up.\n\tshutil.rmtree(tmpdir)\n\treturn xml_files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgrouped_files = _group_files_by_xml_filename(zip_path)\n\treturn grouped_files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t# 1. Get a list of all files in the zip.\n\tfile_list = []\n\twith zipfile.ZipFile(zip_path, \"r\") as z:\n\t\tfor f in z.namelist():\n\t\t\tfile_list.append(f)\n\n\t# 2. Group the files by xml filename.\n\treturn _group_files_by_xml_filename(file_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zipfile.ZipFile(zip_path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgrouped_files = _group_files_by_xml_filename(zip_path)\n\treturn grouped_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as f:\n\t\tfor name in f.namelist():\n\t\t\tfiles.append(name)\n\n\tgroups = _group_files_by_xml_filename(files)\n\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zip_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\treturn _group_files_by_xml_filename(zip_path)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, \"r\") as zip_file:\n\t\tfiles_list = zip_file.namelist()\n\treturn files_list\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\treturn zipfile.ZipFile(zip_path).namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\treturn [\n\t\tf for f in zipfile.ZipFile(zip_path, \"r\").namelist()\n\t\tif os.path.isfile(f)\n\t]\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor name in zf.namelist():\n\t\t\tfiles.append(name)\n\treturn files\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles_list = []\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\tfor file in zip_file.namelist():\n\t\t\tfiles_list.append(file)\n\treturn files_list\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\treturn zf.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zipf:\n\t\tfor file in zipf.namelist():\n\t\t\tfiles.append(file)\n\treturn files\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tfor file in zip_ref.namelist():\n\t\t\tfiles.append(file)\n\treturn files\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t# Open the zip file and its file list\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfile_list = zip_file.namelist()\n\n\treturn file_list\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, \"r\") as zip_file:\n\t\tfiles_list = zip_file.namelist()\n\treturn files_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\treturn zipfile.ZipFile(zip_path).namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\treturn [\n\t\tf for f in zipfile.ZipFile(zip_path, \"r\").namelist()\n\t\tif os.path.isfile(f)\n\t]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor name in zf.namelist():\n\t\t\tfiles.append(name)\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles_list = []\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\tfor file in zip_file.namelist():\n\t\t\tfiles_list.append(file)\n\treturn files_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\treturn zf.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zipf:\n\t\tfor file in zipf.namelist():\n\t\t\tfiles.append(file)\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tfor file in zip_ref.namelist():\n\t\t\tfiles.append(file)\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t# Open the zip file and its file list\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfile_list = zip_file.namelist()\n\n\treturn file_list\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace(b\"w:st\",b\"w-st\")\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace(\"w:st=\", \"w-st=\")\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace('w:st=', 'w-st=')\n\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = re.sub(r'w:st=', r'w-st=', content)\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn re.sub(r\"w:st=\", \"w-st=\", content)\n\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace('w:st=', 'w-st=')\n\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace(\"w:st=\", \"w-st=\")\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t# The regexp is not perfect but it should do the trick.\n\treturn re.sub(r'w:st=', 'w-st=', content)\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace('w:', 'w-')\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace(b\"w:st\",b\"w-st\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace(\"w:st=\", \"w-st=\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace('w:st=', 'w-st=')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = re.sub(r'w:st=', r'w-st=', content)\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn re.sub(r\"w:st=\", \"w-st=\", content)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace('w:st=', 'w-st=')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace(\"w:st=\", \"w-st=\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t# The regexp is not perfect but it should do the trick.\n\treturn re.sub(r'w:st=', 'w-st=', content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\treturn content.replace('w:', 'w-')\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_node = node.find(pubdate_xpath, namespaces=namespaces)\n\t\tif pubdate_node is not None:\n\t\t\treturn pubdate_node.text\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.findtext(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.find(pubdate_xpath)\n\t\tif pubdate is not None:\n\t\t\treturn pubdate\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(xpath)[0].text_content()\n\t\texcept IndexError:\n\t\t\tpass\n\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tif xpath_is_valid(node):\n\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\tpubdate = node.xpath(pubdate_xpath)\n\t\t\tif pubdate:\n\t\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tif not node or not pubdate_xpaths:\n\t\treturn None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate_nodes = node.xpath(xpath)\n\t\tif pubdate_nodes:\n\t\t\treturn pubdate_nodes[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\tfor child in node.xpath(xpath):\n\t\t\treturn child.text\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(xpath)[0].text_content()\n\t\texcept IndexError:\n\t\t\tpass\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(xpath)[0]\n\t\texcept IndexError:\n\t\t\t# nothing found\n\t\t\tpass\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\tpubdate = pubdate[0].text_content()\n\t\t\treturn pubdate\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_node = node.find(pubdate_xpath, namespaces=namespaces)\n\t\tif pubdate_node is not None:\n\t\t\treturn pubdate_node.text\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.findtext(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.find(pubdate_xpath)\n\t\tif pubdate is not None:\n\t\t\treturn pubdate\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(xpath)[0].text_content()\n\t\texcept IndexError:\n\t\t\tpass\n\n\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tif xpath_is_valid(node):\n\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\tpubdate = node.xpath(pubdate_xpath)\n\t\t\tif pubdate:\n\t\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tif not node or not pubdate_xpaths:\n\t\treturn None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate_nodes = node.xpath(xpath)\n\t\tif pubdate_nodes:\n\t\t\treturn pubdate_nodes[0]\n\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\tfor child in node.xpath(xpath):\n\t\t\treturn child.text\n\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(xpath)[0].text_content()\n\t\texcept IndexError:\n\t\t\tpass\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(xpath)[0]\n\t\texcept IndexError:\n\t\t\t# nothing found\n\t\t\tpass\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\tpubdate = pubdate[0].text_content()\n\t\t\treturn pubdate\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tif issue is None:\n\t\treturn None, None\n\tif isinstance(issue, str):\n\t\tissue = [issue]\n\tif isinstance(issue, list):\n\t\tfor i in issue:\n\t\t\tif i is None:\n\t\t\t\treturn None, None\n\t\t\tif not isinstance(i, str):\n\t\t\t\traise TypeError(\"issue element must be a string\")\n\t\treturn issue[0].strip(), issue[1].strip()\n\telse:\n\t\traise TypeError(\"issue must be a string or list\")\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tfor child in issue:\n\t\tif child.tag == \"number\":\n\t\t\tnumber = child.text\n\t\telif child.tag == \"supplement\":\n\t\t\tsup = child.text\n\treturn number, sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t# Extract the number and the supplement\n\tnumber = issue.xpath('./div[1]/text()').extract()[0].strip()\n\tif len(number) == 0:\n\t\tnumber = None\n\telse:\n\t\tnumber = int(number)\n\t\n\tsuppl = issue.xpath('./div[2]/text()').extract()[0].strip()\n\tif len(suppl) == 0:\n\t\tsuppl = None\n\telse:\n\t\tsuppl = int(suppl)\n\t\n\treturn number, suppl\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsuppl = None\n\tif issue.contents:\n\t\tfor item in issue.contents:\n\t\t\tif item.tag == \"number\":\n\t\t\t\tnumber = item.text\n\t\t\telif item.tag == \"suppl\":\n\t\t\t\tsuppl = item.text\n\treturn number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsuppl_text = None\n\tfor child in list(issue):\n\t\tif child.tag == \"number\":\n\t\t\tnumber = child.text\n\t\telif child.tag == \"suppl\":\n\t\t\tsuppl_text = child.text\n\tif suppl_text is not None:\n\t\tsuppl = int(suppl_text)\n\telse:\n\t\tsuppl = 0\n\treturn number, suppl\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tissue_element = issue.get_element_by_id('issue')\n\tnumber = issue_element.get_attribute('number')\n\tsuppl = issue_element.get_attribute('supplement')\n\treturn number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_sup = []\n\tfor child in issue.children:\n\t\tif child.tag == \"number\":\n\t\t\tnumber_and_sup.append(child.text)\n\t\telif child.tag == \"sup\":\n\t\t\tnumber_and_sup.append(child.text)\n\treturn number_and_sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\treturn _extract_number_and_supplment_from_issue_contents(issue.text)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber, sup = 0, 0\n\tfor token in issue:\n\t\tif token.tag == 'number':\n\t\t\tnumber = token.text\n\t\telif token.tag == 'sup':\n\t\t\tsup = token.text\n\treturn number, sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tissue_contents = issue.text\n\tissue_contents = issue_contents.strip()\n\tissue_contents = issue_contents.split(\" \")\n\tnumber = None\n\tsuppl = None\n\tif \"(\" in issue_contents[-1]:\n\t\tnumber = issue_contents[-1][:-1]\n\t\tsuppl = issue_contents[-1][-1]\n\telif \"Suppl.\" in issue_contents[-1]:\n\t\tnumber = issue_contents[-2]\n\t\tsuppl = issue_contents[-1]\n\telse:\n\t\tnumber = issue_contents[-1]\n\t\tsuppl = None\n\treturn number, suppl\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tif issue is None:\n\t\treturn None, None\n\tif isinstance(issue, str):\n\t\tissue = [issue]\n\tif isinstance(issue, list):\n\t\tfor i in issue:\n\t\t\tif i is None:\n\t\t\t\treturn None, None\n\t\t\tif not isinstance(i, str):\n\t\t\t\traise TypeError(\"issue element must be a string\")\n\t\treturn issue[0].strip(), issue[1].strip()\n\telse:\n\t\traise TypeError(\"issue must be a string or list\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tfor child in issue:\n\t\tif child.tag == \"number\":\n\t\t\tnumber = child.text\n\t\telif child.tag == \"supplement\":\n\t\t\tsup = child.text\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t# Extract the number and the supplement\n\tnumber = issue.xpath('./div[1]/text()').extract()[0].strip()\n\tif len(number) == 0:\n\t\tnumber = None\n\telse:\n\t\tnumber = int(number)\n\t\n\tsuppl = issue.xpath('./div[2]/text()').extract()[0].strip()\n\tif len(suppl) == 0:\n\t\tsuppl = None\n\telse:\n\t\tsuppl = int(suppl)\n\t\n\treturn number, suppl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsuppl = None\n\tif issue.contents:\n\t\tfor item in issue.contents:\n\t\t\tif item.tag == \"number\":\n\t\t\t\tnumber = item.text\n\t\t\telif item.tag == \"suppl\":\n\t\t\t\tsuppl = item.text\n\treturn number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsuppl_text = None\n\tfor child in list(issue):\n\t\tif child.tag == \"number\":\n\t\t\tnumber = child.text\n\t\telif child.tag == \"suppl\":\n\t\t\tsuppl_text = child.text\n\tif suppl_text is not None:\n\t\tsuppl = int(suppl_text)\n\telse:\n\t\tsuppl = 0\n\treturn number, suppl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tissue_element = issue.get_element_by_id('issue')\n\tnumber = issue_element.get_attribute('number')\n\tsuppl = issue_element.get_attribute('supplement')\n\treturn number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_sup = []\n\tfor child in issue.children:\n\t\tif child.tag == \"number\":\n\t\t\tnumber_and_sup.append(child.text)\n\t\telif child.tag == \"sup\":\n\t\t\tnumber_and_sup.append(child.text)\n\treturn number_and_sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\treturn _extract_number_and_supplment_from_issue_contents(issue.text)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber, sup = 0, 0\n\tfor token in issue:\n\t\tif token.tag == 'number':\n\t\t\tnumber = token.text\n\t\telif token.tag == 'sup':\n\t\t\tsup = token.text\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tissue_contents = issue.text\n\tissue_contents = issue_contents.strip()\n\tissue_contents = issue_contents.split(\" \")\n\tnumber = None\n\tsuppl = None\n\tif \"(\" in issue_contents[-1]:\n\t\tnumber = issue_contents[-1][:-1]\n\t\tsuppl = issue_contents[-1][-1]\n\telif \"Suppl.\" in issue_contents[-1]:\n\t\tnumber = issue_contents[-2]\n\t\tsuppl = issue_contents[-1]\n\telse:\n\t\tnumber = issue_contents[-1]\n\t\tsuppl = None\n\treturn number, suppl\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.debug_details}\" if self.debug_details else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = indent + 4\n\tdebug_details = \"\"\n\tif self.label != None:\n\t\tdebug_details += f\"label='{self.label}' \"\n\tif self.default != None:\n\t\tdebug_details += f\"default='{self.default}' \"\n\tif self.name != None:\n\t\tdebug_details += f\"name='{self.name}' \"\n\tif self.description != None:\n\t\tdebug_details += f\"description='{self.description}' \"\n\tif self.validators != None:\n\t\tdebug_details += f\"validators={self.validators} \"\n\tif self.type != None:\n\t\tdebug_details += f\"type={self.type} \"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tif debug:\n\t\tdebug_details = f\"obj={repr(self.obj)}, \" if self.obj is not None else \"\"\n\t\tdebug_details += f\"source={repr(self.source)}, \" if self.source is not None else \"\"\n\t\tdebug_details += f\"source_type={repr(self.source_type)}, \" if self.source_type is not None else \"\"\n\t\tdebug_details += f\"source_id={repr(self.source_id)}, \" if self.source_id is not None else \"\"\n\t\tdebug_details += f\"source_field={repr(self.source_field)}, \" if self.source_field is not None else \"\"\n\t\tdebug_details += f\"value={repr(self.value)}, \" if self.value is not None else \"\"\n\t\tdebug_details += f\"value_type={repr(self.value_type)}, \" if self.value_type is not None else \"\"\n\t\tdebug_details += f\"value_id={repr(self.value_id)}, \" if self.value_id is not None else \"\"\n\t\tdebug_details += f\"value_field={", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tif debug:\n\t\tdebug_details = f\"{self.debug_details}\"\n\telse:\n\t\tdebug_details = \"\"\n\tif isinstance(self.obj, str):\n\t\tobj = f\"'{self.obj}'\"\n\telse:\n\t\tobj = repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"obj={obj}, \" if debug else ''\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f'''\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdebug:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttype: {self.type}\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"debug={self.debug}, \" if self.debug else ''\n\treturn f\"{(' ' * indent)}{self.__class__.__name__}({debug_details}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f\"{self.debug_repr(indent=indent + 1)}\"\n\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = indent + 2\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = (\n\t\t\tf\"debug={self.debug}, \"\n\t\t\tf\"error_details={repr(self.error_details)}, \"\n\t\t\tf\"exclude={repr(self.exclude)}, \"\n\t\t\tf\"exclude_regex={repr(self.exclude_regex)}, \"\n\t\t\tf\"include={repr(self.include)}, \"\n\t\t\tf\"include_regex={repr(self.include_regex)}, \"\n\t\t\tf\"obj={repr(self.obj)}, \"\n\t\t\tf\"parent={repr(self.parent)}, \"\n\t\t\tf\"remove_empty_keys={self.remove_empty_keys}, \"\n\t\t\tf\"remove_empty_values={self.remove_empty_values}, \"\n\t\t\tf\"remove_keys={repr(self.remove_keys)}, \"\n\t\t\tf\"remove_none_values={self.remove_none_values}, \"\n\t\t\tf\"remove_values={repr(self.remove_values)}, \"\n\t\t\tf\"replace_values={repr(self.replace_values)}, \"\n\t\t)\n\tobj = f\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 2\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f\"{self.debug_info()}\\n\"\n\tobj = \"\"\n\tif isinstance(self.obj, str):\n\t\tobj = f\"'{self.obj}'\"\n\telse:\n\t\tobj = repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"{self.debug_details}\" if self.debug_details else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = indent + 4\n\tdebug_details = \"\"\n\tif self.label != None:\n\t\tdebug_details += f\"label='{self.label}' \"\n\tif self.default != None:\n\t\tdebug_details += f\"default='{self.default}' \"\n\tif self.name != None:\n\t\tdebug_details += f\"name='{self.name}' \"\n\tif self.description != None:\n\t\tdebug_details += f\"description='{self.description}' \"\n\tif self.validators != None:\n\t\tdebug_details += f\"validators={self.validators} \"\n\tif self.type != None:\n\t\tdebug_details += f\"type={self.type} \"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tif debug:\n\t\tdebug_details = f\"obj={repr(self.obj)}, \" if self.obj is not None else \"\"\n\t\tdebug_details += f\"source={repr(self.source)}, \" if self.source is not None else \"\"\n\t\tdebug_details += f\"source_type={repr(self.source_type)}, \" if self.source_type is not None else \"\"\n\t\tdebug_details += f\"source_id={repr(self.source_id)}, \" if self.source_id is not None else \"\"\n\t\tdebug_details += f\"source_field={repr(self.source_field)}, \" if self.source_field is not None else \"\"\n\t\tdebug_details += f\"value={repr(self.value)}, \" if self.value is not None else \"\"\n\t\tdebug_details += f\"value_type={repr(self.value_type)}, \" if self.value_type is not None else \"\"\n\t\tdebug_details += f\"value_id={repr(self.value_id)}, \" if self.value_id is not None else \"\"\n\t\tdebug_details += f\"value_field={", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tif debug:\n\t\tdebug_details = f\"{self.debug_details}\"\n\telse:\n\t\tdebug_details = \"\"\n\tif isinstance(self.obj, str):\n\t\tobj = f\"'{self.obj}'\"\n\telse:\n\t\tobj = repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"obj={obj}, \" if debug else ''\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f'''\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdebug:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttype: {self.type}\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"debug={self.debug}, \" if self.debug else ''\n\treturn f\"{(' ' * indent)}{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f\"{self.debug_repr(indent=indent + 1)}\"\n\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = indent + 2\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = (\n\t\t\tf\"debug={self.debug}, \"\n\t\t\tf\"error_details={repr(self.error_details)}, \"\n\t\t\tf\"exclude={repr(self.exclude)}, \"\n\t\t\tf\"exclude_regex={repr(self.exclude_regex)}, \"\n\t\t\tf\"include={repr(self.include)}, \"\n\t\t\tf\"include_regex={repr(self.include_regex)}, \"\n\t\t\tf\"obj={repr(self.obj)}, \"\n\t\t\tf\"parent={repr(self.parent)}, \"\n\t\t\tf\"remove_empty_keys={self.remove_empty_keys}, \"\n\t\t\tf\"remove_empty_values={self.remove_empty_values}, \"\n\t\t\tf\"remove_keys={repr(self.remove_keys)}, \"\n\t\t\tf\"remove_none_values={self.remove_none_values}, \"\n\t\t\tf\"remove_values={repr(self.remove_values)}, \"\n\t\t\tf\"replace_values={repr(self.replace_values)}, \"\n\t\t)\n\tobj = f\"", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 2\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f\"{self.debug_info()}\\n\"\n\tobj = \"\"\n\tif isinstance(self.obj, str):\n\t\tobj = f\"'{self.obj}'\"\n\telse:\n\t\tobj = repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif arg.is_Add:\n\t\t\tnew_args.extend(self._absorb(arg.args))\n\t\telse:\n\t\t\tnew_args.append(arg)\n\treturn new_args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif isinstance(arg, Mul):\n\t\t\tnew_args.append(arg.args[0])\n\t\t\tfor arg2 in arg.args[1:]:\n\t\t\t\tnew_arg = Mul(arg2, arg.args[0])\n\t\t\t\tnew_args.append(new_arg)\n\t\telse:\n\t\t\tnew_args.append(arg)\n\treturn new_args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\treturn [self.apply_absorb(a) for a in args]\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\treturn args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\n\tresult = []\n\t\n\tfor arg in args:\n\t\t\n\t\t# The argument expression is a binary operator and the left hand side is a number\n\t\tif isinstance(arg, BinaryOperator) and \\\n\t\t   isinstance(arg.left, Number):\n\t\t\t\n\t\t\t# If the right hand side is a number then we can absorb it\n\t\t\tif isinstance(arg.right, Number):\n\t\t\t\t\n\t\t\t\tif isinstance(arg.left, Integer) and \\\n\t\t\t\t   isinstance(arg.right, Integer):\n\t\t\t\t\t\n\t\t\t\t\t# Integers are immutable so we need to create a new instance\n\t\t\t\t\tresult.append(Integer(arg.left.value + arg.right.value))\n\t\t\t\t\t\n\t\t\t\telif isinstance(arg.left, Float) and \\\n\t\t\t\t     isinstance(arg.right, Float):\n\t\t\t\t\t\n\t\t\t\t\t# Floats are immutable so we need to create a new instance\n\t\t\t\t\tresult.append(Float(arg.left.value + arg.right.value))\n\t\t\t\t\t\n\t\t\t\telif isinstance(arg.left, Complex) and \\\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tif len(args) < 2:\n\t\treturn args\n\n\t# Find the first argument to absorb.\n\tfor i in range(len(args)-1):\n\t\tif args[i] == 0:\n\t\t\tcontinue\n\t\tif args[i].is_number:\n\t\t\tcontinue\n\n\t\t# Found an argument to absorb.\n\t\t# Absorb the argument if it is a binary operator.\n\t\t# If it is a unary operator, the argument is absorbed by the operator.\n\t\targ = args[i+1]\n\t\tif arg.is_binary_operator:\n\t\t\targs[i] += arg\n\t\t\targs.pop(i+1)\n\t\t\treturn self.absorb(args)\n\t\telif arg.is_unary_operator:\n\t\t\targ.operator += args[i]\n\t\t\targs.pop(i)\n\t\t\treturn self.absorb(args)\n\n\treturn args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t# Check we have the same number of terms\n\tif len(args) < 3:\n\t\treturn args\n\n\t# Check for commutativity\n\tif not self.commutative:\n\t\traise ValueError(\"Cannot absorb non-commutative expressions\")\n\n\t# Check that the expressions are the same\n\tfor i in range(len(args)):\n\t\tfor j in range(len(args)):\n\t\t\tif not args[i] == args[j]:\n\t\t\t\traise ValueError(\"Cannot absorb expressions with different terms\")\n\n\t# Absorb the first two expressions\n\tresult = [args[0], args[1]]\n\n\t# Absorb the rest\n\tfor i in range(2, len(args)):\n\t\tresult = self.absorb_terms(result, args[i])\n\n\treturn result\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\treturn [absorb(arg) for arg in args]\n\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tresult = []\n\tfor arg in args:\n\t\tif arg.head == '*' and len(arg.args) == 2:\n\t\t\targ1, arg2 = arg.args\n\t\t\tif arg1.head == '-' and arg2.head == '-':\n\t\t\t\targ1, arg2 = arg1.args, arg2.args\n\t\t\tif arg1.head == '-':\n\t\t\t\targ1, arg2 = arg1.args, arg2\n\t\t\telif arg2.head == '-':\n\t\t\t\targ1, arg2 = arg1, arg2.args\n\t\t\tif arg1.head == '*' and len(arg1.args) == 2:\n\t\t\t\targ11, arg12 = arg1.args\n\t\t\t\tif arg12.is_number:\n\t\t\t\t\targ11, arg12 = arg12, arg11\n\t\t\t\tif arg2.is_number:\n\t\t\t\t\targ11, arg12 = arg11, arg2*arg12\n\t\t\t\tif arg11.is_number:\n\t\t\t\t\targ11, arg12 = arg12, arg11*arg2\n\t\t", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif isinstance(arg, Mul):\n\t\t\tnew_args.extend(arg.args)\n\t\telse:\n\t\t\tnew_args.append(arg)\n\treturn new_args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif arg.is_Add:\n\t\t\tnew_args.extend(self._absorb(arg.args))\n\t\telse:\n\t\t\tnew_args.append(arg)\n\treturn new_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif isinstance(arg, Mul):\n\t\t\tnew_args.append(arg.args[0])\n\t\t\tfor arg2 in arg.args[1:]:\n\t\t\t\tnew_arg = Mul(arg2, arg.args[0])\n\t\t\t\tnew_args.append(new_arg)\n\t\telse:\n\t\t\tnew_args.append(arg)\n\treturn new_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\treturn [self.apply_absorb(a) for a in args]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\n\tresult = []\n\t\n\tfor arg in args:\n\t\t\n\t\t# The argument expression is a binary operator and the left hand side is a number\n\t\tif isinstance(arg, BinaryOperator) and \\\n\t\t   isinstance(arg.left, Number):\n\t\t\t\n\t\t\t# If the right hand side is a number then we can absorb it\n\t\t\tif isinstance(arg.right, Number):\n\t\t\t\t\n\t\t\t\tif isinstance(arg.left, Integer) and \\\n\t\t\t\t   isinstance(arg.right, Integer):\n\t\t\t\t\t\n\t\t\t\t\t# Integers are immutable so we need to create a new instance\n\t\t\t\t\tresult.append(Integer(arg.left.value + arg.right.value))\n\t\t\t\t\t\n\t\t\t\telif isinstance(arg.left, Float) and \\\n\t\t\t\t     isinstance(arg.right, Float):\n\t\t\t\t\t\n\t\t\t\t\t# Floats are immutable so we need to create a new instance\n\t\t\t\t\tresult.append(Float(arg.left.value + arg.right.value))\n\t\t\t\t\t\n\t\t\t\telif isinstance(arg.left, Complex) and \\\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tif len(args) < 2:\n\t\treturn args\n\n\t# Find the first argument to absorb.\n\tfor i in range(len(args)-1):\n\t\tif args[i] == 0:\n\t\t\tcontinue\n\t\tif args[i].is_number:\n\t\t\tcontinue\n\n\t\t# Found an argument to absorb.\n\t\t# Absorb the argument if it is a binary operator.\n\t\t# If it is a unary operator, the argument is absorbed by the operator.\n\t\targ = args[i+1]\n\t\tif arg.is_binary_operator:\n\t\t\targs[i] += arg\n\t\t\targs.pop(i+1)\n\t\t\treturn self.absorb(args)\n\t\telif arg.is_unary_operator:\n\t\t\targ.operator += args[i]\n\t\t\targs.pop(i)\n\t\t\treturn self.absorb(args)\n\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t# Check we have the same number of terms\n\tif len(args) < 3:\n\t\treturn args\n\n\t# Check for commutativity\n\tif not self.commutative:\n\t\traise ValueError(\"Cannot absorb non-commutative expressions\")\n\n\t# Check that the expressions are the same\n\tfor i in range(len(args)):\n\t\tfor j in range(len(args)):\n\t\t\tif not args[i] == args[j]:\n\t\t\t\traise ValueError(\"Cannot absorb expressions with different terms\")\n\n\t# Absorb the first two expressions\n\tresult = [args[0], args[1]]\n\n\t# Absorb the rest\n\tfor i in range(2, len(args)):\n\t\tresult = self.absorb_terms(result, args[i])\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\treturn [absorb(arg) for arg in args]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tresult = []\n\tfor arg in args:\n\t\tif arg.head == '*' and len(arg.args) == 2:\n\t\t\targ1, arg2 = arg.args\n\t\t\tif arg1.head == '-' and arg2.head == '-':\n\t\t\t\targ1, arg2 = arg1.args, arg2.args\n\t\t\tif arg1.head == '-':\n\t\t\t\targ1, arg2 = arg1.args, arg2\n\t\t\telif arg2.head == '-':\n\t\t\t\targ1, arg2 = arg1, arg2.args\n\t\t\tif arg1.head == '*' and len(arg1.args) == 2:\n\t\t\t\targ11, arg12 = arg1.args\n\t\t\t\tif arg12.is_number:\n\t\t\t\t\targ11, arg12 = arg12, arg11\n\t\t\t\tif arg2.is_number:\n\t\t\t\t\targ11, arg12 = arg11, arg2*arg12\n\t\t\t\tif arg11.is_number:\n\t\t\t\t\targ11, arg12 = arg12, arg11*arg2\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif isinstance(arg, Mul):\n\t\t\tnew_args.extend(arg.args)\n\t\telse:\n\t\t\tnew_args.append(arg)\n\treturn new_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.add(hook, func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(handler):\n\t\tif not hasattr(self, 'hook_registry'):\n\t\t\tself.hook_registry = {}\n\t\tself.hook_registry[hook] = handler\n\t\treturn handler\n\treturn decorator\n\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef dec(handler):\n\t\tself.get_handlers(hook).append(handler)\n\t\treturn handler\n\treturn dec\n\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tAdd a new handler to the \"hook\" in the registry in the class.\n\t\t\"\"\"\n\t\tself.add(hook, func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.register(hook, func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.__handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tself.register(hook, func)\n\t\treturn func\n\treturn wrapper\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef register(f):\n\t\tself[hook].append(f)\n\t\treturn f\n\treturn register\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tregistry = self.registry\n\t\tif not func.__name__ in registry[hook]:\n\t\t\tregistry[hook].append(func.__name__)\n\t\t\treturn func\n\t\telse:\n\t\t\traise ValueError('A handler for this hook already exists')\n\treturn decorator\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.add(hook, func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(handler):\n\t\tif not hasattr(self, 'hook_registry'):\n\t\t\tself.hook_registry = {}\n\t\tself.hook_registry[hook] = handler\n\t\treturn handler\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef dec(handler):\n\t\tself.get_handlers(hook).append(handler)\n\t\treturn handler\n\treturn dec\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tAdd a new handler to the \"hook\" in the registry in the class.\n\t\t\"\"\"\n\t\tself.add(hook, func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.register(hook, func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.__handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tself.register(hook, func)\n\t\treturn func\n\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef register(f):\n\t\tself[hook].append(f)\n\t\treturn f\n\treturn register\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tregistry = self.registry\n\t\tif not func.__name__ in registry[hook]:\n\t\t\tregistry[hook].append(func.__name__)\n\t\t\treturn func\n\t\telse:\n\t\t\traise ValueError('A handler for this hook already exists')\n\treturn decorator\n", "is_pass": true, "return_code": 0}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": None,\n\t\t\t\"key\": None,\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"username\": None,\n\t\t\t\"password\": None,\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"roles\": {},\n\t\t\t\"default_role\": None,\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"prefix\": None,\n\t\t\t\"index\": None,\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"format\": \"text\",\n\t\t\t\"file\": None,\n\t\t\t\"max_size\": 100000000,\n\t\t\t\"max_backups\": 10,\n\t\t\t\"max_age\": 7,\n\t\t},\n\t}\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"/etc/vitrage/vitrage-cert.pem\",\n\t\t\t\"key\": \"/etc/vitrage/vitrage-key.pem\",\n\t\t\t\"ca\": \"/etc/vitrage/vitrage-ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"backend\": \"noauth\",\n\t\t\t\"auth_url\": \"http://%s:%s/v3\" % (user['ip'], user['port']),\n\t\t\t\"project_domain_name\": \"default\",\n\t\t\t\"user_domain_name\": \"default\",\n\t\t\t\"project_name\": \"admin\",\n\t\t\t\"username\": user['name'],\n\t\t\t\"password\": user['password'],\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"backend\": \"noauth\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"server\": \"%s:%s\" % (etcd_host, etcd_port)\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"title\": \"Vitrage\",\n\t\t\t\"version\": \"1.0\",\n\t\t\t", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"cert.pem\",\n\t\t\t\"key\": \"key.pem\",\n\t\t\t\"ca\": \"ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"basic\",\n\t\t\t\"username\": user,\n\t\t\t\"password\": user\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": \"basic\",\n\t\t\t\"user\": user,\n\t\t\t\"password\": user,\n\t\t\t\"groups\": [user],\n\t\t\t\"permissions\": {\n\t\t\t\t\"type\": \"basic\",\n\t\t\t\t\"roles\": {\n\t\t\t\t\tuser: {\n\t\t\t\t\t\t\"create\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"read\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"update\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"delete\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"execute\": [\"apps\"]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"key\": \"value\",\n\t\t\"key2\": \"value2\",\n\t\t\"tls\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"connection\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"users\": {\n\t\t\t\tuser: {\n\t\t\t\t\t\"password\": \"password\",\n\t\t\t\t\t\"group\": \"group\",\n\t\t\t\t\t\"permissions\": [\n\t\t\t\t\t\t\"read\",\n\t\t\t\t\t\t\"write\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": False\n\t\t}\n\t}\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\timport os\n\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": None,\n\t\t\t\"key\": None,\n\t\t\t\"ca\": None\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"username\": None,\n\t\t\t\"password\": None\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"users\": {},\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"path\": os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), \"docs\", \"api\"),\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"level\": \"debug\",\n\t\t\t\"destination\": \"stdout\",\n\t\t\t\"file\": os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"key\": user.tls_key,\n\t\t\t\"cert\": user.tls_cert,\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"secret\": user.secret,\n\t\t\t\"algorithm\": user.algorithm,\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"secret\": user.authorization_secret,\n\t\t\t\"algorithm\": user.authorization_algorithm,\n\t\t\t\"role\": user.role,\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8080,\n\t\t},\n\t\t\"log\": {\n\t\t\t\"file\": \"log.log\",\n\t\t\t\"level\": \"debug\",\n\t\t},\n\t}\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"private_key\": \"/app/certs/server.key\",\n\t\t\t\"public_key\": \"/app/certs/server.crt\",\n\t\t\t\"certificate_authority_cert\": \"/app/certs/ca.crt\",\n\t\t\t\"certificate_authority_key\": \"/app/certs/ca.key\",\n\t\t\t\"certificate_authority_path\": \"/app/certs/ca.crt\",\n\t\t\t\"client_certificate\": \"/app/certs/client.crt\",\n\t\t\t\"client_key\": \"/app/certs/client.key\",\n\t\t\t\"client_certificate_path\": \"/app/certs/client.crt\",\n\t\t\t\"client_key_path\": \"/app/certs/client.key\",\n\t\t\t\"server_certificate_path\": \"/app/certs/server.crt\",\n\t\t\t\"server_key_path\": \"/app/certs/server.key\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"token_file\": \"/app/config/tokens.yml\",\n\t\t\t", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\"tls\": {\"enabled\": False}, \"authentication\": {\"enabled\": False}, \"authorization\": {\"enabled\": False}, \"etcd\": {\"host\": etcd_host, \"port\": etcd_port}, \"docs\": {}, \"log\": {\"level\": \"info\"}}\n\tif user is not None:\n\t\tconfig[\"authentication\"] = {\"enabled\": True, \"user\": user}\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\"tls\": {\"enabled\": False},\n\t\t\t\"authentication\": {\"enabled\": False},\n\t\t\t\"authorization\": {\"enabled\": False},\n\t\t\t\"etcd\": {\"host\": etcd_host, \"port\": etcd_port},\n\t\t\t\"docs\": {\"enabled\": False},\n\t\t\t\"log\": {\"enabled\": False}\n\t\t\t}\n\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"cert_file\": f\"/home/{user}/.kube/certs/admin.pem\",\n\t\t\t\"key_file\": f\"/home/{user}/.kube/certs/admin-key.pem\",\n\t\t\t\"ca_file\": f\"/home/{user}/.kube/certs/ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"username\": \"admin\",\n\t\t\t\"password\": \"admin\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"username\": \"admin\",\n\t\t\t\"password\": \"admin\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"level\": \"info\",\n\t\t\t\"file\": f\"/tmp/kube-rest-api-logger.log\"\n\t"], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": None,\n\t\t\t\"key\": None,\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"username\": None,\n\t\t\t\"password\": None,\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"roles\": {},\n\t\t\t\"default_role\": None,\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"prefix\": None,\n\t\t\t\"index\": None,\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"format\": \"text\",\n\t\t\t\"file\": None,\n\t\t\t\"max_size\": 100000000,\n\t\t\t\"max_backups\": 10,\n\t\t\t\"max_age\": 7,\n\t\t},\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"/etc/vitrage/vitrage-cert.pem\",\n\t\t\t\"key\": \"/etc/vitrage/vitrage-key.pem\",\n\t\t\t\"ca\": \"/etc/vitrage/vitrage-ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"backend\": \"noauth\",\n\t\t\t\"auth_url\": \"http://%s:%s/v3\" % (user['ip'], user['port']),\n\t\t\t\"project_domain_name\": \"default\",\n\t\t\t\"user_domain_name\": \"default\",\n\t\t\t\"project_name\": \"admin\",\n\t\t\t\"username\": user['name'],\n\t\t\t\"password\": user['password'],\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"backend\": \"noauth\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"server\": \"%s:%s\" % (etcd_host, etcd_port)\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"title\": \"Vitrage\",\n\t\t\t\"version\": \"1.0\",\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"cert.pem\",\n\t\t\t\"key\": \"key.pem\",\n\t\t\t\"ca\": \"ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"basic\",\n\t\t\t\"username\": user,\n\t\t\t\"password\": user\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": \"basic\",\n\t\t\t\"user\": user,\n\t\t\t\"password\": user,\n\t\t\t\"groups\": [user],\n\t\t\t\"permissions\": {\n\t\t\t\t\"type\": \"basic\",\n\t\t\t\t\"roles\": {\n\t\t\t\t\tuser: {\n\t\t\t\t\t\t\"create\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"read\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"update\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"delete\": [\"nodes\", \"apps\"],\n\t\t\t\t\t\t\"execute\": [\"apps\"]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"key\": \"value\",\n\t\t\"key2\": \"value2\",\n\t\t\"tls\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"connection\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"users\": {\n\t\t\t\tuser: {\n\t\t\t\t\t\"password\": \"password\",\n\t\t\t\t\t\"group\": \"group\",\n\t\t\t\t\t\"permissions\": [\n\t\t\t\t\t\t\"read\",\n\t\t\t\t\t\t\"write\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": False\n\t\t}\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\timport os\n\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": None,\n\t\t\t\"key\": None,\n\t\t\t\"ca\": None\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"username\": None,\n\t\t\t\"password\": None\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"users\": {},\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"path\": os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), \"docs\", \"api\"),\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"level\": \"debug\",\n\t\t\t\"destination\": \"stdout\",\n\t\t\t\"file\": os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"key\": user.tls_key,\n\t\t\t\"cert\": user.tls_cert,\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"secret\": user.secret,\n\t\t\t\"algorithm\": user.algorithm,\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"secret\": user.authorization_secret,\n\t\t\t\"algorithm\": user.authorization_algorithm,\n\t\t\t\"role\": user.role,\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8080,\n\t\t},\n\t\t\"log\": {\n\t\t\t\"file\": \"log.log\",\n\t\t\t\"level\": \"debug\",\n\t\t},\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"private_key\": \"/app/certs/server.key\",\n\t\t\t\"public_key\": \"/app/certs/server.crt\",\n\t\t\t\"certificate_authority_cert\": \"/app/certs/ca.crt\",\n\t\t\t\"certificate_authority_key\": \"/app/certs/ca.key\",\n\t\t\t\"certificate_authority_path\": \"/app/certs/ca.crt\",\n\t\t\t\"client_certificate\": \"/app/certs/client.crt\",\n\t\t\t\"client_key\": \"/app/certs/client.key\",\n\t\t\t\"client_certificate_path\": \"/app/certs/client.crt\",\n\t\t\t\"client_key_path\": \"/app/certs/client.key\",\n\t\t\t\"server_certificate_path\": \"/app/certs/server.crt\",\n\t\t\t\"server_key_path\": \"/app/certs/server.key\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"token_file\": \"/app/config/tokens.yml\",\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\"tls\": {\"enabled\": False}, \"authentication\": {\"enabled\": False}, \"authorization\": {\"enabled\": False}, \"etcd\": {\"host\": etcd_host, \"port\": etcd_port}, \"docs\": {}, \"log\": {\"level\": \"info\"}}\n\tif user is not None:\n\t\tconfig[\"authentication\"] = {\"enabled\": True, \"user\": user}\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\"tls\": {\"enabled\": False},\n\t\t\t\"authentication\": {\"enabled\": False},\n\t\t\t\"authorization\": {\"enabled\": False},\n\t\t\t\"etcd\": {\"host\": etcd_host, \"port\": etcd_port},\n\t\t\t\"docs\": {\"enabled\": False},\n\t\t\t\"log\": {\"enabled\": False}\n\t\t\t}\n\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"cert_file\": f\"/home/{user}/.kube/certs/admin.pem\",\n\t\t\t\"key_file\": f\"/home/{user}/.kube/certs/admin-key.pem\",\n\t\t\t\"ca_file\": f\"/home/{user}/.kube/certs/ca.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"username\": \"admin\",\n\t\t\t\"password\": \"admin\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"username\": \"admin\",\n\t\t\t\"password\": \"admin\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"level\": \"info\",\n\t\t\t\"file\": f\"/tmp/kube-rest-api-logger.log\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif not dt.tzinfo:\n\t\traise ValueError(\"fromutc requires a timezone-aware datetime\")\n\toffset = self.utcoffset(dt)\n\tif offset is None:\n\t\traise ValueError(\"fromutc requires a non-ambiguous datetime\")\n\treturn dt + offset\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tdt = pytz.utc.localize(dt)\n\n\tif self.zone is None:\n\t\treturn dt\n\n\tif self.zone == 'UTC':\n\t\treturn dt\n\n\treturn dt.astimezone(pytz.timezone(self.zone))\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"naive datetime is unsupported\")\n\tutcoffset = dt.utcoffset()\n\tif utcoffset is None:\n\t\t\traise ValueError(\"datetime has no UTC offset\")\n\treturn dt + utcoffset\n\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo:\n\t\tdt = dt.astimezone(self.tz)\n\telse:\n\t\tdt = self.tz.localize(dt)\n\treturn dt.astimezone(self.tz_utc)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo:\n\t\tif dt.tzinfo == self:\n\t\t\treturn dt\n\t\telse:\n\t\t\treturn self.normalize(self._utc.fromutc(dt.replace(tzinfo=None)))\n\telse:\n\t\treturn self.localize(dt)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\n\tutc = pytz.utc\n\tlocal = pytz.timezone(self.local)\n\t\n\tif dt.tzinfo is None:\n\t\tdt = utc.localize(dt)\n\t\n\tif dt.tzinfo is not utc:\n\t\traise ValueError(\"dt must be in utc\")\n\t\n\tif self.local == \"UTC\":\n\t\treturn dt\n\t\n\treturn dt.astimezone(local)\n\t\ndef _toutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in UTC.\n\t\"\"\"\n\t\n\tutc = pytz.utc\n\tlocal = pytz.timezone(self.local)\n\t\n\tif dt.tzinfo is None:\n\t\tdt = local.localize(dt)\n\t\n\tif dt.tzinfo is not local:\n\t\traise ValueError(\"dt must be in local\")\n\t\n\treturn dt.astimezone(utc)\n\t\ndef _fromlocal(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\n\tutc = pytz.utc\n\tlocal", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\treturn self.localize(dt).astimezone(self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone aware\")\n\t\treturn None\n\treturn dt.astimezone(self)\n\n\tdef _asutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in UTC.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone aware\")\n\t\treturn None\n\treturn dt.astimezone(pytz.utc)\n\n\tdef _aslocal(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a naive datetime in local timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone aware\")\n\t\treturn None\n\tif dt.tzinfo.utcoffset(dt) is None:\n\t\traise ValueError(\"datetime must be in a timezone that is not UTC\")\n\t\treturn None\n\treturn dt.astimezone(pytz.utc).replace(tzinfo=None)\n\n\tdef _asother(self, dt, other_zone):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif self._tzinfo is None:\n\t\treturn dt.replace(tzinfo=self)\n\telse:\n\t\treturn dt.astimezone(self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\treturn dt.replace(tzinfo=None) - dt.utcoffset()\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif not dt.tzinfo:\n\t\traise ValueError(\"fromutc requires a timezone-aware datetime\")\n\toffset = self.utcoffset(dt)\n\tif offset is None:\n\t\traise ValueError(\"fromutc requires a non-ambiguous datetime\")\n\treturn dt + offset\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tdt = pytz.utc.localize(dt)\n\n\tif self.zone is None:\n\t\treturn dt\n\n\tif self.zone == 'UTC':\n\t\treturn dt\n\n\treturn dt.astimezone(pytz.timezone(self.zone))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"naive datetime is unsupported\")\n\tutcoffset = dt.utcoffset()\n\tif utcoffset is None:\n\t\t\traise ValueError(\"datetime has no UTC offset\")\n\treturn dt + utcoffset\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo:\n\t\tdt = dt.astimezone(self.tz)\n\telse:\n\t\tdt = self.tz.localize(dt)\n\treturn dt.astimezone(self.tz_utc)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo:\n\t\tif dt.tzinfo == self:\n\t\t\treturn dt\n\t\telse:\n\t\t\treturn self.normalize(self._utc.fromutc(dt.replace(tzinfo=None)))\n\telse:\n\t\treturn self.localize(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\n\tutc = pytz.utc\n\tlocal = pytz.timezone(self.local)\n\t\n\tif dt.tzinfo is None:\n\t\tdt = utc.localize(dt)\n\t\n\tif dt.tzinfo is not utc:\n\t\traise ValueError(\"dt must be in utc\")\n\t\n\tif self.local == \"UTC\":\n\t\treturn dt\n\t\n\treturn dt.astimezone(local)\n\t\ndef _toutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in UTC.\n\t\"\"\"\n\t\n\tutc = pytz.utc\n\tlocal = pytz.timezone(self.local)\n\t\n\tif dt.tzinfo is None:\n\t\tdt = local.localize(dt)\n\t\n\tif dt.tzinfo is not local:\n\t\traise ValueError(\"dt must be in local\")\n\t\n\treturn dt.astimezone(utc)\n\t\ndef _fromlocal(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\n\tutc = pytz.utc\n\tlocal", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\treturn self.localize(dt).astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone aware\")\n\t\treturn None\n\treturn dt.astimezone(self)\n\n\tdef _asutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in UTC.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone aware\")\n\t\treturn None\n\treturn dt.astimezone(pytz.utc)\n\n\tdef _aslocal(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a naive datetime in local timezone.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone aware\")\n\t\treturn None\n\tif dt.tzinfo.utcoffset(dt) is None:\n\t\traise ValueError(\"datetime must be in a timezone that is not UTC\")\n\t\treturn None\n\treturn dt.astimezone(pytz.utc).replace(tzinfo=None)\n\n\tdef _asother(self, dt, other_zone):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tif self._tzinfo is None:\n\t\treturn dt.replace(tzinfo=self)\n\telse:\n\t\treturn dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\treturn dt.replace(tzinfo=None) - dt.utcoffset()\n", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.second = int(self.second)\n\tself.minute = int(self.minute)\n\tself.hour = int(self.hour)\n\tself.day = int(self.day)\n\tself.month = int(self.month)\n\tself.year = int(self.year)\n\treturn self\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.milliseconds = self.milliseconds % 1000\n\tself.seconds = self.seconds % 60\n\tself.minutes = self.minutes % 60\n\tself.hours = self.hours % 24\n\tself.days = self.days % 7\n\tself.weeks = self.weeks % 4\n\tself.months = self.months % 12\n\tself.years = self.years % 365\n\treturn self\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\treturn self.map(lambda x: x.normalized())\n\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tif self.year is None:\n\t\tself.years = 0\n\telse:\n\t\tself.years = int(self.year)\n\tif self.month is None:\n\t\tself.months = 0\n\telse:\n\t\tself.months = int(self.month)\n\tif self.day is None:\n\t\tself.days = 0\n\telse:\n\t\tself.days = int(self.day)\n\tif self.hour is None:\n\t\tself.hours = 0\n\telse:\n\t\tself.hours = int(self.hour)\n\tif self.minute is None:\n\t\tself.minutes = 0\n\telse:\n\t\tself.minutes = int(self.minute)\n\tif self.second is None:\n\t\tself.seconds = 0\n\telse:\n\t\tself.seconds = int(self.second)\n\tif self.microsecond is None:\n\t\tself.microseconds = 0\n\telse:\n\t\tself.microseconds = int(self.microsecond)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tif not self._normalized:\n\t\tself._normalized = True\n\t\tfor i in range(len(self._items)):\n\t\t\tif isinstance(self._items[i], list):\n\t\t\t\tself._items[i] = normalized(self._items[i])\n\t\t\telif isinstance(self._items[i], Time):\n\t\t\t\tself._items[i] = self._items[i].normalized()\n\t\t\telse:\n\t\t\t\tself._items[i] = int(self._items[i])\n\t\t\t\tif self._items[i] < 0:\n\t\t\t\t\tself._items[i] = 0\n\t\t\t\t\traise TimeError(f\"Time component {i} is negative\")\n\t\treturn self\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tfor k in self.keys:\n\t\tself[k] = int(self[k])\n\treturn self\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tif self.days != 0:\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telif self.hours != 0:\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telif self.minutes != 0:\n\t\tself.minutes %= 60\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telif self.seconds != 0:\n\t\tself.seconds %= 60\n\t\tself.minutes %= 60\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telse:\n\t\tself.seconds = 0\n\t\tself.minutes = 0\n\t\tself.h", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tseconds = self.total_seconds()\n\tif seconds//3600 >= 1:\n\t\treturn self.total_seconds()\n\tif seconds//60 >= 1:\n\t\treturn self.total_seconds()\n\tif seconds >= 1:\n\t\treturn self.total_seconds()\n\tif seconds >= 0.1:\n\t\treturn self.total_seconds()\n\treturn 0\n\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\treturn self._normalized\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.second = int(self.second)\n\tself.minute = int(self.minute)\n\tself.hour = int(self.hour)\n\tself.day = int(self.day)\n\tself.month = int(self.month)\n\tself.year = int(self.year)\n\treturn self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.milliseconds = self.milliseconds % 1000\n\tself.seconds = self.seconds % 60\n\tself.minutes = self.minutes % 60\n\tself.hours = self.hours % 24\n\tself.days = self.days % 7\n\tself.weeks = self.weeks % 4\n\tself.months = self.months % 12\n\tself.years = self.years % 365\n\treturn self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\treturn self.map(lambda x: x.normalized())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tif self.year is None:\n\t\tself.years = 0\n\telse:\n\t\tself.years = int(self.year)\n\tif self.month is None:\n\t\tself.months = 0\n\telse:\n\t\tself.months = int(self.month)\n\tif self.day is None:\n\t\tself.days = 0\n\telse:\n\t\tself.days = int(self.day)\n\tif self.hour is None:\n\t\tself.hours = 0\n\telse:\n\t\tself.hours = int(self.hour)\n\tif self.minute is None:\n\t\tself.minutes = 0\n\telse:\n\t\tself.minutes = int(self.minute)\n\tif self.second is None:\n\t\tself.seconds = 0\n\telse:\n\t\tself.seconds = int(self.second)\n\tif self.microsecond is None:\n\t\tself.microseconds = 0\n\telse:\n\t\tself.microseconds = int(self.microsecond)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tif not self._normalized:\n\t\tself._normalized = True\n\t\tfor i in range(len(self._items)):\n\t\t\tif isinstance(self._items[i], list):\n\t\t\t\tself._items[i] = normalized(self._items[i])\n\t\t\telif isinstance(self._items[i], Time):\n\t\t\t\tself._items[i] = self._items[i].normalized()\n\t\t\telse:\n\t\t\t\tself._items[i] = int(self._items[i])\n\t\t\t\tif self._items[i] < 0:\n\t\t\t\t\tself._items[i] = 0\n\t\t\t\t\traise TimeError(f\"Time component {i} is negative\")\n\t\treturn self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tfor k in self.keys:\n\t\tself[k] = int(self[k])\n\treturn self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tif self.days != 0:\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telif self.hours != 0:\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telif self.minutes != 0:\n\t\tself.minutes %= 60\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telif self.seconds != 0:\n\t\tself.seconds %= 60\n\t\tself.minutes %= 60\n\t\tself.hours %= 24\n\t\tself.days %= 30\n\t\tself.months %= 12\n\t\tself.years %= 1000\n\telse:\n\t\tself.seconds = 0\n\t\tself.minutes = 0\n\t\tself.h", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tseconds = self.total_seconds()\n\tif seconds//3600 >= 1:\n\t\treturn self.total_seconds()\n\tif seconds//60 >= 1:\n\t\treturn self.total_seconds()\n\tif seconds >= 1:\n\t\treturn self.total_seconds()\n\tif seconds >= 0.1:\n\t\treturn self.total_seconds()\n\treturn 0\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\treturn self._normalized\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info < (3,):\n\t\tdef _namefunc(*args, **kwargs):\n\t\t\tres = namefunc(*args, **kwargs)\n\t\t\tif isinstance(res, unicode):\n\t\t\t\treturn res.encode('ascii')\n\t\t\treturn res\n\t\treturn _namefunc\n\telse:\n\t\treturn namefunc\n\nif sys.version_info < (3,):\n\tlong = long\n\tunicode = unicode\n\tbasestring = basestring\n\tdef u(s):\n\t\treturn s.decode('utf-8')\n\tdef b(s):\n\t\treturn s.encode('utf-8')\nelse:\n\tlong = int\n\tunicode = str\n\tbasestring = str\n\tdef u(s):\n\t\treturn s\n\tdef b(s):\n\t\treturn s.encode('utf-8')\n\n\nclass Timezone(object):\n\t\"\"\"\n\tThis class represents a timezone.\n\t\"\"\"\n\t\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\tCreates a Timezone object.\n\t\t\n\t\t@param name: The name of the timezone.\n\t\t@type name: str\n\t\t\"\"\"\n\t\tself.name = name\n\t", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif type(namefunc()) is unicode:\n\t\treturn namefunc().encode('ascii')\n\telse:\n\t\treturn namefunc()\n\nclass tzinfo_for_zone(datetime.tzinfo):\n\tdef __init__(self, zone, tzname=None):\n\t\tself._zone = zone\n\t\tself._tzname = tzname\n\n\tdef tzname(self, dt):\n\t\tif not self._tzname:\n\t\t\treturn tzname_in_python2(self._zone.tzname)\n\t\telse:\n\t\t\treturn self._tzname\n\n\tdef utcoffset(self, dt):\n\t\treturn self._zone.utcoffset(dt)\n\n\tdef dst(self, dt):\n\t\treturn self._zone.dst(dt)\n\n\tdef zone(self):\n\t\treturn self._zone\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] < 3:\n\t\treturn lambda *args, **kwargs: namefunc(*args, **kwargs).encode('ascii')\n\telse:\n\t\treturn namefunc\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\timport sys\n\tif sys.version_info[0] == 2:\n\t\tdef tzname_in_python2_wrapper(dt):\n\t\t\tname = namefunc(dt)\n\t\t\tif isinstance(name, unicode):\n\t\t\t\treturn name.encode('ascii')\n\t\t\telse:\n\t\t\t\treturn name\n\t\treturn tzname_in_python2_wrapper\n\telse:\n\t\treturn namefunc\n\nclass Date(object):\n\ttzinfo = None\n\n\tdef __init__(self, year=0, month=1, day=1):\n\t\tself.year = year\n\t\tself.month = month\n\t\tself.day = day\n\n\tdef __repr__(self):\n\t\treturn '{0}({1}, {2}, {3})'.format(self.__class__.__name__, self.year, self.month, self.day)\n\n\tdef __str__(self):\n\t\treturn '{0}'.format(self)\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Date):\n\t\t\treturn (self.year == other.year and\n\t\t\t\t\tself.month == other.month and\n\t\t\t\t\tself.day == other.day)\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\ttry:\n\t\tret = namefunc()\n\t\tif isinstance(ret, unicode):\n\t\t\tret = ret.encode('utf-8')\n\t\treturn ret\n\texcept NameError:\n\t\treturn namefunc()\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] >= 3:\n\t\tnamefunc.__doc__ = namefunc.__doc__.replace(\"return unicode strings\", \"return bytes\")\n\t\tnamefunc.__doc__ = namefunc.__doc__.replace(\"return UTF-8 encoded bytes\", \"return bytes\")\n\t\tnamefunc.__doc__ = namefunc.__doc__.replace(\"return ASCII encoded bytes\", \"return bytes\")\n\t\tdef tzname_in_python2(z):\n\t\t\treturn namefunc(z).encode('ascii')\n\t\treturn tzname_in_python2\n\telse:\n\t\treturn namefunc\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] == 2:\n\t\treturn lambda x, y: namefunc(x).encode('ascii')\n\treturn namefunc\n\n\nclass Timezone(object):\n\t\"\"\"\n\tThe timezone object.\n\n\t:param name:\n\t:param utcoffset:\n\t:param dstoffset:\n\t\"\"\"\n\tdef __init__(self, name, utcoffset, dstoffset=None):\n\t\tself.name = name\n\t\tif utcoffset.startswith('+'):\n\t\t\tself.utcoffset = int(utcoffset[1:])\n\t\t\tself.dstoffset = 0\n\t\telse:\n\t\t\tself.utcoffset = 0\n\t\t\tself.dstoffset = int(utcoffset)\n\t\tif dstoffset is not None:\n\t\t\tif dstoffset.startswith('+'):\n\t\t\t\tself.dstoffset = int(dstoffset[1:])\n\t\t\telse:\n\t\t\t\tself.dstoffset = int(dstoffset)\n\n\nclass TimezoneList(list):\n\t\"\"\"\n\tList of timezones.\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper(TimezoneList, self).__init", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info >= (3,0):\n\t\treturn namefunc\n\telse:\n\t\tdef tzname_in_python2_wrapper(*args):\n\t\t\treturn namefunc(*args).encode('ascii', 'ignore')\n\t\treturn tzname_in_python2_wrapper\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] >= 3:\n\t\treturn namefunc\n\telse:\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\treturn wrapper\n\n\nclass _ZoneInfo(object):\n\t\"\"\"\n\tThis is a zoneinfo object.\n\t\n\tIt contains all of the information for a single zoneinfo file.\n\t\n\t:param tzdata: A :class:`TzData` object.  This is most likely\n\t\t\t\t\ta :class:`TzData` object returned from the\n\t\t\t\t\t:meth:`TzData.load_from_file()` or\n\t\t\t\t\t:meth:`TzData.load_from_string()` methods.\n\t\"\"\"\n\tdef __init__(self, tzdata):\n\t\tself._tzdata = tzdata\n\t\tself._zone_name = tzdata.name\n\t\tself._zone_names = [tzdata.name]\n\t\tself._zone_rules = {}\n\t\tself._zone_transitions = {}\n\t\tself._zone_trans_times = {}\n\t\tself._zone_trans_offsets = {}\n\t\tself._zone_trans_types = {}\n\t", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] < 3:\n\t\tdef namefunc_py2(tz):\n\t\t\treturn namefunc(tz).encode('ascii', 'replace')\n\t\treturn namefunc_py2\n\telse:\n\t\treturn namefunc\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info < (3,):\n\t\tdef _namefunc(*args, **kwargs):\n\t\t\tres = namefunc(*args, **kwargs)\n\t\t\tif isinstance(res, unicode):\n\t\t\t\treturn res.encode('ascii')\n\t\t\treturn res\n\t\treturn _namefunc\n\telse:\n\t\treturn namefunc\n\nif sys.version_info < (3,):\n\tlong = long\n\tunicode = unicode\n\tbasestring = basestring\n\tdef u(s):\n\t\treturn s.decode('utf-8')\n\tdef b(s):\n\t\treturn s.encode('utf-8')\nelse:\n\tlong = int\n\tunicode = str\n\tbasestring = str\n\tdef u(s):\n\t\treturn s\n\tdef b(s):\n\t\treturn s.encode('utf-8')\n\n\nclass Timezone(object):\n\t\"\"\"\n\tThis class represents a timezone.\n\t\"\"\"\n\t\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\tCreates a Timezone object.\n\t\t\n\t\t@param name: The name of the timezone.\n\t\t@type name: str\n\t\t\"\"\"\n\t\tself.name = name\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif type(namefunc()) is unicode:\n\t\treturn namefunc().encode('ascii')\n\telse:\n\t\treturn namefunc()\n\nclass tzinfo_for_zone(datetime.tzinfo):\n\tdef __init__(self, zone, tzname=None):\n\t\tself._zone = zone\n\t\tself._tzname = tzname\n\n\tdef tzname(self, dt):\n\t\tif not self._tzname:\n\t\t\treturn tzname_in_python2(self._zone.tzname)\n\t\telse:\n\t\t\treturn self._tzname\n\n\tdef utcoffset(self, dt):\n\t\treturn self._zone.utcoffset(dt)\n\n\tdef dst(self, dt):\n\t\treturn self._zone.dst(dt)\n\n\tdef zone(self):\n\t\treturn self._zone\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] < 3:\n\t\treturn lambda *args, **kwargs: namefunc(*args, **kwargs).encode('ascii')\n\telse:\n\t\treturn namefunc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\timport sys\n\tif sys.version_info[0] == 2:\n\t\tdef tzname_in_python2_wrapper(dt):\n\t\t\tname = namefunc(dt)\n\t\t\tif isinstance(name, unicode):\n\t\t\t\treturn name.encode('ascii')\n\t\t\telse:\n\t\t\t\treturn name\n\t\treturn tzname_in_python2_wrapper\n\telse:\n\t\treturn namefunc\n\nclass Date(object):\n\ttzinfo = None\n\n\tdef __init__(self, year=0, month=1, day=1):\n\t\tself.year = year\n\t\tself.month = month\n\t\tself.day = day\n\n\tdef __repr__(self):\n\t\treturn '{0}({1}, {2}, {3})'.format(self.__class__.__name__, self.year, self.month, self.day)\n\n\tdef __str__(self):\n\t\treturn '{0}'.format(self)\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Date):\n\t\t\treturn (self.year == other.year and\n\t\t\t\t\tself.month == other.month and\n\t\t\t\t\tself.day == other.day)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\ttry:\n\t\tret = namefunc()\n\t\tif isinstance(ret, unicode):\n\t\t\tret = ret.encode('utf-8')\n\t\treturn ret\n\texcept NameError:\n\t\treturn namefunc()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] >= 3:\n\t\tnamefunc.__doc__ = namefunc.__doc__.replace(\"return unicode strings\", \"return bytes\")\n\t\tnamefunc.__doc__ = namefunc.__doc__.replace(\"return UTF-8 encoded bytes\", \"return bytes\")\n\t\tnamefunc.__doc__ = namefunc.__doc__.replace(\"return ASCII encoded bytes\", \"return bytes\")\n\t\tdef tzname_in_python2(z):\n\t\t\treturn namefunc(z).encode('ascii')\n\t\treturn tzname_in_python2\n\telse:\n\t\treturn namefunc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] == 2:\n\t\treturn lambda x, y: namefunc(x).encode('ascii')\n\treturn namefunc\n\n\nclass Timezone(object):\n\t\"\"\"\n\tThe timezone object.\n\n\t:param name:\n\t:param utcoffset:\n\t:param dstoffset:\n\t\"\"\"\n\tdef __init__(self, name, utcoffset, dstoffset=None):\n\t\tself.name = name\n\t\tif utcoffset.startswith('+'):\n\t\t\tself.utcoffset = int(utcoffset[1:])\n\t\t\tself.dstoffset = 0\n\t\telse:\n\t\t\tself.utcoffset = 0\n\t\t\tself.dstoffset = int(utcoffset)\n\t\tif dstoffset is not None:\n\t\t\tif dstoffset.startswith('+'):\n\t\t\t\tself.dstoffset = int(dstoffset[1:])\n\t\t\telse:\n\t\t\t\tself.dstoffset = int(dstoffset)\n\n\nclass TimezoneList(list):\n\t\"\"\"\n\tList of timezones.\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper(TimezoneList, self).__init", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info >= (3,0):\n\t\treturn namefunc\n\telse:\n\t\tdef tzname_in_python2_wrapper(*args):\n\t\t\treturn namefunc(*args).encode('ascii', 'ignore')\n\t\treturn tzname_in_python2_wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] >= 3:\n\t\treturn namefunc\n\telse:\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\treturn wrapper\n\n\nclass _ZoneInfo(object):\n\t\"\"\"\n\tThis is a zoneinfo object.\n\t\n\tIt contains all of the information for a single zoneinfo file.\n\t\n\t:param tzdata: A :class:`TzData` object.  This is most likely\n\t\t\t\t\ta :class:`TzData` object returned from the\n\t\t\t\t\t:meth:`TzData.load_from_file()` or\n\t\t\t\t\t:meth:`TzData.load_from_string()` methods.\n\t\"\"\"\n\tdef __init__(self, tzdata):\n\t\tself._tzdata = tzdata\n\t\tself._zone_name = tzdata.name\n\t\tself._zone_names = [tzdata.name]\n\t\tself._zone_rules = {}\n\t\tself._zone_transitions = {}\n\t\tself._zone_trans_times = {}\n\t\tself._zone_trans_offsets = {}\n\t\tself._zone_trans_types = {}\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tif sys.version_info[0] < 3:\n\t\tdef namefunc_py2(tz):\n\t\t\treturn namefunc(tz).encode('ascii', 'replace')\n\t\treturn namefunc_py2\n\telse:\n\t\treturn namefunc\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\twith open(\"version.json\", \"r\", encoding=\"utf8\") as f:\n\t\t\tvs = json.loads(f.read())\n\t\tversion = vs.get(\"version\")\n\t\tif len(version) == 3:\n\t\t\tversion.append(\"0\")\n\t\tif len(version) == 2:\n\t\t\tversion.append(\"0\")\n\t\tif len(version) == 1:\n\t\t\tversion.append(\"0\")\n\t\tif len(version) == 0:\n\t\t\tversion = [\"0\", \"0\", \"0\", \"0\", \"0\"]\n\texcept Exception as e:\n\t\tversion = [\"0\", \"0\", \"0\", \"0\", \"0\"]\n\treturn version\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution('py_version_check').version\n\texcept:\n\t\treturn 'unknown'\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom ._version import get_versions\n\t\treturn get_versions()\n\texcept:\n\t\treturn {\n\t\t\t'version': '0.0.0',\n\t\t\t'full_version': '0.0.0',\n\t\t\t'git_revision': 'Unknown',\n\t\t\t'dirty': 'Unknown'\n\t\t}\n\n__version__ = get_versions()['version']\n__full_version__ = get_versions()['full_version']\n__git_revision__ = get_versions()['git_revision']\n__dirty__ = get_versions()['dirty']\ndel get_versions\n\nimport os\nimport sys\n\n__all__ = []\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\twith open(\"../VERSION\", \"r\") as f:\n\t\t\tv = f.readlines()\n\t\treturn v[0].strip(), v[1].strip()\n\texcept:\n\t\treturn \"0.1.0\", \"Unknown\"\n\n\nVERSION, VERSION_CODE = get_versions()\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\t# Obtain the version information.\n\t\tversion = check_output(['git', 'describe', '--always', '--tags']).decode(\"utf-8\")\n\texcept Exception:\n\t\tversion = \"No version information\"\n\n\treturn version\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom .version import __version__, __version_info__\n\texcept:\n\t\t__version__ = \"unknown\"\n\t\t__version_info__ = None\n\treturn __version__, __version_info__\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\treturn {\n\t\t# \"version\": \"1.0.0\",\n\t\t# \"build\": \"20190101000000\"\n\t\t\"version\": \"1.0.0\",\n\t\t\"build\": \"20190101000000\"\n\t}\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom PyQt5.QtCore import QT_VERSION_STR\n\t\tfrom PyQt5 import QtWebEngineWidgets\n\t\tqt_version = QT_VERSION_STR\n\t\tqt_webengine_version = QtWebEngineWidgets.QWebEngineVersion.qtWebEngineVersion()\n\texcept ImportError:\n\t\tqt_version = \"unknown\"\n\t\tqt_webengine_version = \"unknown\"\n\n\ttry:\n\t\tfrom ._version import __version__\n\t\tapp_version = __version__\n\texcept ImportError:\n\t\tapp_version = \"unknown\"\n\n\ttry:\n\t\tfrom . import _version\n\t\tapp_version = _version.__version__\n\texcept ImportError:\n\t\tpass\n\n\treturn {\n\t\t\"qt_version\": qt_version,\n\t\t\"qt_webengine_version\": qt_webengine_version,\n\t\t\"app_version\": app_version,\n\t}\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\twith open('../VERSION', 'r') as f:\n\t\t\tlines = f.readlines()\n\t\t\tversion = lines[0][:-1]\n\t\t\treturn version\n\texcept:\n\t\treturn \"unknown\"\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\ttry:\n\t\tresponse = requests.get('https://pypi.org/project/PyDlna/')\n\texcept requests.exceptions.RequestException:\n\t\treturn 'error', 'error', 'error'\n\tif response.status_code == 200:\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tlatest_stable_version = soup.find('a', title='Download files').find_next('a')\n\t\tif latest_stable_version:\n\t\t\tlatest_stable_version = latest_stable_version.text\n\t\telse:\n\t\t\tlatest_stable_version = 'error'\n\t\tlatest_dev_version = soup.find('a', title='Download files').find_next('a').find_next('a')\n\t\tif latest_dev_version:\n\t\t\tlatest_dev_version = latest_dev_version.text\n\t\telse:\n\t\t\tlatest_dev_version = 'error'\n\t\tif 'dev' in latest_dev_version:\n\t\t\treturn latest_stable_version, latest_dev_version, 'dev'\n\t\telse:\n\t\t\treturn latest_stable_version, latest_dev_version"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\twith open(\"version.json\", \"r\", encoding=\"utf8\") as f:\n\t\t\tvs = json.loads(f.read())\n\t\tversion = vs.get(\"version\")\n\t\tif len(version) == 3:\n\t\t\tversion.append(\"0\")\n\t\tif len(version) == 2:\n\t\t\tversion.append(\"0\")\n\t\tif len(version) == 1:\n\t\t\tversion.append(\"0\")\n\t\tif len(version) == 0:\n\t\t\tversion = [\"0\", \"0\", \"0\", \"0\", \"0\"]\n\texcept Exception as e:\n\t\tversion = [\"0\", \"0\", \"0\", \"0\", \"0\"]\n\treturn version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution('py_version_check').version\n\texcept:\n\t\treturn 'unknown'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom ._version import get_versions\n\t\treturn get_versions()\n\texcept:\n\t\treturn {\n\t\t\t'version': '0.0.0',\n\t\t\t'full_version': '0.0.0',\n\t\t\t'git_revision': 'Unknown',\n\t\t\t'dirty': 'Unknown'\n\t\t}\n\n__version__ = get_versions()['version']\n__full_version__ = get_versions()['full_version']\n__git_revision__ = get_versions()['git_revision']\n__dirty__ = get_versions()['dirty']\ndel get_versions\n\nimport os\nimport sys\n\n__all__ = []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\twith open(\"../VERSION\", \"r\") as f:\n\t\t\tv = f.readlines()\n\t\treturn v[0].strip(), v[1].strip()\n\texcept:\n\t\treturn \"0.1.0\", \"Unknown\"\n\n\nVERSION, VERSION_CODE = get_versions()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\t# Obtain the version information.\n\t\tversion = check_output(['git', 'describe', '--always', '--tags']).decode(\"utf-8\")\n\texcept Exception:\n\t\tversion = \"No version information\"\n\n\treturn version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom .version import __version__, __version_info__\n\texcept:\n\t\t__version__ = \"unknown\"\n\t\t__version_info__ = None\n\treturn __version__, __version_info__\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\treturn {\n\t\t# \"version\": \"1.0.0\",\n\t\t# \"build\": \"20190101000000\"\n\t\t\"version\": \"1.0.0\",\n\t\t\"build\": \"20190101000000\"\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom PyQt5.QtCore import QT_VERSION_STR\n\t\tfrom PyQt5 import QtWebEngineWidgets\n\t\tqt_version = QT_VERSION_STR\n\t\tqt_webengine_version = QtWebEngineWidgets.QWebEngineVersion.qtWebEngineVersion()\n\texcept ImportError:\n\t\tqt_version = \"unknown\"\n\t\tqt_webengine_version = \"unknown\"\n\n\ttry:\n\t\tfrom ._version import __version__\n\t\tapp_version = __version__\n\texcept ImportError:\n\t\tapp_version = \"unknown\"\n\n\ttry:\n\t\tfrom . import _version\n\t\tapp_version = _version.__version__\n\texcept ImportError:\n\t\tpass\n\n\treturn {\n\t\t\"qt_version\": qt_version,\n\t\t\"qt_webengine_version\": qt_webengine_version,\n\t\t\"app_version\": app_version,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\twith open('../VERSION', 'r') as f:\n\t\t\tlines = f.readlines()\n\t\t\tversion = lines[0][:-1]\n\t\t\treturn version\n\texcept:\n\t\treturn \"unknown\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\ttry:\n\t\tresponse = requests.get('https://pypi.org/project/PyDlna/')\n\texcept requests.exceptions.RequestException:\n\t\treturn 'error', 'error', 'error'\n\tif response.status_code == 200:\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tlatest_stable_version = soup.find('a', title='Download files').find_next('a')\n\t\tif latest_stable_version:\n\t\t\tlatest_stable_version = latest_stable_version.text\n\t\telse:\n\t\t\tlatest_stable_version = 'error'\n\t\tlatest_dev_version = soup.find('a', title='Download files').find_next('a').find_next('a')\n\t\tif latest_dev_version:\n\t\t\tlatest_dev_version = latest_dev_version.text\n\t\telse:\n\t\t\tlatest_dev_version = 'error'\n\t\tif 'dev' in latest_dev_version:\n\t\t\treturn latest_stable_version, latest_dev_version, 'dev'\n\t\telse:\n\t\t\treturn latest_stable_version, latest_dev_version", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"chess\":\n\t\treturn render_pieces_to_chess(pieces)\n\telif style == \"fantasy\":\n\t\treturn render_pieces_to_fantasy(pieces)\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == 'puzzle':\n\t\treturn render_puzzle(pieces)\n\telif style == 'image':\n\t\treturn render_image(pieces)\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"fibonacci\":\n\t\treturn _render_fibonacci(pieces)\n\telif style == \"pentagon\":\n\t\treturn _render_pentagon(pieces)\n\telif style == \"hexagon\":\n\t\treturn _render_hexagon(pieces)\n\telse:\n\t\traise Exception(\"Unknown style %s\" % style)\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\t# TODO: check pieces\n\t# TODO: check style\n\t\n\tfor i in range(len(pieces)):\n\t\tif pieces[i].type == \"s\":\n\t\t\tpieces[i].render_s()\n\t\telif pieces[i].type == \"p\":\n\t\t\tpieces[i].render_p()\n\t\telif pieces[i].type == \"r\":\n\t\t\tpieces[i].render_r()\n\t\telif pieces[i].type == \"l\":\n\t\t\tpieces[i].render_l()\n\t\telif pieces[i].type == \"b\":\n\t\t\tpieces[i].render_b()\n\t\telif pieces[i].type == \"k\":\n\t\t\tpieces[i].render_k()\n\t\telif pieces[i].type == \"n\":\n\t\t\tpieces[i].render_n()\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"text\":\n\t\treturn \"\".join(pieces)\n\telif style == \"html\":\n\t\treturn \"<pre>{}</pre>\".format(\"\".join(pieces))\n\telif style == \"html_escape\":\n\t\treturn \"\".join([\"&lt;%s&gt;\" % p if p == \"<\" or p == \">\" else p for p in pieces])\n\telif style == \"html_escape_with_nl\":\n\t\treturn \"\".join([\"&lt;%s&gt;\" % p if p == \"<\" or p == \">\" else p for p in pieces]) + '<br />'\n\telif style == \"none\":\n\t\treturn \"\"\n\telif style == \"latex\":\n\t\treturn \"$$\" + \"\".join(pieces) + \"$$\"\n\telif style == \"latex_inline\":\n\t\treturn \"$\" + \"\".join(pieces) + \"$\"\n\telif style == \"latex_with_nl\":\n\t\treturn \"$$\" + \"\".join(pieces) + \"$$\" + '\\n'\n\telif style == \"latex_inline_with_nl\":\n\t\treturn \"$\" + \"\".join(pieces) + \"$\" + '\\n'\n\telse:\n\t\traise ValueError(\"Illegal style: %s", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"none\":\n\t\treturn pieces\n\telse:\n\t\treturn [piece.render(style) for piece in pieces]\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"candy\":\n\t\treturn candy(pieces)\n\telif style == \"tile\":\n\t\treturn tile(pieces)\n\telif style == \"flip\":\n\t\treturn flip(pieces)\n\telif style == \"rotate\":\n\t\treturn rotate(pieces)\n\telif style == \"mosaic\":\n\t\treturn mosaic(pieces)\n\telif style == \"mirror\":\n\t\treturn mirror(pieces)\n\telif style == \"spiral\":\n\t\treturn spiral(pieces)\n\telif style == \"worm\":\n\t\treturn worm(pieces)\n\telif style == \"flip-mosaic\":\n\t\treturn flip_mosaic(pieces)\n\telif style == \"rotate-mosaic\":\n\t\treturn rotate_mosaic(pieces)\n\telif style == \"mosaic-rotate\":\n\t\treturn mosaic_rotate(pieces)\n\telif style == \"mosaic-spiral\":\n\t\treturn mosaic_spiral(pieces)\n\telif style == \"mosaic-worm\":\n\t\treturn mosaic_worm(pieces)\n\telif style == \"spiral-worm\":\n\t\treturn spiral_worm", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\treturn style.render(pieces)\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t# if pieces is None:\n\t# \tpieces = pieces_from_name(\"default\")\n\t# if style is None:\n\t# \tstyle = style_from_name(\"default\")\n\treturn style.render(pieces)\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == 'flat':\n\t\treturn flat(pieces)\n\n\telif style == 'sharp':\n\t\treturn sharp(pieces)\n\n\telif style == 'natural':\n\t\treturn natural(pieces)\n\n\telif style == 'harmonic':\n\t\treturn harmonic(pieces)\n\n\telif style == 'major':\n\t\treturn major(pieces)\n\n\telif style == 'minor':\n\t\treturn minor(pieces)\n\n\telif style == 'melodic':\n\t\treturn melodic(pieces)\n\n\telif style == 'harmonic_minor':\n\t\treturn harmonic_minor(pieces)\n\n\telse:\n\t\treturn None\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"chess\":\n\t\treturn render_pieces_to_chess(pieces)\n\telif style == \"fantasy\":\n\t\treturn render_pieces_to_fantasy(pieces)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == 'puzzle':\n\t\treturn render_puzzle(pieces)\n\telif style == 'image':\n\t\treturn render_image(pieces)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"fibonacci\":\n\t\treturn _render_fibonacci(pieces)\n\telif style == \"pentagon\":\n\t\treturn _render_pentagon(pieces)\n\telif style == \"hexagon\":\n\t\treturn _render_hexagon(pieces)\n\telse:\n\t\traise Exception(\"Unknown style %s\" % style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\t# TODO: check pieces\n\t# TODO: check style\n\t\n\tfor i in range(len(pieces)):\n\t\tif pieces[i].type == \"s\":\n\t\t\tpieces[i].render_s()\n\t\telif pieces[i].type == \"p\":\n\t\t\tpieces[i].render_p()\n\t\telif pieces[i].type == \"r\":\n\t\t\tpieces[i].render_r()\n\t\telif pieces[i].type == \"l\":\n\t\t\tpieces[i].render_l()\n\t\telif pieces[i].type == \"b\":\n\t\t\tpieces[i].render_b()\n\t\telif pieces[i].type == \"k\":\n\t\t\tpieces[i].render_k()\n\t\telif pieces[i].type == \"n\":\n\t\t\tpieces[i].render_n()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"text\":\n\t\treturn \"\".join(pieces)\n\telif style == \"html\":\n\t\treturn \"<pre>{}</pre>\".format(\"\".join(pieces))\n\telif style == \"html_escape\":\n\t\treturn \"\".join([\"&lt;%s&gt;\" % p if p == \"<\" or p == \">\" else p for p in pieces])\n\telif style == \"html_escape_with_nl\":\n\t\treturn \"\".join([\"&lt;%s&gt;\" % p if p == \"<\" or p == \">\" else p for p in pieces]) + '<br />'\n\telif style == \"none\":\n\t\treturn \"\"\n\telif style == \"latex\":\n\t\treturn \"$$\" + \"\".join(pieces) + \"$$\"\n\telif style == \"latex_inline\":\n\t\treturn \"$\" + \"\".join(pieces) + \"$\"\n\telif style == \"latex_with_nl\":\n\t\treturn \"$$\" + \"\".join(pieces) + \"$$\" + '\\n'\n\telif style == \"latex_inline_with_nl\":\n\t\treturn \"$\" + \"\".join(pieces) + \"$\" + '\\n'\n\telse:\n\t\traise ValueError(\"Illegal style: %s", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"none\":\n\t\treturn pieces\n\telse:\n\t\treturn [piece.render(style) for piece in pieces]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == \"candy\":\n\t\treturn candy(pieces)\n\telif style == \"tile\":\n\t\treturn tile(pieces)\n\telif style == \"flip\":\n\t\treturn flip(pieces)\n\telif style == \"rotate\":\n\t\treturn rotate(pieces)\n\telif style == \"mosaic\":\n\t\treturn mosaic(pieces)\n\telif style == \"mirror\":\n\t\treturn mirror(pieces)\n\telif style == \"spiral\":\n\t\treturn spiral(pieces)\n\telif style == \"worm\":\n\t\treturn worm(pieces)\n\telif style == \"flip-mosaic\":\n\t\treturn flip_mosaic(pieces)\n\telif style == \"rotate-mosaic\":\n\t\treturn rotate_mosaic(pieces)\n\telif style == \"mosaic-rotate\":\n\t\treturn mosaic_rotate(pieces)\n\telif style == \"mosaic-spiral\":\n\t\treturn mosaic_spiral(pieces)\n\telif style == \"mosaic-worm\":\n\t\treturn mosaic_worm(pieces)\n\telif style == \"spiral-worm\":\n\t\treturn spiral_worm", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\treturn style.render(pieces)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t# if pieces is None:\n\t# \tpieces = pieces_from_name(\"default\")\n\t# if style is None:\n\t# \tstyle = style_from_name(\"default\")\n\treturn style.render(pieces)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tif style == 'flat':\n\t\treturn flat(pieces)\n\n\telif style == 'sharp':\n\t\treturn sharp(pieces)\n\n\telif style == 'natural':\n\t\treturn natural(pieces)\n\n\telif style == 'harmonic':\n\t\treturn harmonic(pieces)\n\n\telif style == 'major':\n\t\treturn major(pieces)\n\n\telif style == 'minor':\n\t\treturn minor(pieces)\n\n\telif style == 'melodic':\n\t\treturn melodic(pieces)\n\n\telif style == 'harmonic_minor':\n\t\treturn harmonic_minor(pieces)\n\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tplus_tag = pieces.closet_tag.split(\"+\")\n\tif plus_tag[0] == pieces.closet_tag:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset = pieces.closet_tag\n\tif closet is None:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\treturn \"+\" if \"+\" in pieces else \".\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif pieces[-1].closet:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif pieces.closet_tag.startswith(\"+\"):\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif pieces[1] == \"\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcl = pieces[0]['closetags']\n\tif \"+\" in cl:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tn = len(pieces)\n\tif n < 1:\n\t\treturn \".\"\n\tif pieces[n-1][1] == \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif '+' in pieces:\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces.get(\"closet-tag\")\n\tif not closet_tag:\n\t\treturn \".\"\n\tcloset_tag = closet_tag.lower()\n\tif \"+\" in closet_tag:\n\t\treturn \".\"\n\treturn \"+\"\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tplus_tag = pieces.closet_tag.split(\"+\")\n\tif plus_tag[0] == pieces.closet_tag:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset = pieces.closet_tag\n\tif closet is None:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\treturn \"+\" if \"+\" in pieces else \".\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif pieces[-1].closet:\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif pieces.closet_tag.startswith(\"+\"):\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif pieces[1] == \"\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcl = pieces[0]['closetags']\n\tif \"+\" in cl:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tn = len(pieces)\n\tif n < 1:\n\t\treturn \".\"\n\tif pieces[n-1][1] == \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tif '+' in pieces:\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces.get(\"closet-tag\")\n\tif not closet_tag:\n\t\treturn \".\"\n\tcloset_tag = closet_tag.lower()\n\tif \"+\" in closet_tag:\n\t\treturn \".\"\n\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\treturn run_command_impl(commands, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\texcept Exception:\n\t\tprint_error(\"error: failed to run command '%s'\" % commands)\n\t\traise\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(\"> \" + \" \".join(commands), file=sys.stderr)\n\n\tif hide_stderr:\n\t\tkwargs = {}\n\telse:\n\t\tkwargs = {\"stderr\": subprocess.STDOUT}\n\n\tif env is not None:\n\t\tkwargs[\"env\"] = env\n\n\ttry:\n\t\tsubprocess.check_call(commands, cwd=cwd, stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL, **kwargs)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(\"command failed: \" + \" \".join(commands), file=sys.stderr)\n\t\tprint(e.output.decode(\"utf-8\"), file=sys.stderr)\n\t\traise\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tif not isinstance(args, list):\n\t\targs = [args]\n\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif verbose:\n\t\tlog.debug('command: %s %s', commands, args)\n\n\tfor command in commands:\n\t\ttry:\n\t\t\treturncode = subprocess.call(command + args, cwd=cwd, env=env)\n\t\texcept OSError as err:\n\t\t\tlog.error('failed to execute command: %s', err)\n\t\t\tsys.exit(1)\n\t\telse:\n\t\t\tif returncode != 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tlog.error('command failed with return code %s', returncode)\n\t\t\t\tsys.exit(returncode)\n\n\treturn 0\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tif isinstance(command, str):\n\t\t\tcommand = command.split()\n\t\tcommand = [str(x) for x in command]\n\t\tif hide_stderr:\n\t\t\tcommand.append('2>/dev/null')\n\t\tif args:\n\t\t\tcommand += args\n\t\tif env:\n\t\t\tcommand = ['env', ' '.join(['%s=%s' % (k, v) for k, v in env.items()]), ' '.join(command)]\n\t\tcommand = ' '.join(command)\n\t\tif verbose:\n\t\t\tprint(command)\n\t\tos.system(command)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands) is str:\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tlog.info(' '.join(commands))\n\n\tcmd = subprocess.Popen(\n\t\tcommands,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=(subprocess.PIPE if hide_stderr else subprocess.STDOUT),\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\t(out, err) = cmd.communicate(args)\n\tif cmd.returncode != 0:\n\t\traise Exception('Error running command: %s' % ' '.join(commands))\n\n\treturn out, err\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommand = commands[0]\n\targuments = commands[1:]\n\tcommand_string = command + \" \" + \" \".join(arguments)\n\tif verbose:\n\t\tprint(\"Running: {}\".format(command_string))\n\ttry:\n\t\treturn subprocess.check_output(command_string, shell=True, cwd=cwd, stderr=subprocess.STDOUT, env=env).decode().strip()\n\texcept subprocess.CalledProcessError as e:\n\t\tif e.output:\n\t\t\tprint(e.output.decode())\n\t\tprint(\"Error running {}\".format(command_string))\n\t\tsys.exit(1)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\trun_args = [\" \".join(commands)]\n\tif verbose:\n\t\trun_args.append(\"-v\")\n\tif hide_stderr:\n\t\trun_args.append(\"2>/dev/null\")\n\tif cwd is not None:\n\t\trun_args.append(\"-C {}\".format(cwd))\n\tif env is not None:\n\t\tfor k, v in env.items():\n\t\t\trun_args.append(\"{}={}\".format(k, v))\n\n\trun_args.append(args)\n\treturn subprocess.run(run_args, shell=True)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommand = commands[0]\n\tif len(commands) > 1:\n\t\tcommand = commands\n\t\tcommands = [command]\n\t\t\n\t# call the command\n\tif verbose:\n\t\tprint \"command: %s\" % ' '.join(commands)\n\t\n\tif env is not None:\n\t\tenv = os.environ.copy()\n\t\tenv.update(env)\n\t\n\tproc = subprocess.Popen(commands, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\t\n\tstdout, stderr = proc.communicate(input=args)\n\t\n\tif not hide_stderr:\n\t\tif stderr:\n\t\t\tprint \"STDERR: %s\" % stderr\n\t\t\n\tif proc.returncode != 0:\n\t\tprint \"FAILED: %s\" % ' '.join(commands)\n\t\tprint \"Return code: %d\" % proc.returncode\n\t\tif verbose:\n\t\t\tprint \"STDOUT:\"\n\t\t\tprint stdout\n\t\t\tprint \"STDERR:\"\n\t\t\tprint stderr\n\t\tsys.exit(1)\n\n\treturn stdout\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands) == str:\n\t\tcommands = [commands]\n\t\n\tif not isinstance(args, list):\n\t\targs = list(args)\n\t\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\t\n\tif not isinstance(env, dict):\n\t\tenv = os.environ\n\t\n\tif verbose:\n\t\tprint(\"Running:\", \" \".join(commands), \"in\", cwd)\n\t\n\tp = subprocess.Popen(commands, env=env, cwd=cwd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tstdout, stderr = p.communicate(b\"\\n\".join(args).encode(\"utf-8\"))\n\tstdout = stdout.decode(\"utf-8\")\n\tstderr = stderr.decode(\"utf-8\")\n\t\n\tif not hide_stderr and stderr:\n\t\tprint(\"Error:\", stderr, file=sys.stderr)\n\t\n\treturn stdout\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands) is not list:\n\t\tcommands = [commands]\n\t\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\t\n\tif env is None:\n\t\tenv = os.environ\n\t\n\tfor cmd in commands:\n\t\tif verbose:\n\t\t\tprint(\"> \" + cmd)\n\t\tproc = subprocess.Popen(cmd, cwd=cwd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\tout, err = proc.communicate(input=args)\n\t\tif proc.returncode != 0:\n\t\t\tprint(\"> \" + cmd)\n\t\t\tprint(out)\n\t\t\tif not hide_stderr:\n\t\t\t\tprint(err)\n\t\t\traise Exception(\"Command failed: \" + cmd + \"\\n\" + err)\n\t\t\t\n\treturn out\n\t\t\t\ndef run_command_verbose(commands, args, cwd=None, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\treturn run_command(commands, args, cwd, True, hide_stderr"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\treturn run_command_impl(commands, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\texcept Exception:\n\t\tprint_error(\"error: failed to run command '%s'\" % commands)\n\t\traise\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(\"> \" + \" \".join(commands), file=sys.stderr)\n\n\tif hide_stderr:\n\t\tkwargs = {}\n\telse:\n\t\tkwargs = {\"stderr\": subprocess.STDOUT}\n\n\tif env is not None:\n\t\tkwargs[\"env\"] = env\n\n\ttry:\n\t\tsubprocess.check_call(commands, cwd=cwd, stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL, **kwargs)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(\"command failed: \" + \" \".join(commands), file=sys.stderr)\n\t\tprint(e.output.decode(\"utf-8\"), file=sys.stderr)\n\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tif not isinstance(args, list):\n\t\targs = [args]\n\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif verbose:\n\t\tlog.debug('command: %s %s', commands, args)\n\n\tfor command in commands:\n\t\ttry:\n\t\t\treturncode = subprocess.call(command + args, cwd=cwd, env=env)\n\t\texcept OSError as err:\n\t\t\tlog.error('failed to execute command: %s', err)\n\t\t\tsys.exit(1)\n\t\telse:\n\t\t\tif returncode != 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tlog.error('command failed with return code %s', returncode)\n\t\t\t\tsys.exit(returncode)\n\n\treturn 0\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tif isinstance(command, str):\n\t\t\tcommand = command.split()\n\t\tcommand = [str(x) for x in command]\n\t\tif hide_stderr:\n\t\t\tcommand.append('2>/dev/null')\n\t\tif args:\n\t\t\tcommand += args\n\t\tif env:\n\t\t\tcommand = ['env', ' '.join(['%s=%s' % (k, v) for k, v in env.items()]), ' '.join(command)]\n\t\tcommand = ' '.join(command)\n\t\tif verbose:\n\t\t\tprint(command)\n\t\tos.system(command)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands) is str:\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tlog.info(' '.join(commands))\n\n\tcmd = subprocess.Popen(\n\t\tcommands,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=(subprocess.PIPE if hide_stderr else subprocess.STDOUT),\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\t(out, err) = cmd.communicate(args)\n\tif cmd.returncode != 0:\n\t\traise Exception('Error running command: %s' % ' '.join(commands))\n\n\treturn out, err\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommand = commands[0]\n\targuments = commands[1:]\n\tcommand_string = command + \" \" + \" \".join(arguments)\n\tif verbose:\n\t\tprint(\"Running: {}\".format(command_string))\n\ttry:\n\t\treturn subprocess.check_output(command_string, shell=True, cwd=cwd, stderr=subprocess.STDOUT, env=env).decode().strip()\n\texcept subprocess.CalledProcessError as e:\n\t\tif e.output:\n\t\t\tprint(e.output.decode())\n\t\tprint(\"Error running {}\".format(command_string))\n\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\trun_args = [\" \".join(commands)]\n\tif verbose:\n\t\trun_args.append(\"-v\")\n\tif hide_stderr:\n\t\trun_args.append(\"2>/dev/null\")\n\tif cwd is not None:\n\t\trun_args.append(\"-C {}\".format(cwd))\n\tif env is not None:\n\t\tfor k, v in env.items():\n\t\t\trun_args.append(\"{}={}\".format(k, v))\n\n\trun_args.append(args)\n\treturn subprocess.run(run_args, shell=True)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommand = commands[0]\n\tif len(commands) > 1:\n\t\tcommand = commands\n\t\tcommands = [command]\n\t\t\n\t# call the command\n\tif verbose:\n\t\tprint \"command: %s\" % ' '.join(commands)\n\t\n\tif env is not None:\n\t\tenv = os.environ.copy()\n\t\tenv.update(env)\n\t\n\tproc = subprocess.Popen(commands, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\t\n\tstdout, stderr = proc.communicate(input=args)\n\t\n\tif not hide_stderr:\n\t\tif stderr:\n\t\t\tprint \"STDERR: %s\" % stderr\n\t\t\n\tif proc.returncode != 0:\n\t\tprint \"FAILED: %s\" % ' '.join(commands)\n\t\tprint \"Return code: %d\" % proc.returncode\n\t\tif verbose:\n\t\t\tprint \"STDOUT:\"\n\t\t\tprint stdout\n\t\t\tprint \"STDERR:\"\n\t\t\tprint stderr\n\t\tsys.exit(1)\n\n\treturn stdout\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands) == str:\n\t\tcommands = [commands]\n\t\n\tif not isinstance(args, list):\n\t\targs = list(args)\n\t\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\t\n\tif not isinstance(env, dict):\n\t\tenv = os.environ\n\t\n\tif verbose:\n\t\tprint(\"Running:\", \" \".join(commands), \"in\", cwd)\n\t\n\tp = subprocess.Popen(commands, env=env, cwd=cwd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tstdout, stderr = p.communicate(b\"\\n\".join(args).encode(\"utf-8\"))\n\tstdout = stdout.decode(\"utf-8\")\n\tstderr = stderr.decode(\"utf-8\")\n\t\n\tif not hide_stderr and stderr:\n\t\tprint(\"Error:\", stderr, file=sys.stderr)\n\t\n\treturn stdout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands) is not list:\n\t\tcommands = [commands]\n\t\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\t\n\tif env is None:\n\t\tenv = os.environ\n\t\n\tfor cmd in commands:\n\t\tif verbose:\n\t\t\tprint(\"> \" + cmd)\n\t\tproc = subprocess.Popen(cmd, cwd=cwd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\tout, err = proc.communicate(input=args)\n\t\tif proc.returncode != 0:\n\t\t\tprint(\"> \" + cmd)\n\t\t\tprint(out)\n\t\t\tif not hide_stderr:\n\t\t\t\tprint(err)\n\t\t\traise Exception(\"Command failed: \" + cmd + \"\\n\" + err)\n\t\t\t\n\treturn out\n\t\t\t\ndef run_command_verbose(commands, args, cwd=None, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\treturn run_command(commands, args, cwd, True, hide_stderr", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom .config import VersioneerConfig\n\tversionfile_abs = os.path.join(os.path.dirname(os.path.abspath(__file__)), os.environ.get('VERSIONFILE_SOURCE', 'VERSION'))\n\tversionfile_rel = os.environ.get('VERSIONFILE_SOURCE', 'VERSION')\n\tvcs_backend = os.environ.get('VCS_BACKEND', '').lower()\n\ttag_prefix = os.environ.get('TAG_PREFIX', '')\n\tparentdir_prefix = os.environ.get('PARENTDIR_PREFIX', '')\n\tif os.environ.get('SOURCE_DATE_EPOCH'):\n\t\timport time\n\t\ttime.time = lambda: float(os.environ['SOURCE_DATE_EPOCH'])\n\t\timport datetime\n\t\tdatetime.datetime.utcfromtimestamp = lambda x: datetime.datetime.utcfromtimestamp(x)\n\t\tos.environ['SOURCE_DATE_EPOCH'] = datetime.datetime.utcfromtimestamp(os.environ['SOURCE_DATE_EPOCH']).isoformat()\n\t\timport subprocess\n\t\tsubprocess.call = lambda x: 0\n\tconfig = VersioneerConfig()\n\tconfig.VCS = vcs_backend\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\treturn VersioneerConfig()\n\nclass VersioneerConfig(object):\n\t\"\"\"\n\tConfiguration object for Versioneer\n\t\"\"\"\n\t\n\t# Path to versioneer.py\n\tVERSIONFILE_SOURCE = 'versioneer.py'\n\t# Path to versioneer.pyc\n\tVERSIONFILE_SOURCE_EXTRA = '.pyc'\n\t# Path to _version.py\n\tVERSIONFILE_TARGET = '_version.py'\n\t# Path to _version.pyc\n\tVERSIONFILE_TARGET_EXTRA = '.pyc'\n\t# Regex to match upstream version string in _version.py\n\tVERSION_VARIABLE = '__version__'\n\t# Regex to match tag string in git describe\n\tTAG_PREFIX = 'v'\n\t# Regex to match upstream version string in git describe\n\tVERSION_PATTERN = r'v?(\\d+(?:\\.\\d+)+(?:\\.?(\\d+))?)'\n\t# Regex to match commit count in git describe\n\tCOMMIT_COUNT_PATTERN = r'\\d*'\n\t# Regex to match git describe string in git describe\n\tDESCRIBE_PATTERN = r'(?:^|-)(?P<count>\\d*)-(?P<hash>", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.VCS = 'git'\n\tconfig.style = 'git'\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = '.'\n\tconfig.parentdir_prefix_strip = 0\n\tconfig.versionfile_source = 'setup.py'\n\tconfig.versionfile_build = 'setup.py'\n\tconfig.version_module_main = '__init__.py'\n\tconfig.version_module_build = '__init__.py'\n\tconfig.version_scheme = 'guess-next-dev'\n\tconfig.version_regex = '[^0-9.]+'\n\tconfig.verbose = False\n\tconfig.parents = 2\n\tconfig.style = 'git'\n\tconfig.cwd = os.path.abspath(os.path.dirname(__file__))\n\tconfig.root = os.path.abspath(os.path.join(config.cwd, os.pardir))\n\treturn config\n\nclass VersioneerConfig(object):\n\t\"\"\"\n\tClass to encapsulate the Versioneer configuration options.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.VCS = None\n\t\tself.versionfile_source = None\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tc = VersioneerConfig()\n\tc.VCS = 'git'\n\tc.style = 'pep440'\n\tc.versionfile_source = 'lib/_version.py'\n\tc.tag_prefix = 'v'\n\tc.parentdir_prefix = 'lib/'\n\tc.version_scheme = 'post-release'\n\tc.versionfile_build = 'lib/_version.py'\n\tc.version_variable = '__version__'\n\tc.filemode = True\n\tc.tag_regex = r\"^v(?P<version>.*)$\"\n\tc.parentdir_regex = r\"^(?P<prefix>lib/).*$\"\n\treturn c\n\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.VCS = 'git'\n\tconfig.style = 'pep440'\n\tconfig.versionfile_source = 'src/versioneer.py'\n\tconfig.versionfile_build = 'versioneer.py'\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = 'src/'\n\tconfig.dirty_", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.VCS = 'git'\n\tconfig.style = 'pep440'\n\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = '..'\n\n\tconfig.versionfile_source = 'version.py'\n\tconfig.versionfile_build = 'version.py'\n\tconfig.version_format = '{tag}'\n\tconfig.version_scheme = '{tag}'\n\tconfig.parentdir_prefix = '..'\n\n\t# List all other files that may be in the source directory and should\n\t# be copied to the build directory unchanged.\n\tconfig.versionfile_ignores = ['.*']\n\n\t# The regexp used to parse the output of `git describe`.\n\tconfig.describe_command = \\\n\t\t['git', 'describe', '--dirty=-modified', '--tags', '--always', '--long']\n\tconfig.describe_regex = r'(?P<tag>v\\d+(\\.\\d+)*)-(?P<count>\\d+)-(?P<sha>[a-f0-9]+)'\n\n\tconfig.version_config = {\n\t\t'development': {\n\t\t\t'dirty': '+dev.", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tver_config = VersioneerConfig()\n\n\tver_config.VCS = \"git\"\n\n\tver_config.style = \"pep440\"\n\tver_config.tag_prefix = \"v\"\n\n\tver_config.versionfile_source = \"src/pypipegraph/__init__.py\"\n\tver_config.versionfile_build = \"src/pypipegraph/__init__.py\"\n\tver_config.parentdir_prefix = \"pypipegraph\"\n\n\tver_config.version_scheme = \"post-release\"\n\tver_config.local_scheme = \"node-and-date\"\n\n\tver_config.tag_regex = r\"^(?P<prefix>v)?(?P<version>.*)$\"\n\tver_config.parentdir_regex = r\"^pypipegraph$\"\n\n\tver_config.version_length = 8\n\tver_config.dirty_version = \"0.0.0.dev1+unknown.dirt\"\n\n\treturn ver_config\n\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tv = VersioneerConfig()\n\tv.VCS = 'git'\n\tv.style = 'pep440'\n\tv.tag_prefix = ''\n\tv.parentdir_prefix = 'lib'\n\tv.versionfile_source = 'src/python/_version.py'\n\tv.versionfile_build = 'lib/python/_version.py'\n\tv.version_module_glob = 'src/python/_version.py'\n\tv.version_regex = r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\"\n\tv.version_format = '{tag}.dev{commitcount}+{gitsha}'\n\tv.parentdir_prefix = 'lib'\n\tv.style = 'pep440'\n\tv.tag_prefix = ''\n\tv.version_scheme = 'guess-next-dev'\n\tv.local_scheme = 'node-and-date'\n\tv.untagged_version = '0.0.0.dev0'\n\tv.dirty_", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.VCS = 'git'\n\tconfig.style = 'pep440'\n\n\tconfig.VCS_FRIENDLY_NAME = 'Git'\n\tconfig.VCS_HANDLER_REMOTE_NAME = 'origin'\n\tconfig.VCS_HANDLER_ARGUMENTS = ['git', 'rev-parse', '--short', 'HEAD']\n\tconfig.VCS_HANDLER_CMD_TEMPLATE = 'git'\n\tconfig.VCS_HANDLER_CMD_TEMPLATE_STRICT = False\n\tconfig.VCS_HANDLER_TAG_REGEX = r'^refs/tags/v?(?P<version>.*)$'\n\tconfig.VCS_HANDLER_TAG_REGEX_FULL = r'^(?P<version>.*)$'\n\tconfig.VCS_HANDLER_TAG_FULL_TEMPLATE = 'refs/tags/v{version}'\n\tconfig.VCS_HANDLER_TAG_TEMPLATE = 'refs/tags/v{version}'\n\tconfig.VCS_HANDLER_COMMIT_REGEX = r'", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom distutils.util import strtobool\n\tfrom distutils.version import LooseVersion\n\n\t# Defaults\n\tVERSIONEER_CONFIG = VersioneerConfig()\n\tVERSIONEER_CONFIG.VCS = 'git'\n\tVERSIONEER_CONFIG.VCS_REF_OPTION = '--short'\n\tVERSIONEER_CONFIG.GIT_HASH_OPTION = '--no-pager cat-file -e'\n\tVERSIONEER_CONFIG.GIT_HASH_FALLBACK = 'HEAD'\n\tVERSIONEER_CONFIG.GIT_HASH_PREFIX = 'g'\n\tVERSIONEER_CONFIG.GIT_TAG_PREFIX = 'v'\n\tVERSIONEER_CONFIG.GIT_TAG_PREFIX_OPTION = '--tags'\n\tVERSIONEER_CONFIG.GIT_TAG_OPTION = 'describe --always'\n\tVERSIONEER_CONFIG.GIT_TAG_FALLBACK = 'master'\n\tVERSIONEER_CONFIG.GIT_TAG_FALLBACK_OPTION = '--all'\n\tVERSIONEER_CONFIG.GIT_TAG_PREFIX_FALLBACK = ''\n\tVERSIONEER_CONFIG.GIT_TAG_DIRTY_OPTION = '-dirty'\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom .config import VersioneerConfig\n\tversionfile_abs = os.path.join(os.path.dirname(os.path.abspath(__file__)), os.environ.get('VERSIONFILE_SOURCE', 'VERSION'))\n\tversionfile_rel = os.environ.get('VERSIONFILE_SOURCE', 'VERSION')\n\tvcs_backend = os.environ.get('VCS_BACKEND', '').lower()\n\ttag_prefix = os.environ.get('TAG_PREFIX', '')\n\tparentdir_prefix = os.environ.get('PARENTDIR_PREFIX', '')\n\tif os.environ.get('SOURCE_DATE_EPOCH'):\n\t\timport time\n\t\ttime.time = lambda: float(os.environ['SOURCE_DATE_EPOCH'])\n\t\timport datetime\n\t\tdatetime.datetime.utcfromtimestamp = lambda x: datetime.datetime.utcfromtimestamp(x)\n\t\tos.environ['SOURCE_DATE_EPOCH'] = datetime.datetime.utcfromtimestamp(os.environ['SOURCE_DATE_EPOCH']).isoformat()\n\t\timport subprocess\n\t\tsubprocess.call = lambda x: 0\n\tconfig = VersioneerConfig()\n\tconfig.VCS = vcs_backend\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\treturn VersioneerConfig()\n\nclass VersioneerConfig(object):\n\t\"\"\"\n\tConfiguration object for Versioneer\n\t\"\"\"\n\t\n\t# Path to versioneer.py\n\tVERSIONFILE_SOURCE = 'versioneer.py'\n\t# Path to versioneer.pyc\n\tVERSIONFILE_SOURCE_EXTRA = '.pyc'\n\t# Path to _version.py\n\tVERSIONFILE_TARGET = '_version.py'\n\t# Path to _version.pyc\n\tVERSIONFILE_TARGET_EXTRA = '.pyc'\n\t# Regex to match upstream version string in _version.py\n\tVERSION_VARIABLE = '__version__'\n\t# Regex to match tag string in git describe\n\tTAG_PREFIX = 'v'\n\t# Regex to match upstream version string in git describe\n\tVERSION_PATTERN = r'v?(\\d+(?:\\.\\d+)+(?:\\.?(\\d+))?)'\n\t# Regex to match commit count in git describe\n\tCOMMIT_COUNT_PATTERN = r'\\d*'\n\t# Regex to match git describe string in git describe\n\tDESCRIBE_PATTERN = r'(?:^|-)(?P<count>\\d*)-(?P<hash>", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.VCS = 'git'\n\tconfig.style = 'git'\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = '.'\n\tconfig.parentdir_prefix_strip = 0\n\tconfig.versionfile_source = 'setup.py'\n\tconfig.versionfile_build = 'setup.py'\n\tconfig.version_module_main = '__init__.py'\n\tconfig.version_module_build = '__init__.py'\n\tconfig.version_scheme = 'guess-next-dev'\n\tconfig.version_regex = '[^0-9.]+'\n\tconfig.verbose = False\n\tconfig.parents = 2\n\tconfig.style = 'git'\n\tconfig.cwd = os.path.abspath(os.path.dirname(__file__))\n\tconfig.root = os.path.abspath(os.path.join(config.cwd, os.pardir))\n\treturn config\n\nclass VersioneerConfig(object):\n\t\"\"\"\n\tClass to encapsulate the Versioneer configuration options.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.VCS = None\n\t\tself.versionfile_source = None\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tc = VersioneerConfig()\n\tc.VCS = 'git'\n\tc.style = 'pep440'\n\tc.versionfile_source = 'lib/_version.py'\n\tc.tag_prefix = 'v'\n\tc.parentdir_prefix = 'lib/'\n\tc.version_scheme = 'post-release'\n\tc.versionfile_build = 'lib/_version.py'\n\tc.version_variable = '__version__'\n\tc.filemode = True\n\tc.tag_regex = r\"^v(?P<version>.*)$\"\n\tc.parentdir_regex = r\"^(?P<prefix>lib/).*$\"\n\treturn c\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.VCS = 'git'\n\tconfig.style = 'pep440'\n\tconfig.versionfile_source = 'src/versioneer.py'\n\tconfig.versionfile_build = 'versioneer.py'\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = 'src/'\n\tconfig.dirty_", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.VCS = 'git'\n\tconfig.style = 'pep440'\n\n\tconfig.tag_prefix = 'v'\n\tconfig.parentdir_prefix = '..'\n\n\tconfig.versionfile_source = 'version.py'\n\tconfig.versionfile_build = 'version.py'\n\tconfig.version_format = '{tag}'\n\tconfig.version_scheme = '{tag}'\n\tconfig.parentdir_prefix = '..'\n\n\t# List all other files that may be in the source directory and should\n\t# be copied to the build directory unchanged.\n\tconfig.versionfile_ignores = ['.*']\n\n\t# The regexp used to parse the output of `git describe`.\n\tconfig.describe_command = \\\n\t\t['git', 'describe', '--dirty=-modified', '--tags', '--always', '--long']\n\tconfig.describe_regex = r'(?P<tag>v\\d+(\\.\\d+)*)-(?P<count>\\d+)-(?P<sha>[a-f0-9]+)'\n\n\tconfig.version_config = {\n\t\t'development': {\n\t\t\t'dirty': '+dev.", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tver_config = VersioneerConfig()\n\n\tver_config.VCS = \"git\"\n\n\tver_config.style = \"pep440\"\n\tver_config.tag_prefix = \"v\"\n\n\tver_config.versionfile_source = \"src/pypipegraph/__init__.py\"\n\tver_config.versionfile_build = \"src/pypipegraph/__init__.py\"\n\tver_config.parentdir_prefix = \"pypipegraph\"\n\n\tver_config.version_scheme = \"post-release\"\n\tver_config.local_scheme = \"node-and-date\"\n\n\tver_config.tag_regex = r\"^(?P<prefix>v)?(?P<version>.*)$\"\n\tver_config.parentdir_regex = r\"^pypipegraph$\"\n\n\tver_config.version_length = 8\n\tver_config.dirty_version = \"0.0.0.dev1+unknown.dirt\"\n\n\treturn ver_config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tv = VersioneerConfig()\n\tv.VCS = 'git'\n\tv.style = 'pep440'\n\tv.tag_prefix = ''\n\tv.parentdir_prefix = 'lib'\n\tv.versionfile_source = 'src/python/_version.py'\n\tv.versionfile_build = 'lib/python/_version.py'\n\tv.version_module_glob = 'src/python/_version.py'\n\tv.version_regex = r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\"\n\tv.version_format = '{tag}.dev{commitcount}+{gitsha}'\n\tv.parentdir_prefix = 'lib'\n\tv.style = 'pep440'\n\tv.tag_prefix = ''\n\tv.version_scheme = 'guess-next-dev'\n\tv.local_scheme = 'node-and-date'\n\tv.untagged_version = '0.0.0.dev0'\n\tv.dirty_", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.VCS = 'git'\n\tconfig.style = 'pep440'\n\n\tconfig.VCS_FRIENDLY_NAME = 'Git'\n\tconfig.VCS_HANDLER_REMOTE_NAME = 'origin'\n\tconfig.VCS_HANDLER_ARGUMENTS = ['git', 'rev-parse', '--short', 'HEAD']\n\tconfig.VCS_HANDLER_CMD_TEMPLATE = 'git'\n\tconfig.VCS_HANDLER_CMD_TEMPLATE_STRICT = False\n\tconfig.VCS_HANDLER_TAG_REGEX = r'^refs/tags/v?(?P<version>.*)$'\n\tconfig.VCS_HANDLER_TAG_REGEX_FULL = r'^(?P<version>.*)$'\n\tconfig.VCS_HANDLER_TAG_FULL_TEMPLATE = 'refs/tags/v{version}'\n\tconfig.VCS_HANDLER_TAG_TEMPLATE = 'refs/tags/v{version}'\n\tconfig.VCS_HANDLER_COMMIT_REGEX = r'", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tfrom distutils.util import strtobool\n\tfrom distutils.version import LooseVersion\n\n\t# Defaults\n\tVERSIONEER_CONFIG = VersioneerConfig()\n\tVERSIONEER_CONFIG.VCS = 'git'\n\tVERSIONEER_CONFIG.VCS_REF_OPTION = '--short'\n\tVERSIONEER_CONFIG.GIT_HASH_OPTION = '--no-pager cat-file -e'\n\tVERSIONEER_CONFIG.GIT_HASH_FALLBACK = 'HEAD'\n\tVERSIONEER_CONFIG.GIT_HASH_PREFIX = 'g'\n\tVERSIONEER_CONFIG.GIT_TAG_PREFIX = 'v'\n\tVERSIONEER_CONFIG.GIT_TAG_PREFIX_OPTION = '--tags'\n\tVERSIONEER_CONFIG.GIT_TAG_OPTION = 'describe --always'\n\tVERSIONEER_CONFIG.GIT_TAG_FALLBACK = 'master'\n\tVERSIONEER_CONFIG.GIT_TAG_FALLBACK_OPTION = '--all'\n\tVERSIONEER_CONFIG.GIT_TAG_PREFIX_FALLBACK = ''\n\tVERSIONEER_CONFIG.GIT_TAG_DIRTY_OPTION = '-dirty'\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tvcs.register_handler(method, f)\n\t\treturn f\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tresult = super(self.__class__, self).__getattribute__(method)(*args, **kwargs)\n\t\t\tif not isinstance(result, VCS):\n\t\t\t\traise TypeError('%s() must return a VCS object' % method)\n\t\t\tresult.vcs = vcs\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n\nclass VCS(object):\n\t\"\"\"\n\tVCS interface for all the VCS supported\n\t\"\"\"\n\t_vcs_handlers = dict()\n\n\tdef __init__(self, path):\n\t\t\"\"\"\n\t\tInitialize the VCS object\n\t\t\"\"\"\n\t\tself.path = path\n\t\tself.vcs = None\n\n\tdef __getattribute__(self, attrname):\n\t\t\"\"\"\n\t\tDelegate call to the handler instance\n\t\t\"\"\"\n\t\tif attrname in VCS._vcs_handlers:\n\t\t\treturn VCS._vcs_handlers[attrname](super(self.__class__, self).__getattribute__('path'))\n\t\telse:\n\t\t\treturn super(self.__class__, self).__getattribute__(attr", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef register_method(method):\n\t\tvcs_handlers[vcs] = method\n\t\treturn method\n\treturn register_method\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\treturn func(self, *args, **kwargs)\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__dict__ = func.__dict__\n\t\treturn wrapper\n\tmethod.handler = decorator\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs.register_handler(method, func)\n\t\treturn func\n\treturn decorator\n\n\nclass VCS(object):\n\t\"\"\"\n\tAn interface to a version control system\n\t\"\"\"\n\tdef __init__(self, repository, path):\n\t\tself.repository = repository\n\t\tself.path = path\n\n\tdef __str__(self):\n\t\treturn self.repository\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n\n\tdef is_repository(self):\n\t\t\"\"\"\n\t\tReturn true if the path is a repository\n\t\t\"\"\"\n\t\treturn False\n\n\tdef is_file(self):\n\t\t\"\"\"\n\t\tReturn true if the path is a file\n\t\t\"\"\"\n\t\treturn False\n\n\tdef get_repository(self):\n\t\t\"\"\"\n\t\tReturn the repository path\n\t\t\"\"\"\n\t\treturn self.repository\n\n\tdef get_path(self):\n\t\t\"\"\"\n\t\tReturn the path\n\t\t\"\"\"\n\t\treturn self.path\n\n\tdef get_handlers(self):\n\t\t\"\"\"\n\t\tReturn a list of handler\n\t\t\"\"\"\n\t\treturn []\n\n\tdef register_handler(", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef wrapper(func):\n\t\tmethod.handlers[vcs] = func\n\t\treturn func\n\treturn wrapper\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator\n\t\t\"\"\"\n\t\tvcs.handlers[method] = func\n\t\treturn func\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(handler):\n\t\t\"\"\"\n\t\tDecorator wrapper\n\t\t\"\"\"\n\t\tmethod.add_handler(vcs, handler)\n\t\treturn handler\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(method):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn method(*args, **kwargs)\n\t\twrapper.vcs = vcs\n\t\twrapper.method = method\n\t\treturn wrapper\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tif not callable(method):\n\t\traise TypeError('method must be callable')\n\tif method.__name__ == '<lambda>':\n\t\traise ValueError('lambda functions are not allowed as decorators')\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator for the register_vcs_handler\n\t\t\"\"\"\n\t\tif not callable(func):\n\t\t\traise TypeError('func must be callable')\n\t\tif func.__name__ == '<lambda>':\n\t\t\traise ValueError('lambda functions are not allowed as decorators')\n\t\tvcs_methods = getattr(func, '_vcs_methods', {})\n\t\tvcs_methods[vcs] = method\n\t\tsetattr(func, '_vcs_methods', vcs_methods)\n\t\treturn func\n\n\treturn decorator\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tvcs.register_handler(method, f)\n\t\treturn f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tresult = super(self.__class__, self).__getattribute__(method)(*args, **kwargs)\n\t\t\tif not isinstance(result, VCS):\n\t\t\t\traise TypeError('%s() must return a VCS object' % method)\n\t\t\tresult.vcs = vcs\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n\nclass VCS(object):\n\t\"\"\"\n\tVCS interface for all the VCS supported\n\t\"\"\"\n\t_vcs_handlers = dict()\n\n\tdef __init__(self, path):\n\t\t\"\"\"\n\t\tInitialize the VCS object\n\t\t\"\"\"\n\t\tself.path = path\n\t\tself.vcs = None\n\n\tdef __getattribute__(self, attrname):\n\t\t\"\"\"\n\t\tDelegate call to the handler instance\n\t\t\"\"\"\n\t\tif attrname in VCS._vcs_handlers:\n\t\t\treturn VCS._vcs_handlers[attrname](super(self.__class__, self).__getattribute__('path'))\n\t\telse:\n\t\t\treturn super(self.__class__, self).__getattribute__(attr", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef register_method(method):\n\t\tvcs_handlers[vcs] = method\n\t\treturn method\n\treturn register_method\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\treturn func(self, *args, **kwargs)\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__dict__ = func.__dict__\n\t\treturn wrapper\n\tmethod.handler = decorator\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs.register_handler(method, func)\n\t\treturn func\n\treturn decorator\n\n\nclass VCS(object):\n\t\"\"\"\n\tAn interface to a version control system\n\t\"\"\"\n\tdef __init__(self, repository, path):\n\t\tself.repository = repository\n\t\tself.path = path\n\n\tdef __str__(self):\n\t\treturn self.repository\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n\n\tdef is_repository(self):\n\t\t\"\"\"\n\t\tReturn true if the path is a repository\n\t\t\"\"\"\n\t\treturn False\n\n\tdef is_file(self):\n\t\t\"\"\"\n\t\tReturn true if the path is a file\n\t\t\"\"\"\n\t\treturn False\n\n\tdef get_repository(self):\n\t\t\"\"\"\n\t\tReturn the repository path\n\t\t\"\"\"\n\t\treturn self.repository\n\n\tdef get_path(self):\n\t\t\"\"\"\n\t\tReturn the path\n\t\t\"\"\"\n\t\treturn self.path\n\n\tdef get_handlers(self):\n\t\t\"\"\"\n\t\tReturn a list of handler\n\t\t\"\"\"\n\t\treturn []\n\n\tdef register_handler(", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef wrapper(func):\n\t\tmethod.handlers[vcs] = func\n\t\treturn func\n\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator\n\t\t\"\"\"\n\t\tvcs.handlers[method] = func\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(handler):\n\t\t\"\"\"\n\t\tDecorator wrapper\n\t\t\"\"\"\n\t\tmethod.add_handler(vcs, handler)\n\t\treturn handler\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(method):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn method(*args, **kwargs)\n\t\twrapper.vcs = vcs\n\t\twrapper.method = method\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tif not callable(method):\n\t\traise TypeError('method must be callable')\n\tif method.__name__ == '<lambda>':\n\t\traise ValueError('lambda functions are not allowed as decorators')\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator for the register_vcs_handler\n\t\t\"\"\"\n\t\tif not callable(func):\n\t\t\traise TypeError('func must be callable')\n\t\tif func.__name__ == '<lambda>':\n\t\t\traise ValueError('lambda functions are not allowed as decorators')\n\t\tvcs_methods = getattr(func, '_vcs_methods', {})\n\t\tvcs_methods[vcs] = method\n\t\tsetattr(func, '_vcs_methods', vcs_methods)\n\t\treturn func\n\n\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\timport bpy\n\n\tif validate_objects:\n\t\tif not check_digests:\n\t\t\traise ValueError('validate_objects=True requires check_digests=True')\n\t\tif not show_warnings:\n\t\t\traise ValueError('validate_objects=True requires show_warnings=True')\n\n\t# check_digests and show_warnings are mutually exclusive\n\tif check_digests and show_warnings:\n\t\traise ValueError('check_digests and show_warnings are mutually exclusive')\n\n\t# set up variables\n\tgood_objects = 0\n\tnum_objects = 0\n\n\t# validate root\n\tif bpy.data.is_saved:\n\t\tif not validate_root(self.root_id, validate_objects, check_digests, show_warnings):\n\t\t\t# TODO: log error\n\t\t\treturn num_objects, good_objects\n\telse:\n\t\t# TODO: log error\n\t\treturn num_objects, good_objects\n\n\t# iterate over objects\n\tfor obj_id in bpy.data.objects:\n\t\tif validate_objects:\n\t\t\tif not validate_object(obj_id, check_digests, show_warnings):\n\t\t\t\tcontinue\n\t\t", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tfrom .. import validator\n\tfrom .. import storage_manager\n\n\troot_paths = self.get_paths()\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor root_name in root_paths:\n\t\troot = root_paths[root_name]\n\t\tif root.is_root:\n\t\t\tcontinue\n\t\troot.set_parent(None)\n\t\tfor root_child in root.get_children():\n\t\t\tif root_child.is_root:\n\t\t\t\tcontinue\n\t\t\tnum_objects += 1\n\t\t\tif validate_objects:\n\t\t\t\tif validator.validate_object(root_child, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\t\tgood_objects += 1\n\t\t\telse:\n\t\t\t\tif root_child.validate():\n\t\t\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# TODO: validate_objects and check_digests\n\t# TODO: show_warnings\n\n\t# validate root node\n\tif not self.root_node:\n\t\tprint('No root node')\n\t\treturn 0, 0\n\tif not self.root_node.validate(show_warnings):\n\t\tprint('Root node invalid')\n\t\treturn 0, 0\n\n\t# check digest of root node\n\tif check_digests and not self.root_node.validate_digest():\n\t\tprint('Root node digest mismatch')\n\t\treturn 0, 0\n\n\t# validate any objects\n\tif validate_objects:\n\t\tnum_objects, good_objects = self.root_node.validate_objects(show_warnings)\n\t\treturn num_objects, good_objects\n\n\treturn 0, 0\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj_name in self.dir_list():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj = self.get_object(obj_name, validate_objects=validate_objects, check_digests=check_digests)\n\t\t\tif obj.is_valid:\n\t\t\t\tgood_objects += 1\n\t\texcept ValueError as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"{}: {}\".format(obj_name, e))\n\t\t\telse:\n\t\t\t\tprint(e)\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.get_objects():\n\t\tif validate_objects:\n\t\t\t# validate object\n\t\t\tnum_objects += 1\n\t\t\tif obj.validate(check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\tgood_objects += 1\n\n\t\tif not validate_objects:\n\t\t\tif obj.validate_digest(check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n#------------------------------------------------------------------------------\ndef validate_object(self, obj_id, validate_ancestors=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate single object.\n\t\"\"\"\n\tobj = self.get_object(obj_id)\n\tif obj is not None:\n\t\treturn obj.validate(validate_ancestors=validate_ancestors, check_digests=check_digests, show_warnings=show_warnings)\n\telse:\n\t\treturn False\n\n#------------------------------------------------------------------------------\ndef validate_digest(self", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\thierarchy = self.get_hierarchy()\n\tfor bucket_name in hierarchy:\n\t\tbucket = hierarchy[bucket_name]\n\t\tfor object_key in bucket:\n\t\t\tobject_name = bucket[object_key]\n\t\t\tnum_objects += 1\n\t\t\tif validate_objects:\n\t\t\t\tsuccess, message = self.validate_object(object_name, check_digests=check_digests, show_warnings=show_warnings)\n\t\t\t\tif success:\n\t\t\t\t\tgood_objects += 1\n\t\t\t\telse:\n\t\t\t\t\tself.logger.error(message)\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif validate_objects:\n\t\tif check_digests:\n\t\t\treturn self.validate_objects(show_warnings)\n\t\telse:\n\t\t\treturn self.validate_objects_no_check_digests(show_warnings)\n\telse:\n\t\treturn self.validate_hierarchy_no_check_objects(show_warnings)\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.is_valid_root():\n\t\tself.log_debug(\"Root is valid\")\n\telse:\n\t\tself.log_warn(\"Root is invalid\")\n\t\treturn (0, 0)\n\tif self.root_obj is None:\n\t\tself.log_debug(\"Root is empty\")\n\t\treturn (0, 0)\n\tif validate_objects:\n\t\treturn self._validate_hierarchy(self.root_obj, check_digests, show_warnings)\n\telse:\n\t\treturn self._validate_hierarchy(self.root_obj, False, show_warnings)\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# Check the root directory\n\tnum_objects = len(self.object_list)\n\tgood_objects = 0\n\tif validate_objects:\n\t\tfor object_info in self.object_list:\n\t\t\tobject_path = object_info.path\n\t\t\tif object_info.is_directory:\n\t\t\t\tobject_path += '/'\n\t\t\tobject_digest = self.get_object_digest(object_path)\n\t\t\tif check_digests:\n\t\t\t\tif object_info.digest != object_digest:\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Warning: invalid digest for object:', object_path, object_info.digest, object_digest)\n\t\t\t\telse:\n\t\t\t\t\tgood_objects += 1\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Valid digest for object:', object_path, object_info.digest)\n\t\t\tif validate_objects:\n\t\t\t\tif not object_info.is_directory:\n\t\t\t\t\tif not object_info.validate(show_warnings):\n\t\t\t\t\t\tprint('Warning: invalid object:', object_path", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.validation_state == 'valid':\n\t\treturn\n\t\t\n\tif not show_warnings:\n\t\t# Disable warnings\n\t\timport warnings\n\t\twarnings.simplefilter('ignore')\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\n\t# Check root\n\tif len(self.root.objects) == 0:\n\t\traise Exception('Root object contains no objects')\n\n\t# Check objects\n\tobject_paths = set()\n\tfor obj in self.root.objects:\n\t\tnum_objects += 1\n\t\tif obj.is_valid(validate_objects, check_digests):\n\t\t\tgood_objects += 1\n\t\telse:\n\t\t\traise Exception('Object {} in root is not valid'.format(obj.path))\n\t\t\n\t\t# Check for duplicate object paths\n\t\tif obj.path in object_paths:\n\t\t\traise Exception('Duplicate object path {}'.format(obj.path))\n\t\tobject_paths.add(obj.path)\n\t\t\n\t\t# Check for duplicate object file names\n\t\tif os.path.basename(obj.path) in object_paths:\n\t\t\traise Exception('Duplicate object file name {}'.format(obj.path))"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\timport bpy\n\n\tif validate_objects:\n\t\tif not check_digests:\n\t\t\traise ValueError('validate_objects=True requires check_digests=True')\n\t\tif not show_warnings:\n\t\t\traise ValueError('validate_objects=True requires show_warnings=True')\n\n\t# check_digests and show_warnings are mutually exclusive\n\tif check_digests and show_warnings:\n\t\traise ValueError('check_digests and show_warnings are mutually exclusive')\n\n\t# set up variables\n\tgood_objects = 0\n\tnum_objects = 0\n\n\t# validate root\n\tif bpy.data.is_saved:\n\t\tif not validate_root(self.root_id, validate_objects, check_digests, show_warnings):\n\t\t\t# TODO: log error\n\t\t\treturn num_objects, good_objects\n\telse:\n\t\t# TODO: log error\n\t\treturn num_objects, good_objects\n\n\t# iterate over objects\n\tfor obj_id in bpy.data.objects:\n\t\tif validate_objects:\n\t\t\tif not validate_object(obj_id, check_digests, show_warnings):\n\t\t\t\tcontinue\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tfrom .. import validator\n\tfrom .. import storage_manager\n\n\troot_paths = self.get_paths()\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor root_name in root_paths:\n\t\troot = root_paths[root_name]\n\t\tif root.is_root:\n\t\t\tcontinue\n\t\troot.set_parent(None)\n\t\tfor root_child in root.get_children():\n\t\t\tif root_child.is_root:\n\t\t\t\tcontinue\n\t\t\tnum_objects += 1\n\t\t\tif validate_objects:\n\t\t\t\tif validator.validate_object(root_child, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\t\tgood_objects += 1\n\t\t\telse:\n\t\t\t\tif root_child.validate():\n\t\t\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# TODO: validate_objects and check_digests\n\t# TODO: show_warnings\n\n\t# validate root node\n\tif not self.root_node:\n\t\tprint('No root node')\n\t\treturn 0, 0\n\tif not self.root_node.validate(show_warnings):\n\t\tprint('Root node invalid')\n\t\treturn 0, 0\n\n\t# check digest of root node\n\tif check_digests and not self.root_node.validate_digest():\n\t\tprint('Root node digest mismatch')\n\t\treturn 0, 0\n\n\t# validate any objects\n\tif validate_objects:\n\t\tnum_objects, good_objects = self.root_node.validate_objects(show_warnings)\n\t\treturn num_objects, good_objects\n\n\treturn 0, 0\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj_name in self.dir_list():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj = self.get_object(obj_name, validate_objects=validate_objects, check_digests=check_digests)\n\t\t\tif obj.is_valid:\n\t\t\t\tgood_objects += 1\n\t\texcept ValueError as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"{}: {}\".format(obj_name, e))\n\t\t\telse:\n\t\t\t\tprint(e)\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.get_objects():\n\t\tif validate_objects:\n\t\t\t# validate object\n\t\t\tnum_objects += 1\n\t\t\tif obj.validate(check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\tgood_objects += 1\n\n\t\tif not validate_objects:\n\t\t\tif obj.validate_digest(check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n#------------------------------------------------------------------------------\ndef validate_object(self, obj_id, validate_ancestors=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate single object.\n\t\"\"\"\n\tobj = self.get_object(obj_id)\n\tif obj is not None:\n\t\treturn obj.validate(validate_ancestors=validate_ancestors, check_digests=check_digests, show_warnings=show_warnings)\n\telse:\n\t\treturn False\n\n#------------------------------------------------------------------------------\ndef validate_digest(self", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\thierarchy = self.get_hierarchy()\n\tfor bucket_name in hierarchy:\n\t\tbucket = hierarchy[bucket_name]\n\t\tfor object_key in bucket:\n\t\t\tobject_name = bucket[object_key]\n\t\t\tnum_objects += 1\n\t\t\tif validate_objects:\n\t\t\t\tsuccess, message = self.validate_object(object_name, check_digests=check_digests, show_warnings=show_warnings)\n\t\t\t\tif success:\n\t\t\t\t\tgood_objects += 1\n\t\t\t\telse:\n\t\t\t\t\tself.logger.error(message)\n\t\t\telse:\n\t\t\t\tgood_objects += 1\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif validate_objects:\n\t\tif check_digests:\n\t\t\treturn self.validate_objects(show_warnings)\n\t\telse:\n\t\t\treturn self.validate_objects_no_check_digests(show_warnings)\n\telse:\n\t\treturn self.validate_hierarchy_no_check_objects(show_warnings)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.is_valid_root():\n\t\tself.log_debug(\"Root is valid\")\n\telse:\n\t\tself.log_warn(\"Root is invalid\")\n\t\treturn (0, 0)\n\tif self.root_obj is None:\n\t\tself.log_debug(\"Root is empty\")\n\t\treturn (0, 0)\n\tif validate_objects:\n\t\treturn self._validate_hierarchy(self.root_obj, check_digests, show_warnings)\n\telse:\n\t\treturn self._validate_hierarchy(self.root_obj, False, show_warnings)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# Check the root directory\n\tnum_objects = len(self.object_list)\n\tgood_objects = 0\n\tif validate_objects:\n\t\tfor object_info in self.object_list:\n\t\t\tobject_path = object_info.path\n\t\t\tif object_info.is_directory:\n\t\t\t\tobject_path += '/'\n\t\t\tobject_digest = self.get_object_digest(object_path)\n\t\t\tif check_digests:\n\t\t\t\tif object_info.digest != object_digest:\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Warning: invalid digest for object:', object_path, object_info.digest, object_digest)\n\t\t\t\telse:\n\t\t\t\t\tgood_objects += 1\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Valid digest for object:', object_path, object_info.digest)\n\t\t\tif validate_objects:\n\t\t\t\tif not object_info.is_directory:\n\t\t\t\t\tif not object_info.validate(show_warnings):\n\t\t\t\t\t\tprint('Warning: invalid object:', object_path", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.validation_state == 'valid':\n\t\treturn\n\t\t\n\tif not show_warnings:\n\t\t# Disable warnings\n\t\timport warnings\n\t\twarnings.simplefilter('ignore')\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\n\t# Check root\n\tif len(self.root.objects) == 0:\n\t\traise Exception('Root object contains no objects')\n\n\t# Check objects\n\tobject_paths = set()\n\tfor obj in self.root.objects:\n\t\tnum_objects += 1\n\t\tif obj.is_valid(validate_objects, check_digests):\n\t\t\tgood_objects += 1\n\t\telse:\n\t\t\traise Exception('Object {} in root is not valid'.format(obj.path))\n\t\t\n\t\t# Check for duplicate object paths\n\t\tif obj.path in object_paths:\n\t\t\traise Exception('Duplicate object path {}'.format(obj.path))\n\t\tobject_paths.add(obj.path)\n\t\t\n\t\t# Check for duplicate object file names\n\t\tif os.path.basename(obj.path) in object_paths:\n\t\t\traise Exception('Duplicate object file name {}'.format(obj.path))", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = OCFL.create_storage_root()\n\tself.storage_root.initialize()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = self.create_storage_root()\n\tself.storage_root.validate()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFL.StorageRoot(self.storage_root)\n\tself.root.set_digest(self.digest_algorithm)\n\tself.root.create()\n\tself.root.initialize()\n\treturn self\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\tlogger.info(\"Initializing OCFL storage root at {}\".format(self.root_dir))\n\n\t# Create the root directory\n\tif not os.path.isdir(self.root_dir):\n\t\tos.mkdir(self.root_dir)\n\t\t\n\t# Create the inventory directory\n\tif not os.path.isdir(self.inventory_dir):\n\t\tos.mkdir(self.inventory_dir)\n\t\n\t# Create the config directory\n\tif not os.path.isdir(self.config_dir):\n\t\tos.mkdir(self.config_dir)\n\t\n\t# Create the inventory configuration\n\tif not os.path.isfile(self.inventory_config_file):\n\t\twith open(self.inventory_config_file, 'w') as f:\n\t\t\tf.write(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ocfl_inventory xmlns=\"info:fedora/fedora-system:def/ocfl#inventory-1.0/root\">\n    <id>ROOT</id>\n    <object_root_path>ROOT</object_root_path>\n    <inventory_root_path>ROOT</inventory_root_path", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create()\n\tself.initialize()\n\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.init_storage(OCFL_CONFIGURATION)\n\tself.init_storage_root()\n\tself.init_storage_root_config()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = self.create_storage_root()\n\tself.storage_root_dir = self.storage_root.directory()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.ocfl_root = self.ocfl_storage_path\n\tif not os.path.exists(self.ocfl_root):\n\t\tos.makedirs(self.ocfl_root)\n\t\tos.makedirs(os.path.join(self.ocfl_root, 'version'))\n\t\tos.makedirs(os.path.join(self.ocfl_root, 'version', 'v1'))\n\t\t# Create the OCFL inventories\n\t\t_ocfl_inventory_file = os.path.join(self.ocfl_root, 'inventory.txt')\n\t\t_ocfl_inventory_file_v1 = os.path.join(self.ocfl_root, 'version', 'v1', 'inventory.txt')\n\t\twith open(_ocfl_inventory_file, 'w') as f:\n\t\t\tf.write(self.ocfl_storage_inventory_content())\n\t\twith open(_ocfl_inventory_file_v1, 'w') as f:\n\t\t\tf.write(self.ocfl_storage_inventory_content())\n\t\t# Create the OCFL description file\n\t\t_ocfl_description_file = os.path.join(self.ocfl", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root_dir = os.path.abspath(self.config['storage_root'])\n\tself.repo_root_dir = os.path.dirname(self.root_dir)\n\tself.object_root_dir = os.path.join(self.root_dir, 'objects')\n\tif not os.path.exists(self.object_root_dir):\n\t\tos.makedirs(self.object_root_dir, mode=0o775)\n\tself.content_root_dir = os.path.join(self.root_dir, 'content')\n\tif not os.path.exists(self.content_root_dir):\n\t\tos.makedirs(self.content_root_dir, mode=0o775)\n\tself.version_root_dir = os.path.join(self.root_dir, 'versions')\n\tif not os.path.exists(self.version_root_dir):\n\t\tos.makedirs(self.version_root_dir, mode=0o775)\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.logger.info(\"Creating new OCFL storage root\")\n\n\t# Create the storage root directory\n\tif os.path.exists(self.storage_root):\n\t    self.logger.warning(\"Storage root directory already exists, removing\")\n\t    shutil.rmtree(self.storage_root)\n\tos.mkdir(self.storage_root)\n\n\t# Create the .well-known directory\n\tif os.path.exists(self.well_known_dir):\n\t    self.logger.warning(\"well-known directory already exists, removing\")\n\t    shutil.rmtree(self.well_known_dir)\n\tos.mkdir(self.well_known_dir)\n\n\t# Create the version root\n\tself.version_root = os.path.join(self.storage_root, 'version')\n\tif os.path.exists(self.version_root):\n\t    self.logger.warning(\"version root directory already exists, removing\")\n\t    shutil.rmtree(self.version_root)\n\tos.mkdir(self.version_root)\n\n\t# Write out the .well-known/ocfl_layout file\n\tlayout_file = os.path.join(self.well_known_dir, 'ocfl_layout')\n\tself."], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = OCFL.create_storage_root()\n\tself.storage_root.initialize()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = self.create_storage_root()\n\tself.storage_root.validate()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFL.StorageRoot(self.storage_root)\n\tself.root.set_digest(self.digest_algorithm)\n\tself.root.create()\n\tself.root.initialize()\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\tlogger.info(\"Initializing OCFL storage root at {}\".format(self.root_dir))\n\n\t# Create the root directory\n\tif not os.path.isdir(self.root_dir):\n\t\tos.mkdir(self.root_dir)\n\t\t\n\t# Create the inventory directory\n\tif not os.path.isdir(self.inventory_dir):\n\t\tos.mkdir(self.inventory_dir)\n\t\n\t# Create the config directory\n\tif not os.path.isdir(self.config_dir):\n\t\tos.mkdir(self.config_dir)\n\t\n\t# Create the inventory configuration\n\tif not os.path.isfile(self.inventory_config_file):\n\t\twith open(self.inventory_config_file, 'w') as f:\n\t\t\tf.write(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ocfl_inventory xmlns=\"info:fedora/fedora-system:def/ocfl#inventory-1.0/root\">\n    <id>ROOT</id>\n    <object_root_path>ROOT</object_root_path>\n    <inventory_root_path>ROOT</inventory_root_path", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create()\n\tself.initialize()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.init_storage(OCFL_CONFIGURATION)\n\tself.init_storage_root()\n\tself.init_storage_root_config()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage_root = self.create_storage_root()\n\tself.storage_root_dir = self.storage_root.directory()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.ocfl_root = self.ocfl_storage_path\n\tif not os.path.exists(self.ocfl_root):\n\t\tos.makedirs(self.ocfl_root)\n\t\tos.makedirs(os.path.join(self.ocfl_root, 'version'))\n\t\tos.makedirs(os.path.join(self.ocfl_root, 'version', 'v1'))\n\t\t# Create the OCFL inventories\n\t\t_ocfl_inventory_file = os.path.join(self.ocfl_root, 'inventory.txt')\n\t\t_ocfl_inventory_file_v1 = os.path.join(self.ocfl_root, 'version', 'v1', 'inventory.txt')\n\t\twith open(_ocfl_inventory_file, 'w') as f:\n\t\t\tf.write(self.ocfl_storage_inventory_content())\n\t\twith open(_ocfl_inventory_file_v1, 'w') as f:\n\t\t\tf.write(self.ocfl_storage_inventory_content())\n\t\t# Create the OCFL description file\n\t\t_ocfl_description_file = os.path.join(self.ocfl", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root_dir = os.path.abspath(self.config['storage_root'])\n\tself.repo_root_dir = os.path.dirname(self.root_dir)\n\tself.object_root_dir = os.path.join(self.root_dir, 'objects')\n\tif not os.path.exists(self.object_root_dir):\n\t\tos.makedirs(self.object_root_dir, mode=0o775)\n\tself.content_root_dir = os.path.join(self.root_dir, 'content')\n\tif not os.path.exists(self.content_root_dir):\n\t\tos.makedirs(self.content_root_dir, mode=0o775)\n\tself.version_root_dir = os.path.join(self.root_dir, 'versions')\n\tif not os.path.exists(self.version_root_dir):\n\t\tos.makedirs(self.version_root_dir, mode=0o775)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.logger.info(\"Creating new OCFL storage root\")\n\n\t# Create the storage root directory\n\tif os.path.exists(self.storage_root):\n\t    self.logger.warning(\"Storage root directory already exists, removing\")\n\t    shutil.rmtree(self.storage_root)\n\tos.mkdir(self.storage_root)\n\n\t# Create the .well-known directory\n\tif os.path.exists(self.well_known_dir):\n\t    self.logger.warning(\"well-known directory already exists, removing\")\n\t    shutil.rmtree(self.well_known_dir)\n\tos.mkdir(self.well_known_dir)\n\n\t# Create the version root\n\tself.version_root = os.path.join(self.storage_root, 'version')\n\tif os.path.exists(self.version_root):\n\t    self.logger.warning(\"version root directory already exists, removing\")\n\t    shutil.rmtree(self.version_root)\n\tos.mkdir(self.version_root)\n\n\t# Write out the .well-known/ocfl_layout file\n\tlayout_file = os.path.join(self.well_known_dir, 'ocfl_layout')\n\tself.", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tif version.startswith('v'):\n\t\tversion = version[1:]\n\tparts = version.split('.')\n\tlast = parts[-1]\n\tparts = parts[:-1]\n\tparts.append(str(int(last)+1))\n\treturn '.'.join(parts)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tv_parts = version.split('.')\n\tif len(v_parts) == 1:\n\t\treturn v_parts[0] + '.1'\n\telse:\n\t\treturn '.'.join(v_parts[:-1]) + '.' + str(int(v_parts[-1]) + 1)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tif len(parts) == 3:\n\t\tparts[2] = str(int(parts[2]) + 1)\n\telse:\n\t\tparts.append('1')\n\treturn '.'.join(parts)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tnext_version_id = 0\n\tversion_ids = version.split(\".\")\n\tif len(version_ids) == 3:\n\t\tnext_version_id = int(version_ids[2]) + 1\n\t\tif next_version_id < 10:\n\t\t\treturn \"{}.{}.0{}\".format(version_ids[0], version_ids[1], next_version_id)\n\t\telse:\n\t\t\treturn \"{}.{}.{}\".format(version_ids[0], version_ids[1], next_version_id)\n\telse:\n\t\treturn \"0.1.0\"\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tparts[-1] = str(int(parts[-1]) + 1)\n\treturn '.'.join(parts)\n\nif __name__ == '__main__':\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('version')\n\targs = parser.parse_args()\n\n\tversion = args.version\n\tif version == 'next':\n\t\tversion = next_version(args.version)\n\t\tprint(version)\n\telif version == 'latest':\n\t\timport requests\n\t\tr = requests.get('https://pypi.python.org/pypi/", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[-1] = str(int(version[-1]) + 1)\n\treturn '.'.join(version)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_split = version.split(\".\")\n\tversion_split[-1] = str(int(version_split[-1])+1)\n\treturn \".\".join(version_split)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tparts[-1] = str(int(parts[-1]) + 1)\n\treturn '.'.join(parts)\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\tnext_version = version_parts[0] + '.' + str(int(version_parts[1]) + 1) + '.0'\n\treturn next_version\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch = version.split(\".\")\n\tpatch = int(patch) + 1\n\treturn f\"{major}.{minor}.{patch}\"\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tif version.startswith('v'):\n\t\tversion = version[1:]\n\tparts = version.split('.')\n\tlast = parts[-1]\n\tparts = parts[:-1]\n\tparts.append(str(int(last)+1))\n\treturn '.'.join(parts)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tv_parts = version.split('.')\n\tif len(v_parts) == 1:\n\t\treturn v_parts[0] + '.1'\n\telse:\n\t\treturn '.'.join(v_parts[:-1]) + '.' + str(int(v_parts[-1]) + 1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tif len(parts) == 3:\n\t\tparts[2] = str(int(parts[2]) + 1)\n\telse:\n\t\tparts.append('1')\n\treturn '.'.join(parts)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tnext_version_id = 0\n\tversion_ids = version.split(\".\")\n\tif len(version_ids) == 3:\n\t\tnext_version_id = int(version_ids[2]) + 1\n\t\tif next_version_id < 10:\n\t\t\treturn \"{}.{}.0{}\".format(version_ids[0], version_ids[1], next_version_id)\n\t\telse:\n\t\t\treturn \"{}.{}.{}\".format(version_ids[0], version_ids[1], next_version_id)\n\telse:\n\t\treturn \"0.1.0\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tparts[-1] = str(int(parts[-1]) + 1)\n\treturn '.'.join(parts)\n\nif __name__ == '__main__':\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('version')\n\targs = parser.parse_args()\n\n\tversion = args.version\n\tif version == 'next':\n\t\tversion = next_version(args.version)\n\t\tprint(version)\n\telif version == 'latest':\n\t\timport requests\n\t\tr = requests.get('https://pypi.python.org/pypi/", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[-1] = str(int(version[-1]) + 1)\n\treturn '.'.join(version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_split = version.split(\".\")\n\tversion_split[-1] = str(int(version_split[-1])+1)\n\treturn \".\".join(version_split)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tparts[-1] = str(int(parts[-1]) + 1)\n\treturn '.'.join(parts)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\tnext_version = version_parts[0] + '.' + str(int(version_parts[1]) + 1) + '.0'\n\treturn next_version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch = version.split(\".\")\n\tpatch = int(patch) + 1\n\treturn f\"{major}.{minor}.{patch}\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inv = self.get_root_inventory()\n\troot_content_digests = set(d['digest'] for d in root_inv['content'])\n\tversion_content_digests = []\n\n\t# check that each version directory has an inventory\n\tfor version_dir in version_dirs:\n\t\tversion_inv = self.get_inventory_for_version(version_dir)\n\t\tversion_content_digests.extend(d['digest'] for d in version_inv['content'])\n\t\t# check that each version directory has a content digest\n\t\tif not version_inv['content']:\n\t\t\traise ValidationError(\"Version %s has no content digests\" % version_dir)\n\t\t# check that each version directory has a version digest\n\t\tif not version_inv['version']:\n\t\t\traise ValidationError(\"Version %s has no version digest\" % version_dir)\n\n\t# check that each version directory has a content digest that is different from\n\t# the root inventory\n\tif root_content_digests & set(version_content_digests):\n\t\traise ValidationError(\"Version content digests duplicate those in root inventory\")\n\n\t# check that each version directory has a version digest that is different from", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tcontent_digest_diffs = []\n\n\t# Use the first inventory as the root inventory\n\troot_inventory = self.inventories[0]\n\tcontent_digests = self.get_content_digests(root_inventory)\n\n\t# Validate that each inventory has a content digest\n\tfor version_dir in version_dirs:\n\t\tinventory = self.inventories[version_dir]\n\t\tif not inventory.content_digest:\n\t\t\traise ValidationError('Each inventory MUST have a content digest')\n\n\t\t# Validate that the digest is consistent with root inventory\n\t\tif inventory.content_digest not in content_digests:\n\t\t\tcontent_digest_diffs.append(inventory.content_digest)\n\t\t\tcontinue\n\n\t\t# Validate that the inventory has the same set of files as root\n\t\tinventory_files = self.get_files_from_inventory(inventory)\n\t\tfor f in inventory_files:\n\t\t\tif f not in self.get_files_from_inventory(root_inventory):\n\t\t\t\traise ValidationError(f'inventory {version_dir} has unknown '\n\t\t\t\t", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tif len(version_dirs) == 0:\n\t\traise Exception(\"No version dirs found\")\n\n\troot_inv_path = self.root_inv_path\n\troot_inv = self.root_inv\n\n\tfor version_dir in version_dirs:\n\t\tversion_inv_path = self.version_inv_path(version_dir)\n\t\ttry:\n\t\t\tversion_inv = Inventory(version_inv_path)\n\t\texcept Exception as e:\n\t\t\traise Exception(\"Version inventory %s is not valid: %s\" % (version_inv_path, str(e)))\n\n\t\tself.validate_version_inventory(version_inv_path, version_inv, root_inv_path, root_inv)\n\n\t\t# Keep a record of any content digests different from those in the root inventory\n\t\tversion_digests = version_inv.get_all_digests()\n\t\troot_digests = root_inv.get_all_digests()\n\t\tfor digest in version_digests:\n\t\t\tif digest not in root_digests:\n\t\t\t\tself.version_digests_not_in_root.add(digest)\n\n\t\t\telif version_digests[digest", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_path = os.path.join(self.root_dir, version_dir, INVENTORY_NAME)\n\t\tversion_inventory = Inventory(version_inventory_path)\n\t\tif not os.path.exists(version_inventory.path):\n\t\t\traise Exception(\"Version inventory not found at \" + version_inventory_path)\n\t\tif not version_inventory.validate(self.root_dir, self.root_inventory):\n\t\t\traise Exception(\"Version inventory validation failed for \" + version_inventory_path)\n\n\t\t# check the version inventory against the root inventory\n\t\tif self.root_inventory.compare(version_inventory):\n\t\t\traise Exception(\"Version inventory does not match root inventory: \" + version_inventory_path)\n\n\t\t# check version inventory against previous inventories\n\t\tfor previous_inventory in self.inventories:\n\t\t\tif previous_inventory.compare(version_inventory):\n\t\t\t\traise Exception(\"Version inventory does not match previous inventory: \" + version_inventory_path)\n\n\t\tself.inventories.append(version_inventory)\n\n\t\t", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# This is a list of version directory names, which are also the keys of the\n\t# inventory (as inventory.json), and are assumed to be in version sequence.\n\tfor i in range(len(version_dirs)):\n\t\tversion_dir = version_dirs[i]\n\t\t# Get the inventory for this version directory\n\t\tif not self.inventories.has_key(version_dir):\n\t\t\traise ValidationError('No inventory found for version %s' % version_dir)\n\t\t# If this is the first inventory, then it should be the inventory of the root\n\t\t# directory.\n\t\tif i == 0:\n\t\t\tif self.inventories[version_dir]['directory'] != self.root_dir:\n\t\t\t\traise ValidationError('Expected inventory for version %s to be in root directory, but it was in %s' % version_dir, self.inventories[version_dir]['directory'])\n\t\telse:\n\t\t\t# If this is not the first inventory, then it should be the inventory of the\n\t\t\t# version directory above it.\n\t\t\texpected_parent = version_dirs[i-1]\n\t\t\tif self.inventories[version_dir]['directory", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inventory = self.validate_root_inventory()\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.validate_version_inventory(version_dir, root_inventory)\n\t\tself.validate_version_content(version_dir, version_inventory)\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tversion_path = os.path.join(self.root_dir, version_dir)\n\t\tinventory_path = os.path.join(version_path, \"inventory.json\")\n\t\t# The inventory must exist.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s has no inventory.json\" % version_dir)\n\t\t# The inventory must have the right format.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json\" % version_dir)\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json\" % version_dir)\n\t\t# The inventory must have the right format.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json\" % version_dir)\n\t\t# The inventory must have the right format.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tlogger.debug(\"Validating version inventories\")\n\troot_inventory = self.get_root_inventory()\n\troot_digests = []\n\troot_digests_by_filename = {}\n\tif root_inventory is not None:\n\t\troot_digests = root_inventory.content_digests.digests\n\t\troot_digests_by_filename = root_inventory.content_digests.digests_by_filename\n\tversion_inventories = []\n\tdigests_by_filename = {}\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.get_version_inventory(version_dir)\n\t\tversion_digests = []\n\t\tif version_inventory is not None:\n\t\t\tversion_digests = version_inventory.content_digests.digests\n\t\t\tif version_dir == version_dirs[0]:\n\t\t\t\tversion_digests = root_digests\n\t\t\t\tdigests_by_filename = root_digests_by_filename\n\t\t\telse:\n\t\t\t\tdigests_by_filename = version_inventory.content_digests.digests_by_filename\n\t\tversion_inventories.append(version_inventory)\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.assertEqual(len(version_dirs), len(self.root_inventory.data['versions']),\n\t\t\"Number of version directories should be same as versions in root inventory\")\n\tversion_inventories = []\n\tcontent_digests = []\n\tfor i in range(len(version_dirs)):\n\t\tversion_dir = version_dirs[i]\n\t\tif self.check_version_exists(version_dir):\n\t\t\tversion_inventories.append(self.get_version_inventory(version_dir))\n\t\t\tcontent_digests.append(version_inventories[i].content_digests)\n\t\telse:\n\t\t\tversion_inventories.append(None)\n\t\t\tcontent_digests.append([])\n\n\t# make sure there is an inventory for each version\n\tfor i in range(len(version_inventories)):\n\t\tself.assertIsNotNone(version_inventories[i],\n\t\t\t\"Version {} does not have an inventory\".format(version_dirs[i]))\n\t\t# check that we have a root inventory for each version\n\t\tself.assertIn(version_dirs[i], version_inventories[i].data['versions'],\n\t\t", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_files = []\n\tfor version in version_dirs:\n\t\tinventory_files.append(os.path.join(self.root_dir, version, 'inventory.json'))\n\n\t# keep track of any inventory content digests that are different from the\n\t# root inventory\n\tdifferent_digests = []\n\n\t# validate all the inventories\n\tfor inventory_file in inventory_files:\n\t\tinventory = self.load_inventory(inventory_file)\n\t\tself.validate_inventory(inventory, inventory_file)\n\t\tcontent_digest = inventory['content_digest']\n\t\tif content_digest != self.root_inventory['content_digest']:\n\t\t\t# add a note about the different digest\n\t\t\tdifferent_digests.append({'version': inventory['version'], 'content_digest': content_digest})\n\n\t# validate the different digests\n\tif len(different_digests) > 0:\n\t\tself.fail('different_digest', different_digests)\n\n\t# validate the order\n\tif version_dirs != self.root_inventory['version_dirs']:\n\t\tself.fail('version_order',"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inv = self.get_root_inventory()\n\troot_content_digests = set(d['digest'] for d in root_inv['content'])\n\tversion_content_digests = []\n\n\t# check that each version directory has an inventory\n\tfor version_dir in version_dirs:\n\t\tversion_inv = self.get_inventory_for_version(version_dir)\n\t\tversion_content_digests.extend(d['digest'] for d in version_inv['content'])\n\t\t# check that each version directory has a content digest\n\t\tif not version_inv['content']:\n\t\t\traise ValidationError(\"Version %s has no content digests\" % version_dir)\n\t\t# check that each version directory has a version digest\n\t\tif not version_inv['version']:\n\t\t\traise ValidationError(\"Version %s has no version digest\" % version_dir)\n\n\t# check that each version directory has a content digest that is different from\n\t# the root inventory\n\tif root_content_digests & set(version_content_digests):\n\t\traise ValidationError(\"Version content digests duplicate those in root inventory\")\n\n\t# check that each version directory has a version digest that is different from", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tcontent_digest_diffs = []\n\n\t# Use the first inventory as the root inventory\n\troot_inventory = self.inventories[0]\n\tcontent_digests = self.get_content_digests(root_inventory)\n\n\t# Validate that each inventory has a content digest\n\tfor version_dir in version_dirs:\n\t\tinventory = self.inventories[version_dir]\n\t\tif not inventory.content_digest:\n\t\t\traise ValidationError('Each inventory MUST have a content digest')\n\n\t\t# Validate that the digest is consistent with root inventory\n\t\tif inventory.content_digest not in content_digests:\n\t\t\tcontent_digest_diffs.append(inventory.content_digest)\n\t\t\tcontinue\n\n\t\t# Validate that the inventory has the same set of files as root\n\t\tinventory_files = self.get_files_from_inventory(inventory)\n\t\tfor f in inventory_files:\n\t\t\tif f not in self.get_files_from_inventory(root_inventory):\n\t\t\t\traise ValidationError(f'inventory {version_dir} has unknown '\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tif len(version_dirs) == 0:\n\t\traise Exception(\"No version dirs found\")\n\n\troot_inv_path = self.root_inv_path\n\troot_inv = self.root_inv\n\n\tfor version_dir in version_dirs:\n\t\tversion_inv_path = self.version_inv_path(version_dir)\n\t\ttry:\n\t\t\tversion_inv = Inventory(version_inv_path)\n\t\texcept Exception as e:\n\t\t\traise Exception(\"Version inventory %s is not valid: %s\" % (version_inv_path, str(e)))\n\n\t\tself.validate_version_inventory(version_inv_path, version_inv, root_inv_path, root_inv)\n\n\t\t# Keep a record of any content digests different from those in the root inventory\n\t\tversion_digests = version_inv.get_all_digests()\n\t\troot_digests = root_inv.get_all_digests()\n\t\tfor digest in version_digests:\n\t\t\tif digest not in root_digests:\n\t\t\t\tself.version_digests_not_in_root.add(digest)\n\n\t\t\telif version_digests[digest", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_path = os.path.join(self.root_dir, version_dir, INVENTORY_NAME)\n\t\tversion_inventory = Inventory(version_inventory_path)\n\t\tif not os.path.exists(version_inventory.path):\n\t\t\traise Exception(\"Version inventory not found at \" + version_inventory_path)\n\t\tif not version_inventory.validate(self.root_dir, self.root_inventory):\n\t\t\traise Exception(\"Version inventory validation failed for \" + version_inventory_path)\n\n\t\t# check the version inventory against the root inventory\n\t\tif self.root_inventory.compare(version_inventory):\n\t\t\traise Exception(\"Version inventory does not match root inventory: \" + version_inventory_path)\n\n\t\t# check version inventory against previous inventories\n\t\tfor previous_inventory in self.inventories:\n\t\t\tif previous_inventory.compare(version_inventory):\n\t\t\t\traise Exception(\"Version inventory does not match previous inventory: \" + version_inventory_path)\n\n\t\tself.inventories.append(version_inventory)\n\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# This is a list of version directory names, which are also the keys of the\n\t# inventory (as inventory.json), and are assumed to be in version sequence.\n\tfor i in range(len(version_dirs)):\n\t\tversion_dir = version_dirs[i]\n\t\t# Get the inventory for this version directory\n\t\tif not self.inventories.has_key(version_dir):\n\t\t\traise ValidationError('No inventory found for version %s' % version_dir)\n\t\t# If this is the first inventory, then it should be the inventory of the root\n\t\t# directory.\n\t\tif i == 0:\n\t\t\tif self.inventories[version_dir]['directory'] != self.root_dir:\n\t\t\t\traise ValidationError('Expected inventory for version %s to be in root directory, but it was in %s' % version_dir, self.inventories[version_dir]['directory'])\n\t\telse:\n\t\t\t# If this is not the first inventory, then it should be the inventory of the\n\t\t\t# version directory above it.\n\t\t\texpected_parent = version_dirs[i-1]\n\t\t\tif self.inventories[version_dir]['directory", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inventory = self.validate_root_inventory()\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.validate_version_inventory(version_dir, root_inventory)\n\t\tself.validate_version_content(version_dir, version_inventory)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tversion_path = os.path.join(self.root_dir, version_dir)\n\t\tinventory_path = os.path.join(version_path, \"inventory.json\")\n\t\t# The inventory must exist.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s has no inventory.json\" % version_dir)\n\t\t# The inventory must have the right format.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json\" % version_dir)\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json\" % version_dir)\n\t\t# The inventory must have the right format.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json\" % version_dir)\n\t\t# The inventory must have the right format.\n\t\tself.assertTrue(os.path.exists(inventory_path),\n\t\t\t\t\"version %s inventory is not valid json", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tlogger.debug(\"Validating version inventories\")\n\troot_inventory = self.get_root_inventory()\n\troot_digests = []\n\troot_digests_by_filename = {}\n\tif root_inventory is not None:\n\t\troot_digests = root_inventory.content_digests.digests\n\t\troot_digests_by_filename = root_inventory.content_digests.digests_by_filename\n\tversion_inventories = []\n\tdigests_by_filename = {}\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.get_version_inventory(version_dir)\n\t\tversion_digests = []\n\t\tif version_inventory is not None:\n\t\t\tversion_digests = version_inventory.content_digests.digests\n\t\t\tif version_dir == version_dirs[0]:\n\t\t\t\tversion_digests = root_digests\n\t\t\t\tdigests_by_filename = root_digests_by_filename\n\t\t\telse:\n\t\t\t\tdigests_by_filename = version_inventory.content_digests.digests_by_filename\n\t\tversion_inventories.append(version_inventory)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.assertEqual(len(version_dirs), len(self.root_inventory.data['versions']),\n\t\t\"Number of version directories should be same as versions in root inventory\")\n\tversion_inventories = []\n\tcontent_digests = []\n\tfor i in range(len(version_dirs)):\n\t\tversion_dir = version_dirs[i]\n\t\tif self.check_version_exists(version_dir):\n\t\t\tversion_inventories.append(self.get_version_inventory(version_dir))\n\t\t\tcontent_digests.append(version_inventories[i].content_digests)\n\t\telse:\n\t\t\tversion_inventories.append(None)\n\t\t\tcontent_digests.append([])\n\n\t# make sure there is an inventory for each version\n\tfor i in range(len(version_inventories)):\n\t\tself.assertIsNotNone(version_inventories[i],\n\t\t\t\"Version {} does not have an inventory\".format(version_dirs[i]))\n\t\t# check that we have a root inventory for each version\n\t\tself.assertIn(version_dirs[i], version_inventories[i].data['versions'],\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_files = []\n\tfor version in version_dirs:\n\t\tinventory_files.append(os.path.join(self.root_dir, version, 'inventory.json'))\n\n\t# keep track of any inventory content digests that are different from the\n\t# root inventory\n\tdifferent_digests = []\n\n\t# validate all the inventories\n\tfor inventory_file in inventory_files:\n\t\tinventory = self.load_inventory(inventory_file)\n\t\tself.validate_inventory(inventory, inventory_file)\n\t\tcontent_digest = inventory['content_digest']\n\t\tif content_digest != self.root_inventory['content_digest']:\n\t\t\t# add a note about the different digest\n\t\t\tdifferent_digests.append({'version': inventory['version'], 'content_digest': content_digest})\n\n\t# validate the different digests\n\tif len(different_digests) > 0:\n\t\tself.fail('different_digest', different_digests)\n\n\t# validate the order\n\tif version_dirs != self.root_inventory['version_dirs']:\n\t\tself.fail('version_order',", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tstatbuf = os.lstat(path)\n\tif stat.S_ISDIR(statbuf.st_mode):\n\t\treturn \"dir\"\n\telif stat.S_ISLNK(statbuf.st_mode):\n\t\treturn \"symlink\"\n\telif stat.S_ISCHR(statbuf.st_mode):\n\t\treturn \"char\"\n\telif stat.S_ISBLK(statbuf.st_mode):\n\t\treturn \"block\"\n\telif stat.S_ISREG(statbuf.st_mode):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"unknown\"\n\nclass Path(object):\n\t\"\"\"\n\tAn abstraction for a path.  Paths can be either files or directories.\n\n\tPaths can be absolute or relative, and can be either symlinks or real\n\tfiles.  Paths can also be \"fake\" -- that is, they exist only in the\n\tvirtual filesystem, and do not actually exist on disk.\n\t\"\"\"\n\n\tdef __init__(self, path, fake=False):\n\t\t\"\"\"\n\t\tCreate a Path\n\t\t\"\"\"\n\t\tself.path = path\n\t\tself.fake = fake\n\t\tself.type = find_path_type(path", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn \"Directory\"\n\telif os.path.isfile(path):\n\t\treturn \"File\"\n\telif os.path.islink(path):\n\t\treturn \"Link\"\n\telse:\n\t\treturn \"Unknown\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telse:\n\t\treturn \"file\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.isdir(path):\n\t\treturn \"dir\"\n\telse:\n\t\treturn \"unknown\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif (path[-1] != \"/\"):\n\t\tpath += \"/\"\n\n\ttry:\n\t\treturn os.path.isdir(path)\n\texcept OSError:\n\t\treturn False\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn 'dir'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telse:\n\t\treturn 'other'\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\tif os.path.islink(path):\n\t\treturn \"symlink\"\n\treturn \"unknown\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif not path.exists():\n\t\treturn None\n\telif path.is_dir():\n\t\treturn \"dir\"\n\telif path.is_symlink():\n\t\treturn \"symlink\"\n\telif path.is_file():\n\t\treturn \"file\"\n\telse:\n\t\treturn None\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telse:\n\t\treturn 'unknown'\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tis_directory = os.path.isdir(path)\n\tif is_directory:\n\t\treturn \"dir\"\n\telse:\n\t\tis_file = os.path.isfile(path)\n\t\tif is_file:\n\t\t\treturn \"file\"\n\t\telse:\n\t\t\tis_symlink = os.path.islink(path)\n\t\t\tif is_symlink:\n\t\t\t\treturn \"symlink\"\n\t\t\telse:\n\t\t\t\treturn \"unknown\"\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tstatbuf = os.lstat(path)\n\tif stat.S_ISDIR(statbuf.st_mode):\n\t\treturn \"dir\"\n\telif stat.S_ISLNK(statbuf.st_mode):\n\t\treturn \"symlink\"\n\telif stat.S_ISCHR(statbuf.st_mode):\n\t\treturn \"char\"\n\telif stat.S_ISBLK(statbuf.st_mode):\n\t\treturn \"block\"\n\telif stat.S_ISREG(statbuf.st_mode):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"unknown\"\n\nclass Path(object):\n\t\"\"\"\n\tAn abstraction for a path.  Paths can be either files or directories.\n\n\tPaths can be absolute or relative, and can be either symlinks or real\n\tfiles.  Paths can also be \"fake\" -- that is, they exist only in the\n\tvirtual filesystem, and do not actually exist on disk.\n\t\"\"\"\n\n\tdef __init__(self, path, fake=False):\n\t\t\"\"\"\n\t\tCreate a Path\n\t\t\"\"\"\n\t\tself.path = path\n\t\tself.fake = fake\n\t\tself.type = find_path_type(path", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn \"Directory\"\n\telif os.path.isfile(path):\n\t\treturn \"File\"\n\telif os.path.islink(path):\n\t\treturn \"Link\"\n\telse:\n\t\treturn \"Unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telse:\n\t\treturn \"file\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.isdir(path):\n\t\treturn \"dir\"\n\telse:\n\t\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif (path[-1] != \"/\"):\n\t\tpath += \"/\"\n\n\ttry:\n\t\treturn os.path.isdir(path)\n\texcept OSError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn 'dir'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telse:\n\t\treturn 'other'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\tif os.path.islink(path):\n\t\treturn \"symlink\"\n\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif not path.exists():\n\t\treturn None\n\telif path.is_dir():\n\t\treturn \"dir\"\n\telif path.is_symlink():\n\t\treturn \"symlink\"\n\telif path.is_file():\n\t\treturn \"file\"\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telse:\n\t\treturn 'unknown'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tis_directory = os.path.isdir(path)\n\tif is_directory:\n\t\treturn \"dir\"\n\telse:\n\t\tis_file = os.path.isfile(path)\n\t\tif is_file:\n\t\t\treturn \"file\"\n\t\telse:\n\t\t\tis_symlink = os.path.islink(path)\n\t\t\tif is_symlink:\n\t\t\t\treturn \"symlink\"\n\t\t\telse:\n\t\t\t\treturn \"unknown\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.log.info(\"Amend the Bugzilla params\")\n\tfor bug_id in bug_ids:\n\t\ttry:\n\t\t\tbug = self.client.getbug(bug_id)\n\t\texcept Exception as e:\n\t\t\tself.log.error(\"Error getting bug {0} : {1}\".format(bug_id, e))\n\t\t\traise\n\t\tself.log.debug(\"Getting bug {0}\".format(bug))\n\t\tparams['bug_id'] = bug_id\n\t\tparams['bug_id'] = bug_id\n\t\tparams['product'] = bug.product\n\t\tparams['component'] = bug.component\n\t\tparams['version'] = bug.version\n\t\tparams['assigned_to'] = bug.assigned_to\n\t\tparams['summary'] = bug.summary\n\t\tparams['assigned_to'] = bug.assigned_to\n\t\tparams['priority'] = bug.priority\n\t\tparams['status'] = bug.status\n\t\tparams['resolution'] = bug.resolution\n\t\tparams['summary'] = bug.summary\n\t\tparams['op_sys'] = bug.op_sys\n\t\tparams['platform'] = bug.platform\n\t\tparams['keywords'] = bug.keywords", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# This is the default bugzilla parameters\n\tparams['bug_ids'] = bug_ids\n\tparams['include_fields'] = self.config['include_fields']\n\tparams['include_fields'] += self.config['bug_specific_fields']\n\tparams['include_fields'] = list(set(params['include_fields']))\n\tparams['include_fields'].sort()\n\tparams['include_resolution'] = self.config['include_resolution']\n\tparams['include_status'] = self.config['include_status']\n\tparams['include_priority'] = self.config['include_priority']\n\tparams['include_severity'] = self.config['include_severity']\n\tparams['include_component'] = self.config['include_component']\n\tparams['include_version'] = self.config['include_version']\n\tparams['include_flags'] = self.config['include_flags']\n\tparams['include_assigned_to'] = self.config['include_assigned_to']\n\tparams['include_cc'] = self.config['include_cc']\n\tparams['include_target_milestone'] = self.config['include_target_milestone']\n\tparams['include_product'] = self.config['include_product']\n\tparams['include_product", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in bug_ids:\n\t\tparams[\"ids\"] = bug_id\n\t\tparams[\"include_fields\"] = [\"assigned_to\", \"product\"]\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['f__bug_id'] = []\n\tfor id in bug_ids:\n\t\tparams['f__bug_id'].append(id)\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# Set the list of affected bugs\n\tif bug_ids:\n\t\tparams['bug_id'] = bug_ids\n\t\tparams['product'] = 'all'\n\n\t# Set the list of components\n\tparams['component'] = self.get_components()\n\n\t# Set the list of fields\n\tparams['field'] = self.get_fields()\n\n\t# Set the list of key words\n\tparams['keywords'] = self.get_keywords()\n\n\t# Set the list of platforms\n\tparams['platform'] = self.get_platforms()\n\n\t# Set the list of priorities\n\tparams['priority'] = self.get_priorities()\n\n\t# Set the list of products\n\tparams['product'] = self.get_products()\n\n\t# Set the list of resolutions\n\tparams['resolution'] = self.get_resolutions()\n\n\t# Set the list of severities\n\tparams['severity'] = self.get_severities()\n\n\t# Set the list of statuses\n\tparams['status'] = self.get_statuses()\n\n\t# Set the list of versions\n\tparams['version'] = self.get_versions()\n\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams[\"ids\"] = \",\".join(str(i) for i in bug_ids)\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# Set the bug ids\n\tparams['ids'] = bug_ids\n\n\t# Set the query\n\tparams['query'] = 'id={}'.format('|'.join(bug_ids))\n\n\t# Remove the email template\n\tif 'template' in params:\n\t\tdel params['template']\n\n\t# Set the email template\n\tparams['template'] = 'xmlrpc.email.bug'\n\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['include_fields'] = 'id,status,product'\n\tif bug_ids:\n\t\tparams['idlist'] = ','.join(map(str, bug_ids))\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif len(bug_ids) > 0:\n\t\tparams['bug_id'] = ','.join(str(x) for x in bug_ids)\n\t\tparams['bug_status'] = 'NEW,ASSIGNED,REOPENED,ON_DEV,VERIFIED,CLOSED,RELEASE_PENDING,IN_QA,IN_PROGRESS'\n\t\tparams['include_fields'] = 'summary,status,whiteboard,assigned_to,priority,priority_whiteboard,blocks,depends_on,resolution,resolution_whiteboard,severity,keywords,reporter,creation_time,last_change_time,last_change_time,flags,component,version,platform,product,op_sys,target_milestone,cf_private_status,cf_private_whiteboard,cf_private_whiteboard_updated,cf_private_last_change_time,cf_private_last_change_time_updated'\n\t\tparams['order'] = 'creation_time'\n\t\tparams['f1'] = 'creation_time'\n\t\tparams['o1'] = 'DESC'\n\t\tparams['f2'] = 'status'\n\t\tparams['o2'] = 'DESC'\n\t\tparams"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.log.info(\"Amend the Bugzilla params\")\n\tfor bug_id in bug_ids:\n\t\ttry:\n\t\t\tbug = self.client.getbug(bug_id)\n\t\texcept Exception as e:\n\t\t\tself.log.error(\"Error getting bug {0} : {1}\".format(bug_id, e))\n\t\t\traise\n\t\tself.log.debug(\"Getting bug {0}\".format(bug))\n\t\tparams['bug_id'] = bug_id\n\t\tparams['bug_id'] = bug_id\n\t\tparams['product'] = bug.product\n\t\tparams['component'] = bug.component\n\t\tparams['version'] = bug.version\n\t\tparams['assigned_to'] = bug.assigned_to\n\t\tparams['summary'] = bug.summary\n\t\tparams['assigned_to'] = bug.assigned_to\n\t\tparams['priority'] = bug.priority\n\t\tparams['status'] = bug.status\n\t\tparams['resolution'] = bug.resolution\n\t\tparams['summary'] = bug.summary\n\t\tparams['op_sys'] = bug.op_sys\n\t\tparams['platform'] = bug.platform\n\t\tparams['keywords'] = bug.keywords", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# This is the default bugzilla parameters\n\tparams['bug_ids'] = bug_ids\n\tparams['include_fields'] = self.config['include_fields']\n\tparams['include_fields'] += self.config['bug_specific_fields']\n\tparams['include_fields'] = list(set(params['include_fields']))\n\tparams['include_fields'].sort()\n\tparams['include_resolution'] = self.config['include_resolution']\n\tparams['include_status'] = self.config['include_status']\n\tparams['include_priority'] = self.config['include_priority']\n\tparams['include_severity'] = self.config['include_severity']\n\tparams['include_component'] = self.config['include_component']\n\tparams['include_version'] = self.config['include_version']\n\tparams['include_flags'] = self.config['include_flags']\n\tparams['include_assigned_to'] = self.config['include_assigned_to']\n\tparams['include_cc'] = self.config['include_cc']\n\tparams['include_target_milestone'] = self.config['include_target_milestone']\n\tparams['include_product'] = self.config['include_product']\n\tparams['include_product", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in bug_ids:\n\t\tparams[\"ids\"] = bug_id\n\t\tparams[\"include_fields\"] = [\"assigned_to\", \"product\"]\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['f__bug_id'] = []\n\tfor id in bug_ids:\n\t\tparams['f__bug_id'].append(id)\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# Set the list of affected bugs\n\tif bug_ids:\n\t\tparams['bug_id'] = bug_ids\n\t\tparams['product'] = 'all'\n\n\t# Set the list of components\n\tparams['component'] = self.get_components()\n\n\t# Set the list of fields\n\tparams['field'] = self.get_fields()\n\n\t# Set the list of key words\n\tparams['keywords'] = self.get_keywords()\n\n\t# Set the list of platforms\n\tparams['platform'] = self.get_platforms()\n\n\t# Set the list of priorities\n\tparams['priority'] = self.get_priorities()\n\n\t# Set the list of products\n\tparams['product'] = self.get_products()\n\n\t# Set the list of resolutions\n\tparams['resolution'] = self.get_resolutions()\n\n\t# Set the list of severities\n\tparams['severity'] = self.get_severities()\n\n\t# Set the list of statuses\n\tparams['status'] = self.get_statuses()\n\n\t# Set the list of versions\n\tparams['version'] = self.get_versions()\n\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams[\"ids\"] = \",\".join(str(i) for i in bug_ids)\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_id'] = bug_ids\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t# Set the bug ids\n\tparams['ids'] = bug_ids\n\n\t# Set the query\n\tparams['query'] = 'id={}'.format('|'.join(bug_ids))\n\n\t# Remove the email template\n\tif 'template' in params:\n\t\tdel params['template']\n\n\t# Set the email template\n\tparams['template'] = 'xmlrpc.email.bug'\n\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['include_fields'] = 'id,status,product'\n\tif bug_ids:\n\t\tparams['idlist'] = ','.join(map(str, bug_ids))\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif len(bug_ids) > 0:\n\t\tparams['bug_id'] = ','.join(str(x) for x in bug_ids)\n\t\tparams['bug_status'] = 'NEW,ASSIGNED,REOPENED,ON_DEV,VERIFIED,CLOSED,RELEASE_PENDING,IN_QA,IN_PROGRESS'\n\t\tparams['include_fields'] = 'summary,status,whiteboard,assigned_to,priority,priority_whiteboard,blocks,depends_on,resolution,resolution_whiteboard,severity,keywords,reporter,creation_time,last_change_time,last_change_time,flags,component,version,platform,product,op_sys,target_milestone,cf_private_status,cf_private_whiteboard,cf_private_whiteboard_updated,cf_private_last_change_time,cf_private_last_change_time_updated'\n\t\tparams['order'] = 'creation_time'\n\t\tparams['f1'] = 'creation_time'\n\t\tparams['o1'] = 'DESC'\n\t\tparams['f2'] = 'status'\n\t\tparams['o2'] = 'DESC'\n\t\tparams", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor k, v in node.items():\n\t\t\tif isinstance(v, MappingNode):\n\t\t\t\tif k in result:\n\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = v\n\t\t\telif k in result:\n\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\tresult[k].update(v)\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tdef _deep_merge_nodes(nodes):\n\t\tresult = nodes[0]\n\t\tfor i in range(1, len(nodes)):\n\t\t\tresult = result.deep_merge(nodes[i])\n\t\treturn result\n\t\n\treturn _deep_merge_nodes(nodes)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tres = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in res:\n\t\t\t\tres[key] = value\n\t\t\telif isinstance(res[key], MappingNode):\n\t\t\t\tres[key] = deep_merge_nodes([res[key], value])\n\t\t\telif isinstance(value, MappingNode):\n\t\t\t\tres[key] = deep_merge_nodes([value, res[key]])\n\t\t\telse:\n\t\t\t\tres[key] = value\n\treturn MappingNode(res)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tmerged_nodes.update(node.value)\n\treturn MappingNode(deep_merge_nodes(list(v.value.values()) for v in nodes))\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in merged:\n\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([value, merged[key]])\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\treturn MappingNode(merged)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor k,v in node.items():\n\t\t\tif k not in merged:\n\t\t\t\tmerged[k] = v\n\t\t\telif isinstance(v, dict) and isinstance(merged[k], dict):\n\t\t\t\tmerged[k] = deep_merge_nodes([merged[k],v])\n\treturn merged\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = value\n\t\t\telif isinstance(value, MappingNode):\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, dict):\n\t\t\tcontinue\n\t\tfor key, value in node.items():\n\t\t\tif key in merged:\n\t\t\t\tif isinstance(merged[key], dict) and isinstance(value, dict):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn MappingNode(tag=None, value=merged, flow_style=None)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key in node:\n\t\t\tif key in merged:\n\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], node[key]])\n\t\t\telse:\n\t\t\t\tmerged[key] = node[key]\n\treturn merged\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes:\n\t\t\t\tmerged_nodes[key] = deep_merge(merged_nodes[key], value)\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\treturn merged_nodes\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor k, v in node.items():\n\t\t\tif isinstance(v, MappingNode):\n\t\t\t\tif k in result:\n\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = v\n\t\t\telif k in result:\n\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\tresult[k].update(v)\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tdef _deep_merge_nodes(nodes):\n\t\tresult = nodes[0]\n\t\tfor i in range(1, len(nodes)):\n\t\t\tresult = result.deep_merge(nodes[i])\n\t\treturn result\n\t\n\treturn _deep_merge_nodes(nodes)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tres = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in res:\n\t\t\t\tres[key] = value\n\t\t\telif isinstance(res[key], MappingNode):\n\t\t\t\tres[key] = deep_merge_nodes([res[key], value])\n\t\t\telif isinstance(value, MappingNode):\n\t\t\t\tres[key] = deep_merge_nodes([value, res[key]])\n\t\t\telse:\n\t\t\t\tres[key] = value\n\treturn MappingNode(res)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tmerged_nodes.update(node.value)\n\treturn MappingNode(deep_merge_nodes(list(v.value.values()) for v in nodes))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in merged:\n\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([value, merged[key]])\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\treturn MappingNode(merged)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor k,v in node.items():\n\t\t\tif k not in merged:\n\t\t\t\tmerged[k] = v\n\t\t\telif isinstance(v, dict) and isinstance(merged[k], dict):\n\t\t\t\tmerged[k] = deep_merge_nodes([merged[k],v])\n\treturn merged\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = value\n\t\t\telif isinstance(value, MappingNode):\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, dict):\n\t\t\tcontinue\n\t\tfor key, value in node.items():\n\t\t\tif key in merged:\n\t\t\t\tif isinstance(merged[key], dict) and isinstance(value, dict):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn MappingNode(tag=None, value=merged, flow_style=None)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key in node:\n\t\t\tif key in merged:\n\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], node[key]])\n\t\t\telse:\n\t\t\t\tmerged[key] = node[key]\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes:\n\t\t\t\tmerged_nodes[key] = deep_merge(merged_nodes[key], value)\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument('--url', dest='url', required=True, help='URL of the repository to be cloned')\n\tparser.add_argument('--name', dest='name', required=True, help='Name of the repository to be cloned')\n\tparser.add_argument('--path', dest='path', required=True, help='Path to clone repository to')\n\tparser.add_argument('--branch', dest='branch', required=True, help='Branch to clone')\n\tparser.add_argument('--email', dest='email', required=True, help='Email to use for Git')\n\tparser.add_argument('--username', dest='username', required=True, help='Username to use for Git')\n\tparser.add_argument('--password', dest='password', required=True, help='Password to use for Git')\n\tparser.add_argument('--ssh', dest='ssh', required=False, help='Path to SSH key to use for Git')\n\tparser.add_argument('--ssh_passphrase', dest='ssh_passphrase', required=False, help='Path to SSH passphrase to use for Git')\n\tparser.add_argument('--", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('--input', '-i', required=True, type=str, help='Input image')\n\tparser.add_argument('--output', '-o', type=str, help='Output image', default=None)\n\tparser.add_argument('--mode', '-m', choices=['edge', 'laplacian', 'sobel'], default='edge', type=str, required=False, help='Type of filter')\n\tparser.add_argument('--threshold', '-t', type=int, default=128, required=False, help='Threshold value for the filter')\n\tparser.add_argument('--step', '-s', type=int, default=1, required=False, help='Step of the filter')\n\tparser.add_argument('--output-type', '-ot', choices=['png', 'jpg', 'bmp', 'tif', 'tiff'], default='png', required=False, help='Output file type')\n\tparser.add_argument('--output-path', '-op', type=str, required=False, help='Output file path')\n\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')\n\tparser.add_argument('--debug', '-d', action='store_true', help='Debug output')\n\tparser.add_argument('--quiet', '-q', action='store_true', help='Quiet output')\n\tparser.add_argument('--version', '-V', action='version', version=VERSION)\n\tparser.add_argument('--log-dir', '-l', default=None, help='Log directory')\n\tparser.add_argument('--log-prefix', '-p', default=None, help='Log file prefix')\n\tparser.add_argument('--log-suffix', '-s', default=None, help='Log file suffix')\n\tparser.add_argument('--log-file', '-f', default=None, help='Log file')\n\tparser.add_argument('--log-format', '-F', default=None, help='Log format')\n\tparser.add_argument('--log-level', '-L', default=None, help='Log level')\n\tparser.add_argument('--log-datefmt', '-D', default=None, help='Log date format')\n\tparser.add_argument('--", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--config', help='Configuration file.')\n\tparser.add_argument('--debug', help='Enable debug mode', action='store_true')\n\tparser.add_argument('--version', help='Print version and exit.', action='version', version=VERSION)\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Parse the user-specified arguments\")\n\tparser.add_argument(\"--input\", help=\"Specify the input file\", required=True)\n\tparser.add_argument(\"--out\", help=\"Specify the output file\", required=True)\n\tparser.add_argument(\"--verbose\", help=\"Specify the verbosity level\", required=True, choices=[\"DEBUG\", \"INFO\", \"WARN\", \"ERROR\"])\n\targs = parser.parse_args()\n\treturn args\n\n\nif __name__ == \"__main__\":\n\targuments = parse_arguments()\n\tprint(arguments)\n\n\timport logging\n\tlogger = logging.getLogger('example')\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler = logging.StreamHandler()\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\tif arguments.verbose == \"DEBUG\":\n\t\tlogger.setLevel(logging.DEBUG)\n\telif arguments.verbose == \"INFO\":\n\t\tlogger.setLevel(logging.INFO)\n\telif arguments.verbose == \"WARN", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = ArgumentParser(description=__doc__)\n\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file (default: stdout)\", type=str, default=None)\n\tparser.add_argument(\"-d\", \"--debug\", help=\"Debug mode\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--log\", help=\"Log file (default: stderr)\", type=str, default=None)\n\n\tparser.add_argument(\"--version\", help=\"Display version information and exit\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--profile\", help=\"Profile script\", action=\"store_true\", default=False)\n\n\t# parser.add_argument(\"--list-modules\", help=\"List modules\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-inputs\", help=\"List inputs\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-outputs\", help=\"List outputs\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-commands\", help=\"List commands\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-inputs\", help=\"List inputs\", action", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--input_file\", \"-i\", type=str, help=\"Path to the text file\", required=True)\n\tparser.add_argument(\"--output_file\", \"-o\", type=str, help=\"Path to the output file\", required=True)\n\tparser.add_argument(\"--word_to_find\", \"-w\", type=str, help=\"Word to find\", required=True)\n\tparser.add_argument(\"--word_to_replace\", \"-r\", type=str, help=\"Word to replace\", required=True)\n\tparser.add_argument(\"--case\", \"-c\", type=str, help=\"Case of the word to find. Possible values: upper, lower, title\", required=True)\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Analyze and plot the results of the 1D and 2D simulations')\n\tparser.add_argument(\"-f\", \"--files\",\n\t\t\t\t\t\thelp=\"the path to the directory containing the files to analyze and plot. If not provided, the script will look for the files in the current directory\",\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault=os.getcwd())\n\n\tparser.add_argument(\"-l\", \"--length\",\n\t\t\t\t\t\thelp=\"the length of the chain to analyze\",\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=10000)\n\n\tparser.add_argument(\"-N\", \"--step\",\n\t\t\t\t\t\thelp=\"the number of steps to take at each iteration\",\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=10000)\n\n\tparser.add_argument(\"-d\", \"--directory\",\n\t\t\t\t\t\thelp=\"the path to the directory where the plots will be saved. If not provided, the plots will be saved in the current directory\",\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault=os.", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('--log_level', default='info', help='Log level (default is info)')\n\n\tparser.add_argument('--log_file', default='', help='Log file (default is stdout)')\n\n\tparser.add_argument('--log_format', default='%(asctime)s - %(name)s - %(levelname)s - %(message)s', help='Log format')\n\n\tparser.add_argument('--debug', action='store_true', help='Enable debug mode')\n\n\tparser.add_argument('--verbose', action='store_true', help='Enable verbose mode')\n\n\tparser.add_argument('--quiet', action='store_true', help='Enable quiet mode (logging disabled)')\n\n\tparser.add_argument('--version', action='store_true', help='Print version and exit')\n\n\tparser.add_argument('--help', action='store_true', help='Print this help and exit')\n\n\tparser.add_argument('--config', default='', help='Configuration file')\n\n\tparser.add_argument('--section', default='', help='Configuration file section')\n\n\tparser.add_argument('--ignore_config_errors', action='store", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='This is a script that can be used to parse command line arguments')\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument('--url', dest='url', required=True, help='URL of the repository to be cloned')\n\tparser.add_argument('--name', dest='name', required=True, help='Name of the repository to be cloned')\n\tparser.add_argument('--path', dest='path', required=True, help='Path to clone repository to')\n\tparser.add_argument('--branch', dest='branch', required=True, help='Branch to clone')\n\tparser.add_argument('--email', dest='email', required=True, help='Email to use for Git')\n\tparser.add_argument('--username', dest='username', required=True, help='Username to use for Git')\n\tparser.add_argument('--password', dest='password', required=True, help='Password to use for Git')\n\tparser.add_argument('--ssh', dest='ssh', required=False, help='Path to SSH key to use for Git')\n\tparser.add_argument('--ssh_passphrase', dest='ssh_passphrase', required=False, help='Path to SSH passphrase to use for Git')\n\tparser.add_argument('--", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('--input', '-i', required=True, type=str, help='Input image')\n\tparser.add_argument('--output', '-o', type=str, help='Output image', default=None)\n\tparser.add_argument('--mode', '-m', choices=['edge', 'laplacian', 'sobel'], default='edge', type=str, required=False, help='Type of filter')\n\tparser.add_argument('--threshold', '-t', type=int, default=128, required=False, help='Threshold value for the filter')\n\tparser.add_argument('--step', '-s', type=int, default=1, required=False, help='Step of the filter')\n\tparser.add_argument('--output-type', '-ot', choices=['png', 'jpg', 'bmp', 'tif', 'tiff'], default='png', required=False, help='Output file type')\n\tparser.add_argument('--output-path', '-op', type=str, required=False, help='Output file path')\n\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')\n\tparser.add_argument('--debug', '-d', action='store_true', help='Debug output')\n\tparser.add_argument('--quiet', '-q', action='store_true', help='Quiet output')\n\tparser.add_argument('--version', '-V', action='version', version=VERSION)\n\tparser.add_argument('--log-dir', '-l', default=None, help='Log directory')\n\tparser.add_argument('--log-prefix', '-p', default=None, help='Log file prefix')\n\tparser.add_argument('--log-suffix', '-s', default=None, help='Log file suffix')\n\tparser.add_argument('--log-file', '-f', default=None, help='Log file')\n\tparser.add_argument('--log-format', '-F', default=None, help='Log format')\n\tparser.add_argument('--log-level', '-L', default=None, help='Log level')\n\tparser.add_argument('--log-datefmt', '-D', default=None, help='Log date format')\n\tparser.add_argument('--", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--config', help='Configuration file.')\n\tparser.add_argument('--debug', help='Enable debug mode', action='store_true')\n\tparser.add_argument('--version', help='Print version and exit.', action='version', version=VERSION)\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Parse the user-specified arguments\")\n\tparser.add_argument(\"--input\", help=\"Specify the input file\", required=True)\n\tparser.add_argument(\"--out\", help=\"Specify the output file\", required=True)\n\tparser.add_argument(\"--verbose\", help=\"Specify the verbosity level\", required=True, choices=[\"DEBUG\", \"INFO\", \"WARN\", \"ERROR\"])\n\targs = parser.parse_args()\n\treturn args\n\n\nif __name__ == \"__main__\":\n\targuments = parse_arguments()\n\tprint(arguments)\n\n\timport logging\n\tlogger = logging.getLogger('example')\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler = logging.StreamHandler()\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\tif arguments.verbose == \"DEBUG\":\n\t\tlogger.setLevel(logging.DEBUG)\n\telif arguments.verbose == \"INFO\":\n\t\tlogger.setLevel(logging.INFO)\n\telif arguments.verbose == \"WARN", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = ArgumentParser(description=__doc__)\n\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file (default: stdout)\", type=str, default=None)\n\tparser.add_argument(\"-d\", \"--debug\", help=\"Debug mode\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--log\", help=\"Log file (default: stderr)\", type=str, default=None)\n\n\tparser.add_argument(\"--version\", help=\"Display version information and exit\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--profile\", help=\"Profile script\", action=\"store_true\", default=False)\n\n\t# parser.add_argument(\"--list-modules\", help=\"List modules\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-inputs\", help=\"List inputs\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-outputs\", help=\"List outputs\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-commands\", help=\"List commands\", action=\"store_true\", default=False)\n\t# parser.add_argument(\"--list-inputs\", help=\"List inputs\", action", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--input_file\", \"-i\", type=str, help=\"Path to the text file\", required=True)\n\tparser.add_argument(\"--output_file\", \"-o\", type=str, help=\"Path to the output file\", required=True)\n\tparser.add_argument(\"--word_to_find\", \"-w\", type=str, help=\"Word to find\", required=True)\n\tparser.add_argument(\"--word_to_replace\", \"-r\", type=str, help=\"Word to replace\", required=True)\n\tparser.add_argument(\"--case\", \"-c\", type=str, help=\"Case of the word to find. Possible values: upper, lower, title\", required=True)\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Analyze and plot the results of the 1D and 2D simulations')\n\tparser.add_argument(\"-f\", \"--files\",\n\t\t\t\t\t\thelp=\"the path to the directory containing the files to analyze and plot. If not provided, the script will look for the files in the current directory\",\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault=os.getcwd())\n\n\tparser.add_argument(\"-l\", \"--length\",\n\t\t\t\t\t\thelp=\"the length of the chain to analyze\",\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=10000)\n\n\tparser.add_argument(\"-N\", \"--step\",\n\t\t\t\t\t\thelp=\"the number of steps to take at each iteration\",\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=10000)\n\n\tparser.add_argument(\"-d\", \"--directory\",\n\t\t\t\t\t\thelp=\"the path to the directory where the plots will be saved. If not provided, the plots will be saved in the current directory\",\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault=os.", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('--log_level', default='info', help='Log level (default is info)')\n\n\tparser.add_argument('--log_file', default='', help='Log file (default is stdout)')\n\n\tparser.add_argument('--log_format', default='%(asctime)s - %(name)s - %(levelname)s - %(message)s', help='Log format')\n\n\tparser.add_argument('--debug', action='store_true', help='Enable debug mode')\n\n\tparser.add_argument('--verbose', action='store_true', help='Enable verbose mode')\n\n\tparser.add_argument('--quiet', action='store_true', help='Enable quiet mode (logging disabled)')\n\n\tparser.add_argument('--version', action='store_true', help='Print version and exit')\n\n\tparser.add_argument('--help', action='store_true', help='Print this help and exit')\n\n\tparser.add_argument('--config', default='', help='Configuration file')\n\n\tparser.add_argument('--section', default='', help='Configuration file section')\n\n\tparser.add_argument('--ignore_config_errors', action='store", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='This is a script that can be used to parse command line arguments')\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions.keys())\n\n\n# pylint: disable=too-many-lines\ndef parser_help(parser):\n\t\"\"\"\n\tConstruct a string that contains a help message for the parser and all of its subparsers.\n\t\"\"\"\n\thelp_text = ''\n\n\t# If this is a subparser, we'll need to get the parent parser\n\tif parser._name_parser_map:\n\t\tparent_parser = parser._name_parser_map[parser._prog_name]\n\t\tparent_help_text = parser_help(parent_parser)\n\n\t\tif parent_help_text:\n\t\t\thelp_text += parent_help_text + '\\n'\n\n\t# Parse the parser and its subparsers\n\tfor action in parser._subparsers._actions:\n\t\tif isinstance(action, argparse._SubParsersAction):\n\t\t\tfor subparser in action._name_parser_map.values():\n\t\t\t\thelp_text += _parser_help(subparser)\n\t\t\t\thelp_text += '\\n'\n\t\telse:\n\t\t\thelp_text += _parser_help(action)\n\t\t\thelp_text += '\\n'\n\n\treturn help_", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = ''\n\tfor action in parser._actions:\n\t\toption_string = action.option_strings[0]\n\t\tflags += option_string + ' '\n\treturn flags\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targs = parser._get_args()\n\tflags = ' '.join(arg.option_strings for arg in args if arg.option_strings)\n\treturn flags\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(flag for flag in parser._option_string_actions)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(parser._optionals._option_string_actions.keys())\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(f\"--{flag.replace('_', '-')}\"\n\t                 for flag in parser._option_string_actions)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(parser.get_default_values().keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(f\"--{k}\" for k in parser._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions.keys())\n\n\n# pylint: disable=too-many-lines\ndef parser_help(parser):\n\t\"\"\"\n\tConstruct a string that contains a help message for the parser and all of its subparsers.\n\t\"\"\"\n\thelp_text = ''\n\n\t# If this is a subparser, we'll need to get the parent parser\n\tif parser._name_parser_map:\n\t\tparent_parser = parser._name_parser_map[parser._prog_name]\n\t\tparent_help_text = parser_help(parent_parser)\n\n\t\tif parent_help_text:\n\t\t\thelp_text += parent_help_text + '\\n'\n\n\t# Parse the parser and its subparsers\n\tfor action in parser._subparsers._actions:\n\t\tif isinstance(action, argparse._SubParsersAction):\n\t\t\tfor subparser in action._name_parser_map.values():\n\t\t\t\thelp_text += _parser_help(subparser)\n\t\t\t\thelp_text += '\\n'\n\t\telse:\n\t\t\thelp_text += _parser_help(action)\n\t\t\thelp_text += '\\n'\n\n\treturn help_", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = ''\n\tfor action in parser._actions:\n\t\toption_string = action.option_strings[0]\n\t\tflags += option_string + ' '\n\treturn flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targs = parser._get_args()\n\tflags = ' '.join(arg.option_strings for arg in args if arg.option_strings)\n\treturn flags\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(flag for flag in parser._option_string_actions)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(parser._optionals._option_string_actions.keys())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(f\"--{flag.replace('_', '-')}\"\n\t                 for flag in parser._option_string_actions)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(parser.get_default_values().keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(f\"--{k}\" for k in parser._option_string_actions.keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Docker container to run the NTRIP Client cGnssTimeClient')\n\tparser.add_argument('-n', '--name', help='Name of the container', default='cgnss-time')\n\tparser.add_argument('-e', '--env', help='Environment variables', default='')\n\tparser.add_argument('-v', '--volume', help='Volumes', default='')\n\tparser.add_argument('-p', '--port', help='Ports', default='')\n\tparser.add_argument('-i', '--image', help='Image name', default='cgnss/cgnss-time:latest')\n\tparser.add_argument('-t', '--tag', help='Tag', default='latest')\n\tparser.add_argument('-D', '--debug', help='Enable debug', action='store_true')\n\tparser.add_argument('-s', '--start', help='Start the container', action='store_true')\n\tparser.add_argument('-S', '--stop', help='Stop the container', action='store_true')\n\tparser.add_argument('-r', '--restart', help='Restart the container', action='store_true')\n\t", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Runs simulation on a given set of parameters\")\n\n\tparser.add_argument(\"--simulation\", help=\"The simulation to run\", type=str, required=True)\n\tparser.add_argument(\"--parameter_file\", help=\"The parameter file to use\", type=str, required=True)\n\tparser.add_argument(\"--output_dir\", help=\"Output directory\", type=str, required=True)\n\tparser.add_argument(\"--seed\", help=\"Random seed to use\", type=int, default=None)\n\n\targuments = parser.parse_args(unparsed_arguments)\n\treturn arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Splits a dataset into train, validation and test sets\")\n\tparser.add_argument('--input_dir', type=str, required=True, help='Directory containing the input data')\n\tparser.add_argument('--output_dir', type=str, required=True, help='Directory where to store the generated data')\n\tparser.add_argument('--train_ratio', type=float, required=False, default=0.8, help='Ratio of the training set')\n\tparser.add_argument('--val_ratio', type=float, required=False, default=0.1, help='Ratio of the validation set')\n\tparser.add_argument('--test_ratio', type=float, required=False, default=0.1, help='Ratio of the test set')\n\tparser.add_argument('--shuffle', action='store_true', help='Shuffle the input data')\n\n\targs = parser.parse_args()\n\n\tif args.train_ratio + args.val_ratio + args.test_ratio != 1.0:\n\t\tprint('Error: the sum of the ratios must be 1.0')\n\t\texit()\n\n\treturn vars(args)\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\"--exp_name\", type=str, default=\"\")\n\tparser.add_argument(\"--dataset\", type=str, default=\"\")\n\tparser.add_argument(\"--dataset_dir\", type=str, default=\"\")\n\tparser.add_argument(\"--dataset_mode\", type=str, default=\"\")\n\tparser.add_argument(\"--dataloader_num_workers\", type=int, default=0)\n\tparser.add_argument(\"--batch_size\", type=int, default=1)\n\tparser.add_argument(\"--input_height\", type=int, default=256)\n\tparser.add_argument(\"--input_width\", type=int, default=256)\n\tparser.add_argument(\"--max_dataset_size\", type=int, default=0)\n\tparser.add_argument(\"--num_epochs\", type=int, default=0)\n\tparser.add_argument(\"--num_epochs_decay\", type=int, default=0)\n\n\tparser.add_argument(\"--output_height\", type=int, default=256)\n\tparser.add_argument(\"--output_width\", type=int, default=256)", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='This script is used for running benchmark tests on the different implementations of the GEM algorithm')\n\tparser.add_argument('-d', '--data_path', type=str, help='Path to the data file', required=True)\n\tparser.add_argument('-o', '--output', type=str, help='Path to the output file', required=True)\n\tparser.add_argument('-m', '--model_path', type=str, help='Path to the model file', required=True)\n\tparser.add_argument('-r', '--runs', type=int, default=1, help='Number of runs')\n\tparser.add_argument('-n', '--num_dimensions', type=int, default=25, help='Number of dimensions')\n\tparser.add_argument('-p', '--num_points', type=int, default=1000, help='Number of points')\n\tparser.add_argument('-b', '--num_blocks', type=int, default=1000, help='Number of blocks')\n\tparser.add_argument('-s', '--num_samples', type=int, default=1000, help='Number of samples')\n\tparser", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-i', '--input', help='input file in .json format')\n\tparser.add_argument('-d', '--data', help='data file in .csv format')\n\tparser.add_argument('-o', '--output', help='output file in .json format', default='output.json')\n\n\tparser.add_argument('-l', '--language', help='language to use for the output', default='en')\n\tparser.add_argument('-v', '--verbose', help='verbose mode', action='store_true', default=False)\n\tparser.add_argument('-s', '--silent', help='silent mode', action='store_true', default=False)\n\tparser.add_argument('-t', '--test', help='test mode', action='store_true', default=False)\n\n\treturn vars(parser.parse_args(unparsed_arguments))\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Runs a PARSEC benchmark\")\n\tparser.add_argument(\"--bin_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory containing the binaries\")\n\tparser.add_argument(\"--bench_list\", type=str, required=True,\n\t\tmetavar=\"<file>\", help=\"A file containing a list of benchmarks to run\")\n\tparser.add_argument(\"--log_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory to place log files\")\n\tparser.add_argument(\"--results_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory to place results\")\n\tparser.add_argument(\"--run_count\", type=int, required=True,\n\t\tmetavar=\"<int>\", help=\"The number of times to run each benchmark\")\n\tparser.add_argument(\"--run_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory to run the benchmark\")\n\tparser.add_argument(\"--verbose\", action='store_true',\n\t\thelp=\"Enables verbose output\")\n\t", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('--output', type=str, default='', help='output file')\n\tparser.add_argument('--gpu', type=str, default='', help='which gpu to use')\n\tparser.add_argument('--gpu_mode', type=str, default='', help='which gpu mode to use')\n\tparser.add_argument('--seed', type=int, default=1234, help='random seed')\n\tparser.add_argument('--epochs', type=int, default=100, help='number of epochs')\n\tparser.add_argument('--batch_size', type=int, default=128, help='batch size')\n\tparser.add_argument('--num_workers', type=int, default=1, help='num of workers')\n\tparser.add_argument('--max_seq_len', type=int, default=512, help='max sequence length')\n\tparser.add_argument('--max_word_len', type=int, default=12, help='max word length')\n\tparser.add_argument('--max_tag_len', type=int, default=12, help='", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"A simple script to make it easier to run experiments.\")\n\n\tparser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU to use.\")\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"Run in debug mode.\")\n\tparser.add_argument(\"--output_dir\", type=str, default=\"output\", help=\"Directory to store experiment results.\")\n\tparser.add_argument(\"--log_dir\", type=str, default=\"logs\", help=\"Directory to store experiment logs.\")\n\tparser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to store experiment checkpoints.\")\n\n\tparser.add_argument(\"--model\", type=str, default=\"tiramisu\", help=\"Model to use: 'tiramisu' or 'unet'.\")\n\tparser.add_argument(\"--dataset\", type=str, default=\"mnist\", help=\"Dataset to use: 'mnist' or 'fashion'.\")\n\tparser.add_argument(\"--channels\", type=int, default=1, help=\"Number of channels in the input images.\")\n\tparser.add_argument(\"--input_height\", type=int, default=28, help", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-c', '--config_file', required=True, help='path to config file')\n\tparser.add_argument('-p', '--params_file', required=True, help='path to params file')\n\tparser.add_argument('-o', '--output_dir', required=True, help='output directory')\n\tparser.add_argument('-d', '--debug', action='store_true', help='print debug info')\n\tparser.add_argument('--run_id', default=None, help='run id')\n\targs = parser.parse_args(unparsed_arguments)\n\tif args.debug:\n\t\tprint(args)\n\treturn args\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Docker container to run the NTRIP Client cGnssTimeClient')\n\tparser.add_argument('-n', '--name', help='Name of the container', default='cgnss-time')\n\tparser.add_argument('-e', '--env', help='Environment variables', default='')\n\tparser.add_argument('-v', '--volume', help='Volumes', default='')\n\tparser.add_argument('-p', '--port', help='Ports', default='')\n\tparser.add_argument('-i', '--image', help='Image name', default='cgnss/cgnss-time:latest')\n\tparser.add_argument('-t', '--tag', help='Tag', default='latest')\n\tparser.add_argument('-D', '--debug', help='Enable debug', action='store_true')\n\tparser.add_argument('-s', '--start', help='Start the container', action='store_true')\n\tparser.add_argument('-S', '--stop', help='Stop the container', action='store_true')\n\tparser.add_argument('-r', '--restart', help='Restart the container', action='store_true')\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Runs simulation on a given set of parameters\")\n\n\tparser.add_argument(\"--simulation\", help=\"The simulation to run\", type=str, required=True)\n\tparser.add_argument(\"--parameter_file\", help=\"The parameter file to use\", type=str, required=True)\n\tparser.add_argument(\"--output_dir\", help=\"Output directory\", type=str, required=True)\n\tparser.add_argument(\"--seed\", help=\"Random seed to use\", type=int, default=None)\n\n\targuments = parser.parse_args(unparsed_arguments)\n\treturn arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Splits a dataset into train, validation and test sets\")\n\tparser.add_argument('--input_dir', type=str, required=True, help='Directory containing the input data')\n\tparser.add_argument('--output_dir', type=str, required=True, help='Directory where to store the generated data')\n\tparser.add_argument('--train_ratio', type=float, required=False, default=0.8, help='Ratio of the training set')\n\tparser.add_argument('--val_ratio', type=float, required=False, default=0.1, help='Ratio of the validation set')\n\tparser.add_argument('--test_ratio', type=float, required=False, default=0.1, help='Ratio of the test set')\n\tparser.add_argument('--shuffle', action='store_true', help='Shuffle the input data')\n\n\targs = parser.parse_args()\n\n\tif args.train_ratio + args.val_ratio + args.test_ratio != 1.0:\n\t\tprint('Error: the sum of the ratios must be 1.0')\n\t\texit()\n\n\treturn vars(args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\"--exp_name\", type=str, default=\"\")\n\tparser.add_argument(\"--dataset\", type=str, default=\"\")\n\tparser.add_argument(\"--dataset_dir\", type=str, default=\"\")\n\tparser.add_argument(\"--dataset_mode\", type=str, default=\"\")\n\tparser.add_argument(\"--dataloader_num_workers\", type=int, default=0)\n\tparser.add_argument(\"--batch_size\", type=int, default=1)\n\tparser.add_argument(\"--input_height\", type=int, default=256)\n\tparser.add_argument(\"--input_width\", type=int, default=256)\n\tparser.add_argument(\"--max_dataset_size\", type=int, default=0)\n\tparser.add_argument(\"--num_epochs\", type=int, default=0)\n\tparser.add_argument(\"--num_epochs_decay\", type=int, default=0)\n\n\tparser.add_argument(\"--output_height\", type=int, default=256)\n\tparser.add_argument(\"--output_width\", type=int, default=256)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='This script is used for running benchmark tests on the different implementations of the GEM algorithm')\n\tparser.add_argument('-d', '--data_path', type=str, help='Path to the data file', required=True)\n\tparser.add_argument('-o', '--output', type=str, help='Path to the output file', required=True)\n\tparser.add_argument('-m', '--model_path', type=str, help='Path to the model file', required=True)\n\tparser.add_argument('-r', '--runs', type=int, default=1, help='Number of runs')\n\tparser.add_argument('-n', '--num_dimensions', type=int, default=25, help='Number of dimensions')\n\tparser.add_argument('-p', '--num_points', type=int, default=1000, help='Number of points')\n\tparser.add_argument('-b', '--num_blocks', type=int, default=1000, help='Number of blocks')\n\tparser.add_argument('-s', '--num_samples', type=int, default=1000, help='Number of samples')\n\tparser", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-i', '--input', help='input file in .json format')\n\tparser.add_argument('-d', '--data', help='data file in .csv format')\n\tparser.add_argument('-o', '--output', help='output file in .json format', default='output.json')\n\n\tparser.add_argument('-l', '--language', help='language to use for the output', default='en')\n\tparser.add_argument('-v', '--verbose', help='verbose mode', action='store_true', default=False)\n\tparser.add_argument('-s', '--silent', help='silent mode', action='store_true', default=False)\n\tparser.add_argument('-t', '--test', help='test mode', action='store_true', default=False)\n\n\treturn vars(parser.parse_args(unparsed_arguments))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Runs a PARSEC benchmark\")\n\tparser.add_argument(\"--bin_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory containing the binaries\")\n\tparser.add_argument(\"--bench_list\", type=str, required=True,\n\t\tmetavar=\"<file>\", help=\"A file containing a list of benchmarks to run\")\n\tparser.add_argument(\"--log_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory to place log files\")\n\tparser.add_argument(\"--results_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory to place results\")\n\tparser.add_argument(\"--run_count\", type=int, required=True,\n\t\tmetavar=\"<int>\", help=\"The number of times to run each benchmark\")\n\tparser.add_argument(\"--run_dir\", type=str, required=True,\n\t\tmetavar=\"<path>\", help=\"The directory to run the benchmark\")\n\tparser.add_argument(\"--verbose\", action='store_true',\n\t\thelp=\"Enables verbose output\")\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tparser.add_argument('--output', type=str, default='', help='output file')\n\tparser.add_argument('--gpu', type=str, default='', help='which gpu to use')\n\tparser.add_argument('--gpu_mode', type=str, default='', help='which gpu mode to use')\n\tparser.add_argument('--seed', type=int, default=1234, help='random seed')\n\tparser.add_argument('--epochs', type=int, default=100, help='number of epochs')\n\tparser.add_argument('--batch_size', type=int, default=128, help='batch size')\n\tparser.add_argument('--num_workers', type=int, default=1, help='num of workers')\n\tparser.add_argument('--max_seq_len', type=int, default=512, help='max sequence length')\n\tparser.add_argument('--max_word_len', type=int, default=12, help='max word length')\n\tparser.add_argument('--max_tag_len', type=int, default=12, help='", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"A simple script to make it easier to run experiments.\")\n\n\tparser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU to use.\")\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"Run in debug mode.\")\n\tparser.add_argument(\"--output_dir\", type=str, default=\"output\", help=\"Directory to store experiment results.\")\n\tparser.add_argument(\"--log_dir\", type=str, default=\"logs\", help=\"Directory to store experiment logs.\")\n\tparser.add_argument(\"--checkpoint_dir\", type=str, default=\"checkpoints\", help=\"Directory to store experiment checkpoints.\")\n\n\tparser.add_argument(\"--model\", type=str, default=\"tiramisu\", help=\"Model to use: 'tiramisu' or 'unet'.\")\n\tparser.add_argument(\"--dataset\", type=str, default=\"mnist\", help=\"Dataset to use: 'mnist' or 'fashion'.\")\n\tparser.add_argument(\"--channels\", type=int, default=1, help=\"Number of channels in the input images.\")\n\tparser.add_argument(\"--input_height\", type=int, default=28, help", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-c', '--config_file', required=True, help='path to config file')\n\tparser.add_argument('-p', '--params_file', required=True, help='path to params file')\n\tparser.add_argument('-o', '--output_dir', required=True, help='output directory')\n\tparser.add_argument('-d', '--debug', action='store_true', help='print debug info')\n\tparser.add_argument('--run_id', default=None, help='run id')\n\targs = parser.parse_args(unparsed_arguments)\n\tif args.debug:\n\t\tprint(args)\n\treturn args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = [subparser.name for subparser in subparsers.choices.values()]\n\n\t# If no subparser is specified, give the first one a shot.\n\tif len(unparsed_arguments) == 0 or unparsed_arguments[0] not in subparser_names:\n\t\tunparsed_arguments.insert(0, subparser_names[0])\n\n\taction = subparsers.choices[unparsed_arguments[0]]\n\tremaining_arguments = unparsed_arguments[1:]\n\n\t# Let the subparser parse its arguments first.\n\tnamespace, remaining_arguments = action.parse_known_args(remaining_arguments)\n\n\t# If the subparser didn't take any arguments, we don't have to do anything else.\n\tif len(remaining_arguments) == 0:\n\t\treturn ({'_subparser': action.name}, [])\n\n\t# Otherwise, we need to give the next subparser a shot.\n\treturn parse_subparser_arguments(remaining_arguments, subparsers)\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser.add_argument(\n\t\t\t\"--subparser-name\",\n\t\t\tdest=\"subparser_name\",\n\t\t\tdefault=subparser_name,\n\t\t\thelp=argparse.SUPPRESS,\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"--\" + subparser_name,\n\t\t\tdest=\"subparser_name\",\n\t\t\tdefault=subparser_name,\n\t\t\thelp=argparse.SUPPRESS,\n\t\t)\n\t\tparser.add_argument(\"--\" + subparser_name + \"-help\", action=\"help\")\n\t\tsubparser.set_defaults(func=lambda _: parser.print_help())\n\t\tsubparser.set_defaults(subparser_name=subparser_name)\n\t# Parse the unparsed arguments, to find the subparser to run.\n\targs = parser.parse_args(unparsed_arguments)\n\t# If the user asked for help, just print the help and exit.\n\tif args.subparser_name == \"help\":\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\t# Parse the", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_names = [name for name in subparsers]\n\tsubparsers_dict = {name: subparsers[name] for name in subparsers}\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\"--repository\", help=\"Path to the repository folder\")\n\tparser.add_argument(\"--log-file\", help=\"Log file path\")\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug information\")\n\tparser.add_argument(\"--quiet\", action=\"store_true\", help=\"Disable all output\")\n\targs, unparsed = parser.parse_known_args(unparsed_arguments)\n\tremaining_args = [arg for arg in unparsed_arguments if arg not in unparsed]\n\n\t# Parse arguments for each subparser\n\tsubparsers_args = {}\n\tfor name in subparsers_names:\n\t\tsubparsers_args[name] = subparsers_dict[name].parse_args(remaining_args)\n\n\treturn subparsers_args, remaining_args\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_names = set(subparsers.keys())\n\n\tparser_map = {}\n\tremaining_arguments = []\n\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments.pop(0)\n\n\t\tif argument in parser_names:\n\t\t\tparser_map[argument] = subparsers[argument].parse_args(unparsed_arguments)\n\t\t\tbreak\n\n\t\tremaining_arguments.append(argument)\n\n\treturn parser_map, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_parsers = subparsers.choices.values()\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--repository', default=None)\n\tparser.add_argument('--no-repository', dest='repository', action='store_false')\n\tparser.add_argument('--root-url', default=None)\n\tparser.add_argument('--no-root-url', dest='root_url', action='store_false')\n\tparser.add_argument('--use-ssh', action='store_true')\n\tparser.add_argument('--no-use-ssh', dest='use_ssh', action='store_false')\n\tparser.add_argument('-m', '--mirror-url', default=None)\n\tparser.add_argument('--no-mirror-url', dest='mirror_url', action='store_false')\n\tparser.add_argument('--mirror-url-with-ssh', action='store_true')\n\tparser.add_argument('--no-mirror-url-with-ssh', dest='mirror_url_with_ssh', action='store_false')\n\tparser.add_argument('--local-repo', default=None)\n\tparser.add_argument('--no-local-repo', dest='local_", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsers = dict((name, parser) for name, parser in subparsers.items())\n\tunparsed_arguments = list(unparsed_arguments)\n\tparsed_arguments = {}\n\tfor index, argument in enumerate(unparsed_arguments):\n\t\tif argument.startswith(\"--\"):\n\t\t\tcontinue\n\t\tfor name, parser in parsers.items():\n\t\t\tif parser.parse_known_args([argument])[0]:\n\t\t\t\tparsed_arguments[name] = parser.parse_known_args(unparsed_arguments[index:])[0]\n\t\t\t\tunparsed_arguments = unparsed_arguments[index + len(parser.parse_known_args(unparsed_arguments[index:])[1]):]\n\t\t\t\tbreak\n\treturn parsed_arguments, unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_map = {}\n\tremainder = []\n\n\t# Each subparser has its own argument parser instance.\n\tfor parser in subparsers.choices.values():\n\t\t# We want to use the arguments from the first subparser that claims an argument.\n\t\t# This allows subparsers to share arguments like \"--repository\".\n\t\t#\n\t\t# We also want to use the arguments from the last subparser that claims an argument.\n\t\t# This allows subparsers to add arguments like \"--debug\" and have them take effect.\n\t\t#\n\t\t# We therefore want to keep track of the subparser with the most recent argument\n\t\t# claim, and the subparser with the latest argument claim.\n\t\tclaiming_subparser = None\n\t\tclaiming_subparser_index = 0\n\t\tlatest_subparser = None\n\t\tlatest_subparser_index = 0\n\t\tfor index, arg in enumerate(unparsed_arguments):\n\t\t\t# Check each argument against each subparser's argument parser.\n\t\t\tparser_result = parser.parse_args(args=arg.split())\n\t\t\tif parser_result.repository_name:\n\t\t\t\t# This argument is claimed by a subparser.\n\t\t\t\t#\n\t\t", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = list(unparsed_arguments)\n\tparsed_args = {}\n\twhile remaining_arguments:\n\t\tcurrent_argument = remaining_arguments.pop(0)\n\t\tif current_argument in subparsers:\n\t\t\tparsed_args[current_argument], remaining_arguments = parse_subparser_arguments(remaining_arguments, subparsers[current_argument])\n\t\telse:\n\t\t\traise ValueError('Unknown argument {!r}'.format(current_argument))\n\treturn parsed_args, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# The following is a bit of a hack.\n\t#\n\t# First, we create a new argparse.ArgumentParser instance to hold the common arguments.\n\t# We also create an empty dict to hold subparser names as keys and argparse.Namespace as\n\t# values.\n\t#\n\t# Then, for each subparser, we set the \"default\" argument to be a function that returns\n\t# the subparser's ArgumentParser instance. This allows us to reuse the existing\n\t# subparser.add_argument calls.\n\t#\n\t# Then, we use this new parser to parse the arguments.\n\t#\n\t# Finally, we return the dict of parsed namespaces and the list of remaining arguments.\n\t#\n\t# The code below is a bit ugly. It could be cleaned up by moving the common-argument\n\t# creation and the default-setting out of the loop.\n\n\tcommon_parser = argparse.ArgumentParser(add_help=False)\n\tparsed_namespaces = {}\n\tremaining_arguments = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed_namespaces[subparser_name] = argparse.Namespace()\n\t\tsubparser.set_defaults(func=lambda a: subparser)\n\t\tcommon_parser.add_", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = list(unparsed_arguments)\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments.pop(0)\n\t\tif argument in subparsers:\n\t\t\tparser = subparsers[argument]\n\t\t\tparsed_arguments[argument] = parser.parse_args(unparsed_arguments)\n\t\t\tunparsed_arguments = []\n\t\telse:\n\t\t\tprint(\"Unknown argument: {}\".format(argument))\n\treturn parsed_arguments, unparsed_arguments\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = [subparser.name for subparser in subparsers.choices.values()]\n\n\t# If no subparser is specified, give the first one a shot.\n\tif len(unparsed_arguments) == 0 or unparsed_arguments[0] not in subparser_names:\n\t\tunparsed_arguments.insert(0, subparser_names[0])\n\n\taction = subparsers.choices[unparsed_arguments[0]]\n\tremaining_arguments = unparsed_arguments[1:]\n\n\t# Let the subparser parse its arguments first.\n\tnamespace, remaining_arguments = action.parse_known_args(remaining_arguments)\n\n\t# If the subparser didn't take any arguments, we don't have to do anything else.\n\tif len(remaining_arguments) == 0:\n\t\treturn ({'_subparser': action.name}, [])\n\n\t# Otherwise, we need to give the next subparser a shot.\n\treturn parse_subparser_arguments(remaining_arguments, subparsers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser.add_argument(\n\t\t\t\"--subparser-name\",\n\t\t\tdest=\"subparser_name\",\n\t\t\tdefault=subparser_name,\n\t\t\thelp=argparse.SUPPRESS,\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"--\" + subparser_name,\n\t\t\tdest=\"subparser_name\",\n\t\t\tdefault=subparser_name,\n\t\t\thelp=argparse.SUPPRESS,\n\t\t)\n\t\tparser.add_argument(\"--\" + subparser_name + \"-help\", action=\"help\")\n\t\tsubparser.set_defaults(func=lambda _: parser.print_help())\n\t\tsubparser.set_defaults(subparser_name=subparser_name)\n\t# Parse the unparsed arguments, to find the subparser to run.\n\targs = parser.parse_args(unparsed_arguments)\n\t# If the user asked for help, just print the help and exit.\n\tif args.subparser_name == \"help\":\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\t# Parse the", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_names = [name for name in subparsers]\n\tsubparsers_dict = {name: subparsers[name] for name in subparsers}\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\"--repository\", help=\"Path to the repository folder\")\n\tparser.add_argument(\"--log-file\", help=\"Log file path\")\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug information\")\n\tparser.add_argument(\"--quiet\", action=\"store_true\", help=\"Disable all output\")\n\targs, unparsed = parser.parse_known_args(unparsed_arguments)\n\tremaining_args = [arg for arg in unparsed_arguments if arg not in unparsed]\n\n\t# Parse arguments for each subparser\n\tsubparsers_args = {}\n\tfor name in subparsers_names:\n\t\tsubparsers_args[name] = subparsers_dict[name].parse_args(remaining_args)\n\n\treturn subparsers_args, remaining_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_names = set(subparsers.keys())\n\n\tparser_map = {}\n\tremaining_arguments = []\n\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments.pop(0)\n\n\t\tif argument in parser_names:\n\t\t\tparser_map[argument] = subparsers[argument].parse_args(unparsed_arguments)\n\t\t\tbreak\n\n\t\tremaining_arguments.append(argument)\n\n\treturn parser_map, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_parsers = subparsers.choices.values()\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--repository', default=None)\n\tparser.add_argument('--no-repository', dest='repository', action='store_false')\n\tparser.add_argument('--root-url', default=None)\n\tparser.add_argument('--no-root-url', dest='root_url', action='store_false')\n\tparser.add_argument('--use-ssh', action='store_true')\n\tparser.add_argument('--no-use-ssh', dest='use_ssh', action='store_false')\n\tparser.add_argument('-m', '--mirror-url', default=None)\n\tparser.add_argument('--no-mirror-url', dest='mirror_url', action='store_false')\n\tparser.add_argument('--mirror-url-with-ssh', action='store_true')\n\tparser.add_argument('--no-mirror-url-with-ssh', dest='mirror_url_with_ssh', action='store_false')\n\tparser.add_argument('--local-repo', default=None)\n\tparser.add_argument('--no-local-repo', dest='local_", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsers = dict((name, parser) for name, parser in subparsers.items())\n\tunparsed_arguments = list(unparsed_arguments)\n\tparsed_arguments = {}\n\tfor index, argument in enumerate(unparsed_arguments):\n\t\tif argument.startswith(\"--\"):\n\t\t\tcontinue\n\t\tfor name, parser in parsers.items():\n\t\t\tif parser.parse_known_args([argument])[0]:\n\t\t\t\tparsed_arguments[name] = parser.parse_known_args(unparsed_arguments[index:])[0]\n\t\t\t\tunparsed_arguments = unparsed_arguments[index + len(parser.parse_known_args(unparsed_arguments[index:])[1]):]\n\t\t\t\tbreak\n\treturn parsed_arguments, unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_map = {}\n\tremainder = []\n\n\t# Each subparser has its own argument parser instance.\n\tfor parser in subparsers.choices.values():\n\t\t# We want to use the arguments from the first subparser that claims an argument.\n\t\t# This allows subparsers to share arguments like \"--repository\".\n\t\t#\n\t\t# We also want to use the arguments from the last subparser that claims an argument.\n\t\t# This allows subparsers to add arguments like \"--debug\" and have them take effect.\n\t\t#\n\t\t# We therefore want to keep track of the subparser with the most recent argument\n\t\t# claim, and the subparser with the latest argument claim.\n\t\tclaiming_subparser = None\n\t\tclaiming_subparser_index = 0\n\t\tlatest_subparser = None\n\t\tlatest_subparser_index = 0\n\t\tfor index, arg in enumerate(unparsed_arguments):\n\t\t\t# Check each argument against each subparser's argument parser.\n\t\t\tparser_result = parser.parse_args(args=arg.split())\n\t\t\tif parser_result.repository_name:\n\t\t\t\t# This argument is claimed by a subparser.\n\t\t\t\t#\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = list(unparsed_arguments)\n\tparsed_args = {}\n\twhile remaining_arguments:\n\t\tcurrent_argument = remaining_arguments.pop(0)\n\t\tif current_argument in subparsers:\n\t\t\tparsed_args[current_argument], remaining_arguments = parse_subparser_arguments(remaining_arguments, subparsers[current_argument])\n\t\telse:\n\t\t\traise ValueError('Unknown argument {!r}'.format(current_argument))\n\treturn parsed_args, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# The following is a bit of a hack.\n\t#\n\t# First, we create a new argparse.ArgumentParser instance to hold the common arguments.\n\t# We also create an empty dict to hold subparser names as keys and argparse.Namespace as\n\t# values.\n\t#\n\t# Then, for each subparser, we set the \"default\" argument to be a function that returns\n\t# the subparser's ArgumentParser instance. This allows us to reuse the existing\n\t# subparser.add_argument calls.\n\t#\n\t# Then, we use this new parser to parse the arguments.\n\t#\n\t# Finally, we return the dict of parsed namespaces and the list of remaining arguments.\n\t#\n\t# The code below is a bit ugly. It could be cleaned up by moving the common-argument\n\t# creation and the default-setting out of the loop.\n\n\tcommon_parser = argparse.ArgumentParser(add_help=False)\n\tparsed_namespaces = {}\n\tremaining_arguments = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed_namespaces[subparser_name] = argparse.Namespace()\n\t\tsubparser.set_defaults(func=lambda a: subparser)\n\t\tcommon_parser.add_", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = list(unparsed_arguments)\n\twhile unparsed_arguments:\n\t\targument = unparsed_arguments.pop(0)\n\t\tif argument in subparsers:\n\t\t\tparser = subparsers[argument]\n\t\t\tparsed_arguments[argument] = parser.parse_args(unparsed_arguments)\n\t\t\tunparsed_arguments = []\n\t\telse:\n\t\t\tprint(\"Unknown argument: {}\".format(argument))\n\treturn parsed_arguments, unparsed_arguments\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Process some integers.\",\n\t\tadd_help=True,\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s {}\".format(__version__),\n\t\thelp=\"show program's version number and exit\"\n\t\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"valid subcommands\",\n\t\thelp=\"additional help\",\n\t\tdest=\"subcommand\"\n\t\t)\n\tsubparsers.required = True\n\tparser_run = subparsers.add_parser(\n\t\t\"run\",\n\t\thelp=\"process some integers\",\n\t\t)\n\tparser_run.add_argument(\n\t\t\"integers\",\n\t\tmetavar=\"N\",\n\t\ttype=int,\n\t\tnargs=\"+\",\n\t\thelp=\"an integer for the accumulator\"\n\t\t)\n\tparser_run.add_argument(\n\t\t\"--sum\", \"-s\",\n\t\tdest=\"accumulate\",\n\t\taction=\"store_const\",\n\t\tconst=sum,\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\t\n\tbuild_parser = subparsers.add_parser(\"build\", help=\"Build a file from a template\")\n\tbuild_parser.add_argument(\"--config\", help=\"The config file to use\", default=\"config.json\")\n\tbuild_parser.add_argument(\"--output\", help=\"The output file to write to\", default=\"output.txt\")\n\tbuild_parser.add_argument(\"--template\", help=\"The template file to use\", default=\"template.txt\")\n\tbuild_parser.add_argument(\"--vars\", help=\"The variables file to use\", default=\"vars.json\")\n\t\n\ttest_parser = subparsers.add_parser(\"test\", help=\"Test a template file\")\n\ttest_parser.add_argument(\"--template\", help=\"The template file to use\", default=\"template.txt\")\n\ttest_parser.add_argument(\"--vars\", help=\"The variables file to use\", default=\"vars.json\")\n\t\n\tversion_parser = subparsers.add_parser(\"version\", help=\"Print the version number\")\n\t\n\treturn parser, subparsers\n\t\ndef build_file(config, template, vars, output):\n\t\"\"\"\n\tBuild the", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Convert a KGTK file from one format to another\")\n\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\tparser_from_kgtk = subparsers.add_parser('from-kgtk', aliases=['kgtk'], help=\"Convert from KGTK format to another format\")\n\n\tparser_to_kgtk = subparsers.add_parser('to-kgtk', aliases=['kgtk'], help=\"Convert from another format to KGTK format\")\n\n\tparser_from_kgtk.add_argument('-i', '--input-file', dest=\"input_file\", required=True, help=\"Path to input file\")\n\tparser_from_kgtk.add_argument('-o', '--output-file', dest=\"output_file\", required=True, help=\"Path to output file\")\n\tparser_from_kgtk.add_argument('--force', dest=\"force\", action=\"store_true\", help=\"Force output file creation\")\n\tparser_from_kgtk.add_argument('--skip-row', dest=\"skip_row\", action=\"store_true\", help=\"Skip the current row when an error is encountered\")\n\tparser_from_kgtk", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tfrom . import __version__\n\t\n\tparser = argparse.ArgumentParser(description='''\n\t\tA tool to manage a collection of files.\n\t\t''')\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s v' + __version__)\n\t\n\tsubparsers = parser.add_subparsers()\n\t\n\tparser_add = subparsers.add_parser('add', help='''\n\t\tAdd a file to the collection.\n\t\t''')\n\tparser_add.add_argument('path', help='''\n\t\tThe path of the file to add.\n\t\t''')\n\tparser_add.add_argument('-d', '--description', help='''\n\t\tA description for the file.\n\t\t''')\n\tparser_add.add_argument('-s', '--size', help='''\n\t\tThe size of the file.\n\t\t''')\n\tparser_add.add_argument('-c', '--checksum', help='''\n\t\tA checksum of the file.\n\t\t''')\n\tparser_add.add_argument('-r', '--replace', action='store_true', help='''\n\t\tReplace an existing file with the same check", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(title=\"commands\", dest=\"command\")\n\tsubparsers.required = True\n\tsubparsers.description = \"Commands to control the bot.\"\n\tparser_init = subparsers.add_parser(\"init\", help=\"Initialize the bot\")\n\tparser_init.add_argument(\"--database\", type=str, help=\"Path to the SQLite database\")\n\tparser_init.add_argument(\"--config\", type=str, help=\"Path to the JSON configuration file\")\n\tparser_init.add_argument(\"--token\", type=str, help=\"Token to use\")\n\tparser_init.add_argument(\"--prefix\", type=str, help=\"Prefix to use\")\n\tparser_init.add_argument(\"--owner-id\", type=int, help=\"Owner's ID\")\n\tparser_init.add_argument(\"--debug\", action=\"store_true\", help=\"Run the bot in debug mode\")\n\tparser_start = subparsers.add_parser(\"start\", help=\"Start the bot\")\n\tparser_start.add_argument(\"--delay\", type=int, help=\"Delay between each poll in seconds\")\n\tparser_start.add_argument(\"--shards\", type=int, help=\"Number of sh", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Build a program to run through the command line.')\n\tsubparsers = parser.add_subparsers(dest='command')\n\tadd_parser = subparsers.add_parser('add', help='Add a new item to the database.')\n\tadd_parser.add_argument('name', help='The name of the item to add.', type=str)\n\tadd_parser.add_argument('price', help='The price of the item to add.', type=float)\n\tadd_parser.add_argument('quantity', help='The quantity of the item to add.', type=int)\n\tupdate_parser = subparsers.add_parser('update', help='Update an item in the database.')\n\tupdate_parser.add_argument('item', help='The name of the item to update.', type=str)\n\tupdate_parser.add_argument('name', help='The new name of the item.', type=str)\n\tupdate_parser.add_argument('price', help='The new price of the item.', type=float)\n\tupdate_parser.add_argument('quantity', help='The new quantity of the item.', type=int)\n\tdelete_parser = subparsers.add_parser('delete', help='Delete an item from", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Process a stream of SIGFOX messages')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_list = subparsers.add_parser(\n\t\t'list',\n\t\thelp='List all SIGFOX messages in the store')\n\tparser_list.add_argument(\n\t\t'-m',\n\t\t'--max',\n\t\ttype=int,\n\t\thelp='Maximum number of messages to list')\n\tparser_list.add_argument(\n\t\t'-s',\n\t\t'--since',\n\t\ttype=int,\n\t\thelp='List messages since the given Unix timestamp')\n\n\tparser_show = subparsers.add_parser(\n\t\t'show',\n\t\thelp='Show a SIGFOX message')\n\tparser_show.add_argument(\n\t\t'message',\n\t\ttype=int,\n\t\thelp='Message ID')\n\n\tparser_delete = subparsers.add_parser(\n\t\t'delete',\n\t\thelp='Delete a SIGFOX message')\n\tparser_delete.add_argument(\n\t\t'message',\n\t\ttype=int,\n\t\thelp='Message ID')\n\n\treturn (parser, subparsers", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Construct a TensorFlow model from a configuration file.\")\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\", help=\"Sub-commands\")\n\tbuild = subparsers.add_parser(\"build\")\n\tbuild.add_argument(\"config_file\", metavar=\"CONFIG_FILE\", help=\"A configuration file.\")\n\tbuild.add_argument(\"--output_dir\", \"-o\", metavar=\"OUTPUT_DIR\", help=\"The output directory for the model.\")\n\tbuild.add_argument(\"--checkpoint_dir\", \"-c\", metavar=\"CHECKPOINT_DIR\", help=\"The directory for the checkpoint.\")\n\tbuild.add_argument(\"--checkpoint_file\", \"-f\", metavar=\"CHECKPOINT_FILE\", help=\"The checkpoint file.\")\n\tbuild.add_argument(\"--model_dir\", \"-m\", metavar=\"MODEL_DIR\", help=\"The model directory.\")\n\tbuild.add_argument(\"--model_file\", \"-M\", metavar=\"MODEL_FILE\", help=\"The model file.\")\n\tbuild.add_argument(\"--log_file\", \"-l\", metavar=\"LOG_FILE\", help=\"The log file.\")\n\tbuild.add_argument(\"--log_", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog='wotw',\n\t\tdescription='A tool for working with World of Warcraft addons.',\n\t\tepilog='For more information, see https://github.com/gabrieldemory/wotw'\n\t)\n\n\tsubparsers = parser.add_subparsers(title='subcommands')\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Generate a random file')\n\tsubparsers = parser.add_subparsers()\n\tadd_subparser(subparsers, 'int', int)\n\tadd_subparser(subparsers, 'float', float)\n\tadd_subparser(subparsers, 'char', chr)\n\tadd_subparser(subparsers, 'string', str)\n\tadd_subparser(subparsers, 'binary', lambda x: bin(x)[2:])\n\tadd_subparser(subparsers, 'hex', lambda x: hex(x)[2:])\n\tadd_subparser(subparsers, 'octal', oct)\n\treturn parser, subparsers\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Process some integers.\",\n\t\tadd_help=True,\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s {}\".format(__version__),\n\t\thelp=\"show program's version number and exit\"\n\t\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"valid subcommands\",\n\t\thelp=\"additional help\",\n\t\tdest=\"subcommand\"\n\t\t)\n\tsubparsers.required = True\n\tparser_run = subparsers.add_parser(\n\t\t\"run\",\n\t\thelp=\"process some integers\",\n\t\t)\n\tparser_run.add_argument(\n\t\t\"integers\",\n\t\tmetavar=\"N\",\n\t\ttype=int,\n\t\tnargs=\"+\",\n\t\thelp=\"an integer for the accumulator\"\n\t\t)\n\tparser_run.add_argument(\n\t\t\"--sum\", \"-s\",\n\t\tdest=\"accumulate\",\n\t\taction=\"store_const\",\n\t\tconst=sum,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\t\n\tbuild_parser = subparsers.add_parser(\"build\", help=\"Build a file from a template\")\n\tbuild_parser.add_argument(\"--config\", help=\"The config file to use\", default=\"config.json\")\n\tbuild_parser.add_argument(\"--output\", help=\"The output file to write to\", default=\"output.txt\")\n\tbuild_parser.add_argument(\"--template\", help=\"The template file to use\", default=\"template.txt\")\n\tbuild_parser.add_argument(\"--vars\", help=\"The variables file to use\", default=\"vars.json\")\n\t\n\ttest_parser = subparsers.add_parser(\"test\", help=\"Test a template file\")\n\ttest_parser.add_argument(\"--template\", help=\"The template file to use\", default=\"template.txt\")\n\ttest_parser.add_argument(\"--vars\", help=\"The variables file to use\", default=\"vars.json\")\n\t\n\tversion_parser = subparsers.add_parser(\"version\", help=\"Print the version number\")\n\t\n\treturn parser, subparsers\n\t\ndef build_file(config, template, vars, output):\n\t\"\"\"\n\tBuild the", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Convert a KGTK file from one format to another\")\n\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\tparser_from_kgtk = subparsers.add_parser('from-kgtk', aliases=['kgtk'], help=\"Convert from KGTK format to another format\")\n\n\tparser_to_kgtk = subparsers.add_parser('to-kgtk', aliases=['kgtk'], help=\"Convert from another format to KGTK format\")\n\n\tparser_from_kgtk.add_argument('-i', '--input-file', dest=\"input_file\", required=True, help=\"Path to input file\")\n\tparser_from_kgtk.add_argument('-o', '--output-file', dest=\"output_file\", required=True, help=\"Path to output file\")\n\tparser_from_kgtk.add_argument('--force', dest=\"force\", action=\"store_true\", help=\"Force output file creation\")\n\tparser_from_kgtk.add_argument('--skip-row', dest=\"skip_row\", action=\"store_true\", help=\"Skip the current row when an error is encountered\")\n\tparser_from_kgtk", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\tfrom . import __version__\n\t\n\tparser = argparse.ArgumentParser(description='''\n\t\tA tool to manage a collection of files.\n\t\t''')\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s v' + __version__)\n\t\n\tsubparsers = parser.add_subparsers()\n\t\n\tparser_add = subparsers.add_parser('add', help='''\n\t\tAdd a file to the collection.\n\t\t''')\n\tparser_add.add_argument('path', help='''\n\t\tThe path of the file to add.\n\t\t''')\n\tparser_add.add_argument('-d', '--description', help='''\n\t\tA description for the file.\n\t\t''')\n\tparser_add.add_argument('-s', '--size', help='''\n\t\tThe size of the file.\n\t\t''')\n\tparser_add.add_argument('-c', '--checksum', help='''\n\t\tA checksum of the file.\n\t\t''')\n\tparser_add.add_argument('-r', '--replace', action='store_true', help='''\n\t\tReplace an existing file with the same check", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(title=\"commands\", dest=\"command\")\n\tsubparsers.required = True\n\tsubparsers.description = \"Commands to control the bot.\"\n\tparser_init = subparsers.add_parser(\"init\", help=\"Initialize the bot\")\n\tparser_init.add_argument(\"--database\", type=str, help=\"Path to the SQLite database\")\n\tparser_init.add_argument(\"--config\", type=str, help=\"Path to the JSON configuration file\")\n\tparser_init.add_argument(\"--token\", type=str, help=\"Token to use\")\n\tparser_init.add_argument(\"--prefix\", type=str, help=\"Prefix to use\")\n\tparser_init.add_argument(\"--owner-id\", type=int, help=\"Owner's ID\")\n\tparser_init.add_argument(\"--debug\", action=\"store_true\", help=\"Run the bot in debug mode\")\n\tparser_start = subparsers.add_parser(\"start\", help=\"Start the bot\")\n\tparser_start.add_argument(\"--delay\", type=int, help=\"Delay between each poll in seconds\")\n\tparser_start.add_argument(\"--shards\", type=int, help=\"Number of sh", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Build a program to run through the command line.')\n\tsubparsers = parser.add_subparsers(dest='command')\n\tadd_parser = subparsers.add_parser('add', help='Add a new item to the database.')\n\tadd_parser.add_argument('name', help='The name of the item to add.', type=str)\n\tadd_parser.add_argument('price', help='The price of the item to add.', type=float)\n\tadd_parser.add_argument('quantity', help='The quantity of the item to add.', type=int)\n\tupdate_parser = subparsers.add_parser('update', help='Update an item in the database.')\n\tupdate_parser.add_argument('item', help='The name of the item to update.', type=str)\n\tupdate_parser.add_argument('name', help='The new name of the item.', type=str)\n\tupdate_parser.add_argument('price', help='The new price of the item.', type=float)\n\tupdate_parser.add_argument('quantity', help='The new quantity of the item.', type=int)\n\tdelete_parser = subparsers.add_parser('delete', help='Delete an item from", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Process a stream of SIGFOX messages')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_list = subparsers.add_parser(\n\t\t'list',\n\t\thelp='List all SIGFOX messages in the store')\n\tparser_list.add_argument(\n\t\t'-m',\n\t\t'--max',\n\t\ttype=int,\n\t\thelp='Maximum number of messages to list')\n\tparser_list.add_argument(\n\t\t'-s',\n\t\t'--since',\n\t\ttype=int,\n\t\thelp='List messages since the given Unix timestamp')\n\n\tparser_show = subparsers.add_parser(\n\t\t'show',\n\t\thelp='Show a SIGFOX message')\n\tparser_show.add_argument(\n\t\t'message',\n\t\ttype=int,\n\t\thelp='Message ID')\n\n\tparser_delete = subparsers.add_parser(\n\t\t'delete',\n\t\thelp='Delete a SIGFOX message')\n\tparser_delete.add_argument(\n\t\t'message',\n\t\ttype=int,\n\t\thelp='Message ID')\n\n\treturn (parser, subparsers", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Construct a TensorFlow model from a configuration file.\")\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\", help=\"Sub-commands\")\n\tbuild = subparsers.add_parser(\"build\")\n\tbuild.add_argument(\"config_file\", metavar=\"CONFIG_FILE\", help=\"A configuration file.\")\n\tbuild.add_argument(\"--output_dir\", \"-o\", metavar=\"OUTPUT_DIR\", help=\"The output directory for the model.\")\n\tbuild.add_argument(\"--checkpoint_dir\", \"-c\", metavar=\"CHECKPOINT_DIR\", help=\"The directory for the checkpoint.\")\n\tbuild.add_argument(\"--checkpoint_file\", \"-f\", metavar=\"CHECKPOINT_FILE\", help=\"The checkpoint file.\")\n\tbuild.add_argument(\"--model_dir\", \"-m\", metavar=\"MODEL_DIR\", help=\"The model directory.\")\n\tbuild.add_argument(\"--model_file\", \"-M\", metavar=\"MODEL_FILE\", help=\"The model file.\")\n\tbuild.add_argument(\"--log_file\", \"-l\", metavar=\"LOG_FILE\", help=\"The log file.\")\n\tbuild.add_argument(\"--log_", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog='wotw',\n\t\tdescription='A tool for working with World of Warcraft addons.',\n\t\tepilog='For more information, see https://github.com/gabrieldemory/wotw'\n\t)\n\n\tsubparsers = parser.add_subparsers(title='subcommands')\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Generate a random file')\n\tsubparsers = parser.add_subparsers()\n\tadd_subparser(subparsers, 'int', int)\n\tadd_subparser(subparsers, 'float', float)\n\tadd_subparser(subparsers, 'char', chr)\n\tadd_subparser(subparsers, 'string', str)\n\tadd_subparser(subparsers, 'binary', lambda x: bin(x)[2:])\n\tadd_subparser(subparsers, 'hex', lambda x: hex(x)[2:])\n\tadd_subparser(subparsers, 'octal', oct)\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in nodes.items():\n\t\tif key in result:\n\t\t\tif not isinstance(value, MappingNode):\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = deep_merge_nodes(value.value)\n\t\telse:\n\t\t\tresult[key] = value\n\treturn result\n\n\nclass MappingNode(BaseNode):\n\t\"\"\"\n\tRepresents a mapping of keys to values.\n\t\"\"\"\n\tdef __init__(self, value):\n\t\tsuper().__init__(value)\n\n\tdef __repr__(self):\n\t\treturn f'{self.__class__.__name__}({self.value})'\n\n\tdef merge_nodes(self, nodes):\n\t\treturn deep_merge_nodes(nodes)\n\n\tdef get_node(self, key):\n\t\tif key in self.value:\n\t\t\treturn self.value[key]\n\t\telse:\n\t\t\treturn None\n\n\tdef set_node(self, key, value):\n\t\tif key in self.value:\n\t\t\tself.value[key] = value\n\t\telse:\n\t\t\tself.value[key] = value\n\n\tdef", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif isinstance(value, MappingNode):\n\t\t\t\tif key in result:\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged and isinstance(merged[key], MappingNode):\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\t\t\telif key in merged:\n\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn MappingNode(merged)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node_name in nodes:\n\t\tnode = nodes[node_name]\n\t\tif not isinstance(node, MappingNode):\n\t\t\t# non-dicts are not merged\n\t\t\tmerged_nodes[node_name] = node\n\t\t\tcontinue\n\t\tmerged_nodes[node_name] = deep_merge_dictionaries(node, merged_nodes)\n\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tif isinstance(nodes, MappingNode):\n\t\tresult = {}\n\t\tfor key, val in nodes.value.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] = deep_merge_nodes(result[key], val)\n\t\t\telse:\n\t\t\t\tresult[key] = val\n\t\treturn result\n\telif isinstance(nodes, SequenceNode):\n\t\treturn [deep_merge_nodes(n) for n in nodes.value]\n\telse:\n\t\treturn nodes\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\ttry:\n\t\treturn reduce(deep_merge_nodes, nodes)\n\texcept TypeError:\n\t\treturn nodes\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tres = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in res:\n\t\t\t\tres[key] = value\n\t\t\telif isinstance(res[key], MappingNode):\n\t\t\t\tres[key] = deep_merge_nodes([res[key], value])\n\t\t\telse:\n\t\t\t\tres[key] = value\n\treturn MappingNode(res)\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\treturn reduce(lambda x, y: x.deep_merge(y), nodes, nodes[0])\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif isinstance(value, MappingNode):\n\t\t\t\tif key not in merged:\n\t\t\t\t\tmerged[key] = value\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn MappingNode(merged)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in nodes.items():\n\t\tif key in result:\n\t\t\tif not isinstance(value, MappingNode):\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = deep_merge_nodes(value.value)\n\t\telse:\n\t\t\tresult[key] = value\n\treturn result\n\n\nclass MappingNode(BaseNode):\n\t\"\"\"\n\tRepresents a mapping of keys to values.\n\t\"\"\"\n\tdef __init__(self, value):\n\t\tsuper().__init__(value)\n\n\tdef __repr__(self):\n\t\treturn f'{self.__class__.__name__}({self.value})'\n\n\tdef merge_nodes(self, nodes):\n\t\treturn deep_merge_nodes(nodes)\n\n\tdef get_node(self, key):\n\t\tif key in self.value:\n\t\t\treturn self.value[key]\n\t\telse:\n\t\t\treturn None\n\n\tdef set_node(self, key, value):\n\t\tif key in self.value:\n\t\t\tself.value[key] = value\n\t\telse:\n\t\t\tself.value[key] = value\n\n\tdef", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif isinstance(value, MappingNode):\n\t\t\t\tif key in result:\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged and isinstance(merged[key], MappingNode):\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\t\t\telif key in merged:\n\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn MappingNode(merged)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node_name in nodes:\n\t\tnode = nodes[node_name]\n\t\tif not isinstance(node, MappingNode):\n\t\t\t# non-dicts are not merged\n\t\t\tmerged_nodes[node_name] = node\n\t\t\tcontinue\n\t\tmerged_nodes[node_name] = deep_merge_dictionaries(node, merged_nodes)\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tif isinstance(nodes, MappingNode):\n\t\tresult = {}\n\t\tfor key, val in nodes.value.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] = deep_merge_nodes(result[key], val)\n\t\t\telse:\n\t\t\t\tresult[key] = val\n\t\treturn result\n\telif isinstance(nodes, SequenceNode):\n\t\treturn [deep_merge_nodes(n) for n in nodes.value]\n\telse:\n\t\treturn nodes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\ttry:\n\t\treturn reduce(deep_merge_nodes, nodes)\n\texcept TypeError:\n\t\treturn nodes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tres = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key not in res:\n\t\t\t\tres[key] = value\n\t\t\telif isinstance(res[key], MappingNode):\n\t\t\t\tres[key] = deep_merge_nodes([res[key], value])\n\t\t\telse:\n\t\t\t\tres[key] = value\n\treturn MappingNode(res)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\treturn reduce(lambda x, y: x.deep_merge(y), nodes, nodes[0])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif isinstance(value, MappingNode):\n\t\t\t\tif key not in merged:\n\t\t\t\t\tmerged[key] = value\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn MappingNode(merged)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"integers\", metavar=\"N\", type=int, nargs=\"+\", help=\"an integer for the accumulator\")\n\tparser.add_argument(\"--sum\", \"-s\", action=\"store_true\", help=\"sum the integers (default: find the max)\")\n\tparser.add_argument(\"--file\", \"-f\", metavar=\"FILE\", type=argparse.FileType('r'), help=\"input file\")\n\tparser.add_argument(\"--list\", \"-l\", metavar=\"LIST\", type=str, help=\"comma separated list of arguments\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"print additional output\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", help=\"print debugging output\")\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser=argparse.ArgumentParser()\n\tparser.add_argument('-t','--text',action='store',dest='text',help='The text to be encrypted')\n\tparser.add_argument('-i','--input',action='store',dest='input',help='The input file to be encrypted')\n\tparser.add_argument('-o','--output',action='store',dest='output',help='The output file to be encrypted')\n\tparser.add_argument('-k','--key',action='store',dest='key',help='The key to be used')\n\tparser.add_argument('-s','--symbols',action='store',dest='symbols',help='The number of symbols to be used')\n\tparser.add_argument('-a','--algorithm',action='store',dest='algorithm',help='The algorithm to be used')\n\tparser.add_argument('-n','--normalize',action='store',dest='normalize',help='The normalization method to be used')\n\tparser.add_argument('-d','--debug',action='store_true',dest='debug',help='Turn debug logging on')\n\tparser.add_argument('-u','--unicode',action='store_true',dest='unicode',help='Turn unicode output on')\n\tparser.add_argument('", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t# Configure the argument parser\n\tparser = argparse.ArgumentParser(description='Process arguments for the file_to_sqlite.py script')\n\tparser.add_argument('--input_file', help='The input file to parse into the sqlite database. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--database_file', help='The SQLite file to which to write the data. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--table', help='The name of the SQLite table to which to write the data. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--quiet', help='Suppress the log output', action = 'store_true')\n\tparser.add_argument('--debug', help='Enable debug output', action = 'store_true')\n\tparser.add_argument('--log_file', help='The log file to write debug output to. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--verbose', help='Enable verbose output', action = 'store_true')\n\t# Parse the arguments passed to the script\n\targs = parser.parse_args(arguments)\n\n\t# Return the parsed", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Adds a specified number of random numbers to a specified file\")\n\tparser.add_argument('file', help='The file to which the numbers should be added')\n\tparser.add_argument('number_of_numbers', nargs='?', type=int, default=10, help='The number of numbers to add')\n\tparser.add_argument('-s', '--seed', type=int, help='The seed for the random number generation')\n\t\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"Generate a random number of lines of the specified length, with the specified characters\")\n\tparser.add_argument(\"-n\", \"--lines\", dest=\"lines\", type=int, default=1, help=\"number of lines to generate\")\n\tparser.add_argument(\"-l\", \"--length\", dest=\"length\", type=int, default=1, help=\"length of each line\")\n\tparser.add_argument(\"-c\", \"--characters\", dest=\"characters\", type=str, default=\"abcdefghijklmnopqrstuvwxyz\", help=\"characters to use\")\n\tparser.add_argument(\"-r\", \"--random\", dest=\"random\", action=\"store_true\", default=False, help=\"use random.randint() instead of random.choice()\")\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", type=str, help=\"write output to FILE instead of stdout\")\n\treturn parser.parse_args(*arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-o\", \"--output\", type=str, required=True, help=\"The output filename\")\n\tparser.add_argument(\"-n\", \"--name\", type=str, required=True, help=\"The name of the model\")\n\tparser.add_argument(\"-d\", \"--dimensions\", type=int, default=100, help=\"The size of the vectors\")\n\tparser.add_argument(\"-w\", \"--window\", type=int, default=10, help=\"The window size\")\n\tparser.add_argument(\"-t\", \"--iterations\", type=int, default=5, help=\"The number of iterations\")\n\tparser.add_argument(\"-s\", \"--sample\", type=int, default=1e-3, help=\"The sampling threshold\")\n\tparser.add_argument(\"-c\", \"--cutoff\", type=int, default=10000, help=\"The cutoff value for the model\")\n\tparser.add_argument(\"-r\", \"--repetitions\", type=int, default=1, help=\"The number of repetitions\")\n\tparser.add_argument(\"-l\", \"--languages\", type=str, nargs='+', required=True", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Run a simulation and plot its results', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument('--number_of_particles', type=int, default=500, help='number of particles in the simulation')\n\tparser.add_argument('--simulation_duration', type=float, default=100.0, help='duration of the simulation')\n\tparser.add_argument('--output_file', type=str, default='output.csv', help='name of the output CSV file')\n\tparser.add_argument('--initial_speed_range', type=float, default=0.01, help='range of the speed of the particles')\n\tparser.add_argument('--initial_mass_range', type=float, default=0.5, help='range of the mass of the particles')\n\tparser.add_argument('--initial_position_range', type=float, default=0.5, help='range of the initial position of the particles')\n\tparser.add_argument('--plot_file', type=str, default='plot.png', help='name of the plot file')\n\tparser.add_argument('--plot_x_label', type=str, default='x [m]', help='x axis label", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"integers\", metavar=\"N\", type=int, nargs=\"+\", help=\"an integer for the accumulator\")\n\tparser.add_argument(\"--sum\", dest=\"accumulate\", action=\"store_const\", const=sum, default=max, help=\"sum the integers (default: find the max)\")\n\n\t# if len(sys.argv) == 1:\n\t# \tparser.print_help()\n\t# \tsys.exit(1)\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\"--in_dir\", \"-i\", help=\"directory with input files\", required=True)\n\tparser.add_argument(\"--out_dir\", \"-o\", help=\"directory with output files\", required=True)\n\tparser.add_argument(\"--log_dir\", \"-l\", help=\"directory with log files\", required=True)\n\n\targs = parser.parse_args(arguments)\n\tif len(sys.argv) == 1:\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\treturn args\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\n\t\t\"--debug\",\n\t\taction=\"store_true\",\n\t\thelp=\"Enable debug mode and print debug information to the console\",\n\t\tdefault=False,\n\t)\n\n\tparser.add_argument(\n\t\t\"--test\",\n\t\taction=\"store_true\",\n\t\thelp=\"Enable test mode and print test information to the console\",\n\t\tdefault=False,\n\t)\n\n\tparser.add_argument(\n\t\t\"--verbose\",\n\t\taction=\"store_true\",\n\t\thelp=\"Enable verbose mode and print verbose information to the console\",\n\t\tdefault=False,\n\t)\n\n\tif arguments:\n\t\tparser.add_argument(\n\t\t\t\"--args\",\n\t\t\thelp=\"Additional arguments to pass to the script\",\n\t\t\tnargs=\"*\",\n\t\t\tdefault=[],\n\t\t)\n\n\targs = parser.parse_args()\n\tif arguments:\n\t\targs.args = arguments\n\treturn args\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"integers\", metavar=\"N\", type=int, nargs=\"+\", help=\"an integer for the accumulator\")\n\tparser.add_argument(\"--sum\", \"-s\", action=\"store_true\", help=\"sum the integers (default: find the max)\")\n\tparser.add_argument(\"--file\", \"-f\", metavar=\"FILE\", type=argparse.FileType('r'), help=\"input file\")\n\tparser.add_argument(\"--list\", \"-l\", metavar=\"LIST\", type=str, help=\"comma separated list of arguments\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"print additional output\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", help=\"print debugging output\")\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser=argparse.ArgumentParser()\n\tparser.add_argument('-t','--text',action='store',dest='text',help='The text to be encrypted')\n\tparser.add_argument('-i','--input',action='store',dest='input',help='The input file to be encrypted')\n\tparser.add_argument('-o','--output',action='store',dest='output',help='The output file to be encrypted')\n\tparser.add_argument('-k','--key',action='store',dest='key',help='The key to be used')\n\tparser.add_argument('-s','--symbols',action='store',dest='symbols',help='The number of symbols to be used')\n\tparser.add_argument('-a','--algorithm',action='store',dest='algorithm',help='The algorithm to be used')\n\tparser.add_argument('-n','--normalize',action='store',dest='normalize',help='The normalization method to be used')\n\tparser.add_argument('-d','--debug',action='store_true',dest='debug',help='Turn debug logging on')\n\tparser.add_argument('-u','--unicode',action='store_true',dest='unicode',help='Turn unicode output on')\n\tparser.add_argument('", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t# Configure the argument parser\n\tparser = argparse.ArgumentParser(description='Process arguments for the file_to_sqlite.py script')\n\tparser.add_argument('--input_file', help='The input file to parse into the sqlite database. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--database_file', help='The SQLite file to which to write the data. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--table', help='The name of the SQLite table to which to write the data. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--quiet', help='Suppress the log output', action = 'store_true')\n\tparser.add_argument('--debug', help='Enable debug output', action = 'store_true')\n\tparser.add_argument('--log_file', help='The log file to write debug output to. If this is not provided, the script will attempt to read from standard input')\n\tparser.add_argument('--verbose', help='Enable verbose output', action = 'store_true')\n\t# Parse the arguments passed to the script\n\targs = parser.parse_args(arguments)\n\n\t# Return the parsed", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Adds a specified number of random numbers to a specified file\")\n\tparser.add_argument('file', help='The file to which the numbers should be added')\n\tparser.add_argument('number_of_numbers', nargs='?', type=int, default=10, help='The number of numbers to add')\n\tparser.add_argument('-s', '--seed', type=int, help='The seed for the random number generation')\n\t\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"Generate a random number of lines of the specified length, with the specified characters\")\n\tparser.add_argument(\"-n\", \"--lines\", dest=\"lines\", type=int, default=1, help=\"number of lines to generate\")\n\tparser.add_argument(\"-l\", \"--length\", dest=\"length\", type=int, default=1, help=\"length of each line\")\n\tparser.add_argument(\"-c\", \"--characters\", dest=\"characters\", type=str, default=\"abcdefghijklmnopqrstuvwxyz\", help=\"characters to use\")\n\tparser.add_argument(\"-r\", \"--random\", dest=\"random\", action=\"store_true\", default=False, help=\"use random.randint() instead of random.choice()\")\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", type=str, help=\"write output to FILE instead of stdout\")\n\treturn parser.parse_args(*arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-o\", \"--output\", type=str, required=True, help=\"The output filename\")\n\tparser.add_argument(\"-n\", \"--name\", type=str, required=True, help=\"The name of the model\")\n\tparser.add_argument(\"-d\", \"--dimensions\", type=int, default=100, help=\"The size of the vectors\")\n\tparser.add_argument(\"-w\", \"--window\", type=int, default=10, help=\"The window size\")\n\tparser.add_argument(\"-t\", \"--iterations\", type=int, default=5, help=\"The number of iterations\")\n\tparser.add_argument(\"-s\", \"--sample\", type=int, default=1e-3, help=\"The sampling threshold\")\n\tparser.add_argument(\"-c\", \"--cutoff\", type=int, default=10000, help=\"The cutoff value for the model\")\n\tparser.add_argument(\"-r\", \"--repetitions\", type=int, default=1, help=\"The number of repetitions\")\n\tparser.add_argument(\"-l\", \"--languages\", type=str, nargs='+', required=True", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Run a simulation and plot its results', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument('--number_of_particles', type=int, default=500, help='number of particles in the simulation')\n\tparser.add_argument('--simulation_duration', type=float, default=100.0, help='duration of the simulation')\n\tparser.add_argument('--output_file', type=str, default='output.csv', help='name of the output CSV file')\n\tparser.add_argument('--initial_speed_range', type=float, default=0.01, help='range of the speed of the particles')\n\tparser.add_argument('--initial_mass_range', type=float, default=0.5, help='range of the mass of the particles')\n\tparser.add_argument('--initial_position_range', type=float, default=0.5, help='range of the initial position of the particles')\n\tparser.add_argument('--plot_file', type=str, default='plot.png', help='name of the plot file')\n\tparser.add_argument('--plot_x_label', type=str, default='x [m]', help='x axis label", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"integers\", metavar=\"N\", type=int, nargs=\"+\", help=\"an integer for the accumulator\")\n\tparser.add_argument(\"--sum\", dest=\"accumulate\", action=\"store_const\", const=sum, default=max, help=\"sum the integers (default: find the max)\")\n\n\t# if len(sys.argv) == 1:\n\t# \tparser.print_help()\n\t# \tsys.exit(1)\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\"--in_dir\", \"-i\", help=\"directory with input files\", required=True)\n\tparser.add_argument(\"--out_dir\", \"-o\", help=\"directory with output files\", required=True)\n\tparser.add_argument(\"--log_dir\", \"-l\", help=\"directory with log files\", required=True)\n\n\targs = parser.parse_args(arguments)\n\tif len(sys.argv) == 1:\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\n\t\t\"--debug\",\n\t\taction=\"store_true\",\n\t\thelp=\"Enable debug mode and print debug information to the console\",\n\t\tdefault=False,\n\t)\n\n\tparser.add_argument(\n\t\t\"--test\",\n\t\taction=\"store_true\",\n\t\thelp=\"Enable test mode and print test information to the console\",\n\t\tdefault=False,\n\t)\n\n\tparser.add_argument(\n\t\t\"--verbose\",\n\t\taction=\"store_true\",\n\t\thelp=\"Enable verbose mode and print verbose information to the console\",\n\t\tdefault=False,\n\t)\n\n\tif arguments:\n\t\tparser.add_argument(\n\t\t\t\"--args\",\n\t\t\thelp=\"Additional arguments to pass to the script\",\n\t\t\tnargs=\"*\",\n\t\t\tdefault=[],\n\t\t)\n\n\targs = parser.parse_args()\n\tif arguments:\n\t\targs.args = arguments\n\treturn args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(k.replace(\"_\", \"-\") for k in parser.parse_args().__dict__ if not k.startswith(\"_\"))\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser.parse_known_args().__dict__.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(k for k, v in vars(parser.parse_args([])).items())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join([f for f in parser._get_option_string_actions()])\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(flag for flag in parser._option_string_actions.keys())\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(sorted(map(lambda x: x.replace(\"-\", \"\"), parser.option_list)))\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targs = parser.parse_args([])\n\tflags = [f for f in dir(args) if not f.startswith(\"_\")]\n\treturn \" \".join(f\"--{f}\" for f in flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(k.replace(\"_\", \"-\") for k in parser.parse_args().__dict__ if not k.startswith(\"_\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser.parse_known_args().__dict__.keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions.keys())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(k for k, v in vars(parser.parse_args([])).items())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join([f for f in parser._get_option_string_actions()])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(flag for flag in parser._option_string_actions.keys())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn \" \".join(sorted(map(lambda x: x.replace(\"-\", \"\"), parser.option_list)))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targs = parser.parse_args([])\n\tflags = [f for f in dir(args) if not f.startswith(\"_\")]\n\treturn \" \".join(f\"--{f}\" for f in flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\treturn ' '.join(parser._option_string_actions)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic.commands import Command\n\tfrom borgmatic.arguments import Arguments\n\n\targs = Arguments()\n\tcommands = [\n\t\tCommand(args, 'add', 'Add a job',\n\t\t\targs.job_schedule_add,\n\t\t\t['--dry-run', '--help', '--job-name', '--job-retention', '--job-schedule', '--job-source', '--job-type', '--verbose', '--yes'],\n\t\t\t[],\n\t\t\t['--job-schedule', '--job-source'],\n\t\t\t['--job-name'],\n\t\t),\n\t\tCommand(args, 'delete', 'Delete a job',\n\t\t\targs.job_schedule_delete,\n\t\t\t['--dry-run', '--help', '--job-name', '--verbose', '--yes'],\n\t\t\t[],\n\t\t\t['--job-name'],\n\t\t),\n\t\tCommand(args, 'disable', 'Disable a job',\n\t\t\targs.job_schedule_disable,\n\t\t\t['--dry-run', '--help', '--job-name', '--verbose', '--yes'],\n\t\t\t[],\n\t", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\"--bash-completion\", action=\"store_true\")\n\n\targs, argv = parser.parse_known_args()\n\n\tif args.bash_completion:\n\t\tparser.exit(\n\t\t\tstatus=0,\n\t\t\tmessage=f\"\"\"\\\n_borgmatic_completion() {{\n    local cur prev\n    COMPREPLY=()\n    cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n    prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n\n    # The list of commands\n    local commands=(\"${{COMP_WORDS[@]}}\")\n    commands=(\"${{commands[@]:0:{COMP_CWORD}}}\" --bash-completion)\n\n    local completions\n    completions=(\"${{commands[@]:0}}\" --bash-completion)\n    completions=(\"${{completions[@]:1}}\")\n\n    COMPREPLY=(\"${{completions[@]}}\")\n    return 0\n}}\ncomplete -F _borgmatic_completion borgmatic\n\"\"\",\n\t\t)\n\n", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\timport sys\n\timport inspect\n\tfrom argparse import ArgumentParser\n\tfrom . import __version__ as version\n\tfrom . import __doc__ as description\n\n\tparser = ArgumentParser(prog='borgmatic', add_help=False)\n\tparser.add_argument('-v', '--version', help='Show version and exit', action='version', version=version)\n\tparser.add_argument('-d', '--debug', help='Enable debug output', action='store_true')\n\tparser.add_argument('--list-jobs', help='List all jobs', action='store_true')\n\tparser.add_argument('--show-job', help='Show a job', action='store_true')\n\tparser.add_argument('--show-backup', help='Show a backup', action='store_true')\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_run_job = subparsers.add_parser('run-job', help='Run a job', add_help=False)\n\tparser_run_job.add_argument('job_name', help='Job name')\n\tparser_run_job.add_argument('--no-interactive', help='Do not ask questions', action='store_true')\n\tparser_run_job.", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t# NOTE: we don't use argparse.ArgumentParser.print_help because it doesn't produce the bash completion.\n\t# The bash completion is built-in into the argparse library.\n\t# See: https://bugs.python.org/issue45733\n\t# See: https://bugs.python.org/issue16160\n\t# See: https://bugs.python.org/issue16160#msg209643\n\t# See: https://bugs.python.org/issue31363\n\t# See: https://bugs.python.org/issue31363#msg197586\n\t# See: https://bugs.python.org/issue31363#msg197606\n\t# See: https://bugs.python.org/issue31363#msg197609\n\t# See: https://bugs.python.org/issue31363#msg200856\n\t# See: https://bugs.python.org/issue31363#msg200898\n\t# See: https://bugs.python.org/issue31363#msg200900", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tparser = argparse.ArgumentParser(prog='borgmatic', description='Deduplicating backups with Borg')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_show = subparsers.add_parser('show', help='show the current backup configuration')\n\tparser_show.add_argument('-v', '--verbose', action='store_true', help='show all configuration variables')\n\tparser_show.set_defaults(func=show_config)\n\n\tparser_deploy = subparsers.add_parser('deploy', help='deploy a backup configuration')\n\tparser_deploy.add_argument('--dry-run', action='store_true', help='do not actually deploy the configuration')\n\tparser_deploy.add_argument('--no-lock', action='store_true', help='do not lock the configuration')\n\tparser_deploy.add_argument('--no-unlock', action='store_true', help='do not unlock the configuration')\n\tparser_deploy.add_argument('-f', '--force', action='store_true', help='do not prompt before deploying')\n\tparser_deploy.add_argument('-n', '--name', default='default', help='name of the configuration to deploy')", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport sys\n\timport os\n\timport argparse\n\timport inspect\n\timport textwrap\n\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\n\t\t'--help',\n\t\taction='store_true',\n\t\thelp='Show this help message and exit.',\n\t)\n\tparser.add_argument(\n\t\t'--bash-completion',\n\t\taction='store_true',\n\t\thelp='Produce bash completion script.',\n\t)\n\n\targs, _ = parser.parse_known_args()\n\tif args.bash_completion:\n\t\tprint('Producing bash completion script.')\n\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tfor name, (func, signature) in borgmatic_commands():\n\t\tif name == 'bash-completion':\n\t\t\tcontinue\n\t\tsubparsers.add_parser(name, signature=signature, help=textwrap.dedent(func.__doc__))\n\n\tparser.print_help()\n\tparser.exit(0)\n\n", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(prog=\"borgmatic\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"count\", default=0, help=\"Verbose output.\")\n\tparser.add_argument(\"--quiet\", \"-q\", action=\"count\", default=0, help=\"Quiet output.\")\n\tparser.add_argument(\"--list\", action=\"store_true\", help=\"List available jobs.\")\n\tparser.add_argument(\"--list-jobs\", action=\"store_true\", help=\"List available jobs.\")\n\tparser.add_argument(\"--show\", \"-s\", \"--info\", action=\"store_true\", help=\"Show job info.\")\n\tparser.add_argument(\"--show-jobs\", action=\"store_true\", help=\"Show job info.\")\n\tparser.add_argument(\"--run\", action=\"store_true\", help=\"Run job.\")\n\tparser.add_argument(\"--run-jobs\", action=\"store_true\", help=\"Run job.\")\n\tparser.add_argument(\"--stop\", \"-S\", action=\"store_true\", help=\"Stop job.\")\n\tparser.add_argument(\"--stop-jobs\", action=\"store_true\", help=\"Stop job.\")\n\tparser.add_argument(\"--restart\", action=\"store_true\", help", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic import __version__\n\tfrom argparse import ArgumentParser\n\tfrom borgmatic.config import BorgmaticConfig\n\tfrom borgmatic.helpers import get_config_location\n\tfrom borgmatic.helpers import get_default_config\n\tfrom borgmatic.helpers import get_system_config_location\n\tfrom borgmatic.helpers import get_system_default_config\n\tfrom borgmatic.helpers import get_user_config_location\n\tfrom borgmatic.helpers import get_user_default_config\n\tfrom borgmatic.helpers import is_valid_config\n\tfrom borgmatic.helpers import parse_config\n\tfrom borgmatic.helpers import parse_config_string\n\tfrom borgmatic.helpers import parse_config_string_list\n\tfrom borgmatic.helpers import parse_config_value\n\tfrom borgmatic.helpers import parse_exclude\n\tfrom borgmatic.helpers import parse_exclude_string\n\tfrom borgmatic.helpers import parse_exclude_string_list\n\tfrom borgmatic.helpers import parse_exclude_value\n\tfrom borgmatic.helpers import prepare_exclude\n\t", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t# This regexp parses the borgmatic command-line arguments, and extracts the borgmatic\n\t# command from the arguments.\n\t#\n\t# The regexp first captures the command, then a list of arguments to the command, then a\n\t# list of sub-commands.\n\t#\n\t# It does not capture any arguments to sub-commands, because those are handled by the\n\t# borg sub-command completion function.\n\t#\n\t# It does not capture any sub-commands of the sub-command, because those are handled by\n\t# the borg sub-command completion function.\n\t#\n\t# It does not capture any arguments to the sub-command, because those are handled by the\n\t# borg sub-command completion function.\n\t#\n\t# The regexp assumes that all sub-command names are in lower case.\n\t#\n\t# The regexp assumes that all arguments to the command are in lower case.\n\t#\n\t# The regexp assumes that all sub-commands are in lower case.\n\t#\n\t# The regexp assumes that all arguments to sub-commands are in lower case.\n\t#\n\t# The regexp assumes that all arguments to the sub-command are in lower case.\n\t#\n\t# The regexp assumes that all arguments to the sub-command are", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport sys\n\tfrom tabulate import tabulate\n\tfrom argparse import ArgumentParser\n\tfrom borgmatic import __version__\n\tfrom borgmatic.run import main as run_main\n\tfrom borgmatic.config import Config\n\tfrom borgmatic.logger import get_logger\n\n\tlogger = get_logger()\n\n\tlogger.debug('bash_completion')\n\n\tparser = ArgumentParser(prog='borgmatic')\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\trun_parser = subparsers.add_parser('run', help='Run borgmatic')\n\trun_parser.add_argument('--config', default=b'/etc/borgmatic.conf', metavar='PATH', type=str,\n\t\t\t\t\t\t\thelp='Path to the configuration file')\n\trun_parser.add_argument('--log-level', default=b'info', metavar='LEVEL', type=str,\n\t\t\t\t\t\t\thelp='Log level (default: %(default)s)')\n\trun_parser.add_argument('--log-format', default=b'"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic.commands import Command\n\tfrom borgmatic.arguments import Arguments\n\n\targs = Arguments()\n\tcommands = [\n\t\tCommand(args, 'add', 'Add a job',\n\t\t\targs.job_schedule_add,\n\t\t\t['--dry-run', '--help', '--job-name', '--job-retention', '--job-schedule', '--job-source', '--job-type', '--verbose', '--yes'],\n\t\t\t[],\n\t\t\t['--job-schedule', '--job-source'],\n\t\t\t['--job-name'],\n\t\t),\n\t\tCommand(args, 'delete', 'Delete a job',\n\t\t\targs.job_schedule_delete,\n\t\t\t['--dry-run', '--help', '--job-name', '--verbose', '--yes'],\n\t\t\t[],\n\t\t\t['--job-name'],\n\t\t),\n\t\tCommand(args, 'disable', 'Disable a job',\n\t\t\targs.job_schedule_disable,\n\t\t\t['--dry-run', '--help', '--job-name', '--verbose', '--yes'],\n\t\t\t[],\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\"--bash-completion\", action=\"store_true\")\n\n\targs, argv = parser.parse_known_args()\n\n\tif args.bash_completion:\n\t\tparser.exit(\n\t\t\tstatus=0,\n\t\t\tmessage=f\"\"\"\\\n_borgmatic_completion() {{\n    local cur prev\n    COMPREPLY=()\n    cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n    prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n\n    # The list of commands\n    local commands=(\"${{COMP_WORDS[@]}}\")\n    commands=(\"${{commands[@]:0:{COMP_CWORD}}}\" --bash-completion)\n\n    local completions\n    completions=(\"${{commands[@]:0}}\" --bash-completion)\n    completions=(\"${{completions[@]:1}}\")\n\n    COMPREPLY=(\"${{completions[@]}}\")\n    return 0\n}}\ncomplete -F _borgmatic_completion borgmatic\n\"\"\",\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\timport sys\n\timport inspect\n\tfrom argparse import ArgumentParser\n\tfrom . import __version__ as version\n\tfrom . import __doc__ as description\n\n\tparser = ArgumentParser(prog='borgmatic', add_help=False)\n\tparser.add_argument('-v', '--version', help='Show version and exit', action='version', version=version)\n\tparser.add_argument('-d', '--debug', help='Enable debug output', action='store_true')\n\tparser.add_argument('--list-jobs', help='List all jobs', action='store_true')\n\tparser.add_argument('--show-job', help='Show a job', action='store_true')\n\tparser.add_argument('--show-backup', help='Show a backup', action='store_true')\n\n\tsubparsers = parser.add_subparsers()\n\n\tparser_run_job = subparsers.add_parser('run-job', help='Run a job', add_help=False)\n\tparser_run_job.add_argument('job_name', help='Job name')\n\tparser_run_job.add_argument('--no-interactive', help='Do not ask questions', action='store_true')\n\tparser_run_job.", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t# NOTE: we don't use argparse.ArgumentParser.print_help because it doesn't produce the bash completion.\n\t# The bash completion is built-in into the argparse library.\n\t# See: https://bugs.python.org/issue45733\n\t# See: https://bugs.python.org/issue16160\n\t# See: https://bugs.python.org/issue16160#msg209643\n\t# See: https://bugs.python.org/issue31363\n\t# See: https://bugs.python.org/issue31363#msg197586\n\t# See: https://bugs.python.org/issue31363#msg197606\n\t# See: https://bugs.python.org/issue31363#msg197609\n\t# See: https://bugs.python.org/issue31363#msg200856\n\t# See: https://bugs.python.org/issue31363#msg200898\n\t# See: https://bugs.python.org/issue31363#msg200900", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport sys\n\n\tparser = argparse.ArgumentParser(prog='borgmatic', description='Deduplicating backups with Borg')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_show = subparsers.add_parser('show', help='show the current backup configuration')\n\tparser_show.add_argument('-v', '--verbose', action='store_true', help='show all configuration variables')\n\tparser_show.set_defaults(func=show_config)\n\n\tparser_deploy = subparsers.add_parser('deploy', help='deploy a backup configuration')\n\tparser_deploy.add_argument('--dry-run', action='store_true', help='do not actually deploy the configuration')\n\tparser_deploy.add_argument('--no-lock', action='store_true', help='do not lock the configuration')\n\tparser_deploy.add_argument('--no-unlock', action='store_true', help='do not unlock the configuration')\n\tparser_deploy.add_argument('-f', '--force', action='store_true', help='do not prompt before deploying')\n\tparser_deploy.add_argument('-n', '--name', default='default', help='name of the configuration to deploy')", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport sys\n\timport os\n\timport argparse\n\timport inspect\n\timport textwrap\n\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument(\n\t\t'--help',\n\t\taction='store_true',\n\t\thelp='Show this help message and exit.',\n\t)\n\tparser.add_argument(\n\t\t'--bash-completion',\n\t\taction='store_true',\n\t\thelp='Produce bash completion script.',\n\t)\n\n\targs, _ = parser.parse_known_args()\n\tif args.bash_completion:\n\t\tprint('Producing bash completion script.')\n\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tfor name, (func, signature) in borgmatic_commands():\n\t\tif name == 'bash-completion':\n\t\t\tcontinue\n\t\tsubparsers.add_parser(name, signature=signature, help=textwrap.dedent(func.__doc__))\n\n\tparser.print_help()\n\tparser.exit(0)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(prog=\"borgmatic\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"count\", default=0, help=\"Verbose output.\")\n\tparser.add_argument(\"--quiet\", \"-q\", action=\"count\", default=0, help=\"Quiet output.\")\n\tparser.add_argument(\"--list\", action=\"store_true\", help=\"List available jobs.\")\n\tparser.add_argument(\"--list-jobs\", action=\"store_true\", help=\"List available jobs.\")\n\tparser.add_argument(\"--show\", \"-s\", \"--info\", action=\"store_true\", help=\"Show job info.\")\n\tparser.add_argument(\"--show-jobs\", action=\"store_true\", help=\"Show job info.\")\n\tparser.add_argument(\"--run\", action=\"store_true\", help=\"Run job.\")\n\tparser.add_argument(\"--run-jobs\", action=\"store_true\", help=\"Run job.\")\n\tparser.add_argument(\"--stop\", \"-S\", action=\"store_true\", help=\"Stop job.\")\n\tparser.add_argument(\"--stop-jobs\", action=\"store_true\", help=\"Stop job.\")\n\tparser.add_argument(\"--restart\", action=\"store_true\", help", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic import __version__\n\tfrom argparse import ArgumentParser\n\tfrom borgmatic.config import BorgmaticConfig\n\tfrom borgmatic.helpers import get_config_location\n\tfrom borgmatic.helpers import get_default_config\n\tfrom borgmatic.helpers import get_system_config_location\n\tfrom borgmatic.helpers import get_system_default_config\n\tfrom borgmatic.helpers import get_user_config_location\n\tfrom borgmatic.helpers import get_user_default_config\n\tfrom borgmatic.helpers import is_valid_config\n\tfrom borgmatic.helpers import parse_config\n\tfrom borgmatic.helpers import parse_config_string\n\tfrom borgmatic.helpers import parse_config_string_list\n\tfrom borgmatic.helpers import parse_config_value\n\tfrom borgmatic.helpers import parse_exclude\n\tfrom borgmatic.helpers import parse_exclude_string\n\tfrom borgmatic.helpers import parse_exclude_string_list\n\tfrom borgmatic.helpers import parse_exclude_value\n\tfrom borgmatic.helpers import prepare_exclude\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t# This regexp parses the borgmatic command-line arguments, and extracts the borgmatic\n\t# command from the arguments.\n\t#\n\t# The regexp first captures the command, then a list of arguments to the command, then a\n\t# list of sub-commands.\n\t#\n\t# It does not capture any arguments to sub-commands, because those are handled by the\n\t# borg sub-command completion function.\n\t#\n\t# It does not capture any sub-commands of the sub-command, because those are handled by\n\t# the borg sub-command completion function.\n\t#\n\t# It does not capture any arguments to the sub-command, because those are handled by the\n\t# borg sub-command completion function.\n\t#\n\t# The regexp assumes that all sub-command names are in lower case.\n\t#\n\t# The regexp assumes that all arguments to the command are in lower case.\n\t#\n\t# The regexp assumes that all sub-commands are in lower case.\n\t#\n\t# The regexp assumes that all arguments to sub-commands are in lower case.\n\t#\n\t# The regexp assumes that all arguments to the sub-command are in lower case.\n\t#\n\t# The regexp assumes that all arguments to the sub-command are", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport sys\n\tfrom tabulate import tabulate\n\tfrom argparse import ArgumentParser\n\tfrom borgmatic import __version__\n\tfrom borgmatic.run import main as run_main\n\tfrom borgmatic.config import Config\n\tfrom borgmatic.logger import get_logger\n\n\tlogger = get_logger()\n\n\tlogger.debug('bash_completion')\n\n\tparser = ArgumentParser(prog='borgmatic')\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\trun_parser = subparsers.add_parser('run', help='Run borgmatic')\n\trun_parser.add_argument('--config', default=b'/etc/borgmatic.conf', metavar='PATH', type=str,\n\t\t\t\t\t\t\thelp='Path to the configuration file')\n\trun_parser.add_argument('--log-level', default=b'info', metavar='LEVEL', type=str,\n\t\t\t\t\t\t\thelp='Log level (default: %(default)s)')\n\trun_parser.add_argument('--log-format', default=b'", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Generate a randomized version of a dataset')\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# create the parser for the \"global\" command\n\tglobal_parser = subparsers.add_parser(\"global\", help=\"global help\")\n\tglobal_parser.add_argument(\"--seed\", default=1, type=int, help=\"seed for random number generation\")\n\n\t# create the parser for the \"dataset\" command\n\tdataset_parser = subparsers.add_parser(\"dataset\", help=\"dataset help\")\n\n\t# now create subparsers for the \"dataset\" command\n\tdataset_subparsers = dataset_parser.add_subparsers(help='dataset sub-command help')\n\n\t# create the parser for the \"dataset create\" command\n\tdataset_create_parser = dataset_subparsers.add_parser(\"create\", help=\"create help\")\n\tdataset_create_parser.add_argument(\"--name\", required=True, help=\"name of the dataset to create\")\n\tdataset_create_parser.add_argument(\"--size\", default=1000, type=int, help", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Convert between different formats of the data in the Cytoscape webapp.',\n\t\tadd_help=False)\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\tparser.add_argument('-h', '--help', action='help', help='Show this help message and exit.')\n\tsubparsers = parser.add_subparsers(help='', dest='subparser_name')\n\n\tparser_global = subparsers.add_parser('global', help='Global options')\n\tparser_global.add_argument('-i', '--input', help='Input file', required=True)\n\tparser_global.add_argument('-o', '--output', help='Output file (or directory)', required=True)\n\tparser_global.add_argument('-f', '--format', help='Format of the input file', required=True)\n\tparser_global.set_defaults(func=lambda _: parser.print_help())\n\n\tparser_coords = subparsers.add_parser('coords', help='Convert to 3D coordinates')\n\tparser_coords.add_argument('-i', '--input', help='Input file',", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a Python program in a virtual environment.\",\n\t\tformatter_class=argparse.RawTextHelpFormatter,\n\t)\n\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\n\tparser_global = parser.add_argument_group(title=\"Global arguments\")\n\tparser_global.add_argument(\n\t\t\"--venv-name\",\n\t\tdefault=\"venv\",\n\t\thelp=\"Name of the virtual environment. Default is venv.\",\n\t)\n\tparser_global.add_argument(\n\t\t\"--venv-path\",\n\t\tdefault=os.path.join(\".\", \"venv\"),\n\t\thelp=\"Path to the virtual environment. Default is venv in the current directory.\",\n\t)\n\tparser_global.add_argument(\n\t\t\"--venv-python\",\n\t\thelp=\"Path to the virtual environment Python executable.\",\n\t)\n\tparser_global.add_argument(\n\t\t\"--no-download\",\n\t\taction=\"store_true\",\n\t\thelp=\"Do not download a copy of the Python interpreter. Useful if you already have a copy of Python installed.\",\n\t)\n\tparser_global.add_argument(", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=os.path.basename(sys.argv[0]),\n\t\tdescription='''\n\t\t\tA python script to convert the output of the GATK 4 GenotypeGVCFs tool into a VCF file.\n\t\t\t''',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\tparser = add_subparser(parser)\n\targs = parser.parse_args(unparsed_arguments)\n\treturn args\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Define a command-line argument subparser for each command this script implements.\n\tsubparsers = argparse.ArgumentParser(add_help=False).add_subparsers()\n\tfor command in commands:\n\t\t# Each subparser is defined as a separate ArgparseArgumentParser.\n\t\tsubparser = subparsers.add_parser(command)\n\t\t# The subparser inherits all global arguments by default.\n\t\tsubparser.set_defaults(subparser=command)\n\t\t# The subparser also inherits all arguments of the global parser.\n\t\tfor (argument_name, argument_value) in global_arguments.items():\n\t\t\tsubparser.add_argument(*argument_value.flags, **argument_value.kwargs)\n\t\t# The subparser also inherits all arguments of the command-specific parser.\n\t\tfor (argument_name, argument_value) in command_arguments[command].items():\n\t\t\tsubparser.add_argument(*argument_value.flags, **argument_value.kwargs)\n\n\t# Parse the arguments.\n\targuments = subparsers.parse_args(unparsed_arguments)\n\n\t# The \"global\" subparser inherits all arguments of the global parser.\n\tfor (argument_name, argument_value) in global_arguments.items", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"cwltool\", description=__doc__, add_help=False)\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\tsubparsers.required = True\n\tsubparsers.add_parser(\"convert\", help=\"convert a CWL description to a Workflow Description Language (WDL) description\")\n\tsubparsers.add_parser(\"validate\", help=\"validate a CWL description\")\n\tsubparsers.add_parser(\"exec\", help=\"execute a CWL description\")\n\tsubparsers.add_parser(\"init\", help=\"initialize a new CWL repository\")\n\tsubparsers.add_parser(\"lint\", help=\"lint a CWL description\")\n\tsubparsers.add_parser(\"pack\", help=\"pack CWL tool into a Docker container\")\n\tsubparsers.add_parser(\"extract\", help=\"extract CWL tool from a Docker container\")\n\tsubparsers.add_parser(\"help\", help=\"print this help message and exit\")\n\tsubparsers.add_parser(\"version\", help=\"print version number and exit\")\n\tsubparsers.add_parser(\"update\", help=\"update cwltool\")\n\tsubparsers.add_parser(\"install\",", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(title='Subcommands')\n\tparser_global = subparsers.add_parser('global')\n\tparser_global.set_defaults(func=global_config)\n\tparser_global.add_argument('--verbose', '-v', action='store_true')\n\n\tparser_list = subparsers.add_parser('list')\n\tparser_list.set_defaults(func=list_configs)\n\tparser_list.add_argument('--verbose', '-v', action='store_true')\n\n\tparser_apply = subparsers.add_parser('apply')\n\tparser_apply.set_defaults(func=apply_config)\n\tparser_apply.add_argument('--verbose', '-v', action='store_true')\n\tparser_apply.add_argument('config_name', help='Name of configuration group to use')\n\tparser_apply.add_argument('--no_confirm', '-y', action='store_true', default=False,\n\t\thelp='Skip confirmation when deleting configurations')\n\tparser_apply.add_argument('--no_prompt', '-n', action='store_true', default=False,\n\t\t", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\n\tsubparser_list = [\n\t\tsubparsers.add_parser(\"global\", help=\"global configuration\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"add\", help=\"add a new user\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"list\", help=\"list users\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"remove\", help=\"remove a user\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"set_password\", help=\"set a user's password\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"show\", help=\"show a user\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"unlock\", help=\"unlock a user\")\n\t]\n\n\tfor subparser in subparser_list:\n\t\tsubparser.add_argument(\n\t\t\t\"--insecure\", action=\"store_true\", help=\"insecurely accept", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Generate an HTML report for the given pytest results\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=f\"%(prog)s {__version__}\")\n\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\t# `global` subparser\n\tparser_global = subparsers.add_parser(\"global\", help=_(\"global\"))\n\tparser_global.add_argument(\"--output\", help=_(\"output file\"))\n\tparser_global.add_argument(\"--pytest_results_dir\", help=_(\"pytest results directory\"))\n\tparser_global.add_argument(\"--pytest_results_file\", help=_(\"pytest results file\"))\n\tparser_global.add_argument(\"--pytest_results_file_suffix\", help=_(\"pytest results file suffix\"))\n\tparser_global.add_argument(\"--pytest_results_file_regex\", help=_(\"pytest results file regex\"))\n\tparser_global.add_argument(\"--pytest_results_file_glob\", help=_(\"pytest results file glob\"))\n\tparser_global.add_argument(\"--", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s (v{})\".format(__version__))\n\t# Global arguments.\n\tparser.add_argument(\"--log-level\", dest=\"log_level\", default=None, help=\"set the logging level\")\n\tparser.add_argument(\"--log-file\", dest=\"log_file\", default=None, help=\"set the logging file\")\n\tparser.add_argument(\"--log-format\", dest=\"log_format\", default=None, help=\"set the logging format\")\n\tparser.add_argument(\"--log-to-stdout\", dest=\"log_to_stdout\", default=None, action=\"store_true\",\n\t\t\t\t\t\thelp=\"set the logging file\")\n\n\tsubparsers = parser.add_subparsers()\n\n\tsubparser_list = [\n\t\t\t\t\t  (\"global\", subparsers.add_parser(\"global\", help=\"global arguments\")),\n\t\t\t\t\t  (\"generate\", subparsers.add_parser(\"generate\", help=\"generate arguments\")),\n\t\t\t\t\t  (\"test\", subparsers.add_parser(\"test\", help=\"test arguments\")),\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Generate a randomized version of a dataset')\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# create the parser for the \"global\" command\n\tglobal_parser = subparsers.add_parser(\"global\", help=\"global help\")\n\tglobal_parser.add_argument(\"--seed\", default=1, type=int, help=\"seed for random number generation\")\n\n\t# create the parser for the \"dataset\" command\n\tdataset_parser = subparsers.add_parser(\"dataset\", help=\"dataset help\")\n\n\t# now create subparsers for the \"dataset\" command\n\tdataset_subparsers = dataset_parser.add_subparsers(help='dataset sub-command help')\n\n\t# create the parser for the \"dataset create\" command\n\tdataset_create_parser = dataset_subparsers.add_parser(\"create\", help=\"create help\")\n\tdataset_create_parser.add_argument(\"--name\", required=True, help=\"name of the dataset to create\")\n\tdataset_create_parser.add_argument(\"--size\", default=1000, type=int, help", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Convert between different formats of the data in the Cytoscape webapp.',\n\t\tadd_help=False)\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\tparser.add_argument('-h', '--help', action='help', help='Show this help message and exit.')\n\tsubparsers = parser.add_subparsers(help='', dest='subparser_name')\n\n\tparser_global = subparsers.add_parser('global', help='Global options')\n\tparser_global.add_argument('-i', '--input', help='Input file', required=True)\n\tparser_global.add_argument('-o', '--output', help='Output file (or directory)', required=True)\n\tparser_global.add_argument('-f', '--format', help='Format of the input file', required=True)\n\tparser_global.set_defaults(func=lambda _: parser.print_help())\n\n\tparser_coords = subparsers.add_parser('coords', help='Convert to 3D coordinates')\n\tparser_coords.add_argument('-i', '--input', help='Input file',", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a Python program in a virtual environment.\",\n\t\tformatter_class=argparse.RawTextHelpFormatter,\n\t)\n\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\n\tparser_global = parser.add_argument_group(title=\"Global arguments\")\n\tparser_global.add_argument(\n\t\t\"--venv-name\",\n\t\tdefault=\"venv\",\n\t\thelp=\"Name of the virtual environment. Default is venv.\",\n\t)\n\tparser_global.add_argument(\n\t\t\"--venv-path\",\n\t\tdefault=os.path.join(\".\", \"venv\"),\n\t\thelp=\"Path to the virtual environment. Default is venv in the current directory.\",\n\t)\n\tparser_global.add_argument(\n\t\t\"--venv-python\",\n\t\thelp=\"Path to the virtual environment Python executable.\",\n\t)\n\tparser_global.add_argument(\n\t\t\"--no-download\",\n\t\taction=\"store_true\",\n\t\thelp=\"Do not download a copy of the Python interpreter. Useful if you already have a copy of Python installed.\",\n\t)\n\tparser_global.add_argument(", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=os.path.basename(sys.argv[0]),\n\t\tdescription='''\n\t\t\tA python script to convert the output of the GATK 4 GenotypeGVCFs tool into a VCF file.\n\t\t\t''',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\tparser = add_subparser(parser)\n\targs = parser.parse_args(unparsed_arguments)\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Define a command-line argument subparser for each command this script implements.\n\tsubparsers = argparse.ArgumentParser(add_help=False).add_subparsers()\n\tfor command in commands:\n\t\t# Each subparser is defined as a separate ArgparseArgumentParser.\n\t\tsubparser = subparsers.add_parser(command)\n\t\t# The subparser inherits all global arguments by default.\n\t\tsubparser.set_defaults(subparser=command)\n\t\t# The subparser also inherits all arguments of the global parser.\n\t\tfor (argument_name, argument_value) in global_arguments.items():\n\t\t\tsubparser.add_argument(*argument_value.flags, **argument_value.kwargs)\n\t\t# The subparser also inherits all arguments of the command-specific parser.\n\t\tfor (argument_name, argument_value) in command_arguments[command].items():\n\t\t\tsubparser.add_argument(*argument_value.flags, **argument_value.kwargs)\n\n\t# Parse the arguments.\n\targuments = subparsers.parse_args(unparsed_arguments)\n\n\t# The \"global\" subparser inherits all arguments of the global parser.\n\tfor (argument_name, argument_value) in global_arguments.items", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"cwltool\", description=__doc__, add_help=False)\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\tsubparsers.required = True\n\tsubparsers.add_parser(\"convert\", help=\"convert a CWL description to a Workflow Description Language (WDL) description\")\n\tsubparsers.add_parser(\"validate\", help=\"validate a CWL description\")\n\tsubparsers.add_parser(\"exec\", help=\"execute a CWL description\")\n\tsubparsers.add_parser(\"init\", help=\"initialize a new CWL repository\")\n\tsubparsers.add_parser(\"lint\", help=\"lint a CWL description\")\n\tsubparsers.add_parser(\"pack\", help=\"pack CWL tool into a Docker container\")\n\tsubparsers.add_parser(\"extract\", help=\"extract CWL tool from a Docker container\")\n\tsubparsers.add_parser(\"help\", help=\"print this help message and exit\")\n\tsubparsers.add_parser(\"version\", help=\"print version number and exit\")\n\tsubparsers.add_parser(\"update\", help=\"update cwltool\")\n\tsubparsers.add_parser(\"install\",", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(title='Subcommands')\n\tparser_global = subparsers.add_parser('global')\n\tparser_global.set_defaults(func=global_config)\n\tparser_global.add_argument('--verbose', '-v', action='store_true')\n\n\tparser_list = subparsers.add_parser('list')\n\tparser_list.set_defaults(func=list_configs)\n\tparser_list.add_argument('--verbose', '-v', action='store_true')\n\n\tparser_apply = subparsers.add_parser('apply')\n\tparser_apply.set_defaults(func=apply_config)\n\tparser_apply.add_argument('--verbose', '-v', action='store_true')\n\tparser_apply.add_argument('config_name', help='Name of configuration group to use')\n\tparser_apply.add_argument('--no_confirm', '-y', action='store_true', default=False,\n\t\thelp='Skip confirmation when deleting configurations')\n\tparser_apply.add_argument('--no_prompt', '-n', action='store_true', default=False,\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\n\tsubparser_list = [\n\t\tsubparsers.add_parser(\"global\", help=\"global configuration\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"add\", help=\"add a new user\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"list\", help=\"list users\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"remove\", help=\"remove a user\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"set_password\", help=\"set a user's password\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"show\", help=\"show a user\")\n\t]\n\n\tsubparser_list += [\n\t\tsubparsers.add_parser(\"unlock\", help=\"unlock a user\")\n\t]\n\n\tfor subparser in subparser_list:\n\t\tsubparser.add_argument(\n\t\t\t\"--insecure\", action=\"store_true\", help=\"insecurely accept", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Generate an HTML report for the given pytest results\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=f\"%(prog)s {__version__}\")\n\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\t# `global` subparser\n\tparser_global = subparsers.add_parser(\"global\", help=_(\"global\"))\n\tparser_global.add_argument(\"--output\", help=_(\"output file\"))\n\tparser_global.add_argument(\"--pytest_results_dir\", help=_(\"pytest results directory\"))\n\tparser_global.add_argument(\"--pytest_results_file\", help=_(\"pytest results file\"))\n\tparser_global.add_argument(\"--pytest_results_file_suffix\", help=_(\"pytest results file suffix\"))\n\tparser_global.add_argument(\"--pytest_results_file_regex\", help=_(\"pytest results file regex\"))\n\tparser_global.add_argument(\"--pytest_results_file_glob\", help=_(\"pytest results file glob\"))\n\tparser_global.add_argument(\"--", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s (v{})\".format(__version__))\n\t# Global arguments.\n\tparser.add_argument(\"--log-level\", dest=\"log_level\", default=None, help=\"set the logging level\")\n\tparser.add_argument(\"--log-file\", dest=\"log_file\", default=None, help=\"set the logging file\")\n\tparser.add_argument(\"--log-format\", dest=\"log_format\", default=None, help=\"set the logging format\")\n\tparser.add_argument(\"--log-to-stdout\", dest=\"log_to_stdout\", default=None, action=\"store_true\",\n\t\t\t\t\t\thelp=\"set the logging file\")\n\n\tsubparsers = parser.add_subparsers()\n\n\tsubparser_list = [\n\t\t\t\t\t  (\"global\", subparsers.add_parser(\"global\", help=\"global arguments\")),\n\t\t\t\t\t  (\"generate\", subparsers.add_parser(\"generate\", help=\"generate arguments\")),\n\t\t\t\t\t  (\"test\", subparsers.add_parser(\"test\", help=\"test arguments\")),\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tremaining = []\n\n\twhile len(unparsed_arguments) > 0:\n\t\tfor name, parser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tresult[name] = parser.parse_args(unparsed_arguments)\n\t\t\texcept SystemExit:\n\t\t\t\t# The command line is invalid, and argparse will have already printed an error\n\t\t\t\t# message.\n\t\t\t\tbreak\n\n\t\t\tif result[name].action and result[name].action in (\"show\", \"delete\", \"create\", \"update\"):\n\t\t\t\tresult[name].repo = result[name].repository\n\t\t\t\tresult[name].path = result[name].path\n\t\t\t\tresult[name].package = result[name].package\n\t\t\t\tresult[name].version = result[name].version\n\t\t\t\tresult[name].commit = result[name].commit\n\t\t\t\tresult[name].url = result[name].url\n\t\t\t\tresult[name].hash = result[name].hash\n\t\t\t\tdel result[name].repository\n\t\t\t\tdel result[name].path\n\t\t\t\tdel result[name].package\n\t\t\t\tdel result[name].version\n\t\t", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining = list(unparsed_arguments)\n\tparsed = {}\n\twhile remaining:\n\t\taction_name = remaining[0]\n\t\tif action_name in subparsers:\n\t\t\tparser = subparsers[action_name]\n\t\t\tnamespace = parser.parse_args(remaining[1:])\n\t\t\tparsed[action_name] = namespace\n\t\t\tremaining = []\n\t\telse:\n\t\t\treturn None, remaining\n\treturn parsed, remaining\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_actions = {}\n\tfor subparser_name, parser in subparsers.iteritems():\n\t\tparser.add_argument('--repository', help='Path to repository directory')\n\t\tparser_actions[subparser_name] = parser.parse_known_args(unparsed_arguments)\n\treturn parser_actions\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = subparsers.choices['run']\n\targuments = parser.parse_args(unparsed_arguments)\n\treturn {\n\t\t'run': arguments,\n\t}, []\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif unparsed_argument in subparsers:\n\t\t\tsubparser_name, subparser_arguments = subparsers[unparsed_argument].parse_known_args()\n\t\t\tparsed_arguments[subparser_name] = subparser_arguments\n\t\telse:\n\t\t\tremaining_arguments.append(unparsed_argument)\n\n\treturn (parsed_arguments, remaining_arguments)\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = []\n\taction_maps = {name: subparsers[name]._actions for name in subparsers}\n\tfor arg in unparsed_arguments:\n\t\tfor name, actions in action_maps.items():\n\t\t\tfor action in actions:\n\t\t\t\tif isinstance(action, argparse._SubParsersAction):\n\t\t\t\t\tcontinue\n\t\t\t\tif action.option_strings:\n\t\t\t\t\tfor option in action.option_strings:\n\t\t\t\t\t\tif arg.startswith(option):\n\t\t\t\t\t\t\tif arg != option:\n\t\t\t\t\t\t\t\tremaining_arguments.append(arg[len(option):])\n\t\t\t\t\t\t\tremaining_arguments.extend(action.parse_known_args([arg])[0])\n\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tif arg in action.option_strings:\n\t\t\t\t\t\tremaining_arguments.extend(action.parse_known_args([arg])[0])\n\t\t\t\t\t\tbreak\n\treturn {name: subparsers[name].parse_args(remaining_arguments) for name in subparsers}, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_to_args = {}\n\tremaining_args = []\n\tfor arg in unparsed_arguments:\n\t\tclaimed = False\n\t\tfor subparser_name, parser in subparsers.items():\n\t\t\tif subparser_name not in parser_to_args:\n\t\t\t\ttry:\n\t\t\t\t\tparser_to_args[subparser_name] = parser.parse_args(arg.split())\n\t\t\t\t\tclaimed = True\n\t\t\t\t\tbreak\n\t\t\t\texcept argparse.ArgumentError as e:\n\t\t\t\t\t# If the subparser doesn't know about the argument, ignore it.\n\t\t\t\t\tpass\n\t\tif not claimed:\n\t\t\tremaining_args.append(arg)\n\treturn parser_to_args, remaining_args\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targuments = []\n\tparsed_arguments = {}\n\tfor argument in unparsed_arguments:\n\t\tfor name, subparser in subparsers.items():\n\t\t\tif argument.startswith(name) and len(argument) > len(name):\n\t\t\t\tif name not in parsed_arguments:\n\t\t\t\t\tparsed_arguments[name] = subparser.parse_args(arguments + unparsed_arguments)\n\t\t\t\t\targuments = []\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\targuments.append(argument)\n\treturn parsed_arguments, arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor unparsed_argument in unparsed_arguments:\n\t\t# Some subparsers may not have been created yet, so we need to keep looping until we get\n\t\t# through all arguments.\n\t\twhile True:\n\t\t\t# Attempt to parse the current argument with all subparsers.\n\t\t\tfor subparser in subparsers:\n\t\t\t\t# If the subparser has not been created yet, ignore it.\n\t\t\t\tif subparser not in parsed_arguments:\n\t\t\t\t\tcontinue\n\n\t\t\t\ttry:\n\t\t\t\t\tparsed_arguments[subparser]._parse_optional(unparsed_argument)\n\t\t\t\texcept argparse.ArgumentError:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# The argument was claimed by this subparser, so we're done.\n\t\t\t\tremaining_arguments.append(unparsed_argument)\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t# The argument was not claimed by any subparser, so we're done with this argument.\n\t\t\t\tremaining_arguments.append(unparsed_argument)\n\t\t\t\tbreak\n\n\t# Now that all arguments have", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tunparsed_arguments.insert(0, \"\") # the default subparser\n\tparsed_arguments = {}\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith(\"--\"):\n\t\t\t# parse the argument, it's a global option\n\t\t\tcontinue\n\t\tif arg.startswith(\"-\"):\n\t\t\t# parse the argument, it's a global option\n\t\t\tcontinue\n\t\tif arg in subparsers:\n\t\t\t# parse the argument, it's a subparser\n\t\t\tparsed_arguments[arg] = subparsers[arg].parse_args(unparsed_arguments)\n\t\t\tbreak\n\t\telse:\n\t\t\t# parse the argument, it's a command\n\t\t\tparsed_arguments[arg] = subparsers[arg].parse_args(unparsed_arguments)\n\t\t\tbreak\n\treturn parsed_arguments, unparsed_arguments[len(parsed_arguments):]\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining = list(unparsed_arguments)\n\tparsed = {}\n\twhile remaining:\n\t\taction_name = remaining[0]\n\t\tif action_name in subparsers:\n\t\t\tparser = subparsers[action_name]\n\t\t\tnamespace = parser.parse_args(remaining[1:])\n\t\t\tparsed[action_name] = namespace\n\t\t\tremaining = []\n\t\telse:\n\t\t\treturn None, remaining\n\treturn parsed, remaining\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_actions = {}\n\tfor subparser_name, parser in subparsers.iteritems():\n\t\tparser.add_argument('--repository', help='Path to repository directory')\n\t\tparser_actions[subparser_name] = parser.parse_known_args(unparsed_arguments)\n\treturn parser_actions\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = subparsers.choices['run']\n\targuments = parser.parse_args(unparsed_arguments)\n\treturn {\n\t\t'run': arguments,\n\t}, []\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif unparsed_argument in subparsers:\n\t\t\tsubparser_name, subparser_arguments = subparsers[unparsed_argument].parse_known_args()\n\t\t\tparsed_arguments[subparser_name] = subparser_arguments\n\t\telse:\n\t\t\tremaining_arguments.append(unparsed_argument)\n\n\treturn (parsed_arguments, remaining_arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = []\n\taction_maps = {name: subparsers[name]._actions for name in subparsers}\n\tfor arg in unparsed_arguments:\n\t\tfor name, actions in action_maps.items():\n\t\t\tfor action in actions:\n\t\t\t\tif isinstance(action, argparse._SubParsersAction):\n\t\t\t\t\tcontinue\n\t\t\t\tif action.option_strings:\n\t\t\t\t\tfor option in action.option_strings:\n\t\t\t\t\t\tif arg.startswith(option):\n\t\t\t\t\t\t\tif arg != option:\n\t\t\t\t\t\t\t\tremaining_arguments.append(arg[len(option):])\n\t\t\t\t\t\t\tremaining_arguments.extend(action.parse_known_args([arg])[0])\n\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tif arg in action.option_strings:\n\t\t\t\t\t\tremaining_arguments.extend(action.parse_known_args([arg])[0])\n\t\t\t\t\t\tbreak\n\treturn {name: subparsers[name].parse_args(remaining_arguments) for name in subparsers}, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_to_args = {}\n\tremaining_args = []\n\tfor arg in unparsed_arguments:\n\t\tclaimed = False\n\t\tfor subparser_name, parser in subparsers.items():\n\t\t\tif subparser_name not in parser_to_args:\n\t\t\t\ttry:\n\t\t\t\t\tparser_to_args[subparser_name] = parser.parse_args(arg.split())\n\t\t\t\t\tclaimed = True\n\t\t\t\t\tbreak\n\t\t\t\texcept argparse.ArgumentError as e:\n\t\t\t\t\t# If the subparser doesn't know about the argument, ignore it.\n\t\t\t\t\tpass\n\t\tif not claimed:\n\t\t\tremaining_args.append(arg)\n\treturn parser_to_args, remaining_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targuments = []\n\tparsed_arguments = {}\n\tfor argument in unparsed_arguments:\n\t\tfor name, subparser in subparsers.items():\n\t\t\tif argument.startswith(name) and len(argument) > len(name):\n\t\t\t\tif name not in parsed_arguments:\n\t\t\t\t\tparsed_arguments[name] = subparser.parse_args(arguments + unparsed_arguments)\n\t\t\t\t\targuments = []\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\targuments.append(argument)\n\treturn parsed_arguments, arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor unparsed_argument in unparsed_arguments:\n\t\t# Some subparsers may not have been created yet, so we need to keep looping until we get\n\t\t# through all arguments.\n\t\twhile True:\n\t\t\t# Attempt to parse the current argument with all subparsers.\n\t\t\tfor subparser in subparsers:\n\t\t\t\t# If the subparser has not been created yet, ignore it.\n\t\t\t\tif subparser not in parsed_arguments:\n\t\t\t\t\tcontinue\n\n\t\t\t\ttry:\n\t\t\t\t\tparsed_arguments[subparser]._parse_optional(unparsed_argument)\n\t\t\t\texcept argparse.ArgumentError:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# The argument was claimed by this subparser, so we're done.\n\t\t\t\tremaining_arguments.append(unparsed_argument)\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t# The argument was not claimed by any subparser, so we're done with this argument.\n\t\t\t\tremaining_arguments.append(unparsed_argument)\n\t\t\t\tbreak\n\n\t# Now that all arguments have", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tunparsed_arguments.insert(0, \"\") # the default subparser\n\tparsed_arguments = {}\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith(\"--\"):\n\t\t\t# parse the argument, it's a global option\n\t\t\tcontinue\n\t\tif arg.startswith(\"-\"):\n\t\t\t# parse the argument, it's a global option\n\t\t\tcontinue\n\t\tif arg in subparsers:\n\t\t\t# parse the argument, it's a subparser\n\t\t\tparsed_arguments[arg] = subparsers[arg].parse_args(unparsed_arguments)\n\t\t\tbreak\n\t\telse:\n\t\t\t# parse the argument, it's a command\n\t\t\tparsed_arguments[arg] = subparsers[arg].parse_args(unparsed_arguments)\n\t\t\tbreak\n\treturn parsed_arguments, unparsed_arguments[len(parsed_arguments):]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Parse and reformat a file containing a list of genomic intervals\")\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"valid subcommands\",\n\t\thelp=\"additional help\",\n\t\tdest=\"subparser_name\"\n\t)\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Run a command.\")\n\tparser.add_argument(\"--log-level\", type=str, default=\"info\",\n\t\tchoices=[\"critical\", \"error\", \"warning\", \"info\", \"debug\"],\n\t\thelp=\"Set the log level.\")\n\tsubparsers = parser.add_subparsers()\n\tsubparsers.required = True\n\tsubparsers.dest = \"command\"\n\tsubparsers.add_parser(\"create\", help=\"Create a new game.\")\n\tsubparsers.add_parser(\"run\", help=\"Run the game.\")\n\tsubparsers.add_parser(\"help\", help=\"Show this help message.\")\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"python -m ldtk2d\",\n\t\tdescription=\"Converts LDtk levels into an easy to use format for 2D games.\")\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s \" + __version__)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"The list of subcommands available.\")\n\n\tparser_export = subparsers.add_parser(\n\t\t\"export\",\n\t\tdescription=\"Converts a LDtk level into a game-ready format.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser_export.add_argument(\n\t\t\"--output\",\n\t\ttype=str,\n\t\tmetavar=\"<path>\",\n\t\thelp=\"Output file, defaults to stdout\",\n\t\tdefault=None)\n\tparser_export.add_argument(\n\t\t\"--no-comments\",\n\t\taction=\"store_true\",\n\t\thelp=\"Do not include comments and metadata in the output file\",\n\t\tdefault=False)\n\tparser_export.add_argument(\n\t\t\"--no-auto-tiles", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Check the integrity of the CVE database.\")\n\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"Commands\",\n\t\tdest=\"command\",\n\t\tmetavar=\"COMMAND\")\n\n\tchecker_parser = subparsers.add_parser(\n\t\t\"check\",\n\t\thelp=\"Check the integrity of the CVE database.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--cve-dir\",\n\t\tmetavar=\"DIR\",\n\t\tdefault=os.path.join(os.path.dirname(__file__), \"../cve-data\"),\n\t\thelp=\"Directory containing the CVE database.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--verbose\",\n\t\taction=\"store_true\",\n\t\thelp=\"Print informative messages.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--quiet\",\n\t\taction=\"store_true\",\n\t\thelp=\"Only print error messages.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--no-cve-cache\",\n\t\taction=\"store_true\",\n\t\thelp=\"Do not use the CVE cache.\")\n\n\tchecker_parser", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"A tool for creating and managing databases\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\t# create parser\n\tcreate_parser = subparsers.add_parser(\"create\",\n\t\t\t\t\t\t\t\t\t\thelp=\"Create a new database\")\n\tcreate_parser.add_argument(\"name\", help=\"The name of the database to create\")\n\tcreate_parser.add_argument(\"--path\", help=\"The path to the database\",\n\t\t\t\t\t\t\t   default=\"./data\")\n\tcreate_parser.add_argument(\"--overwrite\", help=\"Overwrite an existing database\",\n\t\t\t\t\t\t\t   action=\"store_true\")\n\tcreate_parser.add_argument(\"--force\", help=\"Forcibly overwrite an existing database\",\n\t\t\t\t\t\t\t   action=\"store_true\")\n\n\t# delete parser\n\tdelete_parser = subparsers.add_parser(\"delete\",\n\t\t\t\t\t\t\t\t\t\thelp=\"Delete an existing database\")\n\tdelete_parser.add_argument(\"name\", help=\"The name of the database to delete\")\n\n\t# list parser\n\tlist_parser = subparsers.add_parser", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='A tool for generating a random password.')\n\tparser.add_argument('-l', '--length', type=int, default=12, help='the length of the password')\n\tparser.add_argument('-s', '--symbols', action='store_true', help='include symbols in the password')\n\tparser.add_argument('-n', '--numbers', action='store_true', help='include numbers in the password')\n\tparser.add_argument('-u', '--uppercase', action='store_true', help='include uppercase letters in the password')\n\tparser.add_argument('-a', '--alphabets', action='store_true', help='include all alphabets in the password')\n\tparser.add_argument('-o', '--output', type=argparse.FileType('w'), help='file to write the password to')\n\tsubparsers = parser.add_subparsers(dest='mode')\n\tsubparsers.required = True\n\tsubparsers.add_parser('generate', help='generate a password of the specified length')\n\tsubparsers.add_parser('gen', help='generate a password of the specified length')\n\tsubparsers.add_parser('random', help='generate a password of", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='An argument parser for the game of 100')\n\tsubparsers = parser.add_subparsers(title='subcommands', dest='subcommand')\n\tsubparsers.required = True\n\n\t# Sub-parser for the new game command\n\tgame_parser = subparsers.add_parser('new', help='Start a new game')\n\tgame_parser.add_argument('--name', help='The name of the game to create')\n\n\t# Sub-parser for the play command\n\tplay_parser = subparsers.add_parser('play', help='Play the game')\n\tplay_parser.add_argument('--name', help='The name of the game to play')\n\tplay_parser.add_argument('--player', help='The name of the player to play')\n\n\t# Sub-parser for the list command\n\tlist_parser = subparsers.add_parser('list', help='List the games')\n\n\t# Sub-parser for the delete command\n\tdelete_parser = subparsers.add_parser('delete', help='Delete a game')\n\tdelete_parser.add_argument('--name', help='The name of the game to delete')\n\n\t# Sub-parser for the join command\n\tjoin_parser = subpar", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool for visualizing and manipulating the output from the \"\n\t\t\t\t\"SAT4J SAT solver.\")\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Add the available subcommands to the parser\n\tsubparsers.add_parser(\n\t\t\"convert\",\n\t\thelp=\"Convert a results file from SAT4J to a file for use with this tool.\")\n\tsubparsers.add_parser(\n\t\t\"visualize\",\n\t\thelp=\"Visualize a results file from SAT4J.\")\n\tsubparsers.add_parser(\n\t\t\"view\",\n\t\thelp=\"View a results file in your default text editor.\")\n\tsubparsers.add_parser(\n\t\t\"print\",\n\t\thelp=\"Print a results file to the standard output.\")\n\t\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Provide the directory with the input data and the output directory\")\n\tsubparsers = parser.add_subparsers()\n\t\n\t# parser for create_dataset\n\tparser_create_dataset = subparsers.add_parser(\"create_dataset\")\n\tparser_create_dataset.add_argument(\"--input_dir\", help=\"The input directory\", required=True)\n\tparser_create_dataset.add_argument(\"--output_dir\", help=\"The output directory\", required=True)\n\tparser_create_dataset.add_argument(\"--num_workers\", help=\"The number of workers for data loading\", type=int, required=True)\n\tparser_create_dataset.add_argument(\"--batch_size\", help=\"The batch size\", type=int, required=True)\n\tparser_create_dataset.add_argument(\"--data_name\", help=\"The name of the data\", required=True)\n\tparser_create_dataset.add_argument(\"--num_classes\", help=\"The number of classes\", type=int, required=True)\n\tparser_create_dataset.add_argument(\"--data_type\", help=\"The type of the data\", required=True)\n\tparser_create_dataset.add_argument(\"--split\", help=\"The", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Clean a CSV file and save it to a new file.',\n\t\tepilog='The default value of the destination file is the same as '\n\t\t\t'the source file with \"cleaned_\" prepended.')\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='commands', dest='command')\n\n\tparser_clean = subparsers.add_parser(\n\t\t'clean',\n\t\thelp='Clean a CSV file.',\n\t\tdescription='Clean a CSV file.',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=textwrap.dedent('''\\\n\t\t\tThe default value of the destination file is the same as the\n\t\t\tsource file with \"cleaned_\" prepended.\n\t\t\t'''))\n\tparser_clean.add_argument(\n\t\t'source_file',\n\t\thelp='The source CSV file.')\n\tparser_clean.add_argument(\n\t\t'destination_file',\n\t\tnargs='?',\n\t\thelp='The destination CSV file.')\n\n\tparser_rename = subparsers.add_parser(\n\t\t'rename',\n\t\thelp='Rename a CSV file.',\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Parse and reformat a file containing a list of genomic intervals\")\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"valid subcommands\",\n\t\thelp=\"additional help\",\n\t\tdest=\"subparser_name\"\n\t)\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Run a command.\")\n\tparser.add_argument(\"--log-level\", type=str, default=\"info\",\n\t\tchoices=[\"critical\", \"error\", \"warning\", \"info\", \"debug\"],\n\t\thelp=\"Set the log level.\")\n\tsubparsers = parser.add_subparsers()\n\tsubparsers.required = True\n\tsubparsers.dest = \"command\"\n\tsubparsers.add_parser(\"create\", help=\"Create a new game.\")\n\tsubparsers.add_parser(\"run\", help=\"Run the game.\")\n\tsubparsers.add_parser(\"help\", help=\"Show this help message.\")\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"python -m ldtk2d\",\n\t\tdescription=\"Converts LDtk levels into an easy to use format for 2D games.\")\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s \" + __version__)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"The list of subcommands available.\")\n\n\tparser_export = subparsers.add_parser(\n\t\t\"export\",\n\t\tdescription=\"Converts a LDtk level into a game-ready format.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser_export.add_argument(\n\t\t\"--output\",\n\t\ttype=str,\n\t\tmetavar=\"<path>\",\n\t\thelp=\"Output file, defaults to stdout\",\n\t\tdefault=None)\n\tparser_export.add_argument(\n\t\t\"--no-comments\",\n\t\taction=\"store_true\",\n\t\thelp=\"Do not include comments and metadata in the output file\",\n\t\tdefault=False)\n\tparser_export.add_argument(\n\t\t\"--no-auto-tiles", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Check the integrity of the CVE database.\")\n\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"Commands\",\n\t\tdest=\"command\",\n\t\tmetavar=\"COMMAND\")\n\n\tchecker_parser = subparsers.add_parser(\n\t\t\"check\",\n\t\thelp=\"Check the integrity of the CVE database.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--cve-dir\",\n\t\tmetavar=\"DIR\",\n\t\tdefault=os.path.join(os.path.dirname(__file__), \"../cve-data\"),\n\t\thelp=\"Directory containing the CVE database.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--verbose\",\n\t\taction=\"store_true\",\n\t\thelp=\"Print informative messages.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--quiet\",\n\t\taction=\"store_true\",\n\t\thelp=\"Only print error messages.\")\n\n\tchecker_parser.add_argument(\n\t\t\"--no-cve-cache\",\n\t\taction=\"store_true\",\n\t\thelp=\"Do not use the CVE cache.\")\n\n\tchecker_parser", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"A tool for creating and managing databases\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\t# create parser\n\tcreate_parser = subparsers.add_parser(\"create\",\n\t\t\t\t\t\t\t\t\t\thelp=\"Create a new database\")\n\tcreate_parser.add_argument(\"name\", help=\"The name of the database to create\")\n\tcreate_parser.add_argument(\"--path\", help=\"The path to the database\",\n\t\t\t\t\t\t\t   default=\"./data\")\n\tcreate_parser.add_argument(\"--overwrite\", help=\"Overwrite an existing database\",\n\t\t\t\t\t\t\t   action=\"store_true\")\n\tcreate_parser.add_argument(\"--force\", help=\"Forcibly overwrite an existing database\",\n\t\t\t\t\t\t\t   action=\"store_true\")\n\n\t# delete parser\n\tdelete_parser = subparsers.add_parser(\"delete\",\n\t\t\t\t\t\t\t\t\t\thelp=\"Delete an existing database\")\n\tdelete_parser.add_argument(\"name\", help=\"The name of the database to delete\")\n\n\t# list parser\n\tlist_parser = subparsers.add_parser", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='A tool for generating a random password.')\n\tparser.add_argument('-l', '--length', type=int, default=12, help='the length of the password')\n\tparser.add_argument('-s', '--symbols', action='store_true', help='include symbols in the password')\n\tparser.add_argument('-n', '--numbers', action='store_true', help='include numbers in the password')\n\tparser.add_argument('-u', '--uppercase', action='store_true', help='include uppercase letters in the password')\n\tparser.add_argument('-a', '--alphabets', action='store_true', help='include all alphabets in the password')\n\tparser.add_argument('-o', '--output', type=argparse.FileType('w'), help='file to write the password to')\n\tsubparsers = parser.add_subparsers(dest='mode')\n\tsubparsers.required = True\n\tsubparsers.add_parser('generate', help='generate a password of the specified length')\n\tsubparsers.add_parser('gen', help='generate a password of the specified length')\n\tsubparsers.add_parser('random', help='generate a password of", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='An argument parser for the game of 100')\n\tsubparsers = parser.add_subparsers(title='subcommands', dest='subcommand')\n\tsubparsers.required = True\n\n\t# Sub-parser for the new game command\n\tgame_parser = subparsers.add_parser('new', help='Start a new game')\n\tgame_parser.add_argument('--name', help='The name of the game to create')\n\n\t# Sub-parser for the play command\n\tplay_parser = subparsers.add_parser('play', help='Play the game')\n\tplay_parser.add_argument('--name', help='The name of the game to play')\n\tplay_parser.add_argument('--player', help='The name of the player to play')\n\n\t# Sub-parser for the list command\n\tlist_parser = subparsers.add_parser('list', help='List the games')\n\n\t# Sub-parser for the delete command\n\tdelete_parser = subparsers.add_parser('delete', help='Delete a game')\n\tdelete_parser.add_argument('--name', help='The name of the game to delete')\n\n\t# Sub-parser for the join command\n\tjoin_parser = subpar", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool for visualizing and manipulating the output from the \"\n\t\t\t\t\"SAT4J SAT solver.\")\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Add the available subcommands to the parser\n\tsubparsers.add_parser(\n\t\t\"convert\",\n\t\thelp=\"Convert a results file from SAT4J to a file for use with this tool.\")\n\tsubparsers.add_parser(\n\t\t\"visualize\",\n\t\thelp=\"Visualize a results file from SAT4J.\")\n\tsubparsers.add_parser(\n\t\t\"view\",\n\t\thelp=\"View a results file in your default text editor.\")\n\tsubparsers.add_parser(\n\t\t\"print\",\n\t\thelp=\"Print a results file to the standard output.\")\n\t\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Provide the directory with the input data and the output directory\")\n\tsubparsers = parser.add_subparsers()\n\t\n\t# parser for create_dataset\n\tparser_create_dataset = subparsers.add_parser(\"create_dataset\")\n\tparser_create_dataset.add_argument(\"--input_dir\", help=\"The input directory\", required=True)\n\tparser_create_dataset.add_argument(\"--output_dir\", help=\"The output directory\", required=True)\n\tparser_create_dataset.add_argument(\"--num_workers\", help=\"The number of workers for data loading\", type=int, required=True)\n\tparser_create_dataset.add_argument(\"--batch_size\", help=\"The batch size\", type=int, required=True)\n\tparser_create_dataset.add_argument(\"--data_name\", help=\"The name of the data\", required=True)\n\tparser_create_dataset.add_argument(\"--num_classes\", help=\"The number of classes\", type=int, required=True)\n\tparser_create_dataset.add_argument(\"--data_type\", help=\"The type of the data\", required=True)\n\tparser_create_dataset.add_argument(\"--split\", help=\"The", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Clean a CSV file and save it to a new file.',\n\t\tepilog='The default value of the destination file is the same as '\n\t\t\t'the source file with \"cleaned_\" prepended.')\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='commands', dest='command')\n\n\tparser_clean = subparsers.add_parser(\n\t\t'clean',\n\t\thelp='Clean a CSV file.',\n\t\tdescription='Clean a CSV file.',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=textwrap.dedent('''\\\n\t\t\tThe default value of the destination file is the same as the\n\t\t\tsource file with \"cleaned_\" prepended.\n\t\t\t'''))\n\tparser_clean.add_argument(\n\t\t'source_file',\n\t\thelp='The source CSV file.')\n\tparser_clean.add_argument(\n\t\t'destination_file',\n\t\tnargs='?',\n\t\thelp='The destination CSV file.')\n\n\tparser_rename = subparsers.add_parser(\n\t\t'rename',\n\t\thelp='Rename a CSV file.',\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infoblox_client/utils.py", "project": "infobloxopen/infoblox-client", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_length = len(response)\n\tpaged_response = []\n\tcurrent_page = 0\n\n\twhile True:\n\t\tif response_length <= max_results:\n\t\t\tpaged_response.append(response)\n\t\t\tbreak\n\t\telse:\n\t\t\tpaged_response.append(response[current_page: current_page + max_results])\n\t\t\tcurrent_page += max_results\n\t\t\tresponse_length -= max_results\n\n\treturn paged_response\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif not isinstance(response, list):\n\t\traise TypeError(\"WAPI response should be a list.\")\n\tnum_pages = len(response) // max_results\n\tnum_pages += 1 if len(response) % max_results != 0 else 0\n\tfor page in range(num_pages):\n\t\tyield response[page * max_results: (page + 1) * max_results]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile True:\n\t\tif len(response) <= max_results:\n\t\t\tyield response\n\t\t\tbreak\n\t\telse:\n\t\t\tyield response[i:i + max_results]\n\t\t\ti += max_results\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t# TODO: add support for pagination with token\n\t# if 'next_token' in response:\n\t#     token = response['next_token']\n\t# else:\n\t#     token = None\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif max_results > 200:\n\t\tmax_results = 200\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile True:\n\t\tresponse_page = response[i:i + max_results]\n\t\tif len(response_page) == 0:\n\t\t\tbreak\n\t\tyield response_page\n\t\ti += max_results\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_length = len(response)\n\tpage_size = max_results\n\n\tfor start_ind in xrange(0, response_length, page_size):\n\t\tyield response[start_ind:start_ind + page_size]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\treturn (response[i:i + max_results] for i in range(0, len(response), max_results))\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i: i + max_results]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_length = len(response)\n\tpaged_response = []\n\tcurrent_page = 0\n\n\twhile True:\n\t\tif response_length <= max_results:\n\t\t\tpaged_response.append(response)\n\t\t\tbreak\n\t\telse:\n\t\t\tpaged_response.append(response[current_page: current_page + max_results])\n\t\t\tcurrent_page += max_results\n\t\t\tresponse_length -= max_results\n\n\treturn paged_response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif not isinstance(response, list):\n\t\traise TypeError(\"WAPI response should be a list.\")\n\tnum_pages = len(response) // max_results\n\tnum_pages += 1 if len(response) % max_results != 0 else 0\n\tfor page in range(num_pages):\n\t\tyield response[page * max_results: (page + 1) * max_results]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t# TODO: add support for pagination with token\n\t# if 'next_token' in response:\n\t#     token = response['next_token']\n\t# else:\n\t#     token = None\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif max_results > 200:\n\t\tmax_results = 200\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile True:\n\t\tresponse_page = response[i:i + max_results]\n\t\tif len(response_page) == 0:\n\t\t\tbreak\n\t\tyield response_page\n\t\ti += max_results\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_length = len(response)\n\tpage_size = max_results\n\n\tfor start_ind in xrange(0, response_length, page_size):\n\t\tyield response[start_ind:start_ind + page_size]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\treturn (response[i:i + max_results] for i in range(0, len(response), max_results))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i: i + max_results]\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tif size.endswith(\"M\"):\n\t\treturn int(size[:-1]) * 1024 * 1024\n\telif size.endswith(\"K\"):\n\t\treturn int(size[:-1]) * 1024\n\telse:\n\t\treturn int(size)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_unit = size[-2:].upper()\n\tsize_num = size[:-2]\n\n\tif size_unit == 'MB':\n\t\tsize_in_bytes = int(size_num) * 1024 * 1024\n\telif size_unit == 'KB':\n\t\tsize_in_bytes = int(size_num) * 1024\n\telif size_unit == 'GB':\n\t\tsize_in_bytes = int(size_num) * 1024 * 1024 * 1024\n\telse:\n\t\tsize_in_bytes = 0\n\n\treturn size_in_bytes\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {\n\t\t'K': 1024,\n\t\t'M': 1024 ** 2,\n\t\t'G': 1024 ** 3,\n\t\t'T': 1024 ** 4,\n\t\t'P': 1024 ** 5,\n\t\t'E': 1024 ** 6,\n\t\t'Z': 1024 ** 7,\n\t\t'Y': 1024 ** 8\n\t}\n\n\tsize = size.upper()\n\n\tif not size[-1].isdigit():\n\t\traise ValueError('size must be a string representing a human readable file size')\n\n\treturn int(size[:-1]) * multiplier[size[-1]]\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t# Remove commas\n\tsize = size.replace(',', '')\n\n\t# Multiply factor by 1024 (2**10)\n\tsize_power = {\n\t\t'K': 1,\n\t\t'M': 2,\n\t\t'G': 3,\n\t\t'T': 4,\n\t\t'P': 5,\n\t\t'E': 6\n\t}\n\tfactor = 1024 ** size_power[size[-1]]\n\n\t# Convert to int\n\tsize = int(size[:-1])\n\treturn size * factor\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_name = ('B', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n\tnum = re.sub(r'([KMGTPEZY])$', r'\\1\\1', size)\n\tnum = re.sub(r'([kMGTPEZY])$', r'\\1\\1', num)\n\tnum = re.sub(r'([KMGTPEZY])', r'\\1\\1\\1', num)\n\tnum = re.sub(r'(.)', r'\\1\\1', num)\n\tfactor = 1000\n\tfor n, s in enumerate(size_name):\n\t\tif s in num:\n\t\t\tnum = num.replace(s, '')\n\t\t\tfactor = 1 << (n + 1) * 10\n\t\t\tbreak\n\treturn int(float(num) * factor)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tm = re.match(r'(\\d*\\.?\\d+) ?([KMGTPEZY]i?)?', size)\n\tif not m:\n\t\traise ValueError('invalid size: ' + size)\n\n\tnum, unit = m.groups()\n\tnum = int(num)\n\n\treturn num * 1024**_unit_to_bytes_exp(unit)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = {\n\t\t'K': 1024,\n\t\t'M': 1024 ** 2,\n\t\t'G': 1024 ** 3,\n\t\t'T': 1024 ** 4,\n\t\t'P': 1024 ** 5,\n\t\t'E': 1024 ** 6,\n\t}\n\tif not size[-1].isalpha():\n\t\traise ValueError('Invalid file size')\n\ttry:\n\t\tsize = float(size[:-1])\n\texcept ValueError:\n\t\traise ValueError('Invalid file size')\n\tif size < 0:\n\t\traise ValueError('Invalid file size')\n\treturn int(size * suffixes[size[-1].upper()])\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tif not size:\n\t\treturn 0\n\n\tmatch = re.search(r'^(\\d+)([KMGT])?$', size)\n\tif not match:\n\t\traise ValueError(f'Invalid size: {size}')\n\n\tsize = match.group(1)\n\tunits = match.group(2)\n\n\tif units:\n\t\treturn int(size) * 1024 ** {'K': 1, 'M': 2, 'G': 3, 'T': 4}[units]\n\telse:\n\t\treturn int(size)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmatch = re.search(r\"^(\\d+)\\s*([KMGTP]?i?B?)?$\", size)\n\tif match:\n\t\tnum = int(match.group(1))\n\t\tunit = match.group(2)\n\telse:\n\t\traise ValueError(f\"Invalid size: {size}\")\n\n\tif unit:\n\t\tunit = unit[0]\n\t\tif unit == \"K\":\n\t\t\tnum *= 1024\n\t\telif unit == \"M\":\n\t\t\tnum *= 1024 * 1024\n\t\telif unit == \"G\":\n\t\t\tnum *= 1024 * 1024 * 1024\n\t\telif unit == \"T\":\n\t\t\tnum *= 1024 * 1024 * 1024 * 1024\n\n\treturn num\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tif size[-2:] in {\"Ki\", \"Mi\", \"Gi\", \"Ti\"}:\n\t\tsize = size[:-2]\n\t\tif size[-1] == \"i\":\n\t\t\tsize = size[:-1]\n\t\tsize = int(size)\n\t\tif size[-2:] == \"Ki\":\n\t\t\tsize *= 1024\n\t\telif size[-2:] == \"Mi\":\n\t\t\tsize *= 1024 ** 2\n\t\telif size[-2:] == \"Gi\":\n\t\t\tsize *= 1024 ** 3\n\t\telif size[-2:] == \"Ti\":\n\t\t\tsize *= 1024 ** 4\n\telse:\n\t\tsize = int(size)\n\treturn size\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tif size.endswith(\"M\"):\n\t\treturn int(size[:-1]) * 1024 * 1024\n\telif size.endswith(\"K\"):\n\t\treturn int(size[:-1]) * 1024\n\telse:\n\t\treturn int(size)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_unit = size[-2:].upper()\n\tsize_num = size[:-2]\n\n\tif size_unit == 'MB':\n\t\tsize_in_bytes = int(size_num) * 1024 * 1024\n\telif size_unit == 'KB':\n\t\tsize_in_bytes = int(size_num) * 1024\n\telif size_unit == 'GB':\n\t\tsize_in_bytes = int(size_num) * 1024 * 1024 * 1024\n\telse:\n\t\tsize_in_bytes = 0\n\n\treturn size_in_bytes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmultiplier = {\n\t\t'K': 1024,\n\t\t'M': 1024 ** 2,\n\t\t'G': 1024 ** 3,\n\t\t'T': 1024 ** 4,\n\t\t'P': 1024 ** 5,\n\t\t'E': 1024 ** 6,\n\t\t'Z': 1024 ** 7,\n\t\t'Y': 1024 ** 8\n\t}\n\n\tsize = size.upper()\n\n\tif not size[-1].isdigit():\n\t\traise ValueError('size must be a string representing a human readable file size')\n\n\treturn int(size[:-1]) * multiplier[size[-1]]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t# Remove commas\n\tsize = size.replace(',', '')\n\n\t# Multiply factor by 1024 (2**10)\n\tsize_power = {\n\t\t'K': 1,\n\t\t'M': 2,\n\t\t'G': 3,\n\t\t'T': 4,\n\t\t'P': 5,\n\t\t'E': 6\n\t}\n\tfactor = 1024 ** size_power[size[-1]]\n\n\t# Convert to int\n\tsize = int(size[:-1])\n\treturn size * factor\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_name = ('B', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n\tnum = re.sub(r'([KMGTPEZY])$', r'\\1\\1', size)\n\tnum = re.sub(r'([kMGTPEZY])$', r'\\1\\1', num)\n\tnum = re.sub(r'([KMGTPEZY])', r'\\1\\1\\1', num)\n\tnum = re.sub(r'(.)', r'\\1\\1', num)\n\tfactor = 1000\n\tfor n, s in enumerate(size_name):\n\t\tif s in num:\n\t\t\tnum = num.replace(s, '')\n\t\t\tfactor = 1 << (n + 1) * 10\n\t\t\tbreak\n\treturn int(float(num) * factor)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tm = re.match(r'(\\d*\\.?\\d+) ?([KMGTPEZY]i?)?', size)\n\tif not m:\n\t\traise ValueError('invalid size: ' + size)\n\n\tnum, unit = m.groups()\n\tnum = int(num)\n\n\treturn num * 1024**_unit_to_bytes_exp(unit)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = {\n\t\t'K': 1024,\n\t\t'M': 1024 ** 2,\n\t\t'G': 1024 ** 3,\n\t\t'T': 1024 ** 4,\n\t\t'P': 1024 ** 5,\n\t\t'E': 1024 ** 6,\n\t}\n\tif not size[-1].isalpha():\n\t\traise ValueError('Invalid file size')\n\ttry:\n\t\tsize = float(size[:-1])\n\texcept ValueError:\n\t\traise ValueError('Invalid file size')\n\tif size < 0:\n\t\traise ValueError('Invalid file size')\n\treturn int(size * suffixes[size[-1].upper()])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tif not size:\n\t\treturn 0\n\n\tmatch = re.search(r'^(\\d+)([KMGT])?$', size)\n\tif not match:\n\t\traise ValueError(f'Invalid size: {size}')\n\n\tsize = match.group(1)\n\tunits = match.group(2)\n\n\tif units:\n\t\treturn int(size) * 1024 ** {'K': 1, 'M': 2, 'G': 3, 'T': 4}[units]\n\telse:\n\t\treturn int(size)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tmatch = re.search(r\"^(\\d+)\\s*([KMGTP]?i?B?)?$\", size)\n\tif match:\n\t\tnum = int(match.group(1))\n\t\tunit = match.group(2)\n\telse:\n\t\traise ValueError(f\"Invalid size: {size}\")\n\n\tif unit:\n\t\tunit = unit[0]\n\t\tif unit == \"K\":\n\t\t\tnum *= 1024\n\t\telif unit == \"M\":\n\t\t\tnum *= 1024 * 1024\n\t\telif unit == \"G\":\n\t\t\tnum *= 1024 * 1024 * 1024\n\t\telif unit == \"T\":\n\t\t\tnum *= 1024 * 1024 * 1024 * 1024\n\n\treturn num\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tif size[-2:] in {\"Ki\", \"Mi\", \"Gi\", \"Ti\"}:\n\t\tsize = size[:-2]\n\t\tif size[-1] == \"i\":\n\t\t\tsize = size[:-1]\n\t\tsize = int(size)\n\t\tif size[-2:] == \"Ki\":\n\t\t\tsize *= 1024\n\t\telif size[-2:] == \"Mi\":\n\t\t\tsize *= 1024 ** 2\n\t\telif size[-2:] == \"Gi\":\n\t\t\tsize *= 1024 ** 3\n\t\telif size[-2:] == \"Ti\":\n\t\t\tsize *= 1024 ** 4\n\telse:\n\t\tsize = int(size)\n\treturn size\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "contrib/planb-swiftsync.py", "project": "ossobv/planb", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tdic[k] = dic.get(k, 0) + v\n\treturn dic\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tif not dicts:\n\t\treturn {}\n\treturn {k: sum(d.get(k, 0) for d in dicts) for k in set().union(*dicts)}\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t# init the dict\n\tdic = {}\n\tfor dic1 in dicts:\n\t\tfor key, value in dic1.items():\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += value\n\t\t\telse:\n\t\t\t\tdic[key] = value\n\treturn dic\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tkeys = set()\n\tfor d in dicts:\n\t\tkeys |= set(d.keys())\n\t\n\tret = {}\n\tfor k in keys:\n\t\tvals = [d[k] for d in dicts if k in d]\n\t\tret[k] = sum(vals)\n\treturn ret\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tif not isinstance(dicts, dict):\n\t\traise TypeError(\"dictsum() only accepts dicts\")\n\t# newdict = {k: sum(d[k] for d in dicts) for k in set().union(*dicts)}\n\tnewdict = {}\n\t# newdict = {}\n\t# for k in set().union(*dicts):\n\t# \tnewdict[k] = sum(d[k] for d in dicts)\n\tfor k in set().union(*dicts):\n\t\t# print(k)\n\t\tnewdict[k] = sum([d[k] for d in dicts if k in d])\n\treturn newdict\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor i in dicts:\n\t\tfor key in i:\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += i[key]\n\t\t\telse:\n\t\t\t\tdic[key] = i[key]\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor key in d:\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += d[key]\n\t\t\telse:\n\t\t\t\tdic[key] = d[key]\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += value\n\t\t\telse:\n\t\t\t\tdic[key] = value\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tresult = {}\n\tfor dict in dicts:\n\t\tfor key in dict.keys():\n\t\t\tif key in result.keys():\n\t\t\t\tresult[key] += dict[key]\n\t\t\telse:\n\t\t\t\tresult[key] = dict[key]\n\treturn result\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += d[k]\n\t\t\telse:\n\t\t\t\tdic[k] = d[k]\n\treturn dic\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tdic[k] = dic.get(k, 0) + v\n\treturn dic\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tif not dicts:\n\t\treturn {}\n\treturn {k: sum(d.get(k, 0) for d in dicts) for k in set().union(*dicts)}\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t# init the dict\n\tdic = {}\n\tfor dic1 in dicts:\n\t\tfor key, value in dic1.items():\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += value\n\t\t\telse:\n\t\t\t\tdic[key] = value\n\treturn dic\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tkeys = set()\n\tfor d in dicts:\n\t\tkeys |= set(d.keys())\n\t\n\tret = {}\n\tfor k in keys:\n\t\tvals = [d[k] for d in dicts if k in d]\n\t\tret[k] = sum(vals)\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tif not isinstance(dicts, dict):\n\t\traise TypeError(\"dictsum() only accepts dicts\")\n\t# newdict = {k: sum(d[k] for d in dicts) for k in set().union(*dicts)}\n\tnewdict = {}\n\t# newdict = {}\n\t# for k in set().union(*dicts):\n\t# \tnewdict[k] = sum(d[k] for d in dicts)\n\tfor k in set().union(*dicts):\n\t\t# print(k)\n\t\tnewdict[k] = sum([d[k] for d in dicts if k in d])\n\treturn newdict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor i in dicts:\n\t\tfor key in i:\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += i[key]\n\t\t\telse:\n\t\t\t\tdic[key] = i[key]\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor key in d:\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += d[key]\n\t\t\telse:\n\t\t\t\tdic[key] = d[key]\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += value\n\t\t\telse:\n\t\t\t\tdic[key] = value\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tresult = {}\n\tfor dict in dicts:\n\t\tfor key in dict.keys():\n\t\t\tif key in result.keys():\n\t\t\t\tresult[key] += dict[key]\n\t\t\telse:\n\t\t\t\tresult[key] = dict[key]\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += d[k]\n\t\t\telse:\n\t\t\t\tdic[k] = d[k]\n\treturn dic\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pyseed/apibase.py", "project": "SEED-platform/py-seed", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif not url_args:\n\t\treturn url\n\treturn url.format(**url_args)\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\n\turl = url.split('?')\n\tif len(url) < 2:\n\t\treturn url[0]\n\n\tquery_string = url[1]\n\tquery_string = query_string.split('&')\n\n\tfor i in range(len(query_string)):\n\t\tsplit_string = query_string[i].split('=')\n\t\tif len(split_string) < 2:\n\t\t\tcontinue\n\t\tif split_string[0] in url_args:\n\t\t\tquery_string[i] = split_string[0] + '=' + str(url_args[split_string[0]])\n\n\turl = url[0] + '?' + '&'.join(query_string)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\tfor key, value in url_args.items():\n\t\tif key in url:\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\tif url_args:\n\t\tfor key in url_args:\n\t\t\turl = url.replace(str(key), str(url_args[key]))\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tfor key, value in url_args.items():\n\t\tif value:\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\telse:\n\t\tfor k, v in url_args.items():\n\t\t\turl = url.replace(\"{{\"+k+\"}}\", str(v))\n\t\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None or len(url_args) == 0:\n\t\treturn url\n\n\tfor key in url_args.keys():\n\t\tif url.find(key) != -1:\n\t\t\turl = url.replace(key, url_args[key])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\tfor key, value in url_args.items():\n\t\turl = url.replace(\"{\"+key+\"}\", value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(\"{{\" + key + \"}}\", value)\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif not url_args:\n\t\treturn url\n\n\tif isinstance(url_args, dict):\n\t\tfor k, v in url_args.items():\n\t\t\turl = url.replace('{' + k + '}', v)\n\n\treturn url\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif not url_args:\n\t\treturn url\n\treturn url.format(**url_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\n\turl = url.split('?')\n\tif len(url) < 2:\n\t\treturn url[0]\n\n\tquery_string = url[1]\n\tquery_string = query_string.split('&')\n\n\tfor i in range(len(query_string)):\n\t\tsplit_string = query_string[i].split('=')\n\t\tif len(split_string) < 2:\n\t\t\tcontinue\n\t\tif split_string[0] in url_args:\n\t\t\tquery_string[i] = split_string[0] + '=' + str(url_args[split_string[0]])\n\n\turl = url[0] + '?' + '&'.join(query_string)\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\tfor key, value in url_args.items():\n\t\tif key in url:\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\tif url_args:\n\t\tfor key in url_args:\n\t\t\turl = url.replace(str(key), str(url_args[key]))\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tfor key, value in url_args.items():\n\t\tif value:\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\telse:\n\t\tfor k, v in url_args.items():\n\t\t\turl = url.replace(\"{{\"+k+\"}}\", str(v))\n\t\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None or len(url_args) == 0:\n\t\treturn url\n\n\tfor key in url_args.keys():\n\t\tif url.find(key) != -1:\n\t\t\turl = url.replace(key, url_args[key])\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args is None:\n\t\treturn url\n\tfor key, value in url_args.items():\n\t\turl = url.replace(\"{\"+key+\"}\", value)\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(\"{{\" + key + \"}}\", value)\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\tif not url_args:\n\t\treturn url\n\n\tif isinstance(url_args, dict):\n\t\tfor k, v in url_args.items():\n\t\t\turl = url.replace('{' + k + '}', v)\n\n\treturn url\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cinder/api/api_utils.py", "project": "openstack/cinder", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif type(val) == str:\n\t\treturn val.lower() == 'none'\n\n\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.lower() == 'none'\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\telif type(val) is str and val.lower() == 'none':\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and (val.lower() == 'none')\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.lower() == 'none'\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif type(val) == str:\n\t\treturn val.lower() == 'none'\n\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.lower() == 'none'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\telif type(val) is str and val.lower() == 'none':\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and (val.lower() == 'none')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.lower() == 'none'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise Exception(\"Path does not start with root\")\n\treturn path[len(root):]\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif root == '':\n\t\treturn path\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise ValueError(f'Path \"{path}\" is not under root \"{root}\"')\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise ValueError('Path does not start with root')\n\treturn path[len(root):]\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Path {} does not start with {}\".format(path, root))\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not root.endswith('/'):\n\t\troot += '/'\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\"Path: {} does not start with root: {}\".format(path, root))\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tstripped_path = path.replace(root, '', 1)\n\tif not stripped_path:\n\t\traise Exception(\"Path %s doesn't start with root %s\" % (path, root))\n\n\treturn stripped_path\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise ValueError(\"{} is not under {}\".format(path, root))\n\telse:\n\t\treturn path[len(root):]\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Can't strip root from path: %s\" % path)\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise RuntimeError(\"Path %s does not start with root %s\" % (path, root))\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif root is None:\n\t\treturn path\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise ValueError('{0} is not a subdirectory of {1}'.format(path, root))\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise Exception(\"Path does not start with root\")\n\treturn path[len(root):]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif root == '':\n\t\treturn path\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise ValueError(f'Path \"{path}\" is not under root \"{root}\"')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise ValueError('Path does not start with root')\n\treturn path[len(root):]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Path {} does not start with {}\".format(path, root))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not root.endswith('/'):\n\t\troot += '/'\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\"Path: {} does not start with root: {}\".format(path, root))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tstripped_path = path.replace(root, '', 1)\n\tif not stripped_path:\n\t\traise Exception(\"Path %s doesn't start with root %s\" % (path, root))\n\n\treturn stripped_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise ValueError(\"{} is not under {}\".format(path, root))\n\telse:\n\t\treturn path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Can't strip root from path: %s\" % path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise RuntimeError(\"Path %s does not start with root %s\" % (path, root))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tif root is None:\n\t\treturn path\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise ValueError('{0} is not a subdirectory of {1}'.format(path, root))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn \" \".join(parser.parse_args([])._get_kwargs())\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser.parse_args()._get_kwargs()[1:])\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflag_strings = []\n\tfor action in parser._actions:\n\t\tflags = action.option_strings\n\t\tif isinstance(flags, list) and len(flags) == 1:\n\t\t\tflag_strings.append(flags[0])\n\t\telif isinstance(flags, list):\n\t\t\tflag_strings.extend(flags)\n\t\telse:\n\t\t\tflag_strings.append(flags)\n\treturn ' '.join(flag_strings)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn \" \".join(parser._optionals._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(flag for flag in parser._optionals._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(arg.split('-')[-1] for arg in parser._option_string_actions)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn \" \".join(parser.parse_args([])._get_kwargs())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions.keys())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser.parse_args()._get_kwargs()[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflag_strings = []\n\tfor action in parser._actions:\n\t\tflags = action.option_strings\n\t\tif isinstance(flags, list) and len(flags) == 1:\n\t\t\tflag_strings.append(flags[0])\n\t\telif isinstance(flags, list):\n\t\t\tflag_strings.extend(flags)\n\t\telse:\n\t\t\tflag_strings.append(flags)\n\treturn ' '.join(flag_strings)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions.keys())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn \" \".join(parser._optionals._option_string_actions.keys())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(parser._optionals._option_string_actions)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(flag for flag in parser._optionals._option_string_actions.keys())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\treturn ' '.join(arg.split('-')[-1] for arg in parser._option_string_actions)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('  [skipped] %s' % path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"    Already processed:\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tpath_name = os.path.abspath(path_name)\n\t\tif path_name in processed:\n\t\t\tprint(f'{path_name} has already been processed. Skipping.', file=sys.stderr)\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\treturn False\n\texcept Exception as e:\n\t\tprint(f'{path_name} is not a valid path. Skipping.', file=sys.stderr)\n\t\treturn True\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif os.path.isabs(path_name):\n\t\tabs_path_name = path_name\n\telse:\n\t\tabs_path_name = os.path.abspath(path_name)\n\n\tif abs_path_name in processed:\n\t\tif verbose:\n\t\t\tprint \"File or directory {0} already processed, skipping\".format(path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed %s' % abs_path)\n\t\treturn True\n\tprocessed.add(abs_path)\n\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(f\"Skipping {path_name}\")\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name not in processed:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\telse:\n\t\tif verbose:\n\t\t\tprint('WARNING: Already processed ' + path_name)\n\t\treturn True\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Warning: {0} was already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint('{} already processed'.format(abs_path))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif not os.path.exists(path_name):\n\t\treturn False\n\n\tif os.path.isdir(path_name):\n\t\tpath_name = os.path.abspath(path_name)\n\t\tif path_name in processed:\n\t\t\tprint(\"{0} has already been processed\".format(path_name), file=sys.stderr)\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\treturn False\n\telif os.path.isfile(path_name):\n\t\tif os.path.getsize(path_name) == 0:\n\t\t\tprint(\"{0} is empty\".format(path_name), file=sys.stderr)\n\t\t\treturn True\n\t\tpath_name = os.path.abspath(path_name)\n\t\tif path_name in processed:\n\t\t\tprint(\"{0} has already been processed\".format(path_name), file=sys.stderr)\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\treturn False\n\tprint(\"{0} is not a directory or file\".format(path_name), file=sys.stderr)\n\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('  [skipped] %s' % path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"    Already processed:\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tpath_name = os.path.abspath(path_name)\n\t\tif path_name in processed:\n\t\t\tprint(f'{path_name} has already been processed. Skipping.', file=sys.stderr)\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\treturn False\n\texcept Exception as e:\n\t\tprint(f'{path_name} is not a valid path. Skipping.', file=sys.stderr)\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif os.path.isabs(path_name):\n\t\tabs_path_name = path_name\n\telse:\n\t\tabs_path_name = os.path.abspath(path_name)\n\n\tif abs_path_name in processed:\n\t\tif verbose:\n\t\t\tprint \"File or directory {0} already processed, skipping\".format(path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path_name)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed %s' % abs_path)\n\t\treturn True\n\tprocessed.add(abs_path)\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(f\"Skipping {path_name}\")\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name not in processed:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\telse:\n\t\tif verbose:\n\t\t\tprint('WARNING: Already processed ' + path_name)\n\t\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Warning: {0} was already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint('{} already processed'.format(abs_path))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tif not os.path.exists(path_name):\n\t\treturn False\n\n\tif os.path.isdir(path_name):\n\t\tpath_name = os.path.abspath(path_name)\n\t\tif path_name in processed:\n\t\t\tprint(\"{0} has already been processed\".format(path_name), file=sys.stderr)\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\treturn False\n\telif os.path.isfile(path_name):\n\t\tif os.path.getsize(path_name) == 0:\n\t\t\tprint(\"{0} is empty\".format(path_name), file=sys.stderr)\n\t\t\treturn True\n\t\tpath_name = os.path.abspath(path_name)\n\t\tif path_name in processed:\n\t\t\tprint(\"{0} has already been processed\".format(path_name), file=sys.stderr)\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\treturn False\n\tprint(\"{0} is not a directory or file\".format(path_name), file=sys.stderr)\n\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "eppy/geometry/surface.py", "project": "santoshphilip/eppy", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tpoints = []\n\tfor i in range(len(vertices)):\n\t\ttry:\n\t\t\tpoints.append((vertices[i], vertices[i+1], vertices[i+2]))\n\t\texcept Exception as e:\n\t\t\tpoints.append((vertices[i], vertices[0], vertices[1]))\n\treturn points\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [[\n\t\tvertices[i][0],\n\t\tvertices[i][1],\n\t\tvertices[i][2],\n\t\tvertices[(i - 1) % len(vertices)][0],\n\t\tvertices[(i - 1) % len(vertices)][1],\n\t\tvertices[(i - 1) % len(vertices)][2],\n\t\tvertices[(i + 1) % len(vertices)][0],\n\t\tvertices[(i + 1) % len(vertices)][1],\n\t\tvertices[(i + 1) % len(vertices)][2]\n\t] for i in range(len(vertices))]\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [(vertices[i], vertices[i + 1], vertices[i + 2]) for i in range(len(vertices) - 2)]\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\tfor vertex_index in range(len(vertices)):\n\t\tif vertex_index + 1 < len(vertices):\n\t\t\tvertex_list.append((vertices[vertex_index], vertices[vertex_index+1], vertices[vertex_index+2]))\n\t\telse:\n\t\t\tvertex_list.append((vertices[vertex_index], vertices[vertex_index+1], vertices[0]))\n\treturn vertex_list\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_list = []\n\tfor i in range(len(vertices)):\n\t\tif i >= len(vertices):\n\t\t\tvertices_list.append((vertices[0], vertices[1], vertices[2]))\n\t\telse:\n\t\t\tvertices_list.append((vertices[i], vertices[i+1], vertices[i+2]))\n\treturn vertices_list\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\tfor i in range(len(vertices)):\n\t\tvertex = vertices[i]\n\t\tif i == 0:\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i][2]]))\n\t\telif i == len(vertices) - 1:\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i-1][2]]))\n\t\telse:\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i-1][2]]))\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i+1][2]]))\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i][2]]))\n\n\treturn vertex_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [vertex3tuple_single(vertices, i) for i in range(len(vertices))]\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_tuple = []\n\tfor i in range(len(vertices)):\n\t\tindices = [i-1, i, i+1]\n\t\tfor index in indices:\n\t\t\tif index < 0:\n\t\t\t\tindex = 0\n\t\t\tif index >= len(vertices):\n\t\t\t\tindex = len(vertices) - 1\n\t\t\tvertices_tuple.append((vertices[i], vertices[index]))\n\treturn vertices_tuple\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_list = []\n\tfor i, vertex in enumerate(vertices):\n\t\ttry:\n\t\t\tvertices_list.append((vertex, vertices[i - 1], vertices[i + 1]))\n\t\texcept IndexError:\n\t\t\tvertices_list.append((vertex, (0,0,0), vertices[i + 1]))\n\t\texcept:\n\t\t\tvertices_list.append((vertex, (0,0,0), (0,0,0)))\n\treturn vertices_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [(vertices[i][0], vertices[i][1], vertices[i][2]) for i in range(len(vertices))]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tpoints = []\n\tfor i in range(len(vertices)):\n\t\ttry:\n\t\t\tpoints.append((vertices[i], vertices[i+1], vertices[i+2]))\n\t\texcept Exception as e:\n\t\t\tpoints.append((vertices[i], vertices[0], vertices[1]))\n\treturn points\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [[\n\t\tvertices[i][0],\n\t\tvertices[i][1],\n\t\tvertices[i][2],\n\t\tvertices[(i - 1) % len(vertices)][0],\n\t\tvertices[(i - 1) % len(vertices)][1],\n\t\tvertices[(i - 1) % len(vertices)][2],\n\t\tvertices[(i + 1) % len(vertices)][0],\n\t\tvertices[(i + 1) % len(vertices)][1],\n\t\tvertices[(i + 1) % len(vertices)][2]\n\t] for i in range(len(vertices))]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [(vertices[i], vertices[i + 1], vertices[i + 2]) for i in range(len(vertices) - 2)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\tfor vertex_index in range(len(vertices)):\n\t\tif vertex_index + 1 < len(vertices):\n\t\t\tvertex_list.append((vertices[vertex_index], vertices[vertex_index+1], vertices[vertex_index+2]))\n\t\telse:\n\t\t\tvertex_list.append((vertices[vertex_index], vertices[vertex_index+1], vertices[0]))\n\treturn vertex_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_list = []\n\tfor i in range(len(vertices)):\n\t\tif i >= len(vertices):\n\t\t\tvertices_list.append((vertices[0], vertices[1], vertices[2]))\n\t\telse:\n\t\t\tvertices_list.append((vertices[i], vertices[i+1], vertices[i+2]))\n\treturn vertices_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_list = []\n\tfor i in range(len(vertices)):\n\t\tvertex = vertices[i]\n\t\tif i == 0:\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i][2]]))\n\t\telif i == len(vertices) - 1:\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i-1][2]]))\n\t\telse:\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i-1][2]]))\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i+1][2]]))\n\t\t\tvertex_list.append(tuple([vertices[i][0], vertices[i][1], vertices[i][2]]))\n\n\treturn vertex_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [vertex3tuple_single(vertices, i) for i in range(len(vertices))]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_tuple = []\n\tfor i in range(len(vertices)):\n\t\tindices = [i-1, i, i+1]\n\t\tfor index in indices:\n\t\t\tif index < 0:\n\t\t\t\tindex = 0\n\t\t\tif index >= len(vertices):\n\t\t\t\tindex = len(vertices) - 1\n\t\t\tvertices_tuple.append((vertices[i], vertices[index]))\n\treturn vertices_tuple\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices_list = []\n\tfor i, vertex in enumerate(vertices):\n\t\ttry:\n\t\t\tvertices_list.append((vertex, vertices[i - 1], vertices[i + 1]))\n\t\texcept IndexError:\n\t\t\tvertices_list.append((vertex, (0,0,0), vertices[i + 1]))\n\t\texcept:\n\t\t\tvertices_list.append((vertex, (0,0,0), (0,0,0)))\n\treturn vertices_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\treturn [(vertices[i][0], vertices[i][1], vertices[i][2]) for i in range(len(vertices))]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\toutput = ''\n\tif padding is not None:\n\t\toutput = '0' * padding\n\t\n\twhile number > 0:\n\t\toutput += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn output[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number < 0:\n\t\traise ValueError('Number should be positive.')\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tresult = ''\n\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\n\tif padding is not None:\n\t\tresult = '0' * (padding - len(result)) + result\n\n\treturn result\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number < 0:\n\t\traise ValueError('The number must be a positive integer')\n\n\tresult = ''\n\twhile number > 0:\n\t\tresult = alphabet[number % len(alphabet)] + result\n\t\tnumber //= len(alphabet)\n\n\tif padding is not None:\n\t\treturn (padding - len(result)) * '0' + result\n\telse:\n\t\treturn result\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif padding is not None:\n\t\tif number < 0:\n\t\t\traise ValueError('Number must be positive when padding is specified.')\n\t\tif number == 0:\n\t\t\treturn alphabet[0] * padding\n\t\treturn alphabet[number % len(alphabet)] + int_to_string(number // len(alphabet), alphabet, padding)\n\n\tif number < 0:\n\t\traise ValueError('Number must be positive when padding is not specified.')\n\n\tout = \"\"\n\twhile number >= len(alphabet):\n\t\tout = alphabet[number % len(alphabet)] + out\n\t\tnumber //= len(alphabet)\n\treturn alphabet[number] + out\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif padding:\n\t\tnumber = number * (10 ** padding)\n\t\tnumber = int(number)\n\t\tstr_number = str(number)\n\t\tstring_value = ''.join(alphabet[int(digit)] for digit in str_number)\n\t\treturn string_value\n\tstring_value = ''\n\twhile number > 0:\n\t\tremainder = number % 10\n\t\tstring_value = alphabet[int(remainder)] + string_value\n\t\tnumber = int(number / 10)\n\treturn string_value\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number < 0:\n\t\traise ValueError(\"Only positive numbers are allowed\")\n\tif len(alphabet) != 256:\n\t\traise ValueError(\"Wrong alphabet length\")\n\tif padding is not None and padding < 0:\n\t\traise ValueError(\"Only positive padding values are allowed\")\n\n\tresult = \"\"\n\n\tif padding is not None:\n\t\tresult += \"0\" * padding\n\n\twhile number > 0:\n\t\tresult += alphabet[number % 256]\n\t\tnumber //= 256\n\n\treturn result[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif padding is None:\n\t\tpadding = 0\n\tif padding:\n\t\treturn ''.join(alphabet[number % len(alphabet)]) + int_to_string(number // len(alphabet), alphabet, padding - 1)\n\telse:\n\t\treturn alphabet[number]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\tresult = \"\"\n\tif padding is not None:\n\t\tnumber = int(str(number).rjust(padding, \"0\"))\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn result[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tassert len(alphabet) >= 16\n\tassert padding is None or padding >= 0\n\n\tif number < 0:\n\t\traise ValueError(\"Number can't be negative\")\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tresult = \"\"\n\twhile number > 0:\n\t\tresult = alphabet[number % len(alphabet)] + result\n\t\tnumber = number // len(alphabet)\n\n\tif padding is not None:\n\t\treturn \"\".join([alphabet[0]] * padding) + result\n\telse:\n\t\treturn result\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\t\n\tresult = ''\n\twhile number > 0:\n\t\tnumber, remainder = divmod(number, padding)\n\t\tresult = alphabet[remainder] + result\n\treturn result\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\toutput = ''\n\tif padding is not None:\n\t\toutput = '0' * padding\n\t\n\twhile number > 0:\n\t\toutput += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn output[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number < 0:\n\t\traise ValueError('Number should be positive.')\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tresult = ''\n\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\n\tif padding is not None:\n\t\tresult = '0' * (padding - len(result)) + result\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number < 0:\n\t\traise ValueError('The number must be a positive integer')\n\n\tresult = ''\n\twhile number > 0:\n\t\tresult = alphabet[number % len(alphabet)] + result\n\t\tnumber //= len(alphabet)\n\n\tif padding is not None:\n\t\treturn (padding - len(result)) * '0' + result\n\telse:\n\t\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif padding is not None:\n\t\tif number < 0:\n\t\t\traise ValueError('Number must be positive when padding is specified.')\n\t\tif number == 0:\n\t\t\treturn alphabet[0] * padding\n\t\treturn alphabet[number % len(alphabet)] + int_to_string(number // len(alphabet), alphabet, padding)\n\n\tif number < 0:\n\t\traise ValueError('Number must be positive when padding is not specified.')\n\n\tout = \"\"\n\twhile number >= len(alphabet):\n\t\tout = alphabet[number % len(alphabet)] + out\n\t\tnumber //= len(alphabet)\n\treturn alphabet[number] + out\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif padding:\n\t\tnumber = number * (10 ** padding)\n\t\tnumber = int(number)\n\t\tstr_number = str(number)\n\t\tstring_value = ''.join(alphabet[int(digit)] for digit in str_number)\n\t\treturn string_value\n\tstring_value = ''\n\twhile number > 0:\n\t\tremainder = number % 10\n\t\tstring_value = alphabet[int(remainder)] + string_value\n\t\tnumber = int(number / 10)\n\treturn string_value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number < 0:\n\t\traise ValueError(\"Only positive numbers are allowed\")\n\tif len(alphabet) != 256:\n\t\traise ValueError(\"Wrong alphabet length\")\n\tif padding is not None and padding < 0:\n\t\traise ValueError(\"Only positive padding values are allowed\")\n\n\tresult = \"\"\n\n\tif padding is not None:\n\t\tresult += \"0\" * padding\n\n\twhile number > 0:\n\t\tresult += alphabet[number % 256]\n\t\tnumber //= 256\n\n\treturn result[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif padding is None:\n\t\tpadding = 0\n\tif padding:\n\t\treturn ''.join(alphabet[number % len(alphabet)]) + int_to_string(number // len(alphabet), alphabet, padding - 1)\n\telse:\n\t\treturn alphabet[number]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\tresult = \"\"\n\tif padding is not None:\n\t\tnumber = int(str(number).rjust(padding, \"0\"))\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn result[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tassert len(alphabet) >= 16\n\tassert padding is None or padding >= 0\n\n\tif number < 0:\n\t\traise ValueError(\"Number can't be negative\")\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tresult = \"\"\n\twhile number > 0:\n\t\tresult = alphabet[number % len(alphabet)] + result\n\t\tnumber = number // len(alphabet)\n\n\tif padding is not None:\n\t\treturn \"\".join([alphabet[0]] * padding) + result\n\telse:\n\t\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\t\n\tresult = ''\n\twhile number > 0:\n\t\tnumber, remainder = divmod(number, padding)\n\t\tresult = alphabet[remainder] + result\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neutron_lib/agent/common/utils.py", "project": "openstack/neutron-lib", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif register_number in flow_params.keys():\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif register_value in flow_params:\n\t\tflow_params[register_number] = flow_params.pop(register_value)\n\telse:\n\t\tflow_params[register_number] = flow_params.pop(register_number)\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tassert isinstance(flow_params, dict)\n\tassert isinstance(register_number, int)\n\tassert isinstance(register_value, int)\n\n\t# Make a copy of flow_params\n\tflow_params_copy = flow_params.copy()\n\n\t# Delete flow_params[register_value]\n\tdel flow_params_copy[register_value]\n\n\t# Replace the value of flow_params[register_number] with flow_params[register_value]\n\tflow_params_copy[register_number] = register_value\n\n\treturn flow_params_copy\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tfor param in flow_params:\n\t\tif param.startswith(register_number):\n\t\t\tflow_params[param].set_value(flow_params[register_value])\n\t\t\tdel flow_params[register_value]\n\t\t\tbreak\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif flow_params[register_number] != None:\n\t\traise Exception(\"Registar number \" + str(register_number) + \" is already used\")\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif register_number not in flow_params.keys():\n\t\traise ValueError(\"Register number is not valid\")\n\tif register_value not in flow_params.keys():\n\t\traise ValueError(\"Register value is not valid\")\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif register_number in flow_params.keys():\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif register_value in flow_params:\n\t\tflow_params[register_number] = flow_params.pop(register_value)\n\telse:\n\t\tflow_params[register_number] = flow_params.pop(register_number)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tassert isinstance(flow_params, dict)\n\tassert isinstance(register_number, int)\n\tassert isinstance(register_value, int)\n\n\t# Make a copy of flow_params\n\tflow_params_copy = flow_params.copy()\n\n\t# Delete flow_params[register_value]\n\tdel flow_params_copy[register_value]\n\n\t# Replace the value of flow_params[register_number] with flow_params[register_value]\n\tflow_params_copy[register_number] = register_value\n\n\treturn flow_params_copy\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tfor param in flow_params:\n\t\tif param.startswith(register_number):\n\t\t\tflow_params[param].set_value(flow_params[register_value])\n\t\t\tdel flow_params[register_value]\n\t\t\tbreak\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif flow_params[register_number] != None:\n\t\traise Exception(\"Registar number \" + str(register_number) + \" is already used\")\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tif register_number not in flow_params.keys():\n\t\traise ValueError(\"Register number is not valid\")\n\tif register_value not in flow_params.keys():\n\t\traise ValueError(\"Register value is not valid\")\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "release_dashboard/templatetags/rd_extras.py", "project": "sipwise/repoapi", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tif isinstance(value, str):\n\t\treturn \"\".join(value.replace('.', arg))\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tif not isinstance(value, str):\n\t\traise ValueError('value is not a string')\n\tif not isinstance(arg, str):\n\t\traise ValueError('arg is not a string')\n\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\nif __name__ == \"__main__\":\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', ' '))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', '~'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', 't'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', 'test'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', '.'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', ''))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', ' '))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('Test.Dot.Test', ' '))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('Test.Dot.Test', '~'))\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', str(arg))\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tif isinstance(value, str):\n\t\treturn \"\".join(value.replace('.', arg))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tif not isinstance(value, str):\n\t\traise ValueError('value is not a string')\n\tif not isinstance(arg, str):\n\t\traise ValueError('arg is not a string')\n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\nif __name__ == \"__main__\":\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', ' '))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', '~'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', 't'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', 'test'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', '.'))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', ''))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('test.dot.test', ' '))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('Test.Dot.Test', ' '))\n\n\tprint(\"Replacing dots in a string\")\n\tprint(replace_dots('Test.Dot.Test', '~'))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', str(arg))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "rows/utils/__init__.py", "project": "turicas/rows", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n\n# --------------------\n# --- Misc Functions\n# --------------------\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses\n\nclass SubClass(object):\n\t\"\"\"\n\tThe base class to inherit from for a subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n\nclass A(SubClass):\n\t\"\"\"\n\tA subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n\nclass B(A):\n\t\"\"\"\n\tA subclass of a subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n\nclass C(A):\n\t\"\"\"\n\tAnother subclass of a subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor sc in cls.__subclasses__():\n\t\tsubclasses.append(sc)\n\t\tsubclasses.extend(subclasses(sc))\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subcls in subclasses:\n\t\tsubclasses.extend(subclasses(subcls))\n\treturn subclasses\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\treturn sum([subclasses(sub) for sub in subs], subs)\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubs.extend(subclasses(subclass))\n\treturn list(cls.__subclasses__()) + subs\n\n\nclass Base:\n\n\tdef __init__(self, data):\n\t\tself.data = data\n\n\tdef __getattr__(self, name):\n\t\tif name not in self.data:\n\t\t\traise AttributeError('data is missing attribute {}.'.format(name))\n\t\treturn self.data[name]\n\n\tdef __repr__(self):\n\t\treturn '{}({!r})'.format(self.__class__.__name__, self.data)\n\n\tdef __dir__(self):\n\t\treturn list(self.data.keys())\n\n\tdef __eq__(self, other):\n\t\treturn self.data == other.data\n\n\tdef __ne__(self, other):\n\t\treturn not (self == other)\n\n\tdef __hash__(self):\n\t\treturn hash(tuple(self.data.items()))\n\n\tdef __bool__(self):\n\t\treturn bool(self.data)\n\n\tdef __iter__(self):\n\t\treturn iter(self.data)\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubcls = cls.__subclasses__()\n\tfor subcls in cls.__subclasses__():\n\t\tsubcls.subclasses(subcls)\n\treturn subcls\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n\n# --------------------\n# --- Misc Functions\n# --------------------\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses\n\nclass SubClass(object):\n\t\"\"\"\n\tThe base class to inherit from for a subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n\nclass A(SubClass):\n\t\"\"\"\n\tA subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n\nclass B(A):\n\t\"\"\"\n\tA subclass of a subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n\nclass C(A):\n\t\"\"\"\n\tAnother subclass of a subclass\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\tdef __str__(self", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor sc in cls.__subclasses__():\n\t\tsubclasses.append(sc)\n\t\tsubclasses.extend(subclasses(sc))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subcls in subclasses:\n\t\tsubclasses.extend(subclasses(subcls))\n\treturn subclasses\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclasses(subclass)\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\treturn sum([subclasses(sub) for sub in subs], subs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubs.extend(subclasses(subclass))\n\treturn list(cls.__subclasses__()) + subs\n\n\nclass Base:\n\n\tdef __init__(self, data):\n\t\tself.data = data\n\n\tdef __getattr__(self, name):\n\t\tif name not in self.data:\n\t\t\traise AttributeError('data is missing attribute {}.'.format(name))\n\t\treturn self.data[name]\n\n\tdef __repr__(self):\n\t\treturn '{}({!r})'.format(self.__class__.__name__, self.data)\n\n\tdef __dir__(self):\n\t\treturn list(self.data.keys())\n\n\tdef __eq__(self, other):\n\t\treturn self.data == other.data\n\n\tdef __ne__(self, other):\n\t\treturn not (self == other)\n\n\tdef __hash__(self):\n\t\treturn hash(tuple(self.data.items()))\n\n\tdef __bool__(self):\n\t\treturn bool(self.data)\n\n\tdef __iter__(self):\n\t\treturn iter(self.data)\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubcls = cls.__subclasses__()\n\tfor subcls in cls.__subclasses__():\n\t\tsubcls.subclasses(subcls)\n\treturn subcls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i, s in enumerate(string):\n\t\tnum = num + alphabet.index(s) * (len(alphabet) ** i)\n\treturn num\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor i, char in enumerate(string):\n\t\tnumber += (ord(char) - ord('a')) * (len(alphabet)**i)\n\treturn number\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor i in range(len(string)):\n\t\tnumber += alphabet.index(string[i]) * (len(alphabet) ** i)\n\treturn number\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\treturn sum([alphabet.index(char) * pow(len(alphabet), len(string) - i - 1) for i, char in enumerate(string)])\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor letter in string:\n\t\tnumber = number * len(alphabet) + alphabet.index(letter)\n\treturn number\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t# check for empty string\n\tif len(string) == 0:\n\t\treturn 0\n\t# check for single letter\n\tif len(string) == 1:\n\t\treturn alphabet.index(string)\n\n\tn = len(alphabet)\n\t# convert string to number\n\tnumber = alphabet.index(string[0])\n\tfor i in range(len(string) - 1):\n\t\tnumber = number * n + alphabet.index(string[i + 1])\n\treturn number\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor index, letter in enumerate(string):\n\t\tresult += (len(alphabet) ** index) * alphabet.index(letter)\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i, c in enumerate(string):\n\t\tnum += alphabet.index(c) * (len(alphabet) ** i)\n\treturn num\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor letter in string:\n\t\tresult = (result * len(alphabet)) + alphabet.index(letter)\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor i, c in enumerate(string):\n\t\tresult += alphabet.index(c) * (len(alphabet) ** i)\n\treturn result\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i, s in enumerate(string):\n\t\tnum = num + alphabet.index(s) * (len(alphabet) ** i)\n\treturn num\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor i, char in enumerate(string):\n\t\tnumber += (ord(char) - ord('a')) * (len(alphabet)**i)\n\treturn number\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor i in range(len(string)):\n\t\tnumber += alphabet.index(string[i]) * (len(alphabet) ** i)\n\treturn number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\treturn sum([alphabet.index(char) * pow(len(alphabet), len(string) - i - 1) for i, char in enumerate(string)])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor letter in string:\n\t\tnumber = number * len(alphabet) + alphabet.index(letter)\n\treturn number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t# check for empty string\n\tif len(string) == 0:\n\t\treturn 0\n\t# check for single letter\n\tif len(string) == 1:\n\t\treturn alphabet.index(string)\n\n\tn = len(alphabet)\n\t# convert string to number\n\tnumber = alphabet.index(string[0])\n\tfor i in range(len(string) - 1):\n\t\tnumber = number * n + alphabet.index(string[i + 1])\n\treturn number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor index, letter in enumerate(string):\n\t\tresult += (len(alphabet) ** index) * alphabet.index(letter)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i, c in enumerate(string):\n\t\tnum += alphabet.index(c) * (len(alphabet) ** i)\n\treturn num\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor letter in string:\n\t\tresult = (result * len(alphabet)) + alphabet.index(letter)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor i, c in enumerate(string):\n\t\tresult += alphabet.index(c) * (len(alphabet) ** i)\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tfrom urllib.request import urlretrieve\n\tfrom tarfile import open as tar_open\n\n\tresponse = urlretrieve(url, str(destination_path / 'repo.tar.gz'))\n\n\ttar = tar_open(response[0], 'r:gz')\n\n\ttar.extractall(path=destination_path)\n\n\ttar.close()\n\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t# TODO: add caching\n\twith TemporaryDirectory() as tmp_dir:\n\t\ttmp_dir = Path(tmp_dir)\n\t\tarchive_path = tmp_dir / 'archive.tar.gz'\n\t\twith urllib.request.urlopen(url) as response:\n\t\t\twith open(archive_path, 'wb') as f:\n\t\t\t\tshutil.copyfileobj(response, f)\n\n\t\twith tarfile.open(archive_path, 'r') as tar:\n\t\t\ttar.extractall(path=tmp_dir)\n\n\t\tshutil.move(tmp_dir / 'desc', destination_path)\n\t\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / \"repo_archive.tar.gz\"\n\twith urlopen(url) as response, archive_path.open('wb') as f:\n\t\tshutil.copyfileobj(response, f)\n\twith tarfile.open(archive_path) as tf:\n\t\ttf.extractall(destination_path)\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\timport requests\n\timport tarfile\n\n\tresponse = requests.get(url)\n\tif response.status_code != 200:\n\t\traise ConnectionError(f\"Failed to download packages archive from {url}\")\n\n\tarchive_path = destination_path / \"packages_archive.tar.gz\"\n\n\twith open(archive_path, \"wb\") as f:\n\t\tf.write(response.content)\n\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(destination_path)\n\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tresponse = requests.get(url)\n\tresponse.raise_for_status()\n\twith tarfile.open(fileobj=BytesIO(response.content)) as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tif not destination_path.exists():\n\t\tdestination_path.mkdir(parents=True)\n\tarchive_path = destination_path / 'repo_archive.tar.gz'\n\tif not archive_path.exists():\n\t\t_LOGGER.info(\"Downloading %s\", url)\n\t\tdownload(url, str(archive_path))\n\twith tarfile.open(archive_path) as tar:\n\t\tpackage_paths = []\n\t\tfor member in tar.getmembers():\n\t\t\tif member.name.endswith('desc'):\n\t\t\t\tpackage_paths.append(destination_path / member.name)\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = Path(destination_path, 'archive')\n\n\twith requests.get(url, stream=True) as r:\n\t\twith tarfile.open(fileobj=r.raw, mode='r|gz') as archive:\n\t\t\tarchive.extractall(path=archive_path)\n\n\treturn archive_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tlogger.info('Archive download and extraction: ' + url)\n\ttmp_dir = tempfile.mkdtemp()\n\tlogger.debug('Download temporary directory: ' + tmp_dir)\n\ttmp_archive_path = Path(tmp_dir) / 'archive.tar.gz'\n\trequest.urlretrieve(url, filename=tmp_archive_path)\n\twith tarfile.open(tmp_archive_path) as archive:\n\t\tarchive.extractall(path=destination_path)\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\timport tarfile\n\timport urllib.request\n\turllib.request.urlretrieve(url, filename=destination_path)\n\tarchive_path = destination_path\n\twith tarfile.open(str(destination_path), \"r:gz\") as tar:\n\t\ttar.extractall(path=destination_path.parent)\n\treturn archive_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = download_file(url)\n\treturn extract_archive(archive_path, destination_path)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tfrom urllib.request import urlretrieve\n\tfrom tarfile import open as tar_open\n\n\tresponse = urlretrieve(url, str(destination_path / 'repo.tar.gz'))\n\n\ttar = tar_open(response[0], 'r:gz')\n\n\ttar.extractall(path=destination_path)\n\n\ttar.close()\n\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t# TODO: add caching\n\twith TemporaryDirectory() as tmp_dir:\n\t\ttmp_dir = Path(tmp_dir)\n\t\tarchive_path = tmp_dir / 'archive.tar.gz'\n\t\twith urllib.request.urlopen(url) as response:\n\t\t\twith open(archive_path, 'wb') as f:\n\t\t\t\tshutil.copyfileobj(response, f)\n\n\t\twith tarfile.open(archive_path, 'r') as tar:\n\t\t\ttar.extractall(path=tmp_dir)\n\n\t\tshutil.move(tmp_dir / 'desc', destination_path)\n\t\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / \"repo_archive.tar.gz\"\n\twith urlopen(url) as response, archive_path.open('wb') as f:\n\t\tshutil.copyfileobj(response, f)\n\twith tarfile.open(archive_path) as tf:\n\t\ttf.extractall(destination_path)\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\timport requests\n\timport tarfile\n\n\tresponse = requests.get(url)\n\tif response.status_code != 200:\n\t\traise ConnectionError(f\"Failed to download packages archive from {url}\")\n\n\tarchive_path = destination_path / \"packages_archive.tar.gz\"\n\n\twith open(archive_path, \"wb\") as f:\n\t\tf.write(response.content)\n\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(destination_path)\n\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tresponse = requests.get(url)\n\tresponse.raise_for_status()\n\twith tarfile.open(fileobj=BytesIO(response.content)) as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tif not destination_path.exists():\n\t\tdestination_path.mkdir(parents=True)\n\tarchive_path = destination_path / 'repo_archive.tar.gz'\n\tif not archive_path.exists():\n\t\t_LOGGER.info(\"Downloading %s\", url)\n\t\tdownload(url, str(archive_path))\n\twith tarfile.open(archive_path) as tar:\n\t\tpackage_paths = []\n\t\tfor member in tar.getmembers():\n\t\t\tif member.name.endswith('desc'):\n\t\t\t\tpackage_paths.append(destination_path / member.name)\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = Path(destination_path, 'archive')\n\n\twith requests.get(url, stream=True) as r:\n\t\twith tarfile.open(fileobj=r.raw, mode='r|gz') as archive:\n\t\t\tarchive.extractall(path=archive_path)\n\n\treturn archive_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tlogger.info('Archive download and extraction: ' + url)\n\ttmp_dir = tempfile.mkdtemp()\n\tlogger.debug('Download temporary directory: ' + tmp_dir)\n\ttmp_archive_path = Path(tmp_dir) / 'archive.tar.gz'\n\trequest.urlretrieve(url, filename=tmp_archive_path)\n\twith tarfile.open(tmp_archive_path) as archive:\n\t\tarchive.extractall(path=destination_path)\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\timport tarfile\n\timport urllib.request\n\turllib.request.urlretrieve(url, filename=destination_path)\n\tarchive_path = destination_path\n\twith tarfile.open(str(destination_path), \"r:gz\") as tar:\n\t\ttar.extractall(path=destination_path.parent)\n\treturn archive_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = download_file(url)\n\treturn extract_archive(archive_path, destination_path)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/systeminfo.py", "project": "cloudmesh/cloudmesh-common", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system() == \"Darwin\"\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn True if platform.system() == 'Darwin' else False\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system().lower() == \"darwin\"\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system() == \"Darwin\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system() == \"Darwin\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn True if platform.system() == 'Darwin' else False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system().lower() == \"darwin\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn sys.platform == \"darwin\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\treturn platform.system() == \"Darwin\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tpatterns = []\n\tfor key in item.keys():\n\t\tpatterns.append(key)\n\t\n\tregex_dict = {}\n\tfor pattern in patterns:\n\t\tregex_dict[re.compile(pattern)] = item[pattern]\n\treturn regex_dict\n\nclass RegexDict(dict):\n\t\"\"\"\n\tCreate a dictionary with regex keys.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(RegexDict, self).__init__(*args, **kwargs)\n\n\tdef __getitem__(self, key):\n\t\tfor k in self:\n\t\t\tif k.match(key):\n\t\t\t\treturn self.get(k)\n\t\traise KeyError\n\n\tdef __setitem__(self, key, value):\n\t\tfor k in self:\n\t\t\tif k.match(key):\n\t\t\t\treturn self.get(k)\n\t\traise KeyError\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor key, value in item.items():\n\t\tret[re.compile(key)] = value\n\n\treturn ret\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tnew_dict = {}\n\tfor key, value in item.items():\n\t\tnew_key = re.compile(key)\n\t\tnew_dict[new_key] = value\n\treturn new_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tr = {}\n\tfor k in item.keys():\n\t\tr[re.compile(k)] = item[k]\n\treturn r\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tresult = {}\n\tfor key in item.keys():\n\t\tresult[re.compile(key)] = item[key]\n\treturn result\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key, values in item.items():\n\t\tregex_dict[re.compile(key)] = values\n\treturn regex_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\treturn {re.compile(key): value for key, value in item.items()}\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item:\n\t\tregex_dict[re.compile(key)] = item[key]\n\treturn regex_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item:\n\t\tregex_dict[re.compile(key)] = item[key]\n\treturn regex_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor regex_key, regex_value in item.items():\n\t\tregex_dict[re.compile(regex_key)] = regex_value\n\treturn regex_dict\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tpatterns = []\n\tfor key in item.keys():\n\t\tpatterns.append(key)\n\t\n\tregex_dict = {}\n\tfor pattern in patterns:\n\t\tregex_dict[re.compile(pattern)] = item[pattern]\n\treturn regex_dict\n\nclass RegexDict(dict):\n\t\"\"\"\n\tCreate a dictionary with regex keys.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(RegexDict, self).__init__(*args, **kwargs)\n\n\tdef __getitem__(self, key):\n\t\tfor k in self:\n\t\t\tif k.match(key):\n\t\t\t\treturn self.get(k)\n\t\traise KeyError\n\n\tdef __setitem__(self, key, value):\n\t\tfor k in self:\n\t\t\tif k.match(key):\n\t\t\t\treturn self.get(k)\n\t\traise KeyError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor key, value in item.items():\n\t\tret[re.compile(key)] = value\n\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\timport re\n\tnew_dict = {}\n\tfor key, value in item.items():\n\t\tnew_key = re.compile(key)\n\t\tnew_dict[new_key] = value\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tr = {}\n\tfor k in item.keys():\n\t\tr[re.compile(k)] = item[k]\n\treturn r\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tresult = {}\n\tfor key in item.keys():\n\t\tresult[re.compile(key)] = item[key]\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key, values in item.items():\n\t\tregex_dict[re.compile(key)] = values\n\treturn regex_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\treturn {re.compile(key): value for key, value in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item:\n\t\tregex_dict[re.compile(key)] = item[key]\n\treturn regex_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor key in item:\n\t\tregex_dict[re.compile(key)] = item[key]\n\treturn regex_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tfor regex_key, regex_value in item.items():\n\t\tregex_dict[re.compile(regex_key)] = regex_value\n\treturn regex_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rdiffweb/core/librdiff.py", "project": "ikus060/rdiffweb", "name": "unquote", "docstring": "Remove quote from the given name.", "_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tpattern = re.compile(r'[\\\"\\']')\n\tname = pattern.sub('', name)\n\treturn name\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'^\"([^\"]+)\"$', r'\\1', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r\"^\\\"|\\\"$\", \"\", name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'\"', '', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tif name[-1] == '\\'':\n\t\treturn name[:-1]\n\telse:\n\t\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(\"['\\\"\\\\\\\\]\", \"\", name)\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tresult = re.compile(r'([\\'\"])').sub('', name)\n\treturn result\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'\"', \"\", name)\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\treturn re.sub(r\"^\\\"\", \"\", re.sub(r\"\\\"$\", \"\", name))\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tpattern = re.compile(r'[\\\"\\']')\n\tname = pattern.sub('', name)\n\treturn name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'^\"([^\"]+)\"$', r'\\1', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r\"^\\\"|\\\"$\", \"\", name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'\"', '', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tif name[-1] == '\\'':\n\t\treturn name[:-1]\n\telse:\n\t\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(\"['\\\"\\\\\\\\]\", \"\", name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tresult = re.compile(r'([\\'\"])').sub('', name)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\treturn re.sub(r'\"', \"\", name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\treturn re.sub(r\"^\\\"\", \"\", re.sub(r\"\\\"$\", \"\", name))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/shlex.py", "project": "cloudmesh/cloudmesh-common", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\t# if platform == 0:\n\t# \treturn s.split(';')\n\treturn s.split(';')\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tplatform = platform_detect()\n\tif platform == 0:\n\t\treturn split_cmd(s)\n\telse:\n\t\treturn s.split()\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\tif platform == 1:\n\t\treturn s.split()\n\telif platform == 0:\n\t\treturn s.split(' ')\n\telse:\n\t\traise ValueError(\"Unknown platform\")\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 1:\n\t\treturn s.split()\n\telif platform == 0:\n\t\treturn s.split(';')\n\telse:\n\t\treturn s.split(os.pathsep)\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'nt':\n\t\t\treturn split(s, platform = 0)\n\t\telse:\n\t\t\treturn split(s, platform = 1)\n\telif platform == 0:\n\t\tif os.name == 'nt':\n\t\t\treturn s.split('\\\\')\n\t\telse:\n\t\t\treturn s.split('/')\n\telif platform == 1:\n\t\treturn s.split('/')\n\telse:\n\t\traise ValueError(\"Unsupported platform\")\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\treturn split(s, os.name)\n\tif platform == 1:\n\t\treturn s.split(':')\n\tif platform == 0:\n\t\treturn s.split(';')\n\treturn []\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tplatform = 1 if os.name == 'posix' else 0\n\tif platform == 1:\n\t\treturn s.split()\n\telse:\n\t\treturn s.split(';')\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\timport platform\n\t\tif platform.system() == 'Windows':\n\t\t\treturn split(s, platform=0)\n\t\telse:\n\t\t\treturn split(s, platform=1)\n\telif platform == 0:\n\t\tif type(s) is not str:\n\t\t\traise TypeError\n\t\tif len(s) == 0:\n\t\t\treturn []\n\t\telif len(s) == 1:\n\t\t\treturn [s]\n\t\treturn [s[0]] + split(s[1:], platform=0)\n\telif platform == 1:\n\t\tif type(s) is not str:\n\t\t\traise TypeError\n\t\tif len(s) == 0:\n\t\t\treturn []\n\t\telif len(s) == 1:\n\t\t\treturn [s]\n\t\telse:\n\t\t\treturn split(s[1:], platform=1) + [s[0]]\n\telse:\n\t\traise ValueError\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\tif platform == 1:\n\t\treturn s.split()\n\telif platform == 0:\n\t\treturn s.split(' ')\n\traise Exception('Invalid platform')\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\treturn split(s, _get_platform())\n\tif platform == 0:\n\t\treturn s.split(';')\n\tif platform == 1:\n\t\treturn s.split(':')\n\tprint('platform must be 0, 1, or \"this\"')\n\treturn []\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\t# if platform == 0:\n\t# \treturn s.split(';')\n\treturn s.split(';')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tplatform = platform_detect()\n\tif platform == 0:\n\t\treturn split_cmd(s)\n\telse:\n\t\treturn s.split()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\tif platform == 1:\n\t\treturn s.split()\n\telif platform == 0:\n\t\treturn s.split(' ')\n\telse:\n\t\traise ValueError(\"Unknown platform\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 1:\n\t\treturn s.split()\n\telif platform == 0:\n\t\treturn s.split(';')\n\telse:\n\t\treturn s.split(os.pathsep)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'nt':\n\t\t\treturn split(s, platform = 0)\n\t\telse:\n\t\t\treturn split(s, platform = 1)\n\telif platform == 0:\n\t\tif os.name == 'nt':\n\t\t\treturn s.split('\\\\')\n\t\telse:\n\t\t\treturn s.split('/')\n\telif platform == 1:\n\t\treturn s.split('/')\n\telse:\n\t\traise ValueError(\"Unsupported platform\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\treturn split(s, os.name)\n\tif platform == 1:\n\t\treturn s.split(':')\n\tif platform == 0:\n\t\treturn s.split(';')\n\treturn []\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tplatform = 1 if os.name == 'posix' else 0\n\tif platform == 1:\n\t\treturn s.split()\n\telse:\n\t\treturn s.split(';')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\timport platform\n\t\tif platform.system() == 'Windows':\n\t\t\treturn split(s, platform=0)\n\t\telse:\n\t\t\treturn split(s, platform=1)\n\telif platform == 0:\n\t\tif type(s) is not str:\n\t\t\traise TypeError\n\t\tif len(s) == 0:\n\t\t\treturn []\n\t\telif len(s) == 1:\n\t\t\treturn [s]\n\t\treturn [s[0]] + split(s[1:], platform=0)\n\telif platform == 1:\n\t\tif type(s) is not str:\n\t\t\traise TypeError\n\t\tif len(s) == 0:\n\t\t\treturn []\n\t\telif len(s) == 1:\n\t\t\treturn [s]\n\t\telse:\n\t\t\treturn split(s[1:], platform=1) + [s[0]]\n\telse:\n\t\traise ValueError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\tif os.name == 'posix':\n\t\t\tplatform = 1\n\t\telse:\n\t\t\tplatform = 0\n\tif platform == 1:\n\t\treturn s.split()\n\telif platform == 0:\n\t\treturn s.split(' ')\n\traise Exception('Invalid platform')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tif platform == 'this':\n\t\treturn split(s, _get_platform())\n\tif platform == 0:\n\t\treturn s.split(';')\n\tif platform == 1:\n\t\treturn s.split(':')\n\tprint('platform must be 0, 1, or \"this\"')\n\treturn []\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/tests/__init__.py", "project": "SoftwareHeritage/swh-lister", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport tarfile\n\timport tempfile\n\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\twith tempfile.TemporaryDirectory(dir=tmp_path) as tmp:\n\t\ttar = tarfile.open(archive_path, \"r:gz\")\n\t\ttar.extractall(tmp)\n\t\ttar.close()\n\n\t\tfor root, dirs, files in os.walk(tmp):\n\t\t\tif len(files) == 1:\n\t\t\t\treturn os.path.join(\"file://\", os.path.abspath(root), files[0])\n\n\t\traise Exception(\"Repository not found in archive\")\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport tempfile\n\n\tif not os.path.isfile(archive_path):\n\t\traise ValueError(\"Archive path does not exist: {}\".format(archive_path))\n\n\twith tempfile.TemporaryDirectory(dir=tmp_path) as temp_dir:\n\t\t# extract archive\n\t\tarchive = zipfile.ZipFile(archive_path)\n\t\tarchive.extractall(temp_dir)\n\n\t\t# get the first file to create a repo\n\t\t# if filename is None, use the first file in the archive\n\t\tif filename is None:\n\t\t\tfiles = os.listdir(temp_dir)\n\t\t\tif len(files) > 0:\n\t\t\t\tfilename = files[0]\n\t\t\telse:\n\t\t\t\traise ValueError(\"No file found in the archive\")\n\n\t\t# create repo\n\t\trepo_dir = os.path.join(temp_dir, filename)\n\t\trepo_url = create_repository(root_dir=repo_dir)\n\t\tshutil.rmtree(temp_dir)\n\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\tarchive_tmp = os.path.join(tmp_path, filename)\n\tshutil.copyfile(archive_path, archive_tmp)\n\trepo_path = extract_archive(archive_tmp)\n\trepo_url = \"file://\" + str(repo_path)\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif not archive_path:\n\t\traise Exception(\"Archive path should be a valid path.\")\n\twith TemporaryDirectory(dir=tmp_path) as tmp:\n\t\tif filename:\n\t\t\ttarfile_path = Path(tmp) / filename\n\t\telse:\n\t\t\ttarfile_path = Path(tmp) / archive_path.split(\"/\")[-1]\n\t\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(path=tmp)\n\n\t\treturn \"file://\" + tarfile_path\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif not filename:\n\t\tfilename = Path(archive_path).name\n\n\tif not Path(archive_path).exists():\n\t\traise FileNotFoundError(f\"No such file: {archive_path}\")\n\n\twith TemporaryDirectory(dir=tmp_path) as tmp_dir:\n\t\tarchive_dir = Path(archive_path).parent\n\t\tarchive_name = Path(archive_path).name\n\t\textracted_path = Path(tmp_dir) / \"extracted\"\n\t\textracted_path.mkdir()\n\t\tsubprocess.check_call([\"tar\", \"-xzf\", archive_path, \"-C\", extracted_path])\n\t\trepo_path = Path(extracted_path) / filename\n\t\trepo_url = \"file://\" + str(repo_path)\n\t\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif not filename:\n\t\tfilename = archive_path.split('/')[-1]\n\t\tfilename = filename.split('.')[0]\n\tif not Path(archive_path).is_file():\n\t\tprint(f\"Could not find {archive_path} at {Path(archive_path).absolute()}\")\n\t\traise FileNotFoundError\n\twith tempfile.TemporaryDirectory(dir=tmp_path) as tmpdirname:\n\t\tfile_path = Path(tmpdirname) / filename\n\t\twith ZipFile(archive_path, 'r') as zipObj:\n\t\t\tzipObj.extractall(tmpdirname)\n\t\t\tzipObj.close()\n\t\tif filename.endswith('.tar.gz'):\n\t\t\treturn prepare_repository_from_tar(file_path)\n\t\telse:\n\t\t\treturn prepare_repository_from_zip(file_path)\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\n\ttmpdir = tempfile.mkdtemp(dir=tmp_path)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(tmpdir)\n\n\treturn f\"file://{os.path.join(tmpdir, filename)}\"\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\t# Remove .gz or .zip extension\n\tif filename.endswith(\".gz\"):\n\t\tfilename = filename.replace(\".gz\", \"\")\n\telif filename.endswith(\".zip\"):\n\t\tfilename = filename.replace(\".zip\", \"\")\n\telse:\n\t\traise Exception(\"Unknown archive format. Only .gz or .zip files are allowed\")\n\n\t# Extract the archive\n\textracted_path = extract_archive(archive_path, tmp_path)\n\n\t# Find the path of the project\n\tproject_path = (\n\t\tPath(extracted_path)\n\t\t.glob(\"**/\" + filename)\n\t\t.filter(lambda x: x.is_dir())\n\t\t.__next__()\n\t)\n\n\tif project_path is None:\n\t\traise Exception(\n\t\t\t\"No project found inside the archive. Please make sure the project name is the same as the archive name\"\n\t\t)\n\n\t# Prepare the repo url\n\trepo_url = \"file://\" + str(project_path)\n\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport zipfile\n\timport tarfile\n\timport urllib.parse\n\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\n\timport tempfile\n\tarchive_file = os.path.join(tempfile.gettempdir(), filename)\n\tshutil.copyfile(archive_path, archive_file)\n\n\tif archive_file.endswith(\".zip\"):\n\t\tzf = zipfile.ZipFile(archive_file, 'r')\n\t\tzf.extractall(tmp_path)\n\t\turl = urllib.parse.urljoin(\n\t\t\t\"file://\",\n\t\t\tos.path.join(tmp_path, zf.namelist()[0]),\n\t\t)\n\telif archive_file.endswith(\".tar.gz\") or archive_file.endswith(\".tgz\"):\n\t\ttf = tarfile.open(archive_file, \"r:gz\")\n\t\ttf.extractall(tmp_path)\n\t\turl = urllib.parse.urljoin(\n\t\t\t\"file://\",\n\t\t\tos.path.join(tmp_path, tf.getnames()[0]),\n\t\t)\n\telse:\n\t\traise ValueError", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport tarfile\n\n\trepo_path = os.path.join(tmp_path, filename or os.path.basename(archive_path))\n\tif os.path.exists(repo_path):\n\t\tshutil.rmtree(repo_path)\n\tos.mkdir(repo_path)\n\twith tarfile.open(archive_path) as archive:\n\t\tarchive.extractall(path=repo_path)\n\treturn f\"file://{repo_path}\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport tarfile\n\timport tempfile\n\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\twith tempfile.TemporaryDirectory(dir=tmp_path) as tmp:\n\t\ttar = tarfile.open(archive_path, \"r:gz\")\n\t\ttar.extractall(tmp)\n\t\ttar.close()\n\n\t\tfor root, dirs, files in os.walk(tmp):\n\t\t\tif len(files) == 1:\n\t\t\t\treturn os.path.join(\"file://\", os.path.abspath(root), files[0])\n\n\t\traise Exception(\"Repository not found in archive\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport tempfile\n\n\tif not os.path.isfile(archive_path):\n\t\traise ValueError(\"Archive path does not exist: {}\".format(archive_path))\n\n\twith tempfile.TemporaryDirectory(dir=tmp_path) as temp_dir:\n\t\t# extract archive\n\t\tarchive = zipfile.ZipFile(archive_path)\n\t\tarchive.extractall(temp_dir)\n\n\t\t# get the first file to create a repo\n\t\t# if filename is None, use the first file in the archive\n\t\tif filename is None:\n\t\t\tfiles = os.listdir(temp_dir)\n\t\t\tif len(files) > 0:\n\t\t\t\tfilename = files[0]\n\t\t\telse:\n\t\t\t\traise ValueError(\"No file found in the archive\")\n\n\t\t# create repo\n\t\trepo_dir = os.path.join(temp_dir, filename)\n\t\trepo_url = create_repository(root_dir=repo_dir)\n\t\tshutil.rmtree(temp_dir)\n\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\tarchive_tmp = os.path.join(tmp_path, filename)\n\tshutil.copyfile(archive_path, archive_tmp)\n\trepo_path = extract_archive(archive_tmp)\n\trepo_url = \"file://\" + str(repo_path)\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif not archive_path:\n\t\traise Exception(\"Archive path should be a valid path.\")\n\twith TemporaryDirectory(dir=tmp_path) as tmp:\n\t\tif filename:\n\t\t\ttarfile_path = Path(tmp) / filename\n\t\telse:\n\t\t\ttarfile_path = Path(tmp) / archive_path.split(\"/\")[-1]\n\t\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(path=tmp)\n\n\t\treturn \"file://\" + tarfile_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif not filename:\n\t\tfilename = Path(archive_path).name\n\n\tif not Path(archive_path).exists():\n\t\traise FileNotFoundError(f\"No such file: {archive_path}\")\n\n\twith TemporaryDirectory(dir=tmp_path) as tmp_dir:\n\t\tarchive_dir = Path(archive_path).parent\n\t\tarchive_name = Path(archive_path).name\n\t\textracted_path = Path(tmp_dir) / \"extracted\"\n\t\textracted_path.mkdir()\n\t\tsubprocess.check_call([\"tar\", \"-xzf\", archive_path, \"-C\", extracted_path])\n\t\trepo_path = Path(extracted_path) / filename\n\t\trepo_url = \"file://\" + str(repo_path)\n\t\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif not filename:\n\t\tfilename = archive_path.split('/')[-1]\n\t\tfilename = filename.split('.')[0]\n\tif not Path(archive_path).is_file():\n\t\tprint(f\"Could not find {archive_path} at {Path(archive_path).absolute()}\")\n\t\traise FileNotFoundError\n\twith tempfile.TemporaryDirectory(dir=tmp_path) as tmpdirname:\n\t\tfile_path = Path(tmpdirname) / filename\n\t\twith ZipFile(archive_path, 'r') as zipObj:\n\t\t\tzipObj.extractall(tmpdirname)\n\t\t\tzipObj.close()\n\t\tif filename.endswith('.tar.gz'):\n\t\t\treturn prepare_repository_from_tar(file_path)\n\t\telse:\n\t\t\treturn prepare_repository_from_zip(file_path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\n\ttmpdir = tempfile.mkdtemp(dir=tmp_path)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(tmpdir)\n\n\treturn f\"file://{os.path.join(tmpdir, filename)}\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\t# Remove .gz or .zip extension\n\tif filename.endswith(\".gz\"):\n\t\tfilename = filename.replace(\".gz\", \"\")\n\telif filename.endswith(\".zip\"):\n\t\tfilename = filename.replace(\".zip\", \"\")\n\telse:\n\t\traise Exception(\"Unknown archive format. Only .gz or .zip files are allowed\")\n\n\t# Extract the archive\n\textracted_path = extract_archive(archive_path, tmp_path)\n\n\t# Find the path of the project\n\tproject_path = (\n\t\tPath(extracted_path)\n\t\t.glob(\"**/\" + filename)\n\t\t.filter(lambda x: x.is_dir())\n\t\t.__next__()\n\t)\n\n\tif project_path is None:\n\t\traise Exception(\n\t\t\t\"No project found inside the archive. Please make sure the project name is the same as the archive name\"\n\t\t)\n\n\t# Prepare the repo url\n\trepo_url = \"file://\" + str(project_path)\n\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport zipfile\n\timport tarfile\n\timport urllib.parse\n\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\n\timport tempfile\n\tarchive_file = os.path.join(tempfile.gettempdir(), filename)\n\tshutil.copyfile(archive_path, archive_file)\n\n\tif archive_file.endswith(\".zip\"):\n\t\tzf = zipfile.ZipFile(archive_file, 'r')\n\t\tzf.extractall(tmp_path)\n\t\turl = urllib.parse.urljoin(\n\t\t\t\"file://\",\n\t\t\tos.path.join(tmp_path, zf.namelist()[0]),\n\t\t)\n\telif archive_file.endswith(\".tar.gz\") or archive_file.endswith(\".tgz\"):\n\t\ttf = tarfile.open(archive_file, \"r:gz\")\n\t\ttf.extractall(tmp_path)\n\t\turl = urllib.parse.urljoin(\n\t\t\t\"file://\",\n\t\t\tos.path.join(tmp_path, tf.getnames()[0]),\n\t\t)\n\telse:\n\t\traise ValueError", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport os\n\timport shutil\n\timport tarfile\n\n\trepo_path = os.path.join(tmp_path, filename or os.path.basename(archive_path))\n\tif os.path.exists(repo_path):\n\t\tshutil.rmtree(repo_path)\n\tos.mkdir(repo_path)\n\twith tarfile.open(archive_path) as archive:\n\t\tarchive.extractall(path=repo_path)\n\treturn f\"file://{repo_path}\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/flashbake/plugins/ignored.py", "project": "commandline/flashbake", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored_filed = subprocess.check_output('git ls-files -o --exclude-standard', shell=True)\n\tignored_file_list = ignored_filed.split('\\n')\n\tignored_file_list.remove('')\n\tignored_file_list.sort()\n\tignored_file_list = [x for x in ignored_file_list if x in ignored]\n\treturn ','.join(ignored_file_list)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = \"git clean -n -x -d\"\n\tignored = subprocess.check_output(cmd, shell=True)\n\tignored = ignored.split()\n\tignored.sort()\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles = subprocess.check_output(['git', 'ls-files']).split('\\n')\n\tignored = ','.join(sorted(set(files) & set(ignored)))\n\treturn ignored\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\toutput = subprocess.check_output(\"git ls-files --others --ignored --exclude-standard\", shell=True).decode(\"utf-8\")\n\tfiles = [file.strip() for file in output.splitlines()]\n\tfiles = sorted(files, key=lambda x: x.lower())\n\t# print(files)\n\tignored = []\n\tif ignored:\n\t\tignored = sorted(ignored, key=lambda x: x.lower())\n\t\t# print(ignored)\n\t\tfor file in files:\n\t\t\tif file in ignored:\n\t\t\t\tfiles.remove(file)\n\t# print(files)\n\treturn ','.join(files)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tif ignored == \"\":\n\t\tignored = 'ignore'\n\tignored_files = git_command.git_ignore_list()\n\tignored_files = [x.strip('\\n') for x in ignored_files]\n\tignored_files.sort()\n\tif not ignored_files:\n\t\treturn \"\"\n\treturn \",\".join(ignored_files)\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tif ignored:\n\t\tignored = ignored.split(',')\n\t\tignored.sort()\n\t\tignored = ','.join(ignored)\n\t\treturn ignored\n\telse:\n\t\treturn None\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_ignore_filenames = subprocess.check_output([\"git\", \"check-ignore\"] + ignored)\n\tgit_ignore_filenames = git_ignore_filenames.decode()\n\tgit_ignore_filenames = filter(None, git_ignore_filenames.split('\\n'))\n\tgit_ignore_filenames = sorted(git_ignore_filenames)\n\treturn \", \".join(git_ignore_filenames)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_output = subprocess.getoutput(\"git ls-files --ignored --exclude-standard -z\")\n\tgit_output = git_output.split('\\x00')\n\tgit_output = set(git_output)\n\tgit_output = sorted(git_output)\n\tignored_files = []\n\tfor file_name in git_output:\n\t\tif ignored.match(file_name):\n\t\t\tignored_files.append(file_name)\n\treturn \",\".join(ignored_files)\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\ttry:\n\t\tgitignore = subprocess.check_output([\"git\", \"check-ignore\"]).decode(encoding=\"utf-8\")\n\t\tgitignore = gitignore.split(\"\\n\")\n\t\tgitignore = [word for word in gitignore if word != \"\"]\n\t\tgitignore = [word for word in gitignore if word[0] != \"#\"]\n\t\tgitignore = [word for word in gitignore if word not in ignored]\n\texcept subprocess.CalledProcessError as error:\n\t\tprint(error)\n\t\treturn \"\"\n\tgitignore.sort()\n\treturn \",\".join(gitignore)\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t# git ls-files --ignored --others --exclude-standard\n\t# git ls-files --ignored --exclude-standard\n\t# git ls-files --ignored --exclude-per-directory\n\t# git ls-files --ignored --exclude-standard --others\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard\n\t# git ls-files --ignored --exclude-standard --others --ignored-file-only\n\t# git ls-files --ignored --others --exclude-standard --ignored-file-only\n\t# git ls-files --ignored --exclude-standard --ignored-file-only --others\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only --others\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only --others"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored_filed = subprocess.check_output('git ls-files -o --exclude-standard', shell=True)\n\tignored_file_list = ignored_filed.split('\\n')\n\tignored_file_list.remove('')\n\tignored_file_list.sort()\n\tignored_file_list = [x for x in ignored_file_list if x in ignored]\n\treturn ','.join(ignored_file_list)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = \"git clean -n -x -d\"\n\tignored = subprocess.check_output(cmd, shell=True)\n\tignored = ignored.split()\n\tignored.sort()\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles = subprocess.check_output(['git', 'ls-files']).split('\\n')\n\tignored = ','.join(sorted(set(files) & set(ignored)))\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\toutput = subprocess.check_output(\"git ls-files --others --ignored --exclude-standard\", shell=True).decode(\"utf-8\")\n\tfiles = [file.strip() for file in output.splitlines()]\n\tfiles = sorted(files, key=lambda x: x.lower())\n\t# print(files)\n\tignored = []\n\tif ignored:\n\t\tignored = sorted(ignored, key=lambda x: x.lower())\n\t\t# print(ignored)\n\t\tfor file in files:\n\t\t\tif file in ignored:\n\t\t\t\tfiles.remove(file)\n\t# print(files)\n\treturn ','.join(files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tif ignored == \"\":\n\t\tignored = 'ignore'\n\tignored_files = git_command.git_ignore_list()\n\tignored_files = [x.strip('\\n') for x in ignored_files]\n\tignored_files.sort()\n\tif not ignored_files:\n\t\treturn \"\"\n\treturn \",\".join(ignored_files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tif ignored:\n\t\tignored = ignored.split(',')\n\t\tignored.sort()\n\t\tignored = ','.join(ignored)\n\t\treturn ignored\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_ignore_filenames = subprocess.check_output([\"git\", \"check-ignore\"] + ignored)\n\tgit_ignore_filenames = git_ignore_filenames.decode()\n\tgit_ignore_filenames = filter(None, git_ignore_filenames.split('\\n'))\n\tgit_ignore_filenames = sorted(git_ignore_filenames)\n\treturn \", \".join(git_ignore_filenames)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_output = subprocess.getoutput(\"git ls-files --ignored --exclude-standard -z\")\n\tgit_output = git_output.split('\\x00')\n\tgit_output = set(git_output)\n\tgit_output = sorted(git_output)\n\tignored_files = []\n\tfor file_name in git_output:\n\t\tif ignored.match(file_name):\n\t\t\tignored_files.append(file_name)\n\treturn \",\".join(ignored_files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\ttry:\n\t\tgitignore = subprocess.check_output([\"git\", \"check-ignore\"]).decode(encoding=\"utf-8\")\n\t\tgitignore = gitignore.split(\"\\n\")\n\t\tgitignore = [word for word in gitignore if word != \"\"]\n\t\tgitignore = [word for word in gitignore if word[0] != \"#\"]\n\t\tgitignore = [word for word in gitignore if word not in ignored]\n\texcept subprocess.CalledProcessError as error:\n\t\tprint(error)\n\t\treturn \"\"\n\tgitignore.sort()\n\treturn \",\".join(gitignore)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t# git ls-files --ignored --others --exclude-standard\n\t# git ls-files --ignored --exclude-standard\n\t# git ls-files --ignored --exclude-per-directory\n\t# git ls-files --ignored --exclude-standard --others\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard\n\t# git ls-files --ignored --exclude-standard --others --ignored-file-only\n\t# git ls-files --ignored --others --exclude-standard --ignored-file-only\n\t# git ls-files --ignored --exclude-standard --ignored-file-only --others\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only --others\n\t# git ls-files --ignored --ignored-file-only --others --exclude-standard --ignored-file-only --others", "is_pass": false, "return_code": 1}]}
{"file_path": "docopt/__init__.py", "project": "bazaar-projects/docopt-ng", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.split('.')[-1].lower() == 'doxyfile'\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tif filename.lower().endswith('doxyfile'):\n\t\treturn True\n\treturn False\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith(('.doxyfile',))\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower() == 'doxyfile'\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tif filename.endswith('.doxyfile'):\n\t\treturn True\n\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('.doxyfile')\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('doxyfile')\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tif filename.lower().endswith('doxyfile'):\n\t\treturn True\n\treturn False\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('doxyfile')\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('doxyfile')\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.split('.')[-1].lower() == 'doxyfile'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tif filename.lower().endswith('doxyfile'):\n\t\treturn True\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith(('.doxyfile',))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower() == 'doxyfile'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tif filename.endswith('.doxyfile'):\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('.doxyfile')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('doxyfile')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tif filename.lower().endswith('doxyfile'):\n\t\treturn True\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('doxyfile')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\treturn filename.lower().endswith('doxyfile')\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/borg/check.py", "project": "witten/borgmatic", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t# if frequency is None or \"always\", return None\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\t# parse the time unit\n\tif frequency == \"day\":\n\t\ttimeunit = datetime.timedelta(days=1)\n\telif frequency == \"hour\":\n\t\ttimeunit = datetime.timedelta(hours=1)\n\telif frequency == \"minute\":\n\t\ttimeunit = datetime.timedelta(minutes=1)\n\telif frequency == \"week\":\n\t\ttimeunit = datetime.timedelta(weeks=1)\n\telif frequency == \"year\":\n\t\ttimeunit = datetime.timedelta(years=1)\n\telse:\n\t\t# not a valid frequency\n\t\traise ValueError(\n\t\t\t\"Frequency string {} is not in the list of accepted frequencies: day, hour, minute, week, year\".format(\n\t\t\t\tfrequency))\n\n\t# parse the number\n\ttry:\n\t\tnumber = int(frequency.split()[0])\n\texcept ValueError:\n\t\t# not a valid frequency\n\t\traise ValueError(\n\t\t\t\"Frequency string {} is not in the list of accepted frequencies: day, hour, minute, week, year\".format(\n\t\t\t\t", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None:\n\t\treturn None\n\tif frequency.lower() == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, timeunit = frequency.split()\n\t\tnumber = int(number)\n\t\tif timeunit.lower() == \"day\" or timeunit.lower() == \"days\":\n\t\t\treturn timedelta(days=number)\n\t\telif timeunit.lower() == \"hour\" or timeunit.lower() == \"hours\":\n\t\t\treturn timedelta(hours=number)\n\t\telif timeunit.lower() == \"minute\" or timeunit.lower() == \"minutes\":\n\t\t\treturn timedelta(minutes=number)\n\t\telif timeunit.lower() == \"second\" or timeunit.lower() == \"seconds\":\n\t\t\treturn timedelta(seconds=number)\n\t\telse:\n\t\t\traise ValueError(\"Invalid time unit: %s\" % timeunit)\n\texcept ValueError as e:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\t# raise Exception(\"Invalid frequency: %s\" % frequency)\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == 'always':\n\t\treturn None\n\ttry:\n\t\tnumber = int(frequency.split()[0])\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency string: %s\" % frequency)\n\tif frequency.split()[1] == 'seconds':\n\t\tunit = timedelta(seconds=number)\n\telif frequency.split()[1] == 'minutes':\n\t\tunit = timedelta(minutes=number)\n\telif frequency.split()[1] == 'hours':\n\t\tunit = timedelta(hours=number)\n\telif frequency.split()[1] == 'days':\n\t\tunit = timedelta(days=number)\n\telif frequency.split()[1] == 'weeks':\n\t\tunit = timedelta(weeks=number)\n\telif frequency.split()[1] == 'months':\n\t\tunit = relativedelta(months=number)\n\telif frequency.split()[1] == 'years':\n\t\tunit = relativedelta(years=number)\n\telse:\n\t\traise ValueError(\"Unknown frequency unit: %s\" % frequency)\n\treturn unit\n\n# Test cases\n# print(parse_frequency(\"3 hours\"))\n#", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == 'always':\n\t\treturn None\n\telse:\n\t\tnumber = 1\n\t\tunit = frequency[-1]\n\t\tif frequency[0] == '-':\n\t\t\tnumber = -1\n\t\t\tunit = frequency[1]\n\n\t\tnumber = int(frequency[0:-1])\n\t\tif unit == 'm':\n\t\t\treturn datetime.timedelta(minutes=number * number)\n\t\telif unit == 'h':\n\t\t\treturn datetime.timedelta(hours=number * number)\n\t\telif unit == 'd':\n\t\t\treturn datetime.timedelta(days=number * number)\n\t\telif unit == 'w':\n\t\t\treturn datetime.timedelta(weeks=number * number)\n\t\telif unit == 'M':\n\t\t\treturn datetime.timedelta(days=number * number * 7)\n\t\telse:\n\t\t\traise ValueError('Frequency could not be parsed')\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t# TODO: Implement the parse_frequency function\n    frequency_dict = {\"hour\": 3600,\n                      \"hours\": 3600,\n                      \"day\": 86400,\n                      \"days\": 86400,\n                      \"week\": 604800,\n                      \"weeks\": 604800,\n                      \"month\": 2629800,\n                      \"months\": 2629800,\n                      \"year\": 31557600,\n                      \"years\": 31557600}\n    if frequency == \"always\":\n        return None\n    else:\n        try:\n            number = int(frequency.split()[0])\n        except ValueError:\n            raise ValueError(\"Frequency must be a number followed by a unit of time. \"\n                             \"For instance: '3 days'\")\n        try:\n            unit = frequency.split()[1].lower()\n        except IndexError:\n            raise ValueError(\"Frequency must be a number followed by a unit of time. \"\n                             \"For instance: '3 days'\")\n        return datetime.timedelta(seconds=frequency_dict", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency == \"always\":\n\t\treturn None\n\telse:\n\t\ttry:\n\t\t\tnumber = int(frequency.split()[0])\n\t\t\tunit = frequency.split()[1]\n\t\t\tif unit == 'week':\n\t\t\t\tunit = 'weeks'\n\t\t\telif unit == 'month':\n\t\t\t\tunit = 'months'\n\t\t\treturn datetime.timedelta(**{unit: number})\n\t\texcept:\n\t\t\traise ValueError(\"Frequency cannot be parsed\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\telse:\n\t\t# look for the space\n\t\ttry:\n\t\t\tpos = frequency.find(\" \")\n\t\texcept:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n\n\t\t# look for a number\n\t\ttry:\n\t\t\tnumber = int(frequency[0:pos])\n\t\texcept:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n\n\t\t# look for the time unit\n\t\ttimeunit = frequency[pos + 1:]\n\t\tif timeunit.lower() == \"days\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif timeunit.lower() == \"hours\":\n\t\t\treturn datetime.timedelta(hours=number)\n\t\telif timeunit.lower() == \"minutes\":\n\t\t\treturn datetime.timedelta(minutes=number)\n\t\telif timeunit.lower() == \"seconds\":\n\t\t\treturn datetime.timedelta(seconds=number)\n\t\telse:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, unit = frequency.split(\" \")\n\texcept ValueError:\n\t\traise ValueError(\"Frequency must be given as a number followed by a time unit. For instance, '3 hour'.\")\n\tif unit == \"hour\":\n\t\treturn timedelta(hours=int(number))\n\telif unit == \"day\":\n\t\treturn timedelta(days=int(number))\n\telif unit == \"week\":\n\t\treturn timedelta(weeks=int(number))\n\telif unit == \"month\":\n\t\treturn timedelta(weeks=int(number) * 4)\n\telif unit == \"year\":\n\t\treturn timedelta(weeks=int(number) * 52)\n\telse:\n\t\traise ValueError(\"Unknown time unit. Please use 'hour', 'day', 'week', 'month' or 'year'.\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\tnumber = int(frequency[0:-3])\n\tunit = frequency[-3:]\n\tif unit == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit == \"min\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit == \"sec\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif unit == \"ms\":\n\t\treturn datetime.timedelta(milliseconds=number)\n\telif unit == \"us\":\n\t\treturn datetime.timedelta(microseconds=number)\n\telif unit == \"ns\":\n\t\treturn datetime.timedelta(microseconds=number)\n\telse:\n\t\traise ValueError(\"Unknown unit of time: \" + unit)\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\t\n\tregex = re.compile(r'^([0-9]+) *([a-z]+)$')\n\tmatch = regex.match(frequency.lower())\n\tif match is None:\n\t\traise ValueError('The given frequency cannot be parsed')\n\t\n\tnumber = match.group(1)\n\tunit = match.group(2)\n\t\n\tif unit == 'second':\n\t\treturn datetime.timedelta(seconds=int(number))\n\t\n\telif unit == 'minute':\n\t\treturn datetime.timedelta(minutes=int(number))\n\t\n\telif unit == 'hour':\n\t\treturn datetime.timedelta(hours=int(number))\n\t\n\telif unit == 'day':\n\t\treturn datetime.timedelta(days=int(number))\n\t\n\telif unit == 'week':\n\t\treturn datetime.timedelta(weeks=int(number))\n\t\n\telif unit == 'month':\n\t\treturn datetime.timedelta(days=30*int(number))\n\t\n\telif unit == 'year':\n\t\treturn datetime.timedelta(days=365*int(number))\n\t\n\telse:\n\t\t"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t# if frequency is None or \"always\", return None\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\t# parse the time unit\n\tif frequency == \"day\":\n\t\ttimeunit = datetime.timedelta(days=1)\n\telif frequency == \"hour\":\n\t\ttimeunit = datetime.timedelta(hours=1)\n\telif frequency == \"minute\":\n\t\ttimeunit = datetime.timedelta(minutes=1)\n\telif frequency == \"week\":\n\t\ttimeunit = datetime.timedelta(weeks=1)\n\telif frequency == \"year\":\n\t\ttimeunit = datetime.timedelta(years=1)\n\telse:\n\t\t# not a valid frequency\n\t\traise ValueError(\n\t\t\t\"Frequency string {} is not in the list of accepted frequencies: day, hour, minute, week, year\".format(\n\t\t\t\tfrequency))\n\n\t# parse the number\n\ttry:\n\t\tnumber = int(frequency.split()[0])\n\texcept ValueError:\n\t\t# not a valid frequency\n\t\traise ValueError(\n\t\t\t\"Frequency string {} is not in the list of accepted frequencies: day, hour, minute, week, year\".format(\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None:\n\t\treturn None\n\tif frequency.lower() == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, timeunit = frequency.split()\n\t\tnumber = int(number)\n\t\tif timeunit.lower() == \"day\" or timeunit.lower() == \"days\":\n\t\t\treturn timedelta(days=number)\n\t\telif timeunit.lower() == \"hour\" or timeunit.lower() == \"hours\":\n\t\t\treturn timedelta(hours=number)\n\t\telif timeunit.lower() == \"minute\" or timeunit.lower() == \"minutes\":\n\t\t\treturn timedelta(minutes=number)\n\t\telif timeunit.lower() == \"second\" or timeunit.lower() == \"seconds\":\n\t\t\treturn timedelta(seconds=number)\n\t\telse:\n\t\t\traise ValueError(\"Invalid time unit: %s\" % timeunit)\n\texcept ValueError as e:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\t# raise Exception(\"Invalid frequency: %s\" % frequency)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == 'always':\n\t\treturn None\n\ttry:\n\t\tnumber = int(frequency.split()[0])\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency string: %s\" % frequency)\n\tif frequency.split()[1] == 'seconds':\n\t\tunit = timedelta(seconds=number)\n\telif frequency.split()[1] == 'minutes':\n\t\tunit = timedelta(minutes=number)\n\telif frequency.split()[1] == 'hours':\n\t\tunit = timedelta(hours=number)\n\telif frequency.split()[1] == 'days':\n\t\tunit = timedelta(days=number)\n\telif frequency.split()[1] == 'weeks':\n\t\tunit = timedelta(weeks=number)\n\telif frequency.split()[1] == 'months':\n\t\tunit = relativedelta(months=number)\n\telif frequency.split()[1] == 'years':\n\t\tunit = relativedelta(years=number)\n\telse:\n\t\traise ValueError(\"Unknown frequency unit: %s\" % frequency)\n\treturn unit\n\n# Test cases\n# print(parse_frequency(\"3 hours\"))\n#", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == 'always':\n\t\treturn None\n\telse:\n\t\tnumber = 1\n\t\tunit = frequency[-1]\n\t\tif frequency[0] == '-':\n\t\t\tnumber = -1\n\t\t\tunit = frequency[1]\n\n\t\tnumber = int(frequency[0:-1])\n\t\tif unit == 'm':\n\t\t\treturn datetime.timedelta(minutes=number * number)\n\t\telif unit == 'h':\n\t\t\treturn datetime.timedelta(hours=number * number)\n\t\telif unit == 'd':\n\t\t\treturn datetime.timedelta(days=number * number)\n\t\telif unit == 'w':\n\t\t\treturn datetime.timedelta(weeks=number * number)\n\t\telif unit == 'M':\n\t\t\treturn datetime.timedelta(days=number * number * 7)\n\t\telse:\n\t\t\traise ValueError('Frequency could not be parsed')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t# TODO: Implement the parse_frequency function\n    frequency_dict = {\"hour\": 3600,\n                      \"hours\": 3600,\n                      \"day\": 86400,\n                      \"days\": 86400,\n                      \"week\": 604800,\n                      \"weeks\": 604800,\n                      \"month\": 2629800,\n                      \"months\": 2629800,\n                      \"year\": 31557600,\n                      \"years\": 31557600}\n    if frequency == \"always\":\n        return None\n    else:\n        try:\n            number = int(frequency.split()[0])\n        except ValueError:\n            raise ValueError(\"Frequency must be a number followed by a unit of time. \"\n                             \"For instance: '3 days'\")\n        try:\n            unit = frequency.split()[1].lower()\n        except IndexError:\n            raise ValueError(\"Frequency must be a number followed by a unit of time. \"\n                             \"For instance: '3 days'\")\n        return datetime.timedelta(seconds=frequency_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency == \"always\":\n\t\treturn None\n\telse:\n\t\ttry:\n\t\t\tnumber = int(frequency.split()[0])\n\t\t\tunit = frequency.split()[1]\n\t\t\tif unit == 'week':\n\t\t\t\tunit = 'weeks'\n\t\t\telif unit == 'month':\n\t\t\t\tunit = 'months'\n\t\t\treturn datetime.timedelta(**{unit: number})\n\t\texcept:\n\t\t\traise ValueError(\"Frequency cannot be parsed\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\telse:\n\t\t# look for the space\n\t\ttry:\n\t\t\tpos = frequency.find(\" \")\n\t\texcept:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n\n\t\t# look for a number\n\t\ttry:\n\t\t\tnumber = int(frequency[0:pos])\n\t\texcept:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n\n\t\t# look for the time unit\n\t\ttimeunit = frequency[pos + 1:]\n\t\tif timeunit.lower() == \"days\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif timeunit.lower() == \"hours\":\n\t\t\treturn datetime.timedelta(hours=number)\n\t\telif timeunit.lower() == \"minutes\":\n\t\t\treturn datetime.timedelta(minutes=number)\n\t\telif timeunit.lower() == \"seconds\":\n\t\t\treturn datetime.timedelta(seconds=number)\n\t\telse:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, unit = frequency.split(\" \")\n\texcept ValueError:\n\t\traise ValueError(\"Frequency must be given as a number followed by a time unit. For instance, '3 hour'.\")\n\tif unit == \"hour\":\n\t\treturn timedelta(hours=int(number))\n\telif unit == \"day\":\n\t\treturn timedelta(days=int(number))\n\telif unit == \"week\":\n\t\treturn timedelta(weeks=int(number))\n\telif unit == \"month\":\n\t\treturn timedelta(weeks=int(number) * 4)\n\telif unit == \"year\":\n\t\treturn timedelta(weeks=int(number) * 52)\n\telse:\n\t\traise ValueError(\"Unknown time unit. Please use 'hour', 'day', 'week', 'month' or 'year'.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\tnumber = int(frequency[0:-3])\n\tunit = frequency[-3:]\n\tif unit == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit == \"min\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit == \"sec\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif unit == \"ms\":\n\t\treturn datetime.timedelta(milliseconds=number)\n\telif unit == \"us\":\n\t\treturn datetime.timedelta(microseconds=number)\n\telif unit == \"ns\":\n\t\treturn datetime.timedelta(microseconds=number)\n\telse:\n\t\traise ValueError(\"Unknown unit of time: \" + unit)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\t\n\tregex = re.compile(r'^([0-9]+) *([a-z]+)$')\n\tmatch = regex.match(frequency.lower())\n\tif match is None:\n\t\traise ValueError('The given frequency cannot be parsed')\n\t\n\tnumber = match.group(1)\n\tunit = match.group(2)\n\t\n\tif unit == 'second':\n\t\treturn datetime.timedelta(seconds=int(number))\n\t\n\telif unit == 'minute':\n\t\treturn datetime.timedelta(minutes=int(number))\n\t\n\telif unit == 'hour':\n\t\treturn datetime.timedelta(hours=int(number))\n\t\n\telif unit == 'day':\n\t\treturn datetime.timedelta(days=int(number))\n\t\n\telif unit == 'week':\n\t\treturn datetime.timedelta(weeks=int(number))\n\t\n\telif unit == 'month':\n\t\treturn datetime.timedelta(days=30*int(number))\n\t\n\telif unit == 'year':\n\t\treturn datetime.timedelta(days=365*int(number))\n\t\n\telse:\n\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_hosts = ['localhost', '127.0.0.1']\n\tlocal_ips = ['127.0.0.1', '::1']\n\tuser_name = os.getenv(\"USER\")\n\tdomain_name = socket.getfqdn()\n\treturn host in local_hosts or host in local_ips or host == user_name or host == domain_name\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ('localhost', 'local', '127.0.0.1')\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ['localhost', '127.0.0.1', '::1', '',\n\t\t\t\t\tsocket.gethostname(), socket.getfqdn(), '0.0.0.0']\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host == 'localhost' or host == '127.0.0.1' or host == '' or host == '::1' or host == '0.0.0.0' or host == '::' or host == '0:0:0:0:0:0:0:1' or host == '::ffff:127.0.0.1' or host in ['::1', '0:0:0:0:0:0:0:1', '127.0.0.1', '0:0:0:0:0:0:0:0', '::', '0.0.0.0'] or host.startswith('127.') or host.lower() == 'localhost' or host.lower() == 'localhost.localdomain' or host.lower() == 'localhost6' or host.lower() == 'localhost6.localdomain6' or host.lower() == 'ip6-localhost' or host.lower() == 'ip6-loopback' or host == 'ip6-localnet' or host.lower() == 'ipv6-localnet' or host == 'ipv6-localnet' or host.lower() == 'ipv6-localhost' or host.lower() == 'ipv6-loopback' or host == 'ipv6", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tif host is None:\n\t\treturn False\n\telif host in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[1] in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[0] in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split('@')[1].split(':')[0] in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[1] == 'localhost':\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[0] == 'localhost':\n\t\treturn True\n\telif host.startswith('http') and host.split('@')[1].split(':')[0] == 'localhost':\n\t\treturn True\n\telif host.startswith('http') and host.split('@')[1].split(':')[1] == 'localhost':\n\t\treturn", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in (socket.gethostname(),\n\t\t\t\t\tsocket.gethostbyname(socket.gethostname()),\n\t\t\t\t\tsocket.gethostname().split('.')[0],\n\t\t\t\t\tsocket.gethostbyname(socket.gethostname().split('.')[0]),\n\t\t\t\t\t'localhost',\n\t\t\t\t\t'127.0.0.1')\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host == 'localhost' or host == '127.0.0.1' or host == '::1' or host == '0.0.0.0' or host == '0:0:0:0:0:0:0:1' or host == socket.gethostname() or host == socket.gethostbyname(socket.gethostname()) or host == socket.gethostbyname(socket.getfqdn()) or host == socket.gethostbyname(socket.getfqdn(host))\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host == 'localhost' or host == '127.0.0.1' or host == socket.gethostname() or host == socket.gethostbyname(socket.gethostname()) or host == socket.getfqdn() or host == socket.gethostbyname(socket.getfqdn())\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ['localhost', 'localhost.localdomain', 'localhost4', 'localhost4.localdomain4',\n\t\t\t\t   'localhost6', 'localhost6.localdomain6', '::1', '127.0.0.1']\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ['localhost', '127.0.0.1', '0.0.0.0', '::1', '::', '0:0:0:0:0:0:0:1'] or socket.gethostbyname(host) in ['127.0.0.1']\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_hosts = ['localhost', '127.0.0.1']\n\tlocal_ips = ['127.0.0.1', '::1']\n\tuser_name = os.getenv(\"USER\")\n\tdomain_name = socket.getfqdn()\n\treturn host in local_hosts or host in local_ips or host == user_name or host == domain_name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ('localhost', 'local', '127.0.0.1')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ['localhost', '127.0.0.1', '::1', '',\n\t\t\t\t\tsocket.gethostname(), socket.getfqdn(), '0.0.0.0']\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host == 'localhost' or host == '127.0.0.1' or host == '' or host == '::1' or host == '0.0.0.0' or host == '::' or host == '0:0:0:0:0:0:0:1' or host == '::ffff:127.0.0.1' or host in ['::1', '0:0:0:0:0:0:0:1', '127.0.0.1', '0:0:0:0:0:0:0:0', '::', '0.0.0.0'] or host.startswith('127.') or host.lower() == 'localhost' or host.lower() == 'localhost.localdomain' or host.lower() == 'localhost6' or host.lower() == 'localhost6.localdomain6' or host.lower() == 'ip6-localhost' or host.lower() == 'ip6-loopback' or host == 'ip6-localnet' or host.lower() == 'ipv6-localnet' or host == 'ipv6-localnet' or host.lower() == 'ipv6-localhost' or host.lower() == 'ipv6-loopback' or host == 'ipv6", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tif host is None:\n\t\treturn False\n\telif host in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[1] in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[0] in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split('@')[1].split(':')[0] in ['127.0.0.1', 'localhost', '::1']:\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[1] == 'localhost':\n\t\treturn True\n\telif host.startswith('http') and host.split(':')[0] == 'localhost':\n\t\treturn True\n\telif host.startswith('http') and host.split('@')[1].split(':')[0] == 'localhost':\n\t\treturn True\n\telif host.startswith('http') and host.split('@')[1].split(':')[1] == 'localhost':\n\t\treturn", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in (socket.gethostname(),\n\t\t\t\t\tsocket.gethostbyname(socket.gethostname()),\n\t\t\t\t\tsocket.gethostname().split('.')[0],\n\t\t\t\t\tsocket.gethostbyname(socket.gethostname().split('.')[0]),\n\t\t\t\t\t'localhost',\n\t\t\t\t\t'127.0.0.1')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host == 'localhost' or host == '127.0.0.1' or host == '::1' or host == '0.0.0.0' or host == '0:0:0:0:0:0:0:1' or host == socket.gethostname() or host == socket.gethostbyname(socket.gethostname()) or host == socket.gethostbyname(socket.getfqdn()) or host == socket.gethostbyname(socket.getfqdn(host))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host == 'localhost' or host == '127.0.0.1' or host == socket.gethostname() or host == socket.gethostbyname(socket.gethostname()) or host == socket.getfqdn() or host == socket.gethostbyname(socket.getfqdn())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ['localhost', 'localhost.localdomain', 'localhost4', 'localhost4.localdomain4',\n\t\t\t\t   'localhost6', 'localhost6.localdomain6', '::1', '127.0.0.1']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\treturn host in ['localhost', '127.0.0.1', '0.0.0.0', '::1', '::', '0:0:0:0:0:0:0:1'] or socket.gethostbyname(host) in ['127.0.0.1']\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/borg/list.py", "project": "witten/borgmatic", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(p if isinstance(p, str) else p.pattern for p in find_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\ttransformed = []\n\tfor path in find_paths:\n\t\tif path.find('*') == -1:\n\t\t\ttransformed.append(path)\n\t\telse:\n\t\t\ttransformed.append(path)\n\treturn tuple(transformed)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tnew_find_paths = []\n\tfor find_path in find_paths:\n\t\tif isinstance(find_path, str):\n\t\t\tif find_path.startswith('*') or find_path.startswith('['):\n\t\t\t\tnew_find_paths.append(find_path)\n\t\t\telse:\n\t\t\t\tnew_find_paths.append('*' + find_path + '*')\n\t\telse:\n\t\t\tnew_find_paths.append(find_path)\n\treturn new_find_paths\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(make_find_path(find_path) for find_path in find_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple([find_paths[i] if not isinstance(find_paths[i], str) else '**/{}'.format(find_paths[i]) for i in range(len(find_paths))])\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(\n\t\tp if isinstance(p, str) and \"*\" not in p else p.replace(\"\\\\\", \"/\")\n\t\tfor p in find_paths\n\t)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tif not find_paths:\n\t\treturn ()\n\telse:\n\t\treturn tuple(map(make_find_path, find_paths))\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(\n\t\tpath if path.startswith(\"*\") else \"*{}\".format(path) for path in find_paths\n\t)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t# return tuple(\n\t#     tuple(\n\t#         tuple(tuple(glob.escape(p) for p in path_element) for path_element in path)\n\t#         for path in find_paths\n\t#     )\n\t# )\n\treturn tuple(\n\t\ttuple(\n\t\t\ttuple(glob.escape(p) for p in path_element) for path_element in path\n\t\t)\n\t\tfor path in find_paths\n\t)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tpaths = []\n\tfor path in find_paths:\n\t\tif path.startswith('*'):\n\t\t\tpaths.append(path)\n\t\telse:\n\t\t\tpaths.append('*%s*' % path)\n\n\treturn tuple(paths)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(p if isinstance(p, str) else p.pattern for p in find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\ttransformed = []\n\tfor path in find_paths:\n\t\tif path.find('*') == -1:\n\t\t\ttransformed.append(path)\n\t\telse:\n\t\t\ttransformed.append(path)\n\treturn tuple(transformed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tnew_find_paths = []\n\tfor find_path in find_paths:\n\t\tif isinstance(find_path, str):\n\t\t\tif find_path.startswith('*') or find_path.startswith('['):\n\t\t\t\tnew_find_paths.append(find_path)\n\t\t\telse:\n\t\t\t\tnew_find_paths.append('*' + find_path + '*')\n\t\telse:\n\t\t\tnew_find_paths.append(find_path)\n\treturn new_find_paths\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(make_find_path(find_path) for find_path in find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple([find_paths[i] if not isinstance(find_paths[i], str) else '**/{}'.format(find_paths[i]) for i in range(len(find_paths))])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(\n\t\tp if isinstance(p, str) and \"*\" not in p else p.replace(\"\\\\\", \"/\")\n\t\tfor p in find_paths\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tif not find_paths:\n\t\treturn ()\n\telse:\n\t\treturn tuple(map(make_find_path, find_paths))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\treturn tuple(\n\t\tpath if path.startswith(\"*\") else \"*{}\".format(path) for path in find_paths\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t# return tuple(\n\t#     tuple(\n\t#         tuple(tuple(glob.escape(p) for p in path_element) for path_element in path)\n\t#         for path in find_paths\n\t#     )\n\t# )\n\treturn tuple(\n\t\ttuple(\n\t\t\ttuple(glob.escape(p) for p in path_element) for path_element in path\n\t\t)\n\t\tfor path in find_paths\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tpaths = []\n\tfor path in find_paths:\n\t\tif path.startswith('*'):\n\t\t\tpaths.append(path)\n\t\telse:\n\t\t\tpaths.append('*%s*' % path)\n\n\treturn tuple(paths)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn 'GIT_PYTHON_GIT_PREFIX' in os.environ\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn hasattr(sys, 'ps1')\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\tif sys.platform == 'win32':\n\t\treturn os.path.exists('C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe')\n\telse:\n\t\treturn False\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn get_shell() == \"git-bash\"\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn 'MSYSTEM' in os.environ\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn 'MSYSTEM' in os.environ and os.environ['MSYSTEM'] == 'MINGW64'\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\ttry:\n\t\tif os.environ['TERM'] == 'msys':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept KeyError:\n\t\treturn False\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn hasattr(sys, 'ps1')\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn os.path.exists('/.git-bash-version')\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn os.name == 'nt' and 'GIT_SHELL_DIR' in os.environ\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn 'GIT_PYTHON_GIT_PREFIX' in os.environ\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn hasattr(sys, 'ps1')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\tif sys.platform == 'win32':\n\t\treturn os.path.exists('C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe')\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn get_shell() == \"git-bash\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn 'MSYSTEM' in os.environ\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn 'MSYSTEM' in os.environ and os.environ['MSYSTEM'] == 'MINGW64'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\ttry:\n\t\tif os.environ['TERM'] == 'msys':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept KeyError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn hasattr(sys, 'ps1')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn os.path.exists('/.git-bash-version')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\treturn os.name == 'nt' and 'GIT_SHELL_DIR' in os.environ\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/generate.py", "project": "witten/borgmatic", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite and os.path.exists(config_filename):\n\t\tprint(\"Configuration already exists at {:s}.  Aborting.\".format(config_filename))\n\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite and os.path.exists(config_filename):\n\t\twith open(config_filename) as old_config:\n\t\t\told_config_contents = old_config.read()\n\t\tif old_config_contents == rendered_config:\n\t\t\treturn\n\t\telse:\n\t\t\tprint(f\"Configuration file {config_filename} exists and is different. \"\n\t\t\t\t\t\"Aborting to avoid overwriting it.\")\n\t\t\tsys.exit(1)\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite and os.path.exists(config_filename):\n\t\tprint(\"Warning: config file already exists. Not overwriting.\")\n\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename):\n\t\tif overwrite:\n\t\t\tprint(\"WARNING: existing configuration file will be overwritten\")\n\t\telse:\n\t\t\tprint(\"WARNING: existing configuration file will not be overwritten\")\n\t\t\tprint(\"To overwrite, run with --overwrite\")\n\t\t\texit(1)\n\tprint(\"Writing configuration to {}\".format(config_filename))\n\twith open(config_filename, 'w') as fd:\n\t\tfd.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename):\n\t\tif not overwrite:\n\t\t\tsys.exit(f\"Configuration file {config_filename} already exists. Exiting.\")\n\t\telse:\n\t\t\twith open(config_filename, 'w') as f:\n\t\t\t\tf.write(rendered_config)\n\t\t\tos.chmod(config_filename, mode)\n\t\t\tsys.exit(f\"Configuration file {config_filename} updated.\")\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\t\tsys.exit(f\"Configuration file {config_filename} created.\")\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not os.path.exists(config_filename) or overwrite:\n\t\twith open(config_filename, \"w\") as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\t\tprint(\"Wrote configuration to file: {0}\".format(config_filename))\n\telse:\n\t\tprint(\"{0} already exists. Aborting.\".format(config_filename))\n\t\tsys.exit(1)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tif not click.confirm(\"Configuration file already exists. Overwrite?\", abort=True):\n\t\t\treturn\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite:\n\t\tif os.path.isfile(config_filename):\n\t\t\tprint(\"Config file ({}) already exists, skipping configuration update.\".format(config_filename))\n\t\t\treturn\n\tif os.path.isfile(config_filename):\n\t\tprint(\"Config file ({}) already exists. Overwriting...\".format(config_filename))\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tprint(\"Wrote config file: {}\".format(config_filename))\n\tos.chmod(config_filename, mode)\n\tprint(\"Set permissions for: {}\".format(config_filename))\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise FileExistsError(f\"{config_filename} already exists.  Set 'overwrite' to 'True' to overwrite.\")\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif isinstance(rendered_config, str):\n\t\trendered_config = yaml.safe_load(rendered_config)\n\tif not overwrite and os.path.exists(config_filename):\n\t\twith open(config_filename, 'r') as f:\n\t\t\ttry:\n\t\t\t\texisting_config = yaml.safe_load(f)\n\t\t\texcept yaml.YAMLError as e:\n\t\t\t\traise Exception(\"Failed to load existing configuration file: {}\".format(e))\n\t\tif existing_config != rendered_config:\n\t\t\traise Exception(\"Existing configuration file is not the same as generated config\")\n\t\telse:\n\t\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tf.write('# Do not edit this file directly. It is generated by the system.')\n\t\tf.write(yaml.dump(rendered_config, default_flow_style=False, encoding='utf-8', allow_unicode=True))\n\tos.chmod(config_filename, mode)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite and os.path.exists(config_filename):\n\t\tprint(\"Configuration already exists at {:s}.  Aborting.\".format(config_filename))\n\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite and os.path.exists(config_filename):\n\t\twith open(config_filename) as old_config:\n\t\t\told_config_contents = old_config.read()\n\t\tif old_config_contents == rendered_config:\n\t\t\treturn\n\t\telse:\n\t\t\tprint(f\"Configuration file {config_filename} exists and is different. \"\n\t\t\t\t\t\"Aborting to avoid overwriting it.\")\n\t\t\tsys.exit(1)\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite and os.path.exists(config_filename):\n\t\tprint(\"Warning: config file already exists. Not overwriting.\")\n\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename):\n\t\tif overwrite:\n\t\t\tprint(\"WARNING: existing configuration file will be overwritten\")\n\t\telse:\n\t\t\tprint(\"WARNING: existing configuration file will not be overwritten\")\n\t\t\tprint(\"To overwrite, run with --overwrite\")\n\t\t\texit(1)\n\tprint(\"Writing configuration to {}\".format(config_filename))\n\twith open(config_filename, 'w') as fd:\n\t\tfd.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename):\n\t\tif not overwrite:\n\t\t\tsys.exit(f\"Configuration file {config_filename} already exists. Exiting.\")\n\t\telse:\n\t\t\twith open(config_filename, 'w') as f:\n\t\t\t\tf.write(rendered_config)\n\t\t\tos.chmod(config_filename, mode)\n\t\t\tsys.exit(f\"Configuration file {config_filename} updated.\")\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\t\tsys.exit(f\"Configuration file {config_filename} created.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not os.path.exists(config_filename) or overwrite:\n\t\twith open(config_filename, \"w\") as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\t\tprint(\"Wrote configuration to file: {0}\".format(config_filename))\n\telse:\n\t\tprint(\"{0} already exists. Aborting.\".format(config_filename))\n\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tif not click.confirm(\"Configuration file already exists. Overwrite?\", abort=True):\n\t\t\treturn\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif not overwrite:\n\t\tif os.path.isfile(config_filename):\n\t\t\tprint(\"Config file ({}) already exists, skipping configuration update.\".format(config_filename))\n\t\t\treturn\n\tif os.path.isfile(config_filename):\n\t\tprint(\"Config file ({}) already exists. Overwriting...\".format(config_filename))\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tprint(\"Wrote config file: {}\".format(config_filename))\n\tos.chmod(config_filename, mode)\n\tprint(\"Set permissions for: {}\".format(config_filename))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise FileExistsError(f\"{config_filename} already exists.  Set 'overwrite' to 'True' to overwrite.\")\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tif isinstance(rendered_config, str):\n\t\trendered_config = yaml.safe_load(rendered_config)\n\tif not overwrite and os.path.exists(config_filename):\n\t\twith open(config_filename, 'r') as f:\n\t\t\ttry:\n\t\t\t\texisting_config = yaml.safe_load(f)\n\t\t\texcept yaml.YAMLError as e:\n\t\t\t\traise Exception(\"Failed to load existing configuration file: {}\".format(e))\n\t\tif existing_config != rendered_config:\n\t\t\traise Exception(\"Existing configuration file is not the same as generated config\")\n\t\telse:\n\t\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tf.write('# Do not edit this file directly. It is generated by the system.')\n\t\tf.write(yaml.dump(rendered_config, default_flow_style=False, encoding='utf-8', allow_unicode=True))\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/Shell.py", "project": "cloudmesh/cloudmesh-common", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.split(\"\\n\"))\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n\tlines = script.splitlines()\n\treturn seperator.join(lines)\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn \" \".join(script.splitlines())\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn \" && \".join(script.split(\"\\n\"))\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tif not isinstance(script, str):\n\t\traise TypeError(\"Argument 'script' must be string.\")\n\tif not isinstance(seperator, str):\n\t\traise TypeError(\"Argument 'seperator' must be string.\")\n\treturn seperator.join(script.split(\"\\n\"))\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.splitlines())\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.split(\"\\n\")\n\treturn seperator.join(script)\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.split(\"\\n\"))\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join([f\"{cmd}\" for cmd in script.split(\"\\n\") if cmd])\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.split(\"\\n\"))\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.split(\"\\n\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n\tlines = script.splitlines()\n\treturn seperator.join(lines)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn \" \".join(script.splitlines())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn \" && \".join(script.split(\"\\n\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tif not isinstance(script, str):\n\t\traise TypeError(\"Argument 'script' must be string.\")\n\tif not isinstance(seperator, str):\n\t\traise TypeError(\"Argument 'seperator' must be string.\")\n\treturn seperator.join(script.split(\"\\n\"))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.splitlines())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.split(\"\\n\")\n\treturn seperator.join(script)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.split(\"\\n\"))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join([f\"{cmd}\" for cmd in script.split(\"\\n\") if cmd])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\treturn seperator.join(script.split(\"\\n\"))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lib/matplotlib/testing/__init__.py", "project": "rougier/matplotlib", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t# Create our environment\n\tenv = os.environ.copy()\n\n\t# Add any extra environment variables\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\t# Create the command\n\tcmd = [sys.executable, \"-m\", func.__module__, func.__name__] + list(args)\n\n\t# Run the subprocess\n\treturn subprocess.run(cmd, timeout=timeout, check=True, env=env)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport json\n\n\tdef get_env_dict():\n\t\t\"\"\"\n\t\tReturn an environment dict that can be used to run a subprocess.\n\t\t\"\"\"\n\t\timport os\n\t\tenv = os.environ.copy()\n\t\tenv[\"PYTHONUNBUFFERED\"] = \"1\"\n\t\tif extra_env is not None:\n\t\t\tenv.update(extra_env)\n\t\treturn env\n\n\tif sys.platform.startswith(\"win\"):\n\t\tcommand = [sys.executable, \"-m\", func.__module__, func.__name__] + list(args)\n\telse:\n\t\tcommand = [sys.executable, \"-m\", f\"{func.__module__}.{func.__name__}\"] + list(args)\n\tcommand_string = \" \".join(command)\n\tprint(f\"Running subprocess: {command_string}\")\n\tres = subprocess.run(command,\n\t\t\t\t\t\t env=get_env_dict(),\n\t\t\t\t\t\t timeout=timeout,\n\t\t\t\t\t\t check=True,\n\t\t\t\t\t\t universal_newlines=True)\n\tprint(f\"Subprocess finished with exit code {res.returncode", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\tif extra_env is None:\n\t\textra_env = {}\n\n\t# Use a unique temporary directory for each test run\n\timport tempfile\n\ttemp_dir = tempfile.TemporaryDirectory()\n\ttmp_env = os.environ.copy()\n\ttmp_env.update(extra_env)\n\ttmp_env[\"PYTHONPATH\"] = os.pathsep.join([\".\", tmp_env.get(\"PYTHONPATH\", \"\")])\n\ttmp_env[\"TEMP\"] = temp_dir.name\n\tcmd = [sys.executable, os.path.join(os.path.dirname(__file__), \"run_test.py\"),\n\t\t\t\t\tos.path.abspath(os.path.dirname(func.__module__)),\n\t\t\t\t\tfunc.__name__] + list(args)\n\tp = subprocess.run(cmd, env=tmp_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\t\t\t\t\t\t\t\t\t\ttimeout=timeout)\n\ttemp_dir.cleanup()\n\treturn p\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t# Use a unique temporary directory for the subprocess.\n\ttemp_dir = tempfile.mkdtemp(prefix='subprocess_run_helper')\n\tos.environ['PYTHONPATH'] = os.getcwd()\n\tos.environ['PYTHONUNBUFFERED'] = '1'\n\tos.environ['PYTHONIOENCODING'] = 'utf8'\n\tif extra_env is not None:\n\t\tos.environ.update(extra_env)\n\n\t# Run the function in a subprocess.\n\tcmd = [sys.executable, '-m', 'mypyc.subprocess_run_helper', func.__module__, func.__name__]\n\tcmd.extend(args)\n\ttry:\n\t\treturn subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\t\t\t\t\t\t  universal_newlines=True, timeout=timeout, check=True,\n\t\t\t\t\t\t\t  env=os.environ, cwd=temp_dir)\n\texcept subprocess.TimeoutExpired as e:\n\t\traise TimeoutError('Timed out while running {}'.format(func)) from e\n\tfinally:\n\t\tshutil.rmtree(temp_dir)\n\n\ndef", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\ttry:\n\t\timport os\n\t\timport subprocess\n\texcept ImportError:\n\t\traise RuntimeError(\"This function is only available in python3\")\n\n\tif extra_env is None:\n\t\textra_env = {}\n\n\targ_str = []\n\tfor arg in args:\n\t\tif isinstance(arg, str):\n\t\t\targ_str.append(arg)\n\t\telse:\n\t\t\targ_str.append(str(arg))\n\n\targ_str = \" \".join(arg_str)\n\n\tp = subprocess.Popen(\n\t\t[\"python3\", \"-c\", \"import {}\".format(func.__module__)],\n\t\tenv=dict(os.environ, **extra_env),\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tclose_fds=True,\n\t)\n\tout, err = p.communicate(\n\t\tinput=\"{}('{}')\".format(func.__name__, arg_str).encode(\"utf-8\")\n\t)\n\tif p.returncode != 0:\n\t\traise RuntimeError(\n\t\t\t\"Error executing python subprocess. Return code {}\".format(p.returncode)\n\t\t)\n\n\tif err", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\t# get the name of the module containing func\n\tmodule = inspect.getmodule(func)\n\tmodule_name = module.__name__\n\n\t# get the name of the function\n\tfunc_name = func.__name__\n\n\t# make the command\n\tcmd = [sys.executable, '-m', module_name, func_name] + list(args)\n\n\t# run the command\n\treturn subprocess.run(cmd, env=env, timeout=timeout)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t# create subprocess\n\tcmd = [sys.executable, '-m', 'autosklearn.util.subprocess', func.__module__, func.__name__]\n\tcmd.extend(args)\n\tlog.debug('Subprocess call: %s', cmd)\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\t# run the subprocess\n\tprocess = subprocess.run(cmd, env=env, timeout=timeout)\n\tif process.returncode != 0:\n\t\traise subprocess.CalledProcessError(process.returncode, process.args)\n\ttry:\n\t\tresult = pickle.loads(base64.b64decode(process.stdout))\n\texcept Exception as e:\n\t\traise RuntimeError(\n\t\t\t'The subprocess returned a result which could not be unpickled: %s' % str(e))\n\treturn result\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tif extra_env is None:\n\t\textra_env = dict()\n\n\timport tempfile\n\timport sys\n\timport os\n\timport subprocess\n\timport importlib\n\timport inspect\n\n\timport torch\n\n\t# save current state\n\tif not os.path.exists(tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save\"):\n\t\tos.makedirs(tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save\")\n\ttorch.save(torch.load('~/.cache/torch/hub/latest.pth'), tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save/latest.pth\")\n\timportlib.reload(sys)\n\tsys.path.insert(0, tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save\")\n\timport latest\n\n\t# run function\n\ttry:\n\t\treturn subprocess.run([sys.executable, '-m', func.__module__, func.__name__] + args, check=True, env=dict(os.environ, **extra_env), timeout=timeout)\n\tfinally:\n\t\t# restore state\n\t\tos.remove(tempfile.gettemp", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport os\n\timport subprocess\n\timport threading\n\timport time\n\n\t# Create an environment for the subprocess\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\tdef run_in_subprocess():\n\t\t# Create a thread to kill the subprocess if it does not complete within timeout seconds\n\t\tt = threading.Timer(timeout, subprocess.Popen.terminate, [p])\n\t\ttry:\n\t\t\tp = subprocess.run(\n\t\t\t\t[\n\t\t\t\t\tsys.executable,\n\t\t\t\t\t'-c',\n\t\t\t\t\t'import %s as m; m.%s(%s)' % (\n\t\t\t\t\t\tfunc.__module__,\n\t\t\t\t\t\tfunc.__name__,\n\t\t\t\t\t\t', '.join(['\"%s\"' % arg for arg in args]),\n\t\t\t\t\t),\n\t\t\t\t],\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\tenv=env,\n\t\t\t\tcheck=False,\n\t\t\t)\n\t\t\tt.cancel()\n\t\texcept Exception as e", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tif extra_env is None:\n\t\textra_env = {}\n\n\t# Get the command line arguments as a list\n\tcmd = [sys.executable, '-m', func.__module__, func.__name__]\n\tcmd.extend(args)\n\n\t# Get the environment\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\n\t# Run the subprocess\n\ttry:\n\t\treturn subprocess.run(cmd, check=True, timeout=timeout, env=env)\n\texcept subprocess.TimeoutExpired:\n\t\t# Print the command line\n\t\tprint(cmd)\n\t\tprint(' '.join('=' * 80))\n\t\t# Print the environment\n\t\tprint(' '.join('=' * 80))\n\t\tfor key, value in env.items():\n\t\t\tprint('{}: {}'.format(key, value))\n\t\tprint(' '.join('=' * 80))\n\t\traise\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t# Create our environment\n\tenv = os.environ.copy()\n\n\t# Add any extra environment variables\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\t# Create the command\n\tcmd = [sys.executable, \"-m\", func.__module__, func.__name__] + list(args)\n\n\t# Run the subprocess\n\treturn subprocess.run(cmd, timeout=timeout, check=True, env=env)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\timport json\n\n\tdef get_env_dict():\n\t\t\"\"\"\n\t\tReturn an environment dict that can be used to run a subprocess.\n\t\t\"\"\"\n\t\timport os\n\t\tenv = os.environ.copy()\n\t\tenv[\"PYTHONUNBUFFERED\"] = \"1\"\n\t\tif extra_env is not None:\n\t\t\tenv.update(extra_env)\n\t\treturn env\n\n\tif sys.platform.startswith(\"win\"):\n\t\tcommand = [sys.executable, \"-m\", func.__module__, func.__name__] + list(args)\n\telse:\n\t\tcommand = [sys.executable, \"-m\", f\"{func.__module__}.{func.__name__}\"] + list(args)\n\tcommand_string = \" \".join(command)\n\tprint(f\"Running subprocess: {command_string}\")\n\tres = subprocess.run(command,\n\t\t\t\t\t\t env=get_env_dict(),\n\t\t\t\t\t\t timeout=timeout,\n\t\t\t\t\t\t check=True,\n\t\t\t\t\t\t universal_newlines=True)\n\tprint(f\"Subprocess finished with exit code {res.returncode", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport sys\n\n\tif extra_env is None:\n\t\textra_env = {}\n\n\t# Use a unique temporary directory for each test run\n\timport tempfile\n\ttemp_dir = tempfile.TemporaryDirectory()\n\ttmp_env = os.environ.copy()\n\ttmp_env.update(extra_env)\n\ttmp_env[\"PYTHONPATH\"] = os.pathsep.join([\".\", tmp_env.get(\"PYTHONPATH\", \"\")])\n\ttmp_env[\"TEMP\"] = temp_dir.name\n\tcmd = [sys.executable, os.path.join(os.path.dirname(__file__), \"run_test.py\"),\n\t\t\t\t\tos.path.abspath(os.path.dirname(func.__module__)),\n\t\t\t\t\tfunc.__name__] + list(args)\n\tp = subprocess.run(cmd, env=tmp_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\t\t\t\t\t\t\t\t\t\ttimeout=timeout)\n\ttemp_dir.cleanup()\n\treturn p\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t# Use a unique temporary directory for the subprocess.\n\ttemp_dir = tempfile.mkdtemp(prefix='subprocess_run_helper')\n\tos.environ['PYTHONPATH'] = os.getcwd()\n\tos.environ['PYTHONUNBUFFERED'] = '1'\n\tos.environ['PYTHONIOENCODING'] = 'utf8'\n\tif extra_env is not None:\n\t\tos.environ.update(extra_env)\n\n\t# Run the function in a subprocess.\n\tcmd = [sys.executable, '-m', 'mypyc.subprocess_run_helper', func.__module__, func.__name__]\n\tcmd.extend(args)\n\ttry:\n\t\treturn subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\t\t\t\t\t\t  universal_newlines=True, timeout=timeout, check=True,\n\t\t\t\t\t\t\t  env=os.environ, cwd=temp_dir)\n\texcept subprocess.TimeoutExpired as e:\n\t\traise TimeoutError('Timed out while running {}'.format(func)) from e\n\tfinally:\n\t\tshutil.rmtree(temp_dir)\n\n\ndef", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\ttry:\n\t\timport os\n\t\timport subprocess\n\texcept ImportError:\n\t\traise RuntimeError(\"This function is only available in python3\")\n\n\tif extra_env is None:\n\t\textra_env = {}\n\n\targ_str = []\n\tfor arg in args:\n\t\tif isinstance(arg, str):\n\t\t\targ_str.append(arg)\n\t\telse:\n\t\t\targ_str.append(str(arg))\n\n\targ_str = \" \".join(arg_str)\n\n\tp = subprocess.Popen(\n\t\t[\"python3\", \"-c\", \"import {}\".format(func.__module__)],\n\t\tenv=dict(os.environ, **extra_env),\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tclose_fds=True,\n\t)\n\tout, err = p.communicate(\n\t\tinput=\"{}('{}')\".format(func.__name__, arg_str).encode(\"utf-8\")\n\t)\n\tif p.returncode != 0:\n\t\traise RuntimeError(\n\t\t\t\"Error executing python subprocess. Return code {}\".format(p.returncode)\n\t\t)\n\n\tif err", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\t# get the name of the module containing func\n\tmodule = inspect.getmodule(func)\n\tmodule_name = module.__name__\n\n\t# get the name of the function\n\tfunc_name = func.__name__\n\n\t# make the command\n\tcmd = [sys.executable, '-m', module_name, func_name] + list(args)\n\n\t# run the command\n\treturn subprocess.run(cmd, env=env, timeout=timeout)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t# create subprocess\n\tcmd = [sys.executable, '-m', 'autosklearn.util.subprocess', func.__module__, func.__name__]\n\tcmd.extend(args)\n\tlog.debug('Subprocess call: %s', cmd)\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\t# run the subprocess\n\tprocess = subprocess.run(cmd, env=env, timeout=timeout)\n\tif process.returncode != 0:\n\t\traise subprocess.CalledProcessError(process.returncode, process.args)\n\ttry:\n\t\tresult = pickle.loads(base64.b64decode(process.stdout))\n\texcept Exception as e:\n\t\traise RuntimeError(\n\t\t\t'The subprocess returned a result which could not be unpickled: %s' % str(e))\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tif extra_env is None:\n\t\textra_env = dict()\n\n\timport tempfile\n\timport sys\n\timport os\n\timport subprocess\n\timport importlib\n\timport inspect\n\n\timport torch\n\n\t# save current state\n\tif not os.path.exists(tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save\"):\n\t\tos.makedirs(tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save\")\n\ttorch.save(torch.load('~/.cache/torch/hub/latest.pth'), tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save/latest.pth\")\n\timportlib.reload(sys)\n\tsys.path.insert(0, tempfile.gettempdir() + \"/\" + \"subprocess_run_helper_save\")\n\timport latest\n\n\t# run function\n\ttry:\n\t\treturn subprocess.run([sys.executable, '-m', func.__module__, func.__name__] + args, check=True, env=dict(os.environ, **extra_env), timeout=timeout)\n\tfinally:\n\t\t# restore state\n\t\tos.remove(tempfile.gettemp", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport os\n\timport subprocess\n\timport threading\n\timport time\n\n\t# Create an environment for the subprocess\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\tdef run_in_subprocess():\n\t\t# Create a thread to kill the subprocess if it does not complete within timeout seconds\n\t\tt = threading.Timer(timeout, subprocess.Popen.terminate, [p])\n\t\ttry:\n\t\t\tp = subprocess.run(\n\t\t\t\t[\n\t\t\t\t\tsys.executable,\n\t\t\t\t\t'-c',\n\t\t\t\t\t'import %s as m; m.%s(%s)' % (\n\t\t\t\t\t\tfunc.__module__,\n\t\t\t\t\t\tfunc.__name__,\n\t\t\t\t\t\t', '.join(['\"%s\"' % arg for arg in args]),\n\t\t\t\t\t),\n\t\t\t\t],\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\tenv=env,\n\t\t\t\tcheck=False,\n\t\t\t)\n\t\t\tt.cancel()\n\t\texcept Exception as e", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tif extra_env is None:\n\t\textra_env = {}\n\n\t# Get the command line arguments as a list\n\tcmd = [sys.executable, '-m', func.__module__, func.__name__]\n\tcmd.extend(args)\n\n\t# Get the environment\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\n\t# Run the subprocess\n\ttry:\n\t\treturn subprocess.run(cmd, check=True, timeout=timeout, env=env)\n\texcept subprocess.TimeoutExpired:\n\t\t# Print the command line\n\t\tprint(cmd)\n\t\tprint(' '.join('=' * 80))\n\t\t# Print the environment\n\t\tprint(' '.join('=' * 80))\n\t\tfor key, value in env.items():\n\t\t\tprint('{}: {}'.format(key, value))\n\t\tprint(' '.join('=' * 80))\n\t\traise\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/override.py", "project": "witten/borgmatic", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tif default:\n\t\tdefault = default.strip()\n\n\tvalue = os.environ.get(name)\n\tif value is not None:\n\t\treturn value.strip()\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Exception(f'Environment variable {name} is not defined and no default value provided.')\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif not matcher:\n\t\treturn matcher\n\tif isinstance(matcher, six.string_types):\n\t\treturn matcher\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tif default is None:\n\t\t\traise Exception(\"Environment variable '{}' is not defined\".format(name))\n\t\telse:\n\t\t\tvalue = default\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group(\"name\")\n\tdefault = matcher.group(\"default\")\n\tif default is not None:\n\t\tdefault = default.strip()\n\treturn os.environ.get(name, default)\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tvalue = os.getenv(name)\n\tdefault = matcher.group('default')\n\n\tif value is None:\n\t\tif default is None:\n\t\t\traise Exception(f'Environment variable {name} is required.')\n\t\telse:\n\t\t\tvalue = default\n\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatch = matcher.match()\n\tif match:\n\t\tif match.group(1) is None:\n\t\t\traise ValueError('Matcher has no name')\n\t\tif match.group(2) is None:\n\t\t\treturn os.environ[match.group(1)]\n\t\tif match.group(2) in os.environ:\n\t\t\treturn os.environ[match.group(2)]\n\t\telse:\n\t\t\treturn match.group(2)\n\telse:\n\t\traise ValueError('Matcher not found')\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tif name is not None:\n\t\tif name in os.environ:\n\t\t\treturn os.environ[name]\n\t\telse:\n\t\t\tif default is not None:\n\t\t\t\treturn default\n\t\t\telse:\n\t\t\t\traise Error('variable %s is not defined in environment' % name)\n\telse:\n\t\traise Error('variable is not defined in environment')\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif isinstance(matcher, str):\n\t\treturn matcher\n\tmatch = matcher.groupdict()\n\tname = match.get(\"name\", matcher)\n\tdefault = match.get(\"default\")\n\tvalue = os.getenv(name, default=default)\n\tif value is None and default is None:\n\t\traise ValueError(\"No value found for variable '{}' and no default defined.\".format(name))\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdefault = matcher.groupdict().get(\"default\", None)\n\tvarname = matcher.group(\"varname\")\n\ttry:\n\t\treturn os.environ[varname]\n\texcept KeyError:\n\t\tif default:\n\t\t\treturn default\n\t\telse:\n\t\t\traise RuntimeError(f\"Required environment variable {varname} not defined\")\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get(matcher.group('name'))\n\tif value is None:\n\t\tif matcher.group('default') is None:\n\t\t\traise ValueError(f'Variable {matcher.group(\"name\")} has no default value and is not defined in environment')\n\t\telse:\n\t\t\tvalue = matcher.group('default')\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvar_name = matcher.group('name')\n\tvar_default = matcher.group('default')\n\tvar_value = os.getenv(var_name, var_default)\n\tif var_value is None:\n\t\traise ValueError(\"Missing required environment variable '%s'\" % var_name)\n\treturn var_value\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tif default:\n\t\tdefault = default.strip()\n\n\tvalue = os.environ.get(name)\n\tif value is not None:\n\t\treturn value.strip()\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Exception(f'Environment variable {name} is not defined and no default value provided.')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif not matcher:\n\t\treturn matcher\n\tif isinstance(matcher, six.string_types):\n\t\treturn matcher\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tif default is None:\n\t\t\traise Exception(\"Environment variable '{}' is not defined\".format(name))\n\t\telse:\n\t\t\tvalue = default\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group(\"name\")\n\tdefault = matcher.group(\"default\")\n\tif default is not None:\n\t\tdefault = default.strip()\n\treturn os.environ.get(name, default)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tvalue = os.getenv(name)\n\tdefault = matcher.group('default')\n\n\tif value is None:\n\t\tif default is None:\n\t\t\traise Exception(f'Environment variable {name} is required.')\n\t\telse:\n\t\t\tvalue = default\n\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatch = matcher.match()\n\tif match:\n\t\tif match.group(1) is None:\n\t\t\traise ValueError('Matcher has no name')\n\t\tif match.group(2) is None:\n\t\t\treturn os.environ[match.group(1)]\n\t\tif match.group(2) in os.environ:\n\t\t\treturn os.environ[match.group(2)]\n\t\telse:\n\t\t\treturn match.group(2)\n\telse:\n\t\traise ValueError('Matcher not found')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tif name is not None:\n\t\tif name in os.environ:\n\t\t\treturn os.environ[name]\n\t\telse:\n\t\t\tif default is not None:\n\t\t\t\treturn default\n\t\t\telse:\n\t\t\t\traise Error('variable %s is not defined in environment' % name)\n\telse:\n\t\traise Error('variable is not defined in environment')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif isinstance(matcher, str):\n\t\treturn matcher\n\tmatch = matcher.groupdict()\n\tname = match.get(\"name\", matcher)\n\tdefault = match.get(\"default\")\n\tvalue = os.getenv(name, default=default)\n\tif value is None and default is None:\n\t\traise ValueError(\"No value found for variable '{}' and no default defined.\".format(name))\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdefault = matcher.groupdict().get(\"default\", None)\n\tvarname = matcher.group(\"varname\")\n\ttry:\n\t\treturn os.environ[varname]\n\texcept KeyError:\n\t\tif default:\n\t\t\treturn default\n\t\telse:\n\t\t\traise RuntimeError(f\"Required environment variable {varname} not defined\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get(matcher.group('name'))\n\tif value is None:\n\t\tif matcher.group('default') is None:\n\t\t\traise ValueError(f'Variable {matcher.group(\"name\")} has no default value and is not defined in environment')\n\t\telse:\n\t\t\tvalue = matcher.group('default')\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvar_name = matcher.group('name')\n\tvar_default = matcher.group('default')\n\tvar_value = os.getenv(var_name, var_default)\n\tif var_value is None:\n\t\traise ValueError(\"Missing required environment variable '%s'\" % var_name)\n\treturn var_value\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cinder/image/glance.py", "project": "openstack/cinder", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tif not image_href:\n\t\traise ValueError(\"Invalid image href: {0}\".format(image_href))\n\tif image_href.startswith(\"http\"):\n\t\tparsed_url = urllib.parse.urlparse(image_href)\n\t\tif parsed_url.scheme not in (\"http\", \"https\"):\n\t\t\traise ValueError(\"Invalid image href: {0}\".format(image_href))\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = parsed_url.scheme == \"https\"\n\t\timage_id = parsed_url.path.split(\"/\")[-1]\n\t\tif not image_id:\n\t\t\traise ValueError(\"Invalid image href: {0}\".format(image_href))\n\telse:\n\t\tnetloc = image_href.split(\":\")[0]\n\t\tuse_ssl = False\n\t\timage_id = image_href\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed_url = urllib.parse.urlparse(image_href)\n\n\tif parsed_url.scheme == 'file':\n\t\treturn parsed_url.path, '', False\n\n\tif parsed_url.scheme == 'https':\n\t\timage_id = parsed_url.path.lstrip('/')\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = True\n\telif parsed_url.scheme == 'http':\n\t\timage_id = parsed_url.path.lstrip('/')\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = False\n\telse:\n\t\traise ValueError(\n\t\t\t'image_href must be a file:/// URL or a '\n\t\t\t'scheme://netloc/path URL')\n\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tif image_href.startswith('http'):\n\t\tnetloc = urlparse(image_href).netloc\n\t\tuse_ssl = True\n\telse:\n\t\tnetloc = urlparse('https://%s' % image_href).netloc\n\t\tuse_ssl = False\n\treturn image_href, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tif not image_href:\n\t\traise ValueError(\"image_href is None\")\n\n\tif not isinstance(image_href, str):\n\t\traise ValueError(\"image_href is not a string\")\n\n\tif not image_href.startswith(\"http\"):\n\t\traise ValueError(\"image_href is not a valid url: %s\" % image_href)\n\n\turl = urlparse.urlsplit(image_href)\n\n\treturn url.netloc, url.path, (url.scheme == \"https\")\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed_image_href = urllib.parse.urlparse(image_href)\n\timage_id = parsed_image_href.path.lstrip('/').rstrip('/')\n\tnetloc = parsed_image_href.netloc\n\tuse_ssl = False\n\tif parsed_image_href.scheme == 'https':\n\t\tuse_ssl = True\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tnetloc = None\n\tuse_ssl = False\n\ttry:\n\t\turl_parts = urlparse(image_href)\n\t\tnetloc = url_parts.netloc\n\t\tscheme = url_parts.scheme\n\t\tif scheme == \"https\":\n\t\t\tuse_ssl = True\n\n\t\timage_id = netloc\n\t\tif \"/\" in image_id:\n\t\t\timage_id = image_id.split(\"/\")[1]\n\t\telif \":\" in image_id:\n\t\t\timage_id = image_id.split(\":\")[1]\n\texcept ValueError:\n\t\tpass\n\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tuse_ssl = True\n\tnetloc = None\n\tif image_href.startswith('file://'):\n\t\timage_id = image_href[7:]\n\telif image_href.startswith('http://'):\n\t\tnetloc = image_href[7:].split('/', 1)[0]\n\telif image_href.startswith('https://'):\n\t\tnetloc = image_href[8:].split('/', 1)[0]\n\t\tuse_ssl = True\n\telse:\n\t\tnetloc = image_href.split('/', 1)[0]\n\t\tuse_ssl = False\n\treturn image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t_validate_href(image_href)\n\n\tif image_href.startswith('http'):\n\t\tnetloc = urlparse(image_href).netloc\n\t\tuse_ssl = urlparse(image_href).scheme == 'https'\n\telse:\n\t\tnetloc = ''\n\t\tuse_ssl = False\n\n\timage_id = image_href.split('/')[-1]\n\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tnetloc = None\n\tuse_ssl = False\n\tif is_valid_href(image_href):\n\t\tnetloc = urlparse(image_href).netloc\n\t\tif netloc is None:\n\t\t\traise ValueError(f\"Invalid image href: {image_href}\")\n\t\telif netloc.endswith(':8443'):\n\t\t\tnetloc = netloc[:-5]\n\t\t\tuse_ssl = True\n\t\telif netloc.endswith(':8080'):\n\t\t\tnetloc = netloc[:-5]\n\t\telif netloc.endswith(':80'):\n\t\t\tnetloc = netloc[:-3]\n\t\tif netloc.startswith('www.'):\n\t\t\tnetloc = netloc[4:]\n\t\tif netloc.startswith('store.'):\n\t\t\tnetloc = netloc[6:]\n\t\tif '/' in netloc:\n\t\t\tnetloc, image_id = netloc.split('/')\n\t\t\tif image_id.endswith('.vhd'):\n\t\t\t\timage_id = image_id[:-4]\n\t\tif netloc.endswith('.com'):\n\t\t\tnetloc = netloc[:-4", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tnetloc = None\n\tuse_ssl = False\n\timg_id = image_href\n\tif img_id.startswith('http://'):\n\t\tnetloc = img_id.split('/')[2]\n\t\tuse_ssl = False\n\telif img_id.startswith('https://'):\n\t\tnetloc = img_id.split('/')[2]\n\t\tuse_ssl = True\n\telif img_id.startswith('file://'):\n\t\timg_id = img_id[7:]\n\t\tnetloc = 'localhost'\n\telse:\n\t\traise ValueError('Invalid image URL {0}'.format(img_id))\n\treturn img_id, netloc, use_ssl\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tif not image_href:\n\t\traise ValueError(\"Invalid image href: {0}\".format(image_href))\n\tif image_href.startswith(\"http\"):\n\t\tparsed_url = urllib.parse.urlparse(image_href)\n\t\tif parsed_url.scheme not in (\"http\", \"https\"):\n\t\t\traise ValueError(\"Invalid image href: {0}\".format(image_href))\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = parsed_url.scheme == \"https\"\n\t\timage_id = parsed_url.path.split(\"/\")[-1]\n\t\tif not image_id:\n\t\t\traise ValueError(\"Invalid image href: {0}\".format(image_href))\n\telse:\n\t\tnetloc = image_href.split(\":\")[0]\n\t\tuse_ssl = False\n\t\timage_id = image_href\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed_url = urllib.parse.urlparse(image_href)\n\n\tif parsed_url.scheme == 'file':\n\t\treturn parsed_url.path, '', False\n\n\tif parsed_url.scheme == 'https':\n\t\timage_id = parsed_url.path.lstrip('/')\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = True\n\telif parsed_url.scheme == 'http':\n\t\timage_id = parsed_url.path.lstrip('/')\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = False\n\telse:\n\t\traise ValueError(\n\t\t\t'image_href must be a file:/// URL or a '\n\t\t\t'scheme://netloc/path URL')\n\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tif image_href.startswith('http'):\n\t\tnetloc = urlparse(image_href).netloc\n\t\tuse_ssl = True\n\telse:\n\t\tnetloc = urlparse('https://%s' % image_href).netloc\n\t\tuse_ssl = False\n\treturn image_href, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tif not image_href:\n\t\traise ValueError(\"image_href is None\")\n\n\tif not isinstance(image_href, str):\n\t\traise ValueError(\"image_href is not a string\")\n\n\tif not image_href.startswith(\"http\"):\n\t\traise ValueError(\"image_href is not a valid url: %s\" % image_href)\n\n\turl = urlparse.urlsplit(image_href)\n\n\treturn url.netloc, url.path, (url.scheme == \"https\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed_image_href = urllib.parse.urlparse(image_href)\n\timage_id = parsed_image_href.path.lstrip('/').rstrip('/')\n\tnetloc = parsed_image_href.netloc\n\tuse_ssl = False\n\tif parsed_image_href.scheme == 'https':\n\t\tuse_ssl = True\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tnetloc = None\n\tuse_ssl = False\n\ttry:\n\t\turl_parts = urlparse(image_href)\n\t\tnetloc = url_parts.netloc\n\t\tscheme = url_parts.scheme\n\t\tif scheme == \"https\":\n\t\t\tuse_ssl = True\n\n\t\timage_id = netloc\n\t\tif \"/\" in image_id:\n\t\t\timage_id = image_id.split(\"/\")[1]\n\t\telif \":\" in image_id:\n\t\t\timage_id = image_id.split(\":\")[1]\n\texcept ValueError:\n\t\tpass\n\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tuse_ssl = True\n\tnetloc = None\n\tif image_href.startswith('file://'):\n\t\timage_id = image_href[7:]\n\telif image_href.startswith('http://'):\n\t\tnetloc = image_href[7:].split('/', 1)[0]\n\telif image_href.startswith('https://'):\n\t\tnetloc = image_href[8:].split('/', 1)[0]\n\t\tuse_ssl = True\n\telse:\n\t\tnetloc = image_href.split('/', 1)[0]\n\t\tuse_ssl = False\n\treturn image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t_validate_href(image_href)\n\n\tif image_href.startswith('http'):\n\t\tnetloc = urlparse(image_href).netloc\n\t\tuse_ssl = urlparse(image_href).scheme == 'https'\n\telse:\n\t\tnetloc = ''\n\t\tuse_ssl = False\n\n\timage_id = image_href.split('/')[-1]\n\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tnetloc = None\n\tuse_ssl = False\n\tif is_valid_href(image_href):\n\t\tnetloc = urlparse(image_href).netloc\n\t\tif netloc is None:\n\t\t\traise ValueError(f\"Invalid image href: {image_href}\")\n\t\telif netloc.endswith(':8443'):\n\t\t\tnetloc = netloc[:-5]\n\t\t\tuse_ssl = True\n\t\telif netloc.endswith(':8080'):\n\t\t\tnetloc = netloc[:-5]\n\t\telif netloc.endswith(':80'):\n\t\t\tnetloc = netloc[:-3]\n\t\tif netloc.startswith('www.'):\n\t\t\tnetloc = netloc[4:]\n\t\tif netloc.startswith('store.'):\n\t\t\tnetloc = netloc[6:]\n\t\tif '/' in netloc:\n\t\t\tnetloc, image_id = netloc.split('/')\n\t\t\tif image_id.endswith('.vhd'):\n\t\t\t\timage_id = image_id[:-4]\n\t\tif netloc.endswith('.com'):\n\t\t\tnetloc = netloc[:-4", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tnetloc = None\n\tuse_ssl = False\n\timg_id = image_href\n\tif img_id.startswith('http://'):\n\t\tnetloc = img_id.split('/')[2]\n\t\tuse_ssl = False\n\telif img_id.startswith('https://'):\n\t\tnetloc = img_id.split('/')[2]\n\t\tuse_ssl = True\n\telif img_id.startswith('file://'):\n\t\timg_id = img_id[7:]\n\t\tnetloc = 'localhost'\n\telse:\n\t\traise ValueError('Invalid image URL {0}'.format(img_id))\n\treturn img_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\treturn [item[:-1] if len(item)>1 and item[-1] == os.sep else item for item in input_list]\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif not input_list:\n\t\treturn []\n\telse:\n\t\treturn [s[:-1] if len(s) > 1 and s[-1] == os.sep else s for s in input_list]\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\treturn [x[:-1] if x[-1] == os.sep else x for x in input_list]\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor file_name in input_list:\n\t\tif(len(file_name) > 1 and file_name[-1] == os.sep):\n\t\t\toutput_list.append(file_name[:-1])\n\t\telse:\n\t\t\toutput_list.append(file_name)\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif not input_list:\n\t\treturn []\n\toutput_list = []\n\tfor input_string in input_list:\n\t\tif len(input_string) > 1 and input_string[-1] == os.sep:\n\t\t\toutput_list.append(input_string[:-1])\n\t\telse:\n\t\t\toutput_list.append(input_string)\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\toutput_list = []\n\tfor string in input_list:\n\t\tif string is None:\n\t\t\toutput_list.append(None)\n\t\telse:\n\t\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\t\toutput_list.append(string[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(string)\n\treturn output_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif not input_list:\n\t\treturn []\n\telse:\n\t\treturn [i[:-1] if len(i) > 1 and i[-1] in (os.path.sep, os.path.altsep) else i for i in input_list]\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor path in input_list:\n\t\tif path is not None:\n\t\t\tif len(path) > 1:\n\t\t\t\tif path[-1] == os.sep:\n\t\t\t\t\tpath = path[:-1]\n\t\t\t\toutput_list.append(path)\n\t\t\telse:\n\t\t\t\toutput_list.append(path)\n\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\tfor i, string in enumerate(input_list):\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tinput_list[i] = string[:-1]\n\treturn input_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\treturn [s[:-1] if s and s[-1] == os.sep else s for s in input_list]\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\treturn [item[:-1] if len(item)>1 and item[-1] == os.sep else item for item in input_list]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif not input_list:\n\t\treturn []\n\telse:\n\t\treturn [s[:-1] if len(s) > 1 and s[-1] == os.sep else s for s in input_list]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\treturn [x[:-1] if x[-1] == os.sep else x for x in input_list]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor file_name in input_list:\n\t\tif(len(file_name) > 1 and file_name[-1] == os.sep):\n\t\t\toutput_list.append(file_name[:-1])\n\t\telse:\n\t\t\toutput_list.append(file_name)\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif not input_list:\n\t\treturn []\n\toutput_list = []\n\tfor input_string in input_list:\n\t\tif len(input_string) > 1 and input_string[-1] == os.sep:\n\t\t\toutput_list.append(input_string[:-1])\n\t\telse:\n\t\t\toutput_list.append(input_string)\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\toutput_list = []\n\tfor string in input_list:\n\t\tif string is None:\n\t\t\toutput_list.append(None)\n\t\telse:\n\t\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\t\toutput_list.append(string[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(string)\n\treturn output_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif not input_list:\n\t\treturn []\n\telse:\n\t\treturn [i[:-1] if len(i) > 1 and i[-1] in (os.path.sep, os.path.altsep) else i for i in input_list]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor path in input_list:\n\t\tif path is not None:\n\t\t\tif len(path) > 1:\n\t\t\t\tif path[-1] == os.sep:\n\t\t\t\t\tpath = path[:-1]\n\t\t\t\toutput_list.append(path)\n\t\t\telse:\n\t\t\t\toutput_list.append(path)\n\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\tfor i, string in enumerate(input_list):\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tinput_list[i] = string[:-1]\n\treturn input_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tif input_list is None:\n\t\treturn []\n\treturn [s[:-1] if s and s[-1] == os.sep else s for s in input_list]\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "shconfparser/search.py", "project": "kirankotari/shconfparser", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tpattern = re.escape(pattern)\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\treturn re.compile(r'^' + pattern.strip() + r'$')\n\t\telse:\n\t\t\treturn re.compile(r'^' + pattern + r'$')\n\telse:\n\t\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\tif strip:\n\t\tpattern = re.compile(r\"^\\s*%s\\s*$\" % pattern.pattern, re.IGNORECASE)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\n\t\tif pattern.startswith('^'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith('$'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('?'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.startswith('*'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.startswith('+'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.startswith('('):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith(')'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('['):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith(']'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('{'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith('}'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('|'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith('|'):\n\t\t\tpattern = pattern[:", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif type(pattern) is str:\n\t\tpattern = re.compile(pattern)\n\tif strip:\n\t\tpattern = pattern.strip()\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\telse:\n\t\treturn pattern\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tpattern = re.escape(pattern)\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\treturn re.compile(r'^' + pattern.strip() + r'$')\n\t\telse:\n\t\t\treturn re.compile(r'^' + pattern + r'$')\n\telse:\n\t\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\tif strip:\n\t\tpattern = re.compile(r\"^\\s*%s\\s*$\" % pattern.pattern, re.IGNORECASE)\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\n\t\tif pattern.startswith('^'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith('$'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('?'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.startswith('*'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.startswith('+'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.startswith('('):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith(')'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('['):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith(']'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('{'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith('}'):\n\t\t\tpattern = pattern[:-1]\n\t\tif pattern.startswith('|'):\n\t\t\tpattern = pattern[1:]\n\t\tif pattern.endswith('|'):\n\t\t\tpattern = pattern[:", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif type(pattern) is str:\n\t\tpattern = re.compile(pattern)\n\tif strip:\n\t\tpattern = pattern.strip()\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\telse:\n\t\treturn pattern\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/core.py", "project": "burgerbecky/makeprojects", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(\"Running command: \" + \" \".join(commands + args))\n\n\tif hide_stderr:\n\t\tstderr = subprocess.DEVNULL\n\telse:\n\t\tstderr = None\n\tp = subprocess.Popen(commands + args, stdout=subprocess.PIPE, stderr=stderr, cwd=cwd, env=env)\n\tstdout, stderr = p.communicate()\n\tif p.returncode != 0:\n\t\tif verbose:\n\t\t\tprint(\"Return code \" + str(p.returncode) + \" for \" + \" \".join(commands + args))\n\t\tprint(stdout.decode(\"utf-8\"))\n\t\tsys.exit(p.returncode)\n\treturn stdout.decode(\"utf-8\"), p.returncode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tfrom sys import stderr\n\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif env is not None:\n\t\tenv = env.copy()\n\telse:\n\t\tenv = os.environ\n\n\t# Run command\n\tif verbose:\n\t\tprint(\"Running command: %s\" % commands)\n\n\tprocess = Popen(commands, cwd=cwd, stdin=PIPE, stdout=PIPE, stderr=(PIPE if hide_stderr else None), env=env)\n\tstdout, stderr = process.communicate(args)\n\n\tif verbose:\n\t\tprint(\"Return code: %d\" % process.returncode)\n\t\tif stdout is not None:\n\t\t\tprint(\"stdout: %s\" % stdout)\n\t\tif stderr is not None:\n\t\t\tprint(\"stderr: %s\" % stderr)\n\n\treturn process.returncode, stdout, stderr\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tresult = RunResult()\n\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tresult.stdout += run_command_once(command, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\n\treturn result\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\toutput = \"\"\n\tif verbose:\n\t\tprint(\"Running command:\", commands, args)\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running command:\", command)\n\t\tif cwd is None:\n\t\t\tcommand_process = subprocess.Popen([command] + args, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else subprocess.STDOUT), universal_newlines=True, env=env)\n\t\telse:\n\t\t\tcommand_process = subprocess.Popen([command] + args, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else subprocess.STDOUT), universal_newlines=True, cwd=os.path.abspath(cwd), env=env)\n\t\tcommand_output = command_process.stdout.read()\n\t\tif verbose:\n\t\t\tprint(\"Command output:\", command_output)\n\t\toutput += command_output\n\t\tcommand_returncode = command_process.wait()\n\t\tif verbose:\n\t\t\tprint(\"Command returncode:\", command_returncode)\n\t\tif command_returncode != 0", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(f\"Running command(s) {commands} with args {args} in {cwd}\")\n\n\t# Run the command(s)\n\tresult = []\n\tfor command in commands:\n\t\t# We need to split the command, if it contains spaces\n\t\tcommand_list = shlex.split(command)\n\t\t# Run the command\n\t\tprocess = subprocess.Popen(command_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\t# Get the result\n\t\tstdout, stderr = process.communicate()\n\t\tresult.append({\n\t\t\t\"returncode\": process.returncode,\n\t\t\t\"stdout\": stdout,\n\t\t\t\"stderr\": stderr,\n\t\t})\n\t\t# Check if the command failed\n\t\tif process.returncode != 0:\n\t\t\treturn process.returncode\n\n\tif verbose:\n\t\tprint(f\"Finished running command(s) {commands} with args {args} in {cwd}\")\n\n\t# Return the result\n\treturn result\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tif not isinstance(args, list):\n\t\targs = [args]\n\tif not isinstance(env, dict):\n\t\tenv = {}\n\n\tif verbose:\n\t\tprint('Running command:', commands, 'with args:', args)\n\n\t# check if commands is None\n\tif commands is None:\n\t\treturn None\n\n\t# check if commands is empty\n\tif not commands:\n\t\treturn None\n\n\t# check if args is None\n\tif args is None:\n\t\targs = []\n\n\t# check if args is empty\n\tif not args:\n\t\targs = []\n\n\t# check if env is None\n\tif env is None:\n\t\tenv = {}\n\n\t# check if env is empty\n\tif not env:\n\t\tenv = {}\n\n\t# check if cwd is None\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\t# check if hide_stderr is None\n\tif hide_stderr is None:\n\t\thide_stderr = False\n\n\t# check if hide_stderr is False\n\tif not hide_stderr:\n\t\thide_stderr = False\n\n\t# check", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tresult = {}\n\tfor command in commands:\n\t\tresult[command] = run_command_single(command, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\treturn result\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t#if not isinstance(commands, list):\n\t#\tcommands = [commands]\n\t#if verbose:\n\t#\tprint \"Running command: %s\" % ' '.join(commands)\n\t#if verbose:\n\t#\tprint \"With arguments: %s\" % args\n\t#if verbose:\n\t#\tprint \"In directory: %s\" % cwd\n\t#if verbose:\n\t#\tprint \"With environment:\"\n\t#\tprint env\n\t#if verbose:\n\t#\tprint\n\t#for i, command in enumerate(commands):\n\t#\tprint \"Running command %d/%d: %s\" % (i+1, len(commands), command)\n\tprocess = subprocess.Popen(commands, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tif not hide_stderr:\n\t\tstdout, stderr = process.communicate(input='\\n'.join(args))\n\telse:\n\t\tstdout, stderr = process.communicate()\n\treturn_code = process.returncode\n\tif verbose:\n\t\tprint \"Output:\\n %s\" % stdout\n\t\tprint \"Error output: %s\" % stderr\n\t\tprint \"", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif verbose:\n\t\tprint(\"Running command: \" + \" \".join(commands))\n\n\tif args:\n\t\tcommands.extend(args)\n\n\tresult = subprocess.run(commands,\n\t\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\t\t\t\tuniversal_newlines=True,\n\t\t\t\t\t\t\tcwd=cwd,\n\t\t\t\t\t\t\tenv=env)\n\n\tif verbose:\n\t\tprint(\"Command result:\")\n\t\tprint(result.stdout)\n\t\tprint()\n\n\tif hide_stderr:\n\t\tresult.stderr = \"\"\n\n\treturn result\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif type(commands) is str:\n\t\tcommands = [commands]\n\n\tif cwd is not None:\n\t\tcwd = os.path.realpath(cwd)\n\t\tif not os.path.exists(cwd):\n\t\t\traise ValueError('The given working directory does not exist {0}'.format(cwd))\n\n\tif env is not None:\n\t\tenv = env.copy()\n\telse:\n\t\tenv = {}\n\n\tresult = None\n\tfor command in commands:\n\t\tfor arg in args:\n\t\t\targs = [command] + arg.split(' ')\n\t\t\ttry:\n\t\t\t\tresult = subprocess.run(args, cwd=cwd, check=True, env=env, capture_output=True)\n\t\t\texcept subprocess.CalledProcessError as e:\n\t\t\t\tprint('Error running with: {0}'.format(' '.join(args)))\n\t\t\t\tprint('Returncode: {0}'.format(e.returncode))\n\t\t\t\tprint('Error: {0}'.format(e.stderr))\n\t\t\t\tprint('Output: {0}'.format(e.stdout))\n\t\t\t\tif verbose:\n\t\t\t\t\ttraceback.print_"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(\"Running command: \" + \" \".join(commands + args))\n\n\tif hide_stderr:\n\t\tstderr = subprocess.DEVNULL\n\telse:\n\t\tstderr = None\n\tp = subprocess.Popen(commands + args, stdout=subprocess.PIPE, stderr=stderr, cwd=cwd, env=env)\n\tstdout, stderr = p.communicate()\n\tif p.returncode != 0:\n\t\tif verbose:\n\t\t\tprint(\"Return code \" + str(p.returncode) + \" for \" + \" \".join(commands + args))\n\t\tprint(stdout.decode(\"utf-8\"))\n\t\tsys.exit(p.returncode)\n\treturn stdout.decode(\"utf-8\"), p.returncode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE\n\tfrom sys import stderr\n\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif env is not None:\n\t\tenv = env.copy()\n\telse:\n\t\tenv = os.environ\n\n\t# Run command\n\tif verbose:\n\t\tprint(\"Running command: %s\" % commands)\n\n\tprocess = Popen(commands, cwd=cwd, stdin=PIPE, stdout=PIPE, stderr=(PIPE if hide_stderr else None), env=env)\n\tstdout, stderr = process.communicate(args)\n\n\tif verbose:\n\t\tprint(\"Return code: %d\" % process.returncode)\n\t\tif stdout is not None:\n\t\t\tprint(\"stdout: %s\" % stdout)\n\t\tif stderr is not None:\n\t\t\tprint(\"stderr: %s\" % stderr)\n\n\treturn process.returncode, stdout, stderr\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tresult = RunResult()\n\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tresult.stdout += run_command_once(command, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\toutput = \"\"\n\tif verbose:\n\t\tprint(\"Running command:\", commands, args)\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running command:\", command)\n\t\tif cwd is None:\n\t\t\tcommand_process = subprocess.Popen([command] + args, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else subprocess.STDOUT), universal_newlines=True, env=env)\n\t\telse:\n\t\t\tcommand_process = subprocess.Popen([command] + args, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else subprocess.STDOUT), universal_newlines=True, cwd=os.path.abspath(cwd), env=env)\n\t\tcommand_output = command_process.stdout.read()\n\t\tif verbose:\n\t\t\tprint(\"Command output:\", command_output)\n\t\toutput += command_output\n\t\tcommand_returncode = command_process.wait()\n\t\tif verbose:\n\t\t\tprint(\"Command returncode:\", command_returncode)\n\t\tif command_returncode != 0", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(f\"Running command(s) {commands} with args {args} in {cwd}\")\n\n\t# Run the command(s)\n\tresult = []\n\tfor command in commands:\n\t\t# We need to split the command, if it contains spaces\n\t\tcommand_list = shlex.split(command)\n\t\t# Run the command\n\t\tprocess = subprocess.Popen(command_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\t# Get the result\n\t\tstdout, stderr = process.communicate()\n\t\tresult.append({\n\t\t\t\"returncode\": process.returncode,\n\t\t\t\"stdout\": stdout,\n\t\t\t\"stderr\": stderr,\n\t\t})\n\t\t# Check if the command failed\n\t\tif process.returncode != 0:\n\t\t\treturn process.returncode\n\n\tif verbose:\n\t\tprint(f\"Finished running command(s) {commands} with args {args} in {cwd}\")\n\n\t# Return the result\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tif not isinstance(args, list):\n\t\targs = [args]\n\tif not isinstance(env, dict):\n\t\tenv = {}\n\n\tif verbose:\n\t\tprint('Running command:', commands, 'with args:', args)\n\n\t# check if commands is None\n\tif commands is None:\n\t\treturn None\n\n\t# check if commands is empty\n\tif not commands:\n\t\treturn None\n\n\t# check if args is None\n\tif args is None:\n\t\targs = []\n\n\t# check if args is empty\n\tif not args:\n\t\targs = []\n\n\t# check if env is None\n\tif env is None:\n\t\tenv = {}\n\n\t# check if env is empty\n\tif not env:\n\t\tenv = {}\n\n\t# check if cwd is None\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\t# check if hide_stderr is None\n\tif hide_stderr is None:\n\t\thide_stderr = False\n\n\t# check if hide_stderr is False\n\tif not hide_stderr:\n\t\thide_stderr = False\n\n\t# check", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tresult = {}\n\tfor command in commands:\n\t\tresult[command] = run_command_single(command, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t#if not isinstance(commands, list):\n\t#\tcommands = [commands]\n\t#if verbose:\n\t#\tprint \"Running command: %s\" % ' '.join(commands)\n\t#if verbose:\n\t#\tprint \"With arguments: %s\" % args\n\t#if verbose:\n\t#\tprint \"In directory: %s\" % cwd\n\t#if verbose:\n\t#\tprint \"With environment:\"\n\t#\tprint env\n\t#if verbose:\n\t#\tprint\n\t#for i, command in enumerate(commands):\n\t#\tprint \"Running command %d/%d: %s\" % (i+1, len(commands), command)\n\tprocess = subprocess.Popen(commands, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tif not hide_stderr:\n\t\tstdout, stderr = process.communicate(input='\\n'.join(args))\n\telse:\n\t\tstdout, stderr = process.communicate()\n\treturn_code = process.returncode\n\tif verbose:\n\t\tprint \"Output:\\n %s\" % stdout\n\t\tprint \"Error output: %s\" % stderr\n\t\tprint \"", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif verbose:\n\t\tprint(\"Running command: \" + \" \".join(commands))\n\n\tif args:\n\t\tcommands.extend(args)\n\n\tresult = subprocess.run(commands,\n\t\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\t\t\t\tuniversal_newlines=True,\n\t\t\t\t\t\t\tcwd=cwd,\n\t\t\t\t\t\t\tenv=env)\n\n\tif verbose:\n\t\tprint(\"Command result:\")\n\t\tprint(result.stdout)\n\t\tprint()\n\n\tif hide_stderr:\n\t\tresult.stderr = \"\"\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tif type(commands) is str:\n\t\tcommands = [commands]\n\n\tif cwd is not None:\n\t\tcwd = os.path.realpath(cwd)\n\t\tif not os.path.exists(cwd):\n\t\t\traise ValueError('The given working directory does not exist {0}'.format(cwd))\n\n\tif env is not None:\n\t\tenv = env.copy()\n\telse:\n\t\tenv = {}\n\n\tresult = None\n\tfor command in commands:\n\t\tfor arg in args:\n\t\t\targs = [command] + arg.split(' ')\n\t\t\ttry:\n\t\t\t\tresult = subprocess.run(args, cwd=cwd, check=True, env=env, capture_output=True)\n\t\t\texcept subprocess.CalledProcessError as e:\n\t\t\t\tprint('Error running with: {0}'.format(' '.join(args)))\n\t\t\t\tprint('Returncode: {0}'.format(e.returncode))\n\t\t\t\tprint('Error: {0}'.format(e.stderr))\n\t\t\t\tprint('Output: {0}'.format(e.stdout))\n\t\t\t\tif verbose:\n\t\t\t\t\ttraceback.print_", "is_pass": false, "return_code": 1}]}
{"file_path": "gopad/rest.py", "project": "gopad/gopad-python", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\timport socket\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\treturn True if re.match(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$', target) else False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\texcept AttributeError:  # no inet_pton here, sorry\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn True\n\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tif not isinstance(target, str):\n\t\treturn False\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\t# Convert to an integer\n\t\tip_num = int(ipaddress.IPv4Address(target))\n\t\t# Convert back to string\n\t\tip_str = str(ipaddress.IPv4Address(ip_num))\n\t\t# Check if target is the same\n\t\tif ip_str == target:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept ValueError:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\texcept AttributeError:  # no inet_pton here, sorry\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn True\n\texcept socket.error:  # not a valid address\n\t\treturn False\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\timport socket\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\treturn True if re.match(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$', target) else False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\texcept AttributeError:  # no inet_pton here, sorry\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\tif not isinstance(target, str):\n\t\treturn False\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\t# Convert to an integer\n\t\tip_num = int(ipaddress.IPv4Address(target))\n\t\t# Convert back to string\n\t\tip_str = str(ipaddress.IPv4Address(ip_num))\n\t\t# Check if target is the same\n\t\tif ip_str == target:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept ValueError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\texcept AttributeError:  # no inet_pton here, sorry\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn True\n\texcept socket.error:  # not a valid address\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rdflib/util.py", "project": "mwatts15/rdflib", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n    for s, o in graph.subject_objects(prop):\n        if s in roots:\n            continue\n        roots.add(s)\n        roots |= find_roots(graph, prop, roots)\n    return roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\tif s in roots:\n\t\t\tcontinue\n\t\tif o in roots:\n\t\t\tcontinue\n\t\troots.add(s)\n\t\troots.update(find_roots(graph, prop, roots))\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\tif s not in roots:\n\t\t\troots.add(s)\n\t\t\troots = find_roots(graph, prop, roots)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t# TODO: support more than one root\n\tif roots is None:\n\t\troots = set()\n\t# TODO: handle multiple roots\n\troot = None\n\tfor s, _, o in graph.triples((None, prop, None)):\n\t\tif s not in roots:\n\t\t\troots.add(s)\n\t\t\troot = s\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s in graph.subjects(prop, None):\n\t\tfor p in graph.objects(s, prop):\n\t\t\tif p not in roots:\n\t\t\t\troots.add(p)\n\t\t\t\troots = find_roots(graph, prop, roots)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph:\n\t\tif s not in roots and p == prop and o in roots:\n\t\t\troots.add(s)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, _, o in graph.triples((None, prop, None)):\n\t\troots.add(s)\n\t\troots |= find_roots(graph, prop, roots)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\t# find all nodes with prop as a direct parent\n\tfor node in graph.objects(None, prop):\n\t\t# add the node to the set of roots\n\t\troots.add(node)\n\t\t# recurse on the parents of the node\n\t\troots |= find_roots(graph, prop, roots)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor (child, prop, parent) in graph.triples((None, prop, None)):\n\t\troots.add(child)\n\t\troots = roots.union(find_roots(graph, prop, roots))\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tr = set()\n\tif roots is None:\n\t\troots = set()\n\tfor p, o in graph.subject_objects(prop):\n\t\tif o in roots:\n\t\t\tr.add(p)\n\t\t\troots.add(p)\n\t\t\troots = roots - r\n\treturn r\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n    for s, o in graph.subject_objects(prop):\n        if s in roots:\n            continue\n        roots.add(s)\n        roots |= find_roots(graph, prop, roots)\n    return roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\tif s in roots:\n\t\t\tcontinue\n\t\tif o in roots:\n\t\t\tcontinue\n\t\troots.add(s)\n\t\troots.update(find_roots(graph, prop, roots))\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\tif s not in roots:\n\t\t\troots.add(s)\n\t\t\troots = find_roots(graph, prop, roots)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t# TODO: support more than one root\n\tif roots is None:\n\t\troots = set()\n\t# TODO: handle multiple roots\n\troot = None\n\tfor s, _, o in graph.triples((None, prop, None)):\n\t\tif s not in roots:\n\t\t\troots.add(s)\n\t\t\troot = s\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s in graph.subjects(prop, None):\n\t\tfor p in graph.objects(s, prop):\n\t\t\tif p not in roots:\n\t\t\t\troots.add(p)\n\t\t\t\troots = find_roots(graph, prop, roots)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph:\n\t\tif s not in roots and p == prop and o in roots:\n\t\t\troots.add(s)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor s, _, o in graph.triples((None, prop, None)):\n\t\troots.add(s)\n\t\troots |= find_roots(graph, prop, roots)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\t# find all nodes with prop as a direct parent\n\tfor node in graph.objects(None, prop):\n\t\t# add the node to the set of roots\n\t\troots.add(node)\n\t\t# recurse on the parents of the node\n\t\troots |= find_roots(graph, prop, roots)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tif roots is None:\n\t\troots = set()\n\tfor (child, prop, parent) in graph.triples((None, prop, None)):\n\t\troots.add(child)\n\t\troots = roots.union(find_roots(graph, prop, roots))\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\tr = set()\n\tif roots is None:\n\t\troots = set()\n\tfor p, o in graph.subject_objects(prop):\n\t\tif o in roots:\n\t\t\tr.add(p)\n\t\t\troots.add(p)\n\t\t\troots = roots - r\n\treturn r\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "tests/unit/mock/yaml_helper.py", "project": "ansible-security/ansible_collections.ibm.qradar", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn dumper(obj, default_flow_style=False)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.SafeDumper\n\treturn yaml.dump(obj, default_flow_style=False, Dumper=dumper)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = _Dumper()\n\treturn yaml.dump(obj, dumper=dumper, default_flow_style=False)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.dumper\n\treturn yaml.dump(obj, dumper)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\treturn dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n\n\nclass YamlDumper(yaml.Dumper):\n\t\"\"\"\n\tA custom YAML dumper for dumping the YAML stream to a string\n\t\"\"\"\n\n\tdef __init__(self, **kwds):\n\t\t\"\"\"\n\t\tA custom YAML dumper for dumping the YAML stream to a string\n\t\t\"\"\"\n\t\tyaml.Dumper.__init__(self, **kwds)\n\t\tself.add_representer(str, _dump_string)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tdumper = dumper or Dumper\n\tdata = []\n\tfor item in obj:\n\t\tdata.append(dumper.represent_data(item))\n\treturn data\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif not dumper:\n\t\tdumper = _yaml\n\treturn dumper.dump(obj)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\treturn yaml.dump(obj, default_flow_style=False, Dumper=dumper)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = _yaml.dumper\n\tdumper.open()\n\tdumper.dump_python_object(obj)\n\tdumper.close()\n\treturn dumper.stream.getvalue()\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\n\ttry:\n\t\toutput = yaml.dump(obj, Dumper=dumper)\n\t\treturn output\n\texcept Exception as ex:\n\t\traise ex\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn dumper(obj, default_flow_style=False)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.SafeDumper\n\treturn yaml.dump(obj, default_flow_style=False, Dumper=dumper)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = _Dumper()\n\treturn yaml.dump(obj, dumper=dumper, default_flow_style=False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.dumper\n\treturn yaml.dump(obj, dumper)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\treturn dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n\n\nclass YamlDumper(yaml.Dumper):\n\t\"\"\"\n\tA custom YAML dumper for dumping the YAML stream to a string\n\t\"\"\"\n\n\tdef __init__(self, **kwds):\n\t\t\"\"\"\n\t\tA custom YAML dumper for dumping the YAML stream to a string\n\t\t\"\"\"\n\t\tyaml.Dumper.__init__(self, **kwds)\n\t\tself.add_representer(str, _dump_string)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tdumper = dumper or Dumper\n\tdata = []\n\tfor item in obj:\n\t\tdata.append(dumper.represent_data(item))\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif not dumper:\n\t\tdumper = _yaml\n\treturn dumper.dump(obj)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\treturn yaml.dump(obj, default_flow_style=False, Dumper=dumper)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = _yaml.dumper\n\tdumper.open()\n\tdumper.dump_python_object(obj)\n\tdumper.close()\n\treturn dumper.stream.getvalue()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\n\ttry:\n\t\toutput = yaml.dump(obj, Dumper=dumper)\n\t\treturn output\n\texcept Exception as ex:\n\t\traise ex\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "apphelpers/loggers.py", "project": "scrolltech/apphelpers", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\tif debug:\n\t\thandler = logging.StreamHandler()\n\telse:\n\t\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif debug:\n\t\t# Format for debugging\n\t\tformatter = logging.Formatter(\n\t\t\t'%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\telse:\n\t\t# Format for actual production\n\t\tformatter = logging.Formatter(\n\t\t\t'%(levelname)s - %(name)s - %(message)s')\n\n\t# Add the handlers to the logger\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport sys\n\tfrom logging import getLogger, StreamHandler, DEBUG, FileHandler, Formatter\n\tfrom logging.handlers import RotatingFileHandler\n\n\tlogger = getLogger(name)  # get a logger object\n\tlogger.setLevel(DEBUG)  # set level\n\tlogger.propagate = False  # disable propagation to root logger\n\n\t# add a stream handler\n\tstream_handler = StreamHandler(sys.stdout)\n\tstream_handler.setLevel(DEBUG)\n\tstream_handler.setFormatter(Formatter('[%(asctime)s] %(levelname)s %(message)s'))\n\tlogger.addHandler(stream_handler)\n\n\t# add a file handler\n\tfile_handler = FileHandler(logfile)\n\tfile_handler.setLevel(DEBUG)\n\tfile_handler.setFormatter(Formatter('[%(asctime)s] %(levelname)s %(message)s'))\n\tlogger.addHandler(file_handler)\n\n\t# add a rotating file handler\n\trotating_file_handler = RotatingFileHandler(logfile, maxBytes=1024 * 1024 * 5, backupCount=5)\n\trotating_file_handler.setLevel(DEBUG)\n\trotating_file_handler.set", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n\tfileHandler = logging.FileHandler(logfile)\n\tfileHandler.setLevel(logging.DEBUG)\n\tfileHandler.setFormatter(formatter)\n\tlogger.addHandler(fileHandler)\n\t# console handler\n\tconsoleHandler = logging.StreamHandler()\n\tconsoleHandler.setFormatter(formatter)\n\tconsoleHandler.setLevel(logging.WARN)\n\tif debug:\n\t\tconsoleHandler.setLevel(logging.DEBUG)\n\tlogger.addHandler(consoleHandler)\n\treturn logger\n\n\nclass Tensorboard_logger():\n\tdef __init__(self, log_dir, name, debug=True):\n\t\t\"\"\"\n\n\t\t:param log_dir:\n\t\t:param name:\n\t\t:param debug:\n\t\t\"\"\"\n\t\tself.debug = debug\n\t\tself.name = name\n\t\tself.log_dir = log_dir\n\t\tself.logger = build_app_logger(name=self.name, logfile=self.log_dir + '/' + self.name + '.log", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.propagate = False\n\tlogger.handlers = []\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tif debug:\n\t\t# log console\n\t\tdebug_handler = logging.StreamHandler()\n\t\tdebug_handler.setFormatter(formatter)\n\t\tdebug_handler.setLevel(logging.DEBUG)\n\t\tlogger.addHandler(debug_handler)\n\t# log file\n\tfile_handler = logging.FileHandler(logfile, mode='a')\n\tfile_handler.setFormatter(formatter)\n\tfile_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.addHandler(file_handler)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.INFO)\n\tfile_handler.setFormatter(formatter)\n\t# stream_handler = logging.StreamHandler()\n\t# stream_handler.setLevel(logging.INFO)\n\t# stream_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\t# logger.addHandler(stream_handler)\n\tif debug:\n\t\tlogger.debug('Logger is ready')\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tif debug:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\t\tch.setFormatter(formatter)\n\t\tlogger.addHandler(ch)\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t# Logging configuration\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tif debug:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\telse:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.INFO)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlog_format = '%(asctime)s - %(levelname)s - %(message)s'\n\tlogging.basicConfig(filename=logfile, level=logging.DEBUG, format=log_format)\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\treturn logger\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\tif debug:\n\t\thandler = logging.StreamHandler()\n\telse:\n\t\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif debug:\n\t\t# Format for debugging\n\t\tformatter = logging.Formatter(\n\t\t\t'%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\telse:\n\t\t# Format for actual production\n\t\tformatter = logging.Formatter(\n\t\t\t'%(levelname)s - %(name)s - %(message)s')\n\n\t# Add the handlers to the logger\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport sys\n\tfrom logging import getLogger, StreamHandler, DEBUG, FileHandler, Formatter\n\tfrom logging.handlers import RotatingFileHandler\n\n\tlogger = getLogger(name)  # get a logger object\n\tlogger.setLevel(DEBUG)  # set level\n\tlogger.propagate = False  # disable propagation to root logger\n\n\t# add a stream handler\n\tstream_handler = StreamHandler(sys.stdout)\n\tstream_handler.setLevel(DEBUG)\n\tstream_handler.setFormatter(Formatter('[%(asctime)s] %(levelname)s %(message)s'))\n\tlogger.addHandler(stream_handler)\n\n\t# add a file handler\n\tfile_handler = FileHandler(logfile)\n\tfile_handler.setLevel(DEBUG)\n\tfile_handler.setFormatter(Formatter('[%(asctime)s] %(levelname)s %(message)s'))\n\tlogger.addHandler(file_handler)\n\n\t# add a rotating file handler\n\trotating_file_handler = RotatingFileHandler(logfile, maxBytes=1024 * 1024 * 5, backupCount=5)\n\trotating_file_handler.setLevel(DEBUG)\n\trotating_file_handler.set", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n\tfileHandler = logging.FileHandler(logfile)\n\tfileHandler.setLevel(logging.DEBUG)\n\tfileHandler.setFormatter(formatter)\n\tlogger.addHandler(fileHandler)\n\t# console handler\n\tconsoleHandler = logging.StreamHandler()\n\tconsoleHandler.setFormatter(formatter)\n\tconsoleHandler.setLevel(logging.WARN)\n\tif debug:\n\t\tconsoleHandler.setLevel(logging.DEBUG)\n\tlogger.addHandler(consoleHandler)\n\treturn logger\n\n\nclass Tensorboard_logger():\n\tdef __init__(self, log_dir, name, debug=True):\n\t\t\"\"\"\n\n\t\t:param log_dir:\n\t\t:param name:\n\t\t:param debug:\n\t\t\"\"\"\n\t\tself.debug = debug\n\t\tself.name = name\n\t\tself.log_dir = log_dir\n\t\tself.logger = build_app_logger(name=self.name, logfile=self.log_dir + '/' + self.name + '.log", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.propagate = False\n\tlogger.handlers = []\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tif debug:\n\t\t# log console\n\t\tdebug_handler = logging.StreamHandler()\n\t\tdebug_handler.setFormatter(formatter)\n\t\tdebug_handler.setLevel(logging.DEBUG)\n\t\tlogger.addHandler(debug_handler)\n\t# log file\n\tfile_handler = logging.FileHandler(logfile, mode='a')\n\tfile_handler.setFormatter(formatter)\n\tfile_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.addHandler(file_handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.INFO)\n\tfile_handler.setFormatter(formatter)\n\t# stream_handler = logging.StreamHandler()\n\t# stream_handler.setLevel(logging.INFO)\n\t# stream_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\t# logger.addHandler(stream_handler)\n\tif debug:\n\t\tlogger.debug('Logger is ready')\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tif debug:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\t\tch.setFormatter(formatter)\n\t\tlogger.addHandler(ch)\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t# Logging configuration\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tif debug:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\telse:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.INFO)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlog_format = '%(asctime)s - %(levelname)s - %(message)s'\n\tlogging.basicConfig(filename=logfile, level=logging.DEBUG, format=log_format)\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\treturn logger\n", "is_pass": false, "return_code": 1}]}
{"file_path": "radiospectra/spectrogram.py", "project": "sunpy/radiospectra", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tif len(shape) == 1:\n\t\treturn np.zeros(shape[0], dtype=dtype)\n\telse:\n\t\treturn np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tif len(shape) == 1:\n\t\treturn np.zeros(shape[0], dtype=dtype)\n\telse:\n\t\treturn np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "concert/tests/unit/devices/test_monochromator.py", "project": "ufo-kit/concert", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x)**2/(2*0.1**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x-0.2)**2/(2*0.1**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x - u)**2/(2*sigma**2))\n\n\nx = np.linspace(-10,10,1000)\ny = gaussian(x)\n\nplt.plot(x,y)\nplt.show()\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn 1/(np.sqrt(2*np.pi)*0.1)*np.exp(-(x-0.2)**2/(2*0.1**2))\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x - 0.2)**2/0.1**2)\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-x**2/(2*(0.1)**2))\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-x**2)\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x - 0.2)**2 / (0.1**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn (1/(x**2)) * np.exp(-x**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-x**2/2)\n\t\ndef gaussian_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*x**2)\n\t\ndef gaussian_sum_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum of sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*(x-0.1)**2)\n\t\ndef gaussian_sum_sum_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum of sum of sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*((x-0.1)**2+0.1))\n\t\ndef gaussian_sum_sum_sum_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum of sum of sum of sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*(((x-0.1)**2+0.1)-0.05))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x)**2/(2*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x-0.2)**2/(2*0.1**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x - u)**2/(2*sigma**2))\n\n\nx = np.linspace(-10,10,1000)\ny = gaussian(x)\n\nplt.plot(x,y)\nplt.show()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn 1/(np.sqrt(2*np.pi)*0.1)*np.exp(-(x-0.2)**2/(2*0.1**2))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x - 0.2)**2/0.1**2)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-x**2/(2*(0.1)**2))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-x**2)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-(x - 0.2)**2 / (0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn (1/(x**2)) * np.exp(-x**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-x**2/2)\n\t\ndef gaussian_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*x**2)\n\t\ndef gaussian_sum_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum of sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*(x-0.1)**2)\n\t\ndef gaussian_sum_sum_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum of sum of sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*((x-0.1)**2+0.1))\n\t\ndef gaussian_sum_sum_sum_sum(x):\n\t\"\"\"\n\tCalculate Gaussian sum of sum of sum of sum centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\treturn np.exp(-0.5*(((x-0.1)**2+0.1)-0.05))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/borgmatic.py", "project": "witten/borgmatic", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\tlogs = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\texcept Exception as e:\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\tfilename,\n\t\t\t\t0,\n\t\t\t\t'Cannot read configuration file %s: %s' % (filename, str(e)),\n\t\t\t\tNone,\n\t\t\t\tNone\n\t\t\t))\n\t\t\tcontinue\n\t\tif not isinstance(config, dict):\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\tfilename,\n\t\t\t\t0,\n\t\t\t\t'Configuration file %s is not a dictionary.' % filename,\n\t\t\t\tNone,\n\t\t\t\tNone\n\t\t\t))\n\t\t\tcontinue\n\t\tif not config:\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.WARN,\n\t\t\t\tfilename,\n\t", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.info('Loading configurations...')\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tlogging.debug('Loading config file: %s', config_filename)\n\t\tconfig_path = os.path.join(config.CONFIG_DIR, config_filename)\n\t\ttry:\n\t\t\tif os.path.exists(config_path):\n\t\t\t\tconfig_file = open(config_path, mode='r')\n\t\t\t\tconfigs[config_filename] = parse_config(config_file.read(), config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\telse:\n\t\t\t\tlogging.warning('Config file not found: %s', config_filename)\n\t\t\t\tconfigs[config_filename] = {}\n\t\texcept (OSError, ValueError) as e:\n\t\t\tlogging.error('Could not load config file %s due to %s', config_filename, e)\n\t\t\tconfigs[config_filename] = {}\n\t\t\terrors.append(logging.LogRecord(logging.ERROR, '', '', '', '', '', '', '', '', e))\n\treturn configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\texcept ConfigurationError as e:\n\t\t\terrors.append(e)\n\t\telse:\n\t\t\tconfigs[filename] = config\n\treturn configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif overrides is None:\n\t\toverrides = {}\n\tresult = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config(filename, resolve_env=resolve_env)\n\t\t\tresult[filename] = config\n\t\texcept Exception as e:\n\t\t\terrors.append(\n\t\t\t\tConfigError(filename, 0, 0, 'Error loading configuration file: ' + str(e)))\n\t\t\tlogger.error('Error loading configuration file: ' + str(e))\n\t\t\tcontinue\n\t\tif not isinstance(config, dict):\n\t\t\terrors.append(ConfigError(filename, 0, 0, 'Configuration file must be a dictionary'))\n\t\t\tlogger.error('Configuration file must be a dictionary')\n\t\t\tcontinue\n\t\tvalidate_config(config, filename, errors)\n\t\tif errors:\n\t\t\tcontinue\n\t\tfor k, v in overrides.items():\n\t\t\ttry:\n\t\t\t\tconfig[k] = v\n\t\t\texcept Exception:\n\t\t\t\terrors.append(\n\t\t\t\t\tConfigError(filename, 0, 0, 'Configuration override failed: ' + k))\n\t\t\t\tlogger.error", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif not config_filenames:\n\t\traise ValueError(\"No config_filenames given\")\n\n\tconfigs = {}\n\tparse_error_logs = []\n\n\tfor filename in config_filenames:\n\t\tconfig = {}\n\t\ttry:\n\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\t\tconfigs[filename] = config\n\t\texcept Exception as e:\n\t\t\tparse_error_logs.append(e)\n\t\t\tlogger.error('Failed to parse configuration file: %s', filename, exc_info=True)\n\n\treturn configs, parse_error_logs\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t_configs = {}\n\t_errors = []\n\tfor config_filename in config_filenames:\n\t\tconfig_dict = parse_config(config_filename, overrides, resolve_env)\n\t\tif config_dict is None:\n\t\t\treturn None, None\n\t\t_configs[config_filename] = config_dict\n\treturn _configs, _errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif resolve_env:\n\t\tconfig_filenames = [os.path.expanduser(filename) for filename in config_filenames]\n\t\tconfig_filenames = [os.path.expandvars(filename) for filename in config_filenames]\n\n\tlogger = logging.getLogger('config')\n\tlogger.setLevel(logging.INFO)\n\tconfig_files = []\n\tlog_records = []\n\tconfigs = {}\n\tfor config_filename in config_filenames:\n\t\tif resolve_env:\n\t\t\tconfig_filename = os.path.expanduser(config_filename)\n\t\t\tconfig_filename = os.path.expandvars(config_filename)\n\t\ttry:\n\t\t\tconfig_text = open(config_filename, 'r').read()\n\t\texcept IOError as e:\n\t\t\tlog_record = logging.LogRecord('config', logging.CRITICAL, config_filename, 0, 'Cannot read configuration file', (e, ))\n\t\t\tlog_records.append(log_record)\n\t\t\tlogger.handle(log_record)\n\t\t\tcontinue\n\t\texcept UnicodeDecodeError as e:\n\t\t\tlog_record = logging.LogRecord('config', logging.CRITIC", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif overrides is None:\n\t\toverrides = {}\n\tconfig_values = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tconfig_value, config_errors = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\tconfig_values[config_filename] = config_value\n\t\terrors += config_errors\n\treturn config_values, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tresult = []\n\terror_logs = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tresult.append(load_configurations_from_file(config_filename, overrides=overrides, resolve_env=resolve_env))\n\t\texcept Exception as err:\n\t\t\terror_logs.append(logging.LogRecord(\"\", 0, \"\", 0, \"Error loading configuration file %s: %s\" % (config_filename, err), \"\", 0))\n\treturn result, error_logs\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\terrors = []\n\tconfigs = {}\n\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename, \"r\") as f:\n\t\t\t\tconfig = json.load(f)\n\t\texcept IOError as e:\n\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t\"msg\": \"Cannot read configuration file {}: {}\".format(filename, e),\n\t\t\t\t\"level\": logging.ERROR,\n\t\t\t\t\"filename\": filename\n\t\t\t}))\n\t\texcept ValueError as e:\n\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t\"msg\": \"Cannot parse configuration file {}: {}\".format(filename, e),\n\t\t\t\t\"level\": logging.ERROR,\n\t\t\t\t\"filename\": filename\n\t\t\t}))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tconfig = validate_config(config, overrides=overrides, resolve_env=resolve_env)\n\t\t\texcept ValueError as e:\n\t\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t\t\"msg\": \"Cannot validate configuration file {}: {}\".format(filename, e),\n\t\t\t\t\t\"level\": logging.ERROR,\n\t"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\tlogs = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\texcept Exception as e:\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\tfilename,\n\t\t\t\t0,\n\t\t\t\t'Cannot read configuration file %s: %s' % (filename, str(e)),\n\t\t\t\tNone,\n\t\t\t\tNone\n\t\t\t))\n\t\t\tcontinue\n\t\tif not isinstance(config, dict):\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.ERROR,\n\t\t\t\tfilename,\n\t\t\t\t0,\n\t\t\t\t'Configuration file %s is not a dictionary.' % filename,\n\t\t\t\tNone,\n\t\t\t\tNone\n\t\t\t))\n\t\t\tcontinue\n\t\tif not config:\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\t'load_configurations',\n\t\t\t\tlogging.WARN,\n\t\t\t\tfilename,\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.info('Loading configurations...')\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tlogging.debug('Loading config file: %s', config_filename)\n\t\tconfig_path = os.path.join(config.CONFIG_DIR, config_filename)\n\t\ttry:\n\t\t\tif os.path.exists(config_path):\n\t\t\t\tconfig_file = open(config_path, mode='r')\n\t\t\t\tconfigs[config_filename] = parse_config(config_file.read(), config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\telse:\n\t\t\t\tlogging.warning('Config file not found: %s', config_filename)\n\t\t\t\tconfigs[config_filename] = {}\n\t\texcept (OSError, ValueError) as e:\n\t\t\tlogging.error('Could not load config file %s due to %s', config_filename, e)\n\t\t\tconfigs[config_filename] = {}\n\t\t\terrors.append(logging.LogRecord(logging.ERROR, '', '', '', '', '', '', '', '', e))\n\treturn configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\texcept ConfigurationError as e:\n\t\t\terrors.append(e)\n\t\telse:\n\t\t\tconfigs[filename] = config\n\treturn configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif overrides is None:\n\t\toverrides = {}\n\tresult = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config(filename, resolve_env=resolve_env)\n\t\t\tresult[filename] = config\n\t\texcept Exception as e:\n\t\t\terrors.append(\n\t\t\t\tConfigError(filename, 0, 0, 'Error loading configuration file: ' + str(e)))\n\t\t\tlogger.error('Error loading configuration file: ' + str(e))\n\t\t\tcontinue\n\t\tif not isinstance(config, dict):\n\t\t\terrors.append(ConfigError(filename, 0, 0, 'Configuration file must be a dictionary'))\n\t\t\tlogger.error('Configuration file must be a dictionary')\n\t\t\tcontinue\n\t\tvalidate_config(config, filename, errors)\n\t\tif errors:\n\t\t\tcontinue\n\t\tfor k, v in overrides.items():\n\t\t\ttry:\n\t\t\t\tconfig[k] = v\n\t\t\texcept Exception:\n\t\t\t\terrors.append(\n\t\t\t\t\tConfigError(filename, 0, 0, 'Configuration override failed: ' + k))\n\t\t\t\tlogger.error", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif not config_filenames:\n\t\traise ValueError(\"No config_filenames given\")\n\n\tconfigs = {}\n\tparse_error_logs = []\n\n\tfor filename in config_filenames:\n\t\tconfig = {}\n\t\ttry:\n\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\t\tconfigs[filename] = config\n\t\texcept Exception as e:\n\t\t\tparse_error_logs.append(e)\n\t\t\tlogger.error('Failed to parse configuration file: %s', filename, exc_info=True)\n\n\treturn configs, parse_error_logs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t_configs = {}\n\t_errors = []\n\tfor config_filename in config_filenames:\n\t\tconfig_dict = parse_config(config_filename, overrides, resolve_env)\n\t\tif config_dict is None:\n\t\t\treturn None, None\n\t\t_configs[config_filename] = config_dict\n\treturn _configs, _errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif resolve_env:\n\t\tconfig_filenames = [os.path.expanduser(filename) for filename in config_filenames]\n\t\tconfig_filenames = [os.path.expandvars(filename) for filename in config_filenames]\n\n\tlogger = logging.getLogger('config')\n\tlogger.setLevel(logging.INFO)\n\tconfig_files = []\n\tlog_records = []\n\tconfigs = {}\n\tfor config_filename in config_filenames:\n\t\tif resolve_env:\n\t\t\tconfig_filename = os.path.expanduser(config_filename)\n\t\t\tconfig_filename = os.path.expandvars(config_filename)\n\t\ttry:\n\t\t\tconfig_text = open(config_filename, 'r').read()\n\t\texcept IOError as e:\n\t\t\tlog_record = logging.LogRecord('config', logging.CRITICAL, config_filename, 0, 'Cannot read configuration file', (e, ))\n\t\t\tlog_records.append(log_record)\n\t\t\tlogger.handle(log_record)\n\t\t\tcontinue\n\t\texcept UnicodeDecodeError as e:\n\t\t\tlog_record = logging.LogRecord('config', logging.CRITIC", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif overrides is None:\n\t\toverrides = {}\n\tconfig_values = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tconfig_value, config_errors = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\tconfig_values[config_filename] = config_value\n\t\terrors += config_errors\n\treturn config_values, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tresult = []\n\terror_logs = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tresult.append(load_configurations_from_file(config_filename, overrides=overrides, resolve_env=resolve_env))\n\t\texcept Exception as err:\n\t\t\terror_logs.append(logging.LogRecord(\"\", 0, \"\", 0, \"Error loading configuration file %s: %s\" % (config_filename, err), \"\", 0))\n\treturn result, error_logs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\terrors = []\n\tconfigs = {}\n\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename, \"r\") as f:\n\t\t\t\tconfig = json.load(f)\n\t\texcept IOError as e:\n\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t\"msg\": \"Cannot read configuration file {}: {}\".format(filename, e),\n\t\t\t\t\"level\": logging.ERROR,\n\t\t\t\t\"filename\": filename\n\t\t\t}))\n\t\texcept ValueError as e:\n\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t\"msg\": \"Cannot parse configuration file {}: {}\".format(filename, e),\n\t\t\t\t\"level\": logging.ERROR,\n\t\t\t\t\"filename\": filename\n\t\t\t}))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tconfig = validate_config(config, overrides=overrides, resolve_env=resolve_env)\n\t\t\texcept ValueError as e:\n\t\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t\t\"msg\": \"Cannot validate configuration file {}: {}\".format(filename, e),\n\t\t\t\t\t\"level\": logging.ERROR,\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "o2sclpy/utils.py", "project": "awsteiner/o2sclpy", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, np.bytes_):\n\t\treturn bytes(obj).decode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, (numpy.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n\nclass ConsistencyConfig(object):\n\t\"\"\"\n\tConfiguration class for consistency\n\n\tAttributes:\n\t\tdefault_config: the default configuration for consistency\n\t\"\"\"\n\tdefault_config = {\n\t\t'consistency_type': 'strong',\n\t\t'consistency_level': 1,\n\t\t'strong_consistency_topic': None,\n\t\t'strong_consistency_topic_replication_factor': 1,\n\t\t'strong_consistency_topic_partitions': 10,\n\t\t'strong_consistency_topic_cleanup_policy': 'compact',\n\t\t'weak_consistency_topic': None,\n\t\t'weak_consistency_topic_replication_factor': 1,\n\t\t'weak_consistency_topic_partitions': 10,\n\t\t'weak_consistency_topic_cleanup_policy': 'compact',\n\t\t'weak_consistency_topic_replication_factor_auto_inc': False,\n\t\t'weak_consistency_topic_replication_factor_min': 1,\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, (numpy.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode(\"utf-8\")\n\telif isinstance(obj, bytes):\n\t\treturn obj.decode(\"utf-8\")\n\telif isinstance(obj, list):\n\t\treturn [force_string(item) for item in obj]\n\telif isinstance(obj, dict):\n\t\treturn {force_string(key): force_string(value) for key, value in obj.items()}\n\telse:\n\t\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, np.bytes_):\n\t\treturn str(obj, \"utf-8\")\n\telif isinstance(obj, bytes):\n\t\treturn str(obj, \"utf-8\")\n\telse:\n\t\treturn obj\n\n\nclass ConsistencyConfig:\n\t\"\"\"\n\tConfig for consistency model.\n\t\"\"\"\n\tdef __init__(self, config):\n\t\tself.max_iter = config[\"max_iter\"]\n\t\tself.consistency_type = config[\"consistency_type\"]\n\t\tself.consistency_type = force_string(self.consistency_type)\n\t\tself.consistency_threshold = config[\"consistency_threshold\"]\n\t\tself.consistency_threshold = force_string(self.consistency_threshold)\n\t\tself.patience = config[\"patience\"]\n\t\tself.patience = force_string(self.patience)\n\t\tself.early_stopping = config[\"early_stopping\"]\n\t\tself.early_stopping = force_string(self.early_stopping)\n\t\tself.verbose = config[\"verbose\"]\n\t\tself.verbose = force_string(self.verbose)\n\t\tself", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif type(obj) == np.bytes_ or type(obj) == bytes:\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj\n\telif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, np.bytes_):\n\t\treturn bytes(obj).decode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, (numpy.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n\nclass ConsistencyConfig(object):\n\t\"\"\"\n\tConfiguration class for consistency\n\n\tAttributes:\n\t\tdefault_config: the default configuration for consistency\n\t\"\"\"\n\tdefault_config = {\n\t\t'consistency_type': 'strong',\n\t\t'consistency_level': 1,\n\t\t'strong_consistency_topic': None,\n\t\t'strong_consistency_topic_replication_factor': 1,\n\t\t'strong_consistency_topic_partitions': 10,\n\t\t'strong_consistency_topic_cleanup_policy': 'compact',\n\t\t'weak_consistency_topic': None,\n\t\t'weak_consistency_topic_replication_factor': 1,\n\t\t'weak_consistency_topic_partitions': 10,\n\t\t'weak_consistency_topic_cleanup_policy': 'compact',\n\t\t'weak_consistency_topic_replication_factor_auto_inc': False,\n\t\t'weak_consistency_topic_replication_factor_min': 1,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, (numpy.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode(\"utf-8\")\n\telif isinstance(obj, bytes):\n\t\treturn obj.decode(\"utf-8\")\n\telif isinstance(obj, list):\n\t\treturn [force_string(item) for item in obj]\n\telif isinstance(obj, dict):\n\t\treturn {force_string(key): force_string(value) for key, value in obj.items()}\n\telse:\n\t\treturn obj\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, np.bytes_):\n\t\treturn str(obj, \"utf-8\")\n\telif isinstance(obj, bytes):\n\t\treturn str(obj, \"utf-8\")\n\telse:\n\t\treturn obj\n\n\nclass ConsistencyConfig:\n\t\"\"\"\n\tConfig for consistency model.\n\t\"\"\"\n\tdef __init__(self, config):\n\t\tself.max_iter = config[\"max_iter\"]\n\t\tself.consistency_type = config[\"consistency_type\"]\n\t\tself.consistency_type = force_string(self.consistency_type)\n\t\tself.consistency_threshold = config[\"consistency_threshold\"]\n\t\tself.consistency_threshold = force_string(self.consistency_threshold)\n\t\tself.patience = config[\"patience\"]\n\t\tself.patience = force_string(self.patience)\n\t\tself.early_stopping = config[\"early_stopping\"]\n\t\tself.early_stopping = force_string(self.early_stopping)\n\t\tself.verbose = config[\"verbose\"]\n\t\tself.verbose = force_string(self.verbose)\n\t\tself", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif type(obj) == np.bytes_ or type(obj) == bytes:\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj\n\telif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"Time ticks must be between 0 and 86400000000000\")\n\n\treturn cls(\n\t\tint(ticks / 1000000000),\n\t\tint((ticks % 1000000000) / 1000000),\n\t\tint((ticks % 1000000) / 1000),\n\t\tint(ticks % 1000),\n\t\ttz\n\t)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks > 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\treturn cls._from_ticks(ticks, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\tticks = int(ticks)\n\tif tz is None:\n\t\treturn cls(ticks)\n\telse:\n\t\treturn cls(ticks, tz)\n\n\n@classmethod\ndef from_datetime(cls, dt, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.\n\n:param dt: datetime\n:type dt: datetime.datetime\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if dt has a microsecond component\n\t\"\"\"\n\tif dt.microsecond:\n\t\traise ValueError('datetime has microsecond component')\n\tticks = int((dt - datetime.datetime(1970, 1, 1)).total_seconds() * 10000000)\n\tif tz is None:\n\t\treturn cls(ticks)\n\telse:\n\t\treturn cls(ticks, tz)\n\n\n@classmethod\ndef from_timedelta(cls, td, tz=None):\n\t\"\"\"", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000000)\")\n\treturn cls(ticks, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks > 86400000000000:\n\t\traise ValueError(\"ticks must be in range [0, 86400000000000]\")\n\treturn cls(datetime.time(0, 0, 0, ticks % 1000000000, ticks / 1000000000, tzinfo=tz))\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of bounds')\n\treturn cls(ticks)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(ticks=ticks, tz=tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(int(ticks), tz=tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of bounds (0 <= ticks < 86400000000000)')\n\treturn cls(ticks=ticks, tz=tz)\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of bounds (0 <= ticks < 86400000000000)')\n\tseconds = ticks // 1000000000\n\tnanoseconds = ticks % 1000000000\n\treturn cls(seconds, nanoseconds, tz)\n\n@classmethod\ndef from_time(cls, time, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.time.\n\n:param time: time value\n:type time: datetime.time\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises TypeError: if time is not a datetime.time\n\t\"\"\"\n\tif not isinstance(time, datetime.time):\n\t\traise TypeError('time must be a datetime.time')\n\treturn cls(time.hour, time.minute, time.second, time.microsecond * 1000, tz)\n\n@classmethod\ndef from_datetime(cls, datetime_, tz=None):\n\t\"\"\"\n\tCreate a time from"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"Time ticks must be between 0 and 86400000000000\")\n\n\treturn cls(\n\t\tint(ticks / 1000000000),\n\t\tint((ticks % 1000000000) / 1000000),\n\t\tint((ticks % 1000000) / 1000),\n\t\tint(ticks % 1000),\n\t\ttz\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks > 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\treturn cls._from_ticks(ticks, tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\tticks = int(ticks)\n\tif tz is None:\n\t\treturn cls(ticks)\n\telse:\n\t\treturn cls(ticks, tz)\n\n\n@classmethod\ndef from_datetime(cls, dt, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.\n\n:param dt: datetime\n:type dt: datetime.datetime\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if dt has a microsecond component\n\t\"\"\"\n\tif dt.microsecond:\n\t\traise ValueError('datetime has microsecond component')\n\tticks = int((dt - datetime.datetime(1970, 1, 1)).total_seconds() * 10000000)\n\tif tz is None:\n\t\treturn cls(ticks)\n\telse:\n\t\treturn cls(ticks, tz)\n\n\n@classmethod\ndef from_timedelta(cls, td, tz=None):\n\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000000)\")\n\treturn cls(ticks, tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0 or ticks > 86400000000000:\n\t\traise ValueError(\"ticks must be in range [0, 86400000000000]\")\n\treturn cls(datetime.time(0, 0, 0, ticks % 1000000000, ticks / 1000000000, tzinfo=tz))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of bounds')\n\treturn cls(ticks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(ticks=ticks, tz=tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(int(ticks), tz=tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of bounds (0 <= ticks < 86400000000000)')\n\treturn cls(ticks=ticks, tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError('ticks out of bounds (0 <= ticks < 86400000000000)')\n\tseconds = ticks // 1000000000\n\tnanoseconds = ticks % 1000000000\n\treturn cls(seconds, nanoseconds, tz)\n\n@classmethod\ndef from_time(cls, time, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.time.\n\n:param time: time value\n:type time: datetime.time\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises TypeError: if time is not a datetime.time\n\t\"\"\"\n\tif not isinstance(time, datetime.time):\n\t\traise TypeError('time must be a datetime.time')\n\treturn cls(time.hour, time.minute, time.second, time.microsecond * 1000, tz)\n\n@classmethod\ndef from_datetime(cls, datetime_, tz=None):\n\t\"\"\"\n\tCreate a time from", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls.all_versions()\n\telif isinstance(protocol_version, tuple):\n\t\tif len(protocol_version) != 2:\n\t\t\traise TypeError(\"protocol versions must be represented \"\n\t\t\t\t\t\t\t\"as a two-element tuple\")\n\t\treturn {protocol_version: cls.get_handler(protocol_version)}\n\telse:\n\t\traise TypeError(\"protocol versions must be passed as a \"\n\t\t\t\t\t\t\"tuple\")\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple\")\n\n\thandlers = {}\n\n\tfor version in cls.SUPPORTED_VERSIONS:\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = cls.get_handler(version)\n\n\treturn handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be a tuple\")\n\t\ttry:\n\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\texcept KeyError:\n\t\t\traise ValueError(\"Unsupported protocol version %s\" % protocol_version)\n\telse:\n\t\treturn cls._protocol_handlers\n\n\nclass Neo4j(object):\n\t\"\"\"\n\t:seealso: :py:class:`.GraphDatabase`\n\t\"\"\"\n\t_protocol_handlers = {}\n\n\tdef __init__(self, uri, auth=None, **config):\n\t\t\"\"\"\n\t\tCreate a new instance of the Bolt object.\n\n\t\t:param uri: URI identifying the target server\n\t\t:param auth: authentication information\n\t\t:param config: configuration dictionary\n\t\t\"\"\"\n\t\tself._uri = uri\n\t\tself._auth = auth\n\t\tself._config = config\n\n\tdef __repr__(self):\n\t\treturn \"Neo4j<%s>\" % self._uri\n\n\tdef _connect(self):\n\t\t\"\"\"\n\t\tEstablish a connection to the server.\n\n\t", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls._protocol_handlers(protocol_version)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn {v:cls for v in SUPPORTED_VERSIONS}\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version in SUPPORTED_VERSIONS:\n\t\t\treturn {protocol_version:cls}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\traise TypeError(\"Protocol version must be tuple\")\n\n\nclass Transaction:\n\t\"\"\"\n\tClass representing a Cypher transaction.\n\n\tThis class is not used directly by application code.\n\tIt is created by :meth:`Neo4j.begin_transaction` and passed to\n\t:meth:`Transaction.run` and :meth:`Transaction.commit` to execute\n\tCypher statements within this transaction.\n\n\tThis class is not thread safe. It should not be shared between threads.\n\t\"\"\"\n\n\tdef __init__(self, connection, bookmarks):\n\t\tself.connection = connection\n\t\tself.bookmarks = bookmarks\n\t\tself.closed = False\n\t\tself.closed_reason = None\n\t\tself.closed_signals = []\n\t\tself.closed_futures = []\n\t\tself.closed_event = asyncio.Event()\n\t\tself.results = []\n\t\tself", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = BoltProtocolV1.protocol_handlers(protocol_version)\n\tprotocol_handlers.update(BoltProtocolV2.protocol_handlers(protocol_version))\n\tprotocol_handlers.update(BoltProtocolV3.protocol_handlers(protocol_version))\n\tprotocol_handlers.update(BoltProtocolV4.protocol_handlers(protocol_version))\n\treturn protocol_handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn {v: cls for v in cls.protocol_versions()}\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Expected protocol version as tuple\")\n\t# Look up the available handlers for this version\n\ttry:\n\t\tcls = cls.protocol_versions()[protocol_version]\n\texcept KeyError:\n\t\traise UnsupportedProtocolError(\"Unsupported protocol version: {}\".format(protocol_version))\n\telse:\n\t\treturn {protocol_version: cls}\n\n\nclass Bolt(object):\n\t\"\"\"\n\tAbstract base class for all Bolt implementations.\n\t\"\"\"\n\n\tdef __init__(self, address, user_agent, auth_token=None,\n\t\t\t\t connection_observer=None, max_connection_lifetime=None,\n\t\t\t\t routing_context=None, use_tlsv1_2=False):\n\n\t\tself.address = address\n\t\tself.user_agent = user_agent\n\t\tself.auth_token = auth_token\n\t\tself.connection_observer = connection_observer\n\t\tself.max_connection_lifetime = max_connection_lifetime\n\t\tself.routing_context =", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn dict(cls.SUPPORTED_VERSIONS)\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version in cls.SUPPORTED_VERSIONS:\n\t\t\treturn {protocol_version: cls.SUPPORTED_VERSIONS[protocol_version]}\n\t\telse:\n\t\t\traise ValueError(\"Unsupported protocol version %s\" % protocol_version)\n\telse:\n\t\traise TypeError(\"Protocol version must be passed as a tuple\")\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn dict(cls._protocol_handlers)\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\t\tif protocol_version in cls._protocol_handlers:\n\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\n\n@classmethod\ndef version_info(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a tuple identifying the highest known Bolt protocol version\n\tsupported by this driver, or the version specified if provided.\n\n\t:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n\t:return: tuple identifying the highest known Bolt protocol version\n\tsupported by this driver, or the version specified if provided\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls._max_protocol_version\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\t\tif protocol_version in cls._protocol", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"`protocol_version` must be a tuple, \"\n\t\t\t\t\t\t\t\"not {}\".format(type(protocol_version)))\n\t\treturn {protocol_version: cls._handlers[protocol_version]}\n\telse:\n\t\treturn cls._handlers\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls.all_versions()\n\telif isinstance(protocol_version, tuple):\n\t\tif len(protocol_version) != 2:\n\t\t\traise TypeError(\"protocol versions must be represented \"\n\t\t\t\t\t\t\t\"as a two-element tuple\")\n\t\treturn {protocol_version: cls.get_handler(protocol_version)}\n\telse:\n\t\traise TypeError(\"protocol versions must be passed as a \"\n\t\t\t\t\t\t\"tuple\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple\")\n\n\thandlers = {}\n\n\tfor version in cls.SUPPORTED_VERSIONS:\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = cls.get_handler(version)\n\n\treturn handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be a tuple\")\n\t\ttry:\n\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\texcept KeyError:\n\t\t\traise ValueError(\"Unsupported protocol version %s\" % protocol_version)\n\telse:\n\t\treturn cls._protocol_handlers\n\n\nclass Neo4j(object):\n\t\"\"\"\n\t:seealso: :py:class:`.GraphDatabase`\n\t\"\"\"\n\t_protocol_handlers = {}\n\n\tdef __init__(self, uri, auth=None, **config):\n\t\t\"\"\"\n\t\tCreate a new instance of the Bolt object.\n\n\t\t:param uri: URI identifying the target server\n\t\t:param auth: authentication information\n\t\t:param config: configuration dictionary\n\t\t\"\"\"\n\t\tself._uri = uri\n\t\tself._auth = auth\n\t\tself._config = config\n\n\tdef __repr__(self):\n\t\treturn \"Neo4j<%s>\" % self._uri\n\n\tdef _connect(self):\n\t\t\"\"\"\n\t\tEstablish a connection to the server.\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls._protocol_handlers(protocol_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn {v:cls for v in SUPPORTED_VERSIONS}\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version in SUPPORTED_VERSIONS:\n\t\t\treturn {protocol_version:cls}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\traise TypeError(\"Protocol version must be tuple\")\n\n\nclass Transaction:\n\t\"\"\"\n\tClass representing a Cypher transaction.\n\n\tThis class is not used directly by application code.\n\tIt is created by :meth:`Neo4j.begin_transaction` and passed to\n\t:meth:`Transaction.run` and :meth:`Transaction.commit` to execute\n\tCypher statements within this transaction.\n\n\tThis class is not thread safe. It should not be shared between threads.\n\t\"\"\"\n\n\tdef __init__(self, connection, bookmarks):\n\t\tself.connection = connection\n\t\tself.bookmarks = bookmarks\n\t\tself.closed = False\n\t\tself.closed_reason = None\n\t\tself.closed_signals = []\n\t\tself.closed_futures = []\n\t\tself.closed_event = asyncio.Event()\n\t\tself.results = []\n\t\tself", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = BoltProtocolV1.protocol_handlers(protocol_version)\n\tprotocol_handlers.update(BoltProtocolV2.protocol_handlers(protocol_version))\n\tprotocol_handlers.update(BoltProtocolV3.protocol_handlers(protocol_version))\n\tprotocol_handlers.update(BoltProtocolV4.protocol_handlers(protocol_version))\n\treturn protocol_handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn {v: cls for v in cls.protocol_versions()}\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Expected protocol version as tuple\")\n\t# Look up the available handlers for this version\n\ttry:\n\t\tcls = cls.protocol_versions()[protocol_version]\n\texcept KeyError:\n\t\traise UnsupportedProtocolError(\"Unsupported protocol version: {}\".format(protocol_version))\n\telse:\n\t\treturn {protocol_version: cls}\n\n\nclass Bolt(object):\n\t\"\"\"\n\tAbstract base class for all Bolt implementations.\n\t\"\"\"\n\n\tdef __init__(self, address, user_agent, auth_token=None,\n\t\t\t\t connection_observer=None, max_connection_lifetime=None,\n\t\t\t\t routing_context=None, use_tlsv1_2=False):\n\n\t\tself.address = address\n\t\tself.user_agent = user_agent\n\t\tself.auth_token = auth_token\n\t\tself.connection_observer = connection_observer\n\t\tself.max_connection_lifetime = max_connection_lifetime\n\t\tself.routing_context =", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn dict(cls.SUPPORTED_VERSIONS)\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version in cls.SUPPORTED_VERSIONS:\n\t\t\treturn {protocol_version: cls.SUPPORTED_VERSIONS[protocol_version]}\n\t\telse:\n\t\t\traise ValueError(\"Unsupported protocol version %s\" % protocol_version)\n\telse:\n\t\traise TypeError(\"Protocol version must be passed as a tuple\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn dict(cls._protocol_handlers)\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\t\tif protocol_version in cls._protocol_handlers:\n\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\n\n@classmethod\ndef version_info(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a tuple identifying the highest known Bolt protocol version\n\tsupported by this driver, or the version specified if provided.\n\n\t:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n\t:return: tuple identifying the highest known Bolt protocol version\n\tsupported by this driver, or the version specified if provided\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls._max_protocol_version\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\t\tif protocol_version in cls._protocol", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"`protocol_version` must be a tuple, \"\n\t\t\t\t\t\t\t\"not {}\".format(type(protocol_version)))\n\t\treturn {protocol_version: cls._handlers[protocol_version]}\n\telse:\n\t\treturn cls._handlers\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\n\treturn cls([Bookmark.from_raw_value(value) for value in values])\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor bookmark_string in values:\n\t\tbookmarks.add_from_raw_value(bookmark_string)\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.raw_values = values\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tif not values:\n\t\treturn None\n\tbookmarks = []\n\tfor value in values:\n\t\tif value:\n\t\t\tbookmarks.append(cls.from_raw_value(value))\n\treturn cls(bookmarks)\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\tfor value in values:\n\t\tif value == '':\n\t\t\tbookmarks.append(None)\n\t\telse:\n\t\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\treturn cls(bookmarks)\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\treturn cls(Bookmark.from_raw_value(val) for val in values)\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\tfor value in values:\n\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\treturn cls(bookmarks)\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\tfor value in values:\n\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\treturn cls(bookmarks)\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.add(value)\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\n\tfor value in values:\n\t\tbookmarks.add_bookmark(value)\n\n\treturn bookmarks\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\n\treturn cls([Bookmark.from_raw_value(value) for value in values])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor bookmark_string in values:\n\t\tbookmarks.add_from_raw_value(bookmark_string)\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.raw_values = values\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tif not values:\n\t\treturn None\n\tbookmarks = []\n\tfor value in values:\n\t\tif value:\n\t\t\tbookmarks.append(cls.from_raw_value(value))\n\treturn cls(bookmarks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\tfor value in values:\n\t\tif value == '':\n\t\t\tbookmarks.append(None)\n\t\telse:\n\t\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\treturn cls(bookmarks)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\treturn cls(Bookmark.from_raw_value(val) for val in values)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\tfor value in values:\n\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\treturn cls(bookmarks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\tfor value in values:\n\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\treturn cls(bookmarks)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.add(value)\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\n\tfor value in values:\n\t\tbookmarks.add_bookmark(value)\n\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\tseq = list(seq)\n\t# try to find a subsequence of *seq* that matches the type\n\tfor i in range(1,len(seq)):\n\t\tif type(seq[i]) != type(seq[i-1]):\n\t\t\tbreak\n\t\tif i >= bufsize:\n\t\t\treturn seq, type(seq[i])\n\t# if we can't find a subsequence of *seq* that matches the type,\n\t# return *seq* itself\n\treturn seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = seq.nbytes\n\treturn (seq.view(np.uint8).reshape((bufsize,)),\n\t\t\ttype(seq))\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif (isinstance(seq, str) or isinstance(seq, bytes)):\n\t\tseq = bytearray(seq)\n\tif isinstance(seq, memoryview):\n\t\tseq = seq.obj\n\tif not isinstance(seq, buffer):\n\t\tseq = memoryview(seq)\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\treturn (seq, type(seq))\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, (tuple, list)):\n\t\treturn seq, seq.__class__\n\telif isinstance(seq, str):\n\t\tif bufsize is not None:\n\t\t\treturn seq[:bufsize], str\n\t\treturn seq, str\n\telse:\n\t\treturn seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t_seq = seq\n\t_seq_type = type(_seq)\n\tif isinstance(_seq, (str, bytes)):\n\t\tif bufsize is None:\n\t\t\t_seq = _seq\n\t\telse:\n\t\t\t_seq = _seq[:bufsize]\n\telif not isinstance(_seq, bytearray):\n\t\tif bufsize is None:\n\t\t\t_seq = list(_seq)\n\t\telse:\n\t\t\t_seq = list(_seq)[:bufsize]\n\telse:\n\t\tif bufsize is None:\n\t\t\t_seq = _seq\n\t\telse:\n\t\t\t_seq = bytearray(_seq)[:bufsize]\n\treturn _seq, _seq_type\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\n\tif isinstance(seq, basestring):\n\t\tseq = seq.encode('UTF-8')\n\t\treturn seq, ctypes.c_char_p\n\telif isinstance(seq, int):\n\t\tctype = ctypes.c_int\n\telif isinstance(seq, float):\n\t\tctype = ctypes.c_double\n\telse:\n\t\tctype = ctypes.c_char_p\n\t\n\tif len(seq) > bufsize:\n\t\tseq = seq[:bufsize]\n\tif isinstance(seq, unicode):\n\t\tseq = seq.encode('UTF-8')\n\treturn seq, ctype\n\n_get_seq_with_type_cached = _lru_cache(4)(_get_seq_with_type)\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = 1\n\treturn seq, _get_type(seq, bufsize=bufsize)\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is not None:\n\t\treturn (seq, type(seq[0:bufsize]))\n\telse:\n\t\treturn (seq, type(seq))\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = 8\n\tif isinstance(seq, (list, tuple)):\n\t\treturn (seq, 'i')\n\telif isinstance(seq, (bytes, bytearray)):\n\t\treturn (seq, 'b')\n\telif isinstance(seq, memoryview):\n\t\treturn (seq, 'B')\n\telif isinstance(seq, byte_type):\n\t\treturn (seq, 'B')\n\telse:\n\t\treturn (seq, 'i')\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = _seq_or_none(seq)\n\tif seq is None:\n\t\treturn seq, None\n\tseqtype = type(seq)\n\t# If this is a string buffer, return the underlying bytes\n\tif bufsize is not None and seqtype is bytes:\n\t\tif len(seq) > bufsize:\n\t\t\traise ValueError('buffer is too small')\n\t\tseq = seq.rstrip(b'\\0')\n\treturn seq, seqtype\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\tseq = list(seq)\n\t# try to find a subsequence of *seq* that matches the type\n\tfor i in range(1,len(seq)):\n\t\tif type(seq[i]) != type(seq[i-1]):\n\t\t\tbreak\n\t\tif i >= bufsize:\n\t\t\treturn seq, type(seq[i])\n\t# if we can't find a subsequence of *seq* that matches the type,\n\t# return *seq* itself\n\treturn seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = seq.nbytes\n\treturn (seq.view(np.uint8).reshape((bufsize,)),\n\t\t\ttype(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif (isinstance(seq, str) or isinstance(seq, bytes)):\n\t\tseq = bytearray(seq)\n\tif isinstance(seq, memoryview):\n\t\tseq = seq.obj\n\tif not isinstance(seq, buffer):\n\t\tseq = memoryview(seq)\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\treturn (seq, type(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, (tuple, list)):\n\t\treturn seq, seq.__class__\n\telif isinstance(seq, str):\n\t\tif bufsize is not None:\n\t\t\treturn seq[:bufsize], str\n\t\treturn seq, str\n\telse:\n\t\treturn seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t_seq = seq\n\t_seq_type = type(_seq)\n\tif isinstance(_seq, (str, bytes)):\n\t\tif bufsize is None:\n\t\t\t_seq = _seq\n\t\telse:\n\t\t\t_seq = _seq[:bufsize]\n\telif not isinstance(_seq, bytearray):\n\t\tif bufsize is None:\n\t\t\t_seq = list(_seq)\n\t\telse:\n\t\t\t_seq = list(_seq)[:bufsize]\n\telse:\n\t\tif bufsize is None:\n\t\t\t_seq = _seq\n\t\telse:\n\t\t\t_seq = bytearray(_seq)[:bufsize]\n\treturn _seq, _seq_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\n\tif isinstance(seq, basestring):\n\t\tseq = seq.encode('UTF-8')\n\t\treturn seq, ctypes.c_char_p\n\telif isinstance(seq, int):\n\t\tctype = ctypes.c_int\n\telif isinstance(seq, float):\n\t\tctype = ctypes.c_double\n\telse:\n\t\tctype = ctypes.c_char_p\n\t\n\tif len(seq) > bufsize:\n\t\tseq = seq[:bufsize]\n\tif isinstance(seq, unicode):\n\t\tseq = seq.encode('UTF-8')\n\treturn seq, ctype\n\n_get_seq_with_type_cached = _lru_cache(4)(_get_seq_with_type)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = 1\n\treturn seq, _get_type(seq, bufsize=bufsize)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is not None:\n\t\treturn (seq, type(seq[0:bufsize]))\n\telse:\n\t\treturn (seq, type(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = 8\n\tif isinstance(seq, (list, tuple)):\n\t\treturn (seq, 'i')\n\telif isinstance(seq, (bytes, bytearray)):\n\t\treturn (seq, 'b')\n\telif isinstance(seq, memoryview):\n\t\treturn (seq, 'B')\n\telif isinstance(seq, byte_type):\n\t\treturn (seq, 'B')\n\telse:\n\t\treturn (seq, 'i')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = _seq_or_none(seq)\n\tif seq is None:\n\t\treturn seq, None\n\tseqtype = type(seq)\n\t# If this is a string buffer, return the underlying bytes\n\tif bufsize is not None and seqtype is bytes:\n\t\tif len(seq) > bufsize:\n\t\t\traise ValueError('buffer is too small')\n\t\tseq = seq.rstrip(b'\\0')\n\treturn seq, seqtype\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"cannot compute scale of an empty histogram\")\n\t\treturn self.scale\n\tif recompute:\n\t\tself._scale = None\n\tif self.scale != 0:\n\t\tself.scale = other\n\t\tself.scale_set = True\n\t\treturn\n\tif other == 0:\n\t\traise LenaValueError(\n\t\t\t\"cannot scale an histogram to zero\")\n\tself.scale = other\n\tself.scale_set = True\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\t# Compute and store scale\n\t\t\tself._scale = self.integrate()\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"Can't rescale a histogram with scale equal to zero\")\n\t# Rescale\n\tif self._scale == 1:\n\t\tself.data[:] = other * self.data\n\telse:\n\t\tself.data[:] = other * self.data / self._scale\n\tself._scale = other\n\treturn self\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not self.scale or recompute:\n\t\t\tself.recompute_scale()\n\t\treturn self.scale\n\telif isinstance(other, numbers.Real):\n\t\tif self.scale:\n\t\t\tself.scale = other\n\t\telse:\n\t\t\traise LenaValueError('Scale must be calculated before rescaling.')\n\telse:\n\t\traise LenaTypeError('Other must be a number or None.')\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not hasattr(self, '_scale'):\n\t\t\tif not recompute:\n\t\t\t\traise LenaValueError('scale not computed')\n\t\t\tself._scale = float(sum(self._histogram))\n\t\treturn self._scale\n\telse:\n\t\tif not isinstance(other, (int, float)):\n\t\t\traise TypeError(\n\t\t\t\t'float or None expected, not {0}'.format(type(other)))\n\t\tif float(other) == 0:\n\t\t\traise LenaValueError('scale cannot be zero')\n\t\tif not hasattr(self, '_scale'):\n\t\t\tself._scale = float(sum(self._histogram))\n\t\tif not recompute:\n\t\t\tself._histogram = [float(h)/self._scale for h in self._histogram]\n\t\telse:\n\t\t\tself._histogram = [float(h)/other for h in self._histogram]\n\t\tself._scale = float(other)\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif len(self) == 0:\n\t\t\treturn 0.0\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self._sum()\n\t\treturn self._scale\n\tif self._scale is not None:\n\t\tif other == 0.0:\n\t\t\traise LenaValueError('histogram scale is zero')\n\t\tself._scale = other\n\t\treturn self\n\tif other == 0.0:\n\t\traise LenaValueError('histogram scale is zero')\n\tself._scale = other\n\treturn self\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError('scale = 0')\n\t\treturn self.scale\n\telif isinstance(other, float):\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError('scale = 0')\n\t\tself.scale = other\n\t\tself.integral = None\n\t\treturn\n\telse:\n\t\traise LenaTypeError('float or None')\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale == 0.0:\n\t\t\traise LenaValueError('histogram with scale of 0.0 cannot be rescaled.')\n\t\treturn self.scale\n\tif other == 0.0:\n\t\traise LenaValueError('histogram with scale of 0.0 cannot be rescaled.')\n\tif self.scale == 0.0 or recompute:\n\t\tself.scale = self._get_scale(recompute)\n\telse:\n\t\tself.scale = other / self._get_scale(recompute)\n\treturn self\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not self.scale == 0.:\n\t\t\treturn self.scale\n\t\telif recompute:\n\t\t\tself.scale = self.histogram.sum()\n\t\t\treturn self.scale\n\t\telse:\n\t\t\treturn 0.\n\telif not isinstance(other, numbers.Real):\n\t\traise TypeError(\"other must be a number\")\n\telif other == 0.:\n\t\traise LenaValueError(\"Can't scale to zero\")\n\telse:\n\t\tself.histogram /= self.scale\n\t\tself.scale = other\n\t\tself.histogram *= self.scale\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not self._scale and not recompute:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Scale of histogram with no data is undefined\"\n\t\t\t)\n\t\treturn self._scale\n\tif self._scale and not recompute:\n\t\traise LenaValueError(\n\t\t\t\"Scale of histogram is already computed, \"\n\t\t\t\"use recompute=True to force recomputation.\"\n\t\t)\n\tself._scale = self.sum()\n\tif other == 0:\n\t\traise LenaValueError(\n\t\t\t\"Histograms with scale equal to zero can't be rescaled\"\n\t\t)\n\tself.scale = other / self._scale\n\treturn self\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale is None:\n\t\t\tif recompute:\n\t\t\t\tself.scale = self._compute_scale()\n\t\t\telse:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\t\"scale is not computed, \"\n\t\t\t\t\t\t\"use recompute=True to compute scale\"\n\t\t\t\t\t\t)\n\t\treturn self.scale\n\tif other != 0.:\n\t\tif self.scale != 0.:\n\t\t\tself.scale = other\n\t\telse:\n\t\t\traise LenaValueError(\n\t\t\t\t\t\"histogram scale is zero, \"\n\t\t\t\t\t\"can't rescale histogram to {}\"\n\t\t\t\t\t.format(other)\n\t\t\t\t\t)\n\telse:\n\t\traise LenaValueError(\n\t\t\t\t\"can't rescale histogram to zero\"\n\t\t\t\t)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"cannot compute scale of an empty histogram\")\n\t\treturn self.scale\n\tif recompute:\n\t\tself._scale = None\n\tif self.scale != 0:\n\t\tself.scale = other\n\t\tself.scale_set = True\n\t\treturn\n\tif other == 0:\n\t\traise LenaValueError(\n\t\t\t\"cannot scale an histogram to zero\")\n\tself.scale = other\n\tself.scale_set = True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\t# Compute and store scale\n\t\t\tself._scale = self.integrate()\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"Can't rescale a histogram with scale equal to zero\")\n\t# Rescale\n\tif self._scale == 1:\n\t\tself.data[:] = other * self.data\n\telse:\n\t\tself.data[:] = other * self.data / self._scale\n\tself._scale = other\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not self.scale or recompute:\n\t\t\tself.recompute_scale()\n\t\treturn self.scale\n\telif isinstance(other, numbers.Real):\n\t\tif self.scale:\n\t\t\tself.scale = other\n\t\telse:\n\t\t\traise LenaValueError('Scale must be calculated before rescaling.')\n\telse:\n\t\traise LenaTypeError('Other must be a number or None.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not hasattr(self, '_scale'):\n\t\t\tif not recompute:\n\t\t\t\traise LenaValueError('scale not computed')\n\t\t\tself._scale = float(sum(self._histogram))\n\t\treturn self._scale\n\telse:\n\t\tif not isinstance(other, (int, float)):\n\t\t\traise TypeError(\n\t\t\t\t'float or None expected, not {0}'.format(type(other)))\n\t\tif float(other) == 0:\n\t\t\traise LenaValueError('scale cannot be zero')\n\t\tif not hasattr(self, '_scale'):\n\t\t\tself._scale = float(sum(self._histogram))\n\t\tif not recompute:\n\t\t\tself._histogram = [float(h)/self._scale for h in self._histogram]\n\t\telse:\n\t\t\tself._histogram = [float(h)/other for h in self._histogram]\n\t\tself._scale = float(other)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif len(self) == 0:\n\t\t\treturn 0.0\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self._sum()\n\t\treturn self._scale\n\tif self._scale is not None:\n\t\tif other == 0.0:\n\t\t\traise LenaValueError('histogram scale is zero')\n\t\tself._scale = other\n\t\treturn self\n\tif other == 0.0:\n\t\traise LenaValueError('histogram scale is zero')\n\tself._scale = other\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError('scale = 0')\n\t\treturn self.scale\n\telif isinstance(other, float):\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError('scale = 0')\n\t\tself.scale = other\n\t\tself.integral = None\n\t\treturn\n\telse:\n\t\traise LenaTypeError('float or None')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale == 0.0:\n\t\t\traise LenaValueError('histogram with scale of 0.0 cannot be rescaled.')\n\t\treturn self.scale\n\tif other == 0.0:\n\t\traise LenaValueError('histogram with scale of 0.0 cannot be rescaled.')\n\tif self.scale == 0.0 or recompute:\n\t\tself.scale = self._get_scale(recompute)\n\telse:\n\t\tself.scale = other / self._get_scale(recompute)\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not self.scale == 0.:\n\t\t\treturn self.scale\n\t\telif recompute:\n\t\t\tself.scale = self.histogram.sum()\n\t\t\treturn self.scale\n\t\telse:\n\t\t\treturn 0.\n\telif not isinstance(other, numbers.Real):\n\t\traise TypeError(\"other must be a number\")\n\telif other == 0.:\n\t\traise LenaValueError(\"Can't scale to zero\")\n\telse:\n\t\tself.histogram /= self.scale\n\t\tself.scale = other\n\t\tself.histogram *= self.scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not self._scale and not recompute:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Scale of histogram with no data is undefined\"\n\t\t\t)\n\t\treturn self._scale\n\tif self._scale and not recompute:\n\t\traise LenaValueError(\n\t\t\t\"Scale of histogram is already computed, \"\n\t\t\t\"use recompute=True to force recomputation.\"\n\t\t)\n\tself._scale = self.sum()\n\tif other == 0:\n\t\traise LenaValueError(\n\t\t\t\"Histograms with scale equal to zero can't be rescaled\"\n\t\t)\n\tself.scale = other / self._scale\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale is None:\n\t\t\tif recompute:\n\t\t\t\tself.scale = self._compute_scale()\n\t\t\telse:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\t\"scale is not computed, \"\n\t\t\t\t\t\t\"use recompute=True to compute scale\"\n\t\t\t\t\t\t)\n\t\treturn self.scale\n\tif other != 0.:\n\t\tif self.scale != 0.:\n\t\t\tself.scale = other\n\t\telse:\n\t\t\traise LenaValueError(\n\t\t\t\t\t\"histogram scale is zero, \"\n\t\t\t\t\t\"can't rescale histogram to {}\"\n\t\t\t\t\t.format(other)\n\t\t\t\t\t)\n\telse:\n\t\traise LenaValueError(\n\t\t\t\t\"can't rescale histogram to zero\"\n\t\t\t\t)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tself._scale = other\n\treturn self\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif not isinstance(other, (int, float)):\n\t\traise TypeError(\n\t\t\t\t\"Expected a numeric value, got %r.\"\n\t\t\t\t\t\t% other.__class__.__name__)\n\tif not self._scale:\n\t\traise LenaValueError(\n\t\t\t\t\"Cannot rescale a graph with unknown or zero scale.\")\n\tself._scale = other\n\treturn self\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\treturn self._scale\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif not isinstance(other, (int, float)):\n\t\traise TypeError('scale() takes 1 positional argument but 2 were given')\n\tself._scale = other\n\tself.errors.scale(other)\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Unknown scale of the graph (%r).\" % self\n\t\t\t)\n\t\treturn self.scale\n\telse:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Unknown scale of the graph (%r).\" % self\n\t\t\t)\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Zero scale of the graph (%r).\" % self\n\t\t\t)\n\t\tself.scale = float(other)\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale is None:\n\t\traise LenaValueError('graph has unknown scale')\n\tif self._scale == 0:\n\t\traise LenaValueError('graph has zero scale')\n\tif not is_numeric(other):\n\t\traise LenaTypeError('numeric other', type(other))\n\tself._scale = other / self._scale\n\treturn self._scale\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.scale\n\tif other == 0:\n\t\traise LenaValueError('scale is zero')\n\tif other < 0:\n\t\traise LenaValueError('negative scale')\n\tif self.scale == 0:\n\t\traise LenaValueError('unknown scale')\n\tif other == self.scale:\n\t\treturn\n\tfor error in self.errors:\n\t\terror.scale(other / self.scale)\n\tself.scale = other\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"unknown scale\")\n\tif other == 0:\n\t\traise LenaValueError(\"zero scale\")\n\tfor e in self._errors:\n\t\te._scale = other / self._scale\n\tself._scale = other\n\treturn self\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.xscale\n\telse:\n\t\tfor coord in self.coordinates:\n\t\t\tif coord.scale == 0:\n\t\t\t\traise LenaValueError('scale', 'zero')\n\t\t\tcoord.scale *= other\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tself._scale = other\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tself._scale = other\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif not isinstance(other, (int, float)):\n\t\traise TypeError(\n\t\t\t\t\"Expected a numeric value, got %r.\"\n\t\t\t\t\t\t% other.__class__.__name__)\n\tif not self._scale:\n\t\traise LenaValueError(\n\t\t\t\t\"Cannot rescale a graph with unknown or zero scale.\")\n\tself._scale = other\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\treturn self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif not isinstance(other, (int, float)):\n\t\traise TypeError('scale() takes 1 positional argument but 2 were given')\n\tself._scale = other\n\tself.errors.scale(other)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Unknown scale of the graph (%r).\" % self\n\t\t\t)\n\t\treturn self.scale\n\telse:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Unknown scale of the graph (%r).\" % self\n\t\t\t)\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Zero scale of the graph (%r).\" % self\n\t\t\t)\n\t\tself.scale = float(other)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale is None:\n\t\traise LenaValueError('graph has unknown scale')\n\tif self._scale == 0:\n\t\traise LenaValueError('graph has zero scale')\n\tif not is_numeric(other):\n\t\traise LenaTypeError('numeric other', type(other))\n\tself._scale = other / self._scale\n\treturn self._scale\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.scale\n\tif other == 0:\n\t\traise LenaValueError('scale is zero')\n\tif other < 0:\n\t\traise LenaValueError('negative scale')\n\tif self.scale == 0:\n\t\traise LenaValueError('unknown scale')\n\tif other == self.scale:\n\t\treturn\n\tfor error in self.errors:\n\t\terror.scale(other / self.scale)\n\tself.scale = other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"unknown scale\")\n\tif other == 0:\n\t\traise LenaValueError(\"zero scale\")\n\tfor e in self._errors:\n\t\te._scale = other / self._scale\n\tself._scale = other\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.xscale\n\telse:\n\t\tfor coord in self.coordinates:\n\t\t\tif coord.scale == 0:\n\t\t\t\traise LenaValueError('scale', 'zero')\n\t\t\tcoord.scale *= other\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tself._scale = other\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tif scale is None:\n\t\tscale = hist.scale\n\tif scale is True:\n\t\tscale = hist.scale\n\tif isinstance(field_names, str):\n\t\tfield_names = (field_names,)\n\tif len(field_names) != hist.dimension:\n\t\traise ValueError(\"number of field names must be the same as the dimension of the histogram\")\n\tif get_coordinate == \"left\":\n\t\tpos = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tpos = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tpos = lambda bin_: bin_.middle\n\telse:\n\t\traise ValueError(\"unknown get_coordinate\")\n\tgraph = Graph(\n\t\thist.dimension,\n\t\tscale=scale,\n\t\tfield_names=field_names,\n\t\tmake_value=make_value,\n\t\t)\n\tfor bin_ in hist:\n\t\tgraph.add_point(\n\t\t\tpos(bin_),\n\t\t\tbin_.content,\n\t\t\t)\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\tgraph = graph_cls(scale=scale, value_field_names=field_names)\n\tfor bin_ in hist:\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.context\n\t\tvalue = make_value(bin_)\n\t\tif get_coordinate == \"left\":\n\t\t\tbin_left = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tbin_left = bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tbin_left = bin_.middle\n\t\telse:\n\t\t\traise ValueError(\"Unknown get_coordinate: %r\" % get_coordinate)\n\t\tgraph.append(bin_left, value)\n\treturn graph\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\tif scale is True:\n\t\tscale = hist.scale\n\tif get_coordinate is None:\n\t\traise ValueError(\"`get_coordinate` must not be None\")\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif len(field_names) != 2:\n\t\traise ValueError(\"`field_names` must have length 2\")\n\tif hist.context:\n\t\tif make_value is None:\n\t\t\traise ValueError(\"`make_value` must not be None if histogram has a context\")\n\t\tif not callable(make_value):\n\t\t\traise TypeError(\"`make_value` must be a callable if histogram has a context\")\n\t\tif len(field_names) != len(make_value(*hist.bins[0])):\n\t\t\traise ValueError(\"`field_names` must have same length as `make_value` returns\")\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\"`get_coordinate` must be None, 'left', 'right' or 'middle' if histogram has a context\")\n\t\tif get_coordinate == \"middle\":\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\thist = histogram.as_histogram(hist)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tscale = histogram.get_scale(hist) if scale is True else scale\n\treturn graph.graph(\n\t\t\tmake_value=make_value,\n\t\t\tget_coordinate=get_coordinate,\n\t\t\tfield_names=field_names,\n\t\t\tscale=scale)(hist)\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tfrom .context import Context\n\n\tif isinstance(hist, Histogram):\n\t\thist = hist.bins\n\tgraph = Graph(scale=scale)\n\tfor bin_ in hist:\n\t\tif isinstance(bin_.value, Context):\n\t\t\traise TypeError(\"histogram with contexts is not supported\")\n\t\tif make_value is None:\n\t\t\tvalue = bin_.value\n\t\telse:\n\t\t\tvalue = make_value(bin_.value)\n\t\tif isinstance(value, tuple):\n\t\t\tpoints = []\n\t\t\tfor p in value:\n\t\t\t\tif isinstance(p, Context):\n\t\t\t\t\traise TypeError(\"histogram with contexts is not supported\")\n\t\t\t\tpoints.append(p)\n\t\t\tif len(points) == 1:\n\t\t\t\tvalue = points[0]\n\t\t\telse:\n\t\t\t\tgraph.add_point(*points)\n\t\t\t\tcontinue\n\t\tif get_coordinate == \"left\":\n\t\t\tx = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tx = bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tx = bin_.left", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Unknown get_coordinate value\")\n\tif not scale:\n\t\tscale = hist.scale\n\tgraph = Graph(scale=scale)\n\tfor field in field_names:\n\t\tgraph.add_field(field, hist.field_type(field))\n\tif hist.context:\n\t\tif make_value is None:\n\t\t\traise ValueError(\"No make_value for a context histogram\")\n\t\telse:\n\t\t\tmake_value = make_value_with_context(make_value, hist.context)\n\tfor bin_ in hist.bins:\n\t\tgraph_point = graph.add_point()\n\t\tfor field, value in zip(field_names, make_value(bin_)):\n\t\t\tgraph_point.set_field(field, value)\n\t\tif get_coordinate == \"left\":\n\t\t\tgraph_point.coordinate = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tgraph_point.coordinate = bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tgraph_point.coordinate = (bin_.left + bin_.right) / 2\n\treturn graph\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\treturn graph(hist.edges(make_edges=True),\n\t\t\t\t hist.make_bin_content(make_value),\n\t\t\t\t get_coordinate, field_names,\n\t\t\t\t hist.name, scale)\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tassert len(field_names) == 2\n\tif get_coordinate == \"left\":\n\t\tcoordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tcoordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tcoordinate = lambda bin_: bin_.middle\n\telse:\n\t\traise ValueError(f\"Unknown get_coordinate: {get_coordinate}\")\n\n\tif scale is True:\n\t\tscale = hist.scale\n\n\tpoints = []\n\n\tfor bin_ in hist:\n\t\tx = coordinate(bin_)\n\t\tif make_value is None:\n\t\t\ty = bin_\n\t\telse:\n\t\t\ty = make_value(bin_)\n\n\t\tif isinstance(y, tuple):\n\t\t\ty = list(y)\n\n\t\tif isinstance(y, (list, tuple)):\n\t\t\tif len(y) == 1:\n\t\t\t\ty = y[0]\n\t\t\telse:\n\t\t\t\tassert len(y) == 2\n\t\t\t\ty = [y[0], y[1], 0]\n\t\telse:\n\t\t\ty = [y, 0, 0]\n\n\t\tpoints.append", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\tif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\tif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.middle\n\tif not hasattr(get_coordinate, \"__call__\"):\n\t\traise TypeError(\"get_coordinate must be a callable\")\n\tif not hasattr(make_value, \"__call__\"):\n\t\traise TypeError(\"make_value must be a callable\")\n\tif len(field_names) != hist.dimension:\n\t\traise ValueError(\"field_names length must be the same as dimension\")\n\tgr = graph(scale=scale)\n\tfor i, field_name in enumerate(field_names):\n\t\tgr.add_field(field_name)\n\tfor bin_ in hist.bins:\n\t\tgraph_point = gr.add_point()\n\t\tmake_value_result = make_value(bin_)\n\t\tif len(make_value_result) != hist.dimension:", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif isinstance(hist, histogram1d):\n\t\treturn _hist_to_graph(hist, make_value, get_coordinate, field_names, scale)\n\telif isinstance(hist, histogram2d):\n\t\treturn _hist2d_to_graph(hist, make_value, get_coordinate, field_names, scale)\n\telse:\n\t\traise ValueError(\"Unsupported histogram type\")\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tif scale is None:\n\t\tscale = hist.scale\n\tif scale is True:\n\t\tscale = hist.scale\n\tif isinstance(field_names, str):\n\t\tfield_names = (field_names,)\n\tif len(field_names) != hist.dimension:\n\t\traise ValueError(\"number of field names must be the same as the dimension of the histogram\")\n\tif get_coordinate == \"left\":\n\t\tpos = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tpos = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tpos = lambda bin_: bin_.middle\n\telse:\n\t\traise ValueError(\"unknown get_coordinate\")\n\tgraph = Graph(\n\t\thist.dimension,\n\t\tscale=scale,\n\t\tfield_names=field_names,\n\t\tmake_value=make_value,\n\t\t)\n\tfor bin_ in hist:\n\t\tgraph.add_point(\n\t\t\tpos(bin_),\n\t\t\tbin_.content,\n\t\t\t)\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\tgraph = graph_cls(scale=scale, value_field_names=field_names)\n\tfor bin_ in hist:\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.context\n\t\tvalue = make_value(bin_)\n\t\tif get_coordinate == \"left\":\n\t\t\tbin_left = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tbin_left = bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tbin_left = bin_.middle\n\t\telse:\n\t\t\traise ValueError(\"Unknown get_coordinate: %r\" % get_coordinate)\n\t\tgraph.append(bin_left, value)\n\treturn graph\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\tif scale is True:\n\t\tscale = hist.scale\n\tif get_coordinate is None:\n\t\traise ValueError(\"`get_coordinate` must not be None\")\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif len(field_names) != 2:\n\t\traise ValueError(\"`field_names` must have length 2\")\n\tif hist.context:\n\t\tif make_value is None:\n\t\t\traise ValueError(\"`make_value` must not be None if histogram has a context\")\n\t\tif not callable(make_value):\n\t\t\traise TypeError(\"`make_value` must be a callable if histogram has a context\")\n\t\tif len(field_names) != len(make_value(*hist.bins[0])):\n\t\t\traise ValueError(\"`field_names` must have same length as `make_value` returns\")\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\"`get_coordinate` must be None, 'left', 'right' or 'middle' if histogram has a context\")\n\t\tif get_coordinate == \"middle\":\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\thist = histogram.as_histogram(hist)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tscale = histogram.get_scale(hist) if scale is True else scale\n\treturn graph.graph(\n\t\t\tmake_value=make_value,\n\t\t\tget_coordinate=get_coordinate,\n\t\t\tfield_names=field_names,\n\t\t\tscale=scale)(hist)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tfrom .context import Context\n\n\tif isinstance(hist, Histogram):\n\t\thist = hist.bins\n\tgraph = Graph(scale=scale)\n\tfor bin_ in hist:\n\t\tif isinstance(bin_.value, Context):\n\t\t\traise TypeError(\"histogram with contexts is not supported\")\n\t\tif make_value is None:\n\t\t\tvalue = bin_.value\n\t\telse:\n\t\t\tvalue = make_value(bin_.value)\n\t\tif isinstance(value, tuple):\n\t\t\tpoints = []\n\t\t\tfor p in value:\n\t\t\t\tif isinstance(p, Context):\n\t\t\t\t\traise TypeError(\"histogram with contexts is not supported\")\n\t\t\t\tpoints.append(p)\n\t\t\tif len(points) == 1:\n\t\t\t\tvalue = points[0]\n\t\t\telse:\n\t\t\t\tgraph.add_point(*points)\n\t\t\t\tcontinue\n\t\tif get_coordinate == \"left\":\n\t\t\tx = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tx = bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tx = bin_.left", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Unknown get_coordinate value\")\n\tif not scale:\n\t\tscale = hist.scale\n\tgraph = Graph(scale=scale)\n\tfor field in field_names:\n\t\tgraph.add_field(field, hist.field_type(field))\n\tif hist.context:\n\t\tif make_value is None:\n\t\t\traise ValueError(\"No make_value for a context histogram\")\n\t\telse:\n\t\t\tmake_value = make_value_with_context(make_value, hist.context)\n\tfor bin_ in hist.bins:\n\t\tgraph_point = graph.add_point()\n\t\tfor field, value in zip(field_names, make_value(bin_)):\n\t\t\tgraph_point.set_field(field, value)\n\t\tif get_coordinate == \"left\":\n\t\t\tgraph_point.coordinate = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tgraph_point.coordinate = bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tgraph_point.coordinate = (bin_.left + bin_.right) / 2\n\treturn graph\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\treturn graph(hist.edges(make_edges=True),\n\t\t\t\t hist.make_bin_content(make_value),\n\t\t\t\t get_coordinate, field_names,\n\t\t\t\t hist.name, scale)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tassert len(field_names) == 2\n\tif get_coordinate == \"left\":\n\t\tcoordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tcoordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tcoordinate = lambda bin_: bin_.middle\n\telse:\n\t\traise ValueError(f\"Unknown get_coordinate: {get_coordinate}\")\n\n\tif scale is True:\n\t\tscale = hist.scale\n\n\tpoints = []\n\n\tfor bin_ in hist:\n\t\tx = coordinate(bin_)\n\t\tif make_value is None:\n\t\t\ty = bin_\n\t\telse:\n\t\t\ty = make_value(bin_)\n\n\t\tif isinstance(y, tuple):\n\t\t\ty = list(y)\n\n\t\tif isinstance(y, (list, tuple)):\n\t\t\tif len(y) == 1:\n\t\t\t\ty = y[0]\n\t\t\telse:\n\t\t\t\tassert len(y) == 2\n\t\t\t\ty = [y[0], y[1], 0]\n\t\telse:\n\t\t\ty = [y, 0, 0]\n\n\t\tpoints.append", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif scale is None:\n\t\tscale = hist.scale\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.content\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\tif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\tif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.middle\n\tif not hasattr(get_coordinate, \"__call__\"):\n\t\traise TypeError(\"get_coordinate must be a callable\")\n\tif not hasattr(make_value, \"__call__\"):\n\t\traise TypeError(\"make_value must be a callable\")\n\tif len(field_names) != hist.dimension:\n\t\traise ValueError(\"field_names length must be the same as dimension\")\n\tgr = graph(scale=scale)\n\tfor i, field_name in enumerate(field_names):\n\t\tgr.add_field(field_name)\n\tfor bin_ in hist.bins:\n\t\tgraph_point = gr.add_point()\n\t\tmake_value_result = make_value(bin_)\n\t\tif len(make_value_result) != hist.dimension:", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tif isinstance(hist, histogram1d):\n\t\treturn _hist_to_graph(hist, make_value, get_coordinate, field_names, scale)\n\telif isinstance(hist, histogram2d):\n\t\treturn _hist2d_to_graph(hist, make_value, get_coordinate, field_names, scale)\n\telse:\n\t\traise ValueError(\"Unsupported histogram type\")\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface = provideInterface(iface)\n\t\tiface.verifyClass(candidate)\n\tfor name, attr in iface.namesAndDescriptors():\n\t\tif isinstance(attr, (Interface, Attribute)):\n\t\t\tcontinue\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\n\t\t\t\tu\"Missing attribute %r on %r.\" % (name, candidate),\n\t\t\t\tcandidate, name, iface)\n\t\tifattr = getattr(candidate, name)\n\t\tif not callable(ifattr):\n\t\t\traise Invalid(\n\t\t\t\tu\"Attribute %r on %r is not callable.\" % (name, candidate),\n\t\t\t\tcandidate, name, iface)\n\t\tif attr.required is None:\n\t\t\tcontinue\n\t\tif len(attr.required) != len(ifattr.__code__.co_varnames):\n\t\t\traise Invalid(\n\t\t\t\tu\"Incorrect signature for %r on %r.\" % (name, candidate),\n\t\t\t\tcandidate, name, iface)\n\t\tfor index, argname in enumerate(ifattr.__code__.co_varnames):\n\t\t\t", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Candidate %r does not claim to provide \"\n\t\t\t\t\t\t  \"interface %r\" % (candidate, iface))\n\n\tfor requirement in iface.getAttributeRequirements():\n\t\tif not hasattr(candidate, requirement.name):\n\t\t\traise Invalid(\"Candidate %r does not define \"\n\t\t\t\t\t\t  \"attribute %r\" % (candidate, requirement))\n\n\tfor requirement in iface.getMethodRequirements():\n\t\tif not hasattr(candidate, requirement.name):\n\t\t\traise Invalid(\"Candidate %r does not define \"\n\t\t\t\t\t\t  \"method %r\" % (candidate, requirement))\n\t\tif vtype is not None and not _verify_signature(\n\t\t\t\tcandidate, requirement.name, vtype):\n\t\t\traise Invalid(\"Candidate %r does not define \"\n\t\t\t\t\t\t  \"method %r with a signature that is \"\n\t\t\t\t\t\t  \"compatible with the interface\" % (\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcandidate,", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate)\n\tfor attrname in iface.names():\n\t\tif getattr(iface, attrname, None) is None:\n\t\t\tcontinue\n\t\tattr = getattr(candidate, attrname)\n\t\tif not callable(attr):\n\t\t\traise Invalid('%r does not define %r()' % (candidate, attrname))\n\t\tif vtype is not None:\n\t\t\tif not _is_valid_signature(attr, iface, attrname, vtype):\n\t\t\t\traise Invalid('%r does not define %r() correctly' % (candidate, attrname))\n\tfor method in iface.functions():\n\t\tif method in skip_checks:\n\t\t\tcontinue\n\t\tattr = getattr(candidate, method)\n\t\tif not callable(attr):\n\t\t\traise Invalid('%r does not define %r()' % (candidate, method))\n\t\tif vtype is not None:\n\t\t\tif not _is_valid_signature(attr, iface, method, vtype):\n\t\t\t\traise Invalid('%r does not define %r() correctly' % (candidate, method))\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"Candidate does not provide the interface: %r\" % iface)\n\n\t# Check that all required methods are present\n\n\tfor method in iface.names:\n\t\tif method.startswith('_') or method == '__getattr__':\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tgetattr(candidate, method)\n\t\texcept AttributeError:\n\t\t\tif method not in candidate.names():\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"Candidate does not provide method %s: %r\"\n\t\t\t\t\t% (method, candidate))\n\t\t\telse:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"Candidate does not provide method %s: %r\"\n\t\t\t\t\t% (method, candidate))\n\n\t# Check that all required attributes are present\n\n\tfor attr in iface.names:\n\t\tif attr.startswith('_') or attr == '__getattr__':\n\t\t\tcontinue\n\t\ttry:\n\t\t\tgetattr(candidate, attr)\n\t\texcept AttributeError:\n\t\t\tif attr not in candidate.names():\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif vtype is None:\n\t\tvtype = _verify_method\n\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Object does not provide interface\", iface, candidate)\n\n\tfor name, method in iface.namesAndDescriptions():\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError as e:\n\t\t\t# Get the actual name of the method, not the mangled one\n\t\t\tif name.startswith('_'):\n\t\t\t\tname = name[1:]\n\t\t\traise Invalid(\"Method does not exist\", iface, candidate, name, e)\n\t\ttry:\n\t\t\tvtype(name, iface, candidate)\n\t\texcept Invalid as e:\n\t\t\t# Get the actual name of the method, not the mangled one\n\t\t\tif name.startswith('_'):\n\t\t\t\tname = name[1:]\n\t\t\traise Invalid(\"Method is invalid\", iface, candidate, name, e)\n\n\tfor name, attribute in iface.namesAndDescriptions(attributes=True):\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError as e:", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"does not provide %s\" % (iface,))\n\n\tfor name in iface.names(provided=True):\n\t\ttry:\n\t\t\tmethod = candidate.__getattr__(name)\n\t\texcept AttributeError:\n\t\t\tif iface.names(required=True, provided=False):\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"does not provide required attribute %s\" % (name,))\n\t\telse:\n\t\t\tmethod = _get_actual_method(method)\n\t\t\t# This is a bit of a hack, but we need to inspect\n\t\t\t# the method to make sure it has the right signature.\n\t\t\t# Unfortunately, inspect.ismethod doesn't work right\n\t\t\t# on bound methods, so we need to check it in a bit\n\t\t\t# more detail.\n\t\t\tif not inspect.isfunction(method) and not inspect.ismethod(method):\n\t\t\t\traise Invalid(\"attribute %s is not a function\" % (name,))\n\t\t\ttry:\n\t\t\t\tiface.getMethod(name)\n\t\t\texcept UnknownMethod:\n\t\t\t\traise", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"Interface %r not provided by %r\"\n\t\t\t\t% (iface, candidate), iface, candidate)\n\tfor name in iface.names():\n\t\t# Get the method from the interface, not the candidate\n\t\timeth = getattr(iface, name)\n\t\t# Get the method from the candidate, not the interface\n\t\tcmeth = getattr(candidate, name, None)\n\t\tif not _verifyMethod(imeth, cmeth, tentative, vtype):\n\t\t\treturn False\n\tfor name in iface.names(True):\n\t\t# Get the attribute from the interface, not the candidate\n\t\tattr = getattr(iface, name)\n\t\t# Get the attribute from the candidate, not the interface\n\t\tcattr = getattr(candidate, name, None)\n\t\tif not _verifyAttribute(attr, cattr, tentative, vtype):\n\t\t\treturn False\n\treturn True\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t# pylint:disable=too-many-branches,too-many-statements\n\t# pylint:disable=too-many-locals,too-many-nested-blocks\n\t# pylint:disable=too-many-return-statements\n\t# pylint:disable=too-many-branches\n\t# pylint:disable=too-many-statements\n\t# pylint:disable=too-many-locals\n\t# pylint:disable=too-many-nested-blocks\n\t# pylint:disable=too-many-branches\n\t# pylint:disable=too-many-return-statements\n\n\t# Check the class hierarchy\n\tfor base in getattr(candidate, '__bases__', ()):\n\t\tif _verify(iface, base, tentative, vtype):\n\t\t\tcontinue\n\t\traise Invalid(\"Candidate %r cannot be used as %r, \"\n\t\t\t\t\t  \"because its base class %r does not\"\n\t\t\t\t\t  \" implement %r\" % (candidate, iface,\n\t\t\t\t\t\t\t\t\t\tbase, iface))\n\n\t# Check the class attributes\n\t", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t# Check the IFace itself\n\t# This is a very quick check, so don't bother to cache the result\n\tif not iface.providedBy(candidate, default=False):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise Invalid(\"Object does not provide %r\" % (iface,),\n\t\t\t\t\t  (None, candidate, iface))\n\n\t# Check the methods\n\tfor name in iface.names():\n\t\tmethod = getattr(iface, name, None)\n\t\tif method is None:\n\t\t\tcontinue\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\"Object does not implement %r\" % (method,),\n\t\t\t\t\t\t  (None, candidate, iface))\n\t\tmethod = getattr(candidate, name)\n\t\tif method is None:\n\t\t\tcontinue\n\t\tifaces = getattr(method, _ifaceAttr, ())\n\t\tif iface not in ifaces:\n\t\t\traise Invalid(\"Method does not provide %r\" % (iface,),\n\t\t\t\t\t\t  (None, candidate, iface))\n\t\tif method is None:\n\t\t\tcontinue\n\t\tif hasattr(method, '", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate)\n\tfor name, method in iface.namesAndDescriptors():\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid('%s does not define %s' % (candidate, name))\n\t\tcandidate_method = getattr(candidate, name)\n\t\tif method.kind is method.METHOD:\n\t\t\tif not callable(candidate_method):\n\t\t\t\traise Invalid('%s.%s is not callable' % (candidate, name))\n\n\t\t\tiface_args = method.getSignatureInfo()[1]\n\t\t\tcandidate_args = getargspec(candidate_method)[0]\n\t\t\tif len(iface_args) != len(candidate_args):\n\t\t\t\traise Invalid('%s.%s has wrong signature' % (candidate, name))\n\t\t\tfor iface_arg, candidate_arg in zip(iface_args, candidate_args):\n\t\t\t\tif iface_arg.name != candidate_arg:\n\t\t\t\t\traise Invalid('%s.%s has wrong signature' % (candidate, name"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface = provideInterface(iface)\n\t\tiface.verifyClass(candidate)\n\tfor name, attr in iface.namesAndDescriptors():\n\t\tif isinstance(attr, (Interface, Attribute)):\n\t\t\tcontinue\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\n\t\t\t\tu\"Missing attribute %r on %r.\" % (name, candidate),\n\t\t\t\tcandidate, name, iface)\n\t\tifattr = getattr(candidate, name)\n\t\tif not callable(ifattr):\n\t\t\traise Invalid(\n\t\t\t\tu\"Attribute %r on %r is not callable.\" % (name, candidate),\n\t\t\t\tcandidate, name, iface)\n\t\tif attr.required is None:\n\t\t\tcontinue\n\t\tif len(attr.required) != len(ifattr.__code__.co_varnames):\n\t\t\traise Invalid(\n\t\t\t\tu\"Incorrect signature for %r on %r.\" % (name, candidate),\n\t\t\t\tcandidate, name, iface)\n\t\tfor index, argname in enumerate(ifattr.__code__.co_varnames):\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Candidate %r does not claim to provide \"\n\t\t\t\t\t\t  \"interface %r\" % (candidate, iface))\n\n\tfor requirement in iface.getAttributeRequirements():\n\t\tif not hasattr(candidate, requirement.name):\n\t\t\traise Invalid(\"Candidate %r does not define \"\n\t\t\t\t\t\t  \"attribute %r\" % (candidate, requirement))\n\n\tfor requirement in iface.getMethodRequirements():\n\t\tif not hasattr(candidate, requirement.name):\n\t\t\traise Invalid(\"Candidate %r does not define \"\n\t\t\t\t\t\t  \"method %r\" % (candidate, requirement))\n\t\tif vtype is not None and not _verify_signature(\n\t\t\t\tcandidate, requirement.name, vtype):\n\t\t\traise Invalid(\"Candidate %r does not define \"\n\t\t\t\t\t\t  \"method %r with a signature that is \"\n\t\t\t\t\t\t  \"compatible with the interface\" % (\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcandidate,", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate)\n\tfor attrname in iface.names():\n\t\tif getattr(iface, attrname, None) is None:\n\t\t\tcontinue\n\t\tattr = getattr(candidate, attrname)\n\t\tif not callable(attr):\n\t\t\traise Invalid('%r does not define %r()' % (candidate, attrname))\n\t\tif vtype is not None:\n\t\t\tif not _is_valid_signature(attr, iface, attrname, vtype):\n\t\t\t\traise Invalid('%r does not define %r() correctly' % (candidate, attrname))\n\tfor method in iface.functions():\n\t\tif method in skip_checks:\n\t\t\tcontinue\n\t\tattr = getattr(candidate, method)\n\t\tif not callable(attr):\n\t\t\traise Invalid('%r does not define %r()' % (candidate, method))\n\t\tif vtype is not None:\n\t\t\tif not _is_valid_signature(attr, iface, method, vtype):\n\t\t\t\traise Invalid('%r does not define %r() correctly' % (candidate, method))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"Candidate does not provide the interface: %r\" % iface)\n\n\t# Check that all required methods are present\n\n\tfor method in iface.names:\n\t\tif method.startswith('_') or method == '__getattr__':\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tgetattr(candidate, method)\n\t\texcept AttributeError:\n\t\t\tif method not in candidate.names():\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"Candidate does not provide method %s: %r\"\n\t\t\t\t\t% (method, candidate))\n\t\t\telse:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"Candidate does not provide method %s: %r\"\n\t\t\t\t\t% (method, candidate))\n\n\t# Check that all required attributes are present\n\n\tfor attr in iface.names:\n\t\tif attr.startswith('_') or attr == '__getattr__':\n\t\t\tcontinue\n\t\ttry:\n\t\t\tgetattr(candidate, attr)\n\t\texcept AttributeError:\n\t\t\tif attr not in candidate.names():\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif vtype is None:\n\t\tvtype = _verify_method\n\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Object does not provide interface\", iface, candidate)\n\n\tfor name, method in iface.namesAndDescriptions():\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError as e:\n\t\t\t# Get the actual name of the method, not the mangled one\n\t\t\tif name.startswith('_'):\n\t\t\t\tname = name[1:]\n\t\t\traise Invalid(\"Method does not exist\", iface, candidate, name, e)\n\t\ttry:\n\t\t\tvtype(name, iface, candidate)\n\t\texcept Invalid as e:\n\t\t\t# Get the actual name of the method, not the mangled one\n\t\t\tif name.startswith('_'):\n\t\t\t\tname = name[1:]\n\t\t\traise Invalid(\"Method is invalid\", iface, candidate, name, e)\n\n\tfor name, attribute in iface.namesAndDescriptions(attributes=True):\n\t\ttry:\n\t\t\tgetattr(candidate, name)\n\t\texcept AttributeError as e:", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"does not provide %s\" % (iface,))\n\n\tfor name in iface.names(provided=True):\n\t\ttry:\n\t\t\tmethod = candidate.__getattr__(name)\n\t\texcept AttributeError:\n\t\t\tif iface.names(required=True, provided=False):\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"does not provide required attribute %s\" % (name,))\n\t\telse:\n\t\t\tmethod = _get_actual_method(method)\n\t\t\t# This is a bit of a hack, but we need to inspect\n\t\t\t# the method to make sure it has the right signature.\n\t\t\t# Unfortunately, inspect.ismethod doesn't work right\n\t\t\t# on bound methods, so we need to check it in a bit\n\t\t\t# more detail.\n\t\t\tif not inspect.isfunction(method) and not inspect.ismethod(method):\n\t\t\t\traise Invalid(\"attribute %s is not a function\" % (name,))\n\t\t\ttry:\n\t\t\t\tiface.getMethod(name)\n\t\t\texcept UnknownMethod:\n\t\t\t\traise", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"Interface %r not provided by %r\"\n\t\t\t\t% (iface, candidate), iface, candidate)\n\tfor name in iface.names():\n\t\t# Get the method from the interface, not the candidate\n\t\timeth = getattr(iface, name)\n\t\t# Get the method from the candidate, not the interface\n\t\tcmeth = getattr(candidate, name, None)\n\t\tif not _verifyMethod(imeth, cmeth, tentative, vtype):\n\t\t\treturn False\n\tfor name in iface.names(True):\n\t\t# Get the attribute from the interface, not the candidate\n\t\tattr = getattr(iface, name)\n\t\t# Get the attribute from the candidate, not the interface\n\t\tcattr = getattr(candidate, name, None)\n\t\tif not _verifyAttribute(attr, cattr, tentative, vtype):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t# pylint:disable=too-many-branches,too-many-statements\n\t# pylint:disable=too-many-locals,too-many-nested-blocks\n\t# pylint:disable=too-many-return-statements\n\t# pylint:disable=too-many-branches\n\t# pylint:disable=too-many-statements\n\t# pylint:disable=too-many-locals\n\t# pylint:disable=too-many-nested-blocks\n\t# pylint:disable=too-many-branches\n\t# pylint:disable=too-many-return-statements\n\n\t# Check the class hierarchy\n\tfor base in getattr(candidate, '__bases__', ()):\n\t\tif _verify(iface, base, tentative, vtype):\n\t\t\tcontinue\n\t\traise Invalid(\"Candidate %r cannot be used as %r, \"\n\t\t\t\t\t  \"because its base class %r does not\"\n\t\t\t\t\t  \" implement %r\" % (candidate, iface,\n\t\t\t\t\t\t\t\t\t\tbase, iface))\n\n\t# Check the class attributes\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t# Check the IFace itself\n\t# This is a very quick check, so don't bother to cache the result\n\tif not iface.providedBy(candidate, default=False):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise Invalid(\"Object does not provide %r\" % (iface,),\n\t\t\t\t\t  (None, candidate, iface))\n\n\t# Check the methods\n\tfor name in iface.names():\n\t\tmethod = getattr(iface, name, None)\n\t\tif method is None:\n\t\t\tcontinue\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\"Object does not implement %r\" % (method,),\n\t\t\t\t\t\t  (None, candidate, iface))\n\t\tmethod = getattr(candidate, name)\n\t\tif method is None:\n\t\t\tcontinue\n\t\tifaces = getattr(method, _ifaceAttr, ())\n\t\tif iface not in ifaces:\n\t\t\traise Invalid(\"Method does not provide %r\" % (iface,),\n\t\t\t\t\t\t  (None, candidate, iface))\n\t\tif method is None:\n\t\t\tcontinue\n\t\tif hasattr(method, '", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate)\n\tfor name, method in iface.namesAndDescriptors():\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid('%s does not define %s' % (candidate, name))\n\t\tcandidate_method = getattr(candidate, name)\n\t\tif method.kind is method.METHOD:\n\t\t\tif not callable(candidate_method):\n\t\t\t\traise Invalid('%s.%s is not callable' % (candidate, name))\n\n\t\t\tiface_args = method.getSignatureInfo()[1]\n\t\t\tcandidate_args = getargspec(candidate_method)[0]\n\t\t\tif len(iface_args) != len(candidate_args):\n\t\t\t\traise Invalid('%s.%s has wrong signature' % (candidate, name))\n\t\t\tfor iface_arg, candidate_arg in zip(iface_args, candidate_args):\n\t\t\t\tif iface_arg.name != candidate_arg:\n\t\t\t\t\traise Invalid('%s.%s has wrong signature' % (candidate, name", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t# FIXME: We should probably check for a few other things here as well\n\t# like that the candidate is not None\n\tif not tentative:\n\t\tif not _checkProvidedBy(candidate, iface):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide the required interface %r\"\n\t\t\t\t% (candidate, iface),\n\t\t\t\tcandidate, iface)\n\n\t# FIXME: We should probably check for a few other things here as well\n\t# like that the candidate is not None\n\tmethods = inspect.getmembers(candidate, inspect.ismethod)\n\tfor name, method in methods.items():\n\t\t# Skip special methods\n\t\tif name.startswith(\"__\") and name.endswith(\"__\"):\n\t\t\tcontinue\n\t\tif not _checkMethod(method, iface):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide the required interface %r\"\n\t\t\t\t% (candidate, iface),\n\t\t\t\tcandidate, iface)\n\n\t# FIXME: We should probably check for a few other things here as well\n\t# like that the candidate is not None\n\tattrs = inspect.getmembers(c", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\t# Check that the candidate claims to provide the interface\n\t\t# using implementedBy()\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Candidate does not provide interface\",\n\t\t\t\t\t\t  candidate, iface)\n\tfor name in dir(iface):\n\t\tif name.startswith('_'):\n\t\t\t# Skip special names\n\t\t\tcontinue\n\t\tiface_member = getattr(iface, name)\n\t\tif not isinstance(iface_member, Interface):\n\t\t\t# Skip attributes\n\t\t\tcontinue\n\t\tcandidate_member = getattr(candidate, name, None)\n\t\tif candidate_member is None:\n\t\t\traise Invalid(\"Candidate lacks required attribute\",\n\t\t\t\t\t\t  candidate, iface)\n\t\tif not isCallable(candidate_member):\n\t\t\traise Invalid(\"Candidate lacks required method\",\n\t\t\t\t\t\t  candidate, iface)\n\t\tif not verifyObject(iface_member, candidate_member, tentative):\n\t\t\traise Invalid(\"Invalid method\", candidate, iface)\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate)\n\tfor name, method in iface.namesAndDescriptions():\n\t\ttry:\n\t\t\tmethod = getattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\traise Invalid(\"missing attribute %s\" % name)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tverifyObjectMethod(iface, name, method)\n\t\t\texcept Invalid as e:\n\t\t\t\traise Invalid(\"invalid method %s: %s\" %\n\t\t\t\t\t\t\t\t\t\t\t(name, e.args[0]))\n\tfor name, attr in iface.namesAndDescriptions(attributes=True):\n\t\ttry:\n\t\t\tattr_value = getattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\traise Invalid(\"missing attribute %s\" % name)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tverifyObjectAttribute(iface, name, attr_value)\n\t\t\texcept Invalid as e:\n\t\t\t\traise Invalid(\"invalid attribute %s: %s\" %\n\t\t\t\t\t\t\t\t\t\t\t(name, e.args[0]))\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\"'%s' does not provide '%s'\" % (candidate, iface))\n\tfor attr, value in iface.namesAndDescriptions():\n\t\tif not hasattr(candidate, attr):\n\t\t\traise Invalid(\"'%s' lacks attribute '%s'\" % (candidate, attr))\n\t\t# XXX: This shouldn't be necessary, but is for backward\n\t\t# compatibility.\n\t\tif not hasattr(getattr(candidate, attr), \"__call__\"):\n\t\t\traise Invalid(\"'%s' has attribute '%s' that is not callable\"\n\t\t\t\t\t\t  % (candidate, attr))\n\t\tsig = iface[attr].__signature__\n\t\tif sig is None:\n\t\t\tcontinue\n\t\t# XXX: This shouldn't be necessary, but is for backward\n\t\t# compatibility.\n\t\tif isinstance(sig, six.string_types):\n\t\t\traise Invalid(\"'%s' has attribute '%s' with illegal signature\"\n\t\t\t\t\t\t  % (candidate, attr))\n\t\tif not sig.bind(candidate).apply_default", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\t# noinspection PyUnresolvedReferences\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Candidate does not provide the interface\", candidate, iface)\n\tfor name in iface.names():\n\t\tiface_method = iface[name]\n\t\tcandidate_method = getattr(candidate, name, None)\n\t\tif candidate_method is None:\n\t\t\traise Invalid(\"Candidate does not define %r\" % (name,), candidate, iface)\n\t\tif not iface_method.implementedBy(candidate_method):\n\t\t\traise Invalid(\"Candidate defines %r with the wrong signature\"\n\t\t\t\t\t\t  % (name,), candidate, iface)\n\tfor name in iface.names():\n\t\tattr = getattr(iface, name, None)\n\t\tif attr is not None:\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\traise Invalid(\"Candidate does not define %r\" % (name,), candidate, iface)\n\t\t\tif attr is not getattr(candidate, name):\n\t\t\t\traise Invalid(\"Candidate defines %r with the", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\t_verifyProvides(candidate, iface)\n\tfor method in iface.names():\n\t\tif method == '__of__':\n\t\t\tcontinue\n\t\ttry:\n\t\t\t_verifyMethod(candidate, iface, method)\n\t\texcept TypeError as e:\n\t\t\traise Invalid(candidate, iface, method, e)\n\tfor attr in iface.names(all=True, public=False, predicate=isAttribute):\n\t\tif not hasattr(candidate, attr):\n\t\t\traise Invalid(candidate, iface, attr,\n\t\t\t\t\t\t  AttributeError('no such attribute %r' % (attr,)))\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate, _check_bases=False)\n\tfor name in iface.names():\n\t\tif name in candidate.__dict__:\n\t\t\ttry:\n\t\t\t\tmethod = candidate.__dict__[name]\n\t\t\texcept TypeError:\n\t\t\t\t# if __dict__[name] raises a TypeError, it's\n\t\t\t\t# probably a method descriptor, so we can\n\t\t\t\t# safely assume it's a method.\n\t\t\t\tmethod = candidate.__dict__[name].__func__\n\t\t\tif not isMethod(method):\n\t\t\t\traise Invalid(\"Object %s does not define a method %s\"\n\t\t\t\t\t% (candidate, name))\n\t\t\telif not verifyMethod(method, iface[name], tentative=tentative):\n\t\t\t\treturn False\n\t\telif name in iface.names(True):\n\t\t\traise Invalid(\"Object %s does not define an attribute %s\"\n\t\t\t\t% (candidate, name))\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tcheckImplements(iface, candidate)\n\n\tfor method in iface.names():\n\t\tcheckMethod(iface, iface[method], candidate)\n\n\tfor attr in iface.names(iface.ATTRIBUTE):\n\t\tcheckAttribute(iface, iface[attr], candidate)\n\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Does not provide interface\", iface=iface, candidate=candidate)\n\ttry:\n\t\tverifyProvides(iface, candidate)\n\texcept Invalid:\n\t\tif not tentative:\n\t\t\traise\n\tfor attrname in iface.names():\n\t\ttry:\n\t\t\tif attrname in candidate.__dict__:\n\t\t\t\tcontinue\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tgetattr(candidate, attrname)\n\t\texcept AttributeError:\n\t\t\tif not tentative:\n\t\t\t\traise Invalid(\"Doesn't define attribute %s\" % attrname, iface=iface, candidate=candidate)\n\tfor methodname in iface.names(interfaces.IAttribute):\n\t\ttry:\n\t\t\tif methodname in candidate.__dict__:\n\t\t\t\tcontinue\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tgetattr(candidate, methodname)\n\t\texcept AttributeError:\n\t\t\tif not tentative:\n\t\t\t\traise Invalid(\"Doesn't define attribute %s\"", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.verifyObject(candidate)\n\tfor name, method in iface.namesAndDescriptions():\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\n\t\t\t\t\"The object does not have the attribute %r\" % (name,)\n\t\t\t)\n\t\telse:\n\t\t\tifaceMethod = getattr(iface, name)\n\t\t\tcandidateMethod = getattr(candidate, name)\n\t\t\tif ifaceMethod.signature != candidateMethod.signature:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The attribute %r has a different signature than in the interface\" % (\n\t\t\t\t\t\tname,\n\t\t\t\t\t)\n\t\t\t\t)\n\n\tfor name, attr in iface.namesAndDescriptions(attributes=True):\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\n\t\t\t\t\"The object does not have the attribute %r\" % (name,)\n\t\t\t)\n\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t# FIXME: We should probably check for a few other things here as well\n\t# like that the candidate is not None\n\tif not tentative:\n\t\tif not _checkProvidedBy(candidate, iface):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide the required interface %r\"\n\t\t\t\t% (candidate, iface),\n\t\t\t\tcandidate, iface)\n\n\t# FIXME: We should probably check for a few other things here as well\n\t# like that the candidate is not None\n\tmethods = inspect.getmembers(candidate, inspect.ismethod)\n\tfor name, method in methods.items():\n\t\t# Skip special methods\n\t\tif name.startswith(\"__\") and name.endswith(\"__\"):\n\t\t\tcontinue\n\t\tif not _checkMethod(method, iface):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide the required interface %r\"\n\t\t\t\t% (candidate, iface),\n\t\t\t\tcandidate, iface)\n\n\t# FIXME: We should probably check for a few other things here as well\n\t# like that the candidate is not None\n\tattrs = inspect.getmembers(c", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\t# Check that the candidate claims to provide the interface\n\t\t# using implementedBy()\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Candidate does not provide interface\",\n\t\t\t\t\t\t  candidate, iface)\n\tfor name in dir(iface):\n\t\tif name.startswith('_'):\n\t\t\t# Skip special names\n\t\t\tcontinue\n\t\tiface_member = getattr(iface, name)\n\t\tif not isinstance(iface_member, Interface):\n\t\t\t# Skip attributes\n\t\t\tcontinue\n\t\tcandidate_member = getattr(candidate, name, None)\n\t\tif candidate_member is None:\n\t\t\traise Invalid(\"Candidate lacks required attribute\",\n\t\t\t\t\t\t  candidate, iface)\n\t\tif not isCallable(candidate_member):\n\t\t\traise Invalid(\"Candidate lacks required method\",\n\t\t\t\t\t\t  candidate, iface)\n\t\tif not verifyObject(iface_member, candidate_member, tentative):\n\t\t\traise Invalid(\"Invalid method\", candidate, iface)\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate)\n\tfor name, method in iface.namesAndDescriptions():\n\t\ttry:\n\t\t\tmethod = getattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\traise Invalid(\"missing attribute %s\" % name)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tverifyObjectMethod(iface, name, method)\n\t\t\texcept Invalid as e:\n\t\t\t\traise Invalid(\"invalid method %s: %s\" %\n\t\t\t\t\t\t\t\t\t\t\t(name, e.args[0]))\n\tfor name, attr in iface.namesAndDescriptions(attributes=True):\n\t\ttry:\n\t\t\tattr_value = getattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\traise Invalid(\"missing attribute %s\" % name)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tverifyObjectAttribute(iface, name, attr_value)\n\t\t\texcept Invalid as e:\n\t\t\t\traise Invalid(\"invalid attribute %s: %s\" %\n\t\t\t\t\t\t\t\t\t\t\t(name, e.args[0]))\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\"'%s' does not provide '%s'\" % (candidate, iface))\n\tfor attr, value in iface.namesAndDescriptions():\n\t\tif not hasattr(candidate, attr):\n\t\t\traise Invalid(\"'%s' lacks attribute '%s'\" % (candidate, attr))\n\t\t# XXX: This shouldn't be necessary, but is for backward\n\t\t# compatibility.\n\t\tif not hasattr(getattr(candidate, attr), \"__call__\"):\n\t\t\traise Invalid(\"'%s' has attribute '%s' that is not callable\"\n\t\t\t\t\t\t  % (candidate, attr))\n\t\tsig = iface[attr].__signature__\n\t\tif sig is None:\n\t\t\tcontinue\n\t\t# XXX: This shouldn't be necessary, but is for backward\n\t\t# compatibility.\n\t\tif isinstance(sig, six.string_types):\n\t\t\traise Invalid(\"'%s' has attribute '%s' with illegal signature\"\n\t\t\t\t\t\t  % (candidate, attr))\n\t\tif not sig.bind(candidate).apply_default", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\t# noinspection PyUnresolvedReferences\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Candidate does not provide the interface\", candidate, iface)\n\tfor name in iface.names():\n\t\tiface_method = iface[name]\n\t\tcandidate_method = getattr(candidate, name, None)\n\t\tif candidate_method is None:\n\t\t\traise Invalid(\"Candidate does not define %r\" % (name,), candidate, iface)\n\t\tif not iface_method.implementedBy(candidate_method):\n\t\t\traise Invalid(\"Candidate defines %r with the wrong signature\"\n\t\t\t\t\t\t  % (name,), candidate, iface)\n\tfor name in iface.names():\n\t\tattr = getattr(iface, name, None)\n\t\tif attr is not None:\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\traise Invalid(\"Candidate does not define %r\" % (name,), candidate, iface)\n\t\t\tif attr is not getattr(candidate, name):\n\t\t\t\traise Invalid(\"Candidate defines %r with the", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\t_verifyProvides(candidate, iface)\n\tfor method in iface.names():\n\t\tif method == '__of__':\n\t\t\tcontinue\n\t\ttry:\n\t\t\t_verifyMethod(candidate, iface, method)\n\t\texcept TypeError as e:\n\t\t\traise Invalid(candidate, iface, method, e)\n\tfor attr in iface.names(all=True, public=False, predicate=isAttribute):\n\t\tif not hasattr(candidate, attr):\n\t\t\traise Invalid(candidate, iface, attr,\n\t\t\t\t\t\t  AttributeError('no such attribute %r' % (attr,)))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.providedBy(candidate, _check_bases=False)\n\tfor name in iface.names():\n\t\tif name in candidate.__dict__:\n\t\t\ttry:\n\t\t\t\tmethod = candidate.__dict__[name]\n\t\t\texcept TypeError:\n\t\t\t\t# if __dict__[name] raises a TypeError, it's\n\t\t\t\t# probably a method descriptor, so we can\n\t\t\t\t# safely assume it's a method.\n\t\t\t\tmethod = candidate.__dict__[name].__func__\n\t\t\tif not isMethod(method):\n\t\t\t\traise Invalid(\"Object %s does not define a method %s\"\n\t\t\t\t\t% (candidate, name))\n\t\t\telif not verifyMethod(method, iface[name], tentative=tentative):\n\t\t\t\treturn False\n\t\telif name in iface.names(True):\n\t\t\traise Invalid(\"Object %s does not define an attribute %s\"\n\t\t\t\t% (candidate, name))\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tcheckImplements(iface, candidate)\n\n\tfor method in iface.names():\n\t\tcheckMethod(iface, iface[method], candidate)\n\n\tfor attr in iface.names(iface.ATTRIBUTE):\n\t\tcheckAttribute(iface, iface[attr], candidate)\n\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"Does not provide interface\", iface=iface, candidate=candidate)\n\ttry:\n\t\tverifyProvides(iface, candidate)\n\texcept Invalid:\n\t\tif not tentative:\n\t\t\traise\n\tfor attrname in iface.names():\n\t\ttry:\n\t\t\tif attrname in candidate.__dict__:\n\t\t\t\tcontinue\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tgetattr(candidate, attrname)\n\t\texcept AttributeError:\n\t\t\tif not tentative:\n\t\t\t\traise Invalid(\"Doesn't define attribute %s\" % attrname, iface=iface, candidate=candidate)\n\tfor methodname in iface.names(interfaces.IAttribute):\n\t\ttry:\n\t\t\tif methodname in candidate.__dict__:\n\t\t\t\tcontinue\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tgetattr(candidate, methodname)\n\t\texcept AttributeError:\n\t\t\tif not tentative:\n\t\t\t\traise Invalid(\"Doesn't define attribute %s\"", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\tiface.verifyObject(candidate)\n\tfor name, method in iface.namesAndDescriptions():\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\n\t\t\t\t\"The object does not have the attribute %r\" % (name,)\n\t\t\t)\n\t\telse:\n\t\t\tifaceMethod = getattr(iface, name)\n\t\t\tcandidateMethod = getattr(candidate, name)\n\t\t\tif ifaceMethod.signature != candidateMethod.signature:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The attribute %r has a different signature than in the interface\" % (\n\t\t\t\t\t\tname,\n\t\t\t\t\t)\n\t\t\t\t)\n\n\tfor name, attr in iface.namesAndDescriptions(attributes=True):\n\t\tif not hasattr(candidate, name):\n\t\t\traise Invalid(\n\t\t\t\t\"The object does not have the attribute %r\" % (name,)\n\t\t\t)\n\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif candidate is None:\n\t\treturn False\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"candidate must be a type\")\n\tif not issubclass(candidate, iface):\n\t\treturn False\n\tif tentative:\n\t\treturn True\n\tif iface == object:\n\t\treturn True\n\tfor base in iface.__bases__:\n\t\tif not verifyClass(base, candidate, tentative):\n\t\t\treturn False\n\treturn True\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif isinstance(iface, (type, types.ClassType)):\n\t\tiface = getattr(iface,'__name__', iface)\n\tif isinstance(candidate, (type, types.ClassType)):\n\t\tcandidate = getattr(candidate,'__name__', candidate)\n\treturn '__provides__' in candidate.__dict__ and \\\n\t\tiface in candidate.__provides__ and \\\n\t\t(not tentative or candidate.__provides__[iface] is True)\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tif tentative:\n\t\treturn True\n\t\n\tif isinstance(iface, (list, tuple)):\n\t\treturn all(verifyClass(i, c) for i in iface for c in candidate)\n\t\n\tif interface.isinterface(iface):\n\t\treturn verifyInterface(iface, candidate)\n\t\n\tif isinstance(iface, (type, types.ClassType)):\n\t\treturn verifyClass(iface, candidate, tentative=True)\n\t\n\tif iface is object:\n\t\treturn True\n\t\n\tif issubclass(iface, candidate):\n\t\treturn True\n\t\n\tif candidate is object:\n\t\treturn True\n\t\n\treturn False\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif iface is None:\n\t\traise TypeError(\"iface must be a class\")\n\tif iface == object:\n\t\treturn True\n\tif not isinstance(candidate, type) and not isinstance(candidate, type(None)):\n\t\traise TypeError(\"candidate must be a class\")\n\tif candidate is None or iface is candidate:\n\t\treturn True\n\tfor base in candidate.__mro__:\n\t\tif iface is base:\n\t\t\treturn True\n\tif tentative:\n\t\treturn False\n\traise IncompatibleClass(candidate, iface)\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not tentative:\n\t\t# Get the class hierarchy for the candidate.\n\t\t# (1) If the candidate does not define a metaclass, it is not a class, and\n\t\t#     therefore cannot be a provider.\n\t\t# (2) If the candidate defines a metaclass, it is a class.\n\t\t# (3) If the candidate is a class, it might provide the interface.\n\t\tif not hasattr(candidate, \"__bases__\"):\n\t\t\treturn False\n\t\tbases = candidate.__bases__\n\t\t# (4) If the metaclass does not define an interfacemeta attribute, it is\n\t\t#     an ordinary class.\n\t\t# (5) If the metaclass defines an interfacemeta attribute, it is an\n\t\t#     interface metaclass.\n\t\t# (6) If the metaclass is an interface metaclass, it might provide the\n\t\t#     interface.\n\t\t# (7) If the metaclass is an ordinary class, it is not a provider.\n\t\tif not hasattr(candidate.__class__, \"interfacemeta\"):\n\t\t\treturn False\n\t\tmetaclass = candidate.__class__\n\t\tif hasattr(metaclass, \"", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif isinstance(iface, str):\n\t\tiface = getClass(iface)\n\n\tif not isinstance(iface, type):\n\t\traise TypeError(\"iface must be a class or a string referencing a class\")\n\n\tif isinstance(candidate, str):\n\t\tcandidate = getClass(candidate)\n\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"candidate must be a class or a string referencing a class\")\n\n\tif tentative:\n\t\treturn issubclass(candidate, iface)\n\telse:\n\t\treturn isinstance(candidate, iface)\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\treturn True\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not tentative:\n\t\tiface = iface.interfaces()[0]\n\t\tif not iface.isSubtypeOf(candidate.interfaces()[0]):\n\t\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\t\tif iface.implements() and iface.implements() != candidate.implements():\n\t\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\treturn candidate\n\n\nclass _EmptyClass(object):\n\t\"\"\"\n\tA class that implements nothing.\n\t\"\"\"\n\t__implements__ = ()\n\t__provides__ = ()\n\t\n\tdef __new__(cls, *args, **kwds):\n\t\traise NotImplementedError\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# check if we already have a class defined\n\tif iface in _classes:\n\t\t# check if it is a tentative implementation\n\t\tif tentative:\n\t\t\t# check if it is a tentative implementation of the same class\n\t\t\tif _classes[iface] is candidate:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\t# candidate is a different implementation of the same class\n\t\t\t\traise TypeError(\"%s is a different implementation of %s\" % (candidate, iface))\n\t\telse:\n\t\t\t# check if it is an implementation of the same class\n\t\t\tif _classes[iface] is candidate:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\t# candidate is a different implementation of the same class\n\t\t\t\traise TypeError(\"%s is a different implementation of %s\" % (candidate, iface))\n\telse:\n\t\t# check if it is an implementation of the same class\n\t\tif _classes[iface] is candidate:\n\t\t\treturn True\n\t\telse:\n\t\t\t# candidate is a different implementation of the same class\n\t\t\traise TypeError(\"%s is a different implementation of %s\" % (candidate, iface))\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif iface is None and candidate is None:\n\t\treturn True\n\tif iface is None:\n\t\treturn False\n\tif candidate is None:\n\t\treturn False\n\tif isinstance(candidate, iface):\n\t\treturn True\n\tif issubclass(candidate, iface):\n\t\treturn True\n\tif not tentative:\n\t\tcandidate = type(candidate)\n\t\tif candidate in iface.__mro__:\n\t\t\treturn True\n\t\tif iface in candidate.__mro__:\n\t\t\treturn True\n\treturn False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif candidate is None:\n\t\treturn False\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"candidate must be a type\")\n\tif not issubclass(candidate, iface):\n\t\treturn False\n\tif tentative:\n\t\treturn True\n\tif iface == object:\n\t\treturn True\n\tfor base in iface.__bases__:\n\t\tif not verifyClass(base, candidate, tentative):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif isinstance(iface, (type, types.ClassType)):\n\t\tiface = getattr(iface,'__name__', iface)\n\tif isinstance(candidate, (type, types.ClassType)):\n\t\tcandidate = getattr(candidate,'__name__', candidate)\n\treturn '__provides__' in candidate.__dict__ and \\\n\t\tiface in candidate.__provides__ and \\\n\t\t(not tentative or candidate.__provides__[iface] is True)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tif tentative:\n\t\treturn True\n\t\n\tif isinstance(iface, (list, tuple)):\n\t\treturn all(verifyClass(i, c) for i in iface for c in candidate)\n\t\n\tif interface.isinterface(iface):\n\t\treturn verifyInterface(iface, candidate)\n\t\n\tif isinstance(iface, (type, types.ClassType)):\n\t\treturn verifyClass(iface, candidate, tentative=True)\n\t\n\tif iface is object:\n\t\treturn True\n\t\n\tif issubclass(iface, candidate):\n\t\treturn True\n\t\n\tif candidate is object:\n\t\treturn True\n\t\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif iface is None:\n\t\traise TypeError(\"iface must be a class\")\n\tif iface == object:\n\t\treturn True\n\tif not isinstance(candidate, type) and not isinstance(candidate, type(None)):\n\t\traise TypeError(\"candidate must be a class\")\n\tif candidate is None or iface is candidate:\n\t\treturn True\n\tfor base in candidate.__mro__:\n\t\tif iface is base:\n\t\t\treturn True\n\tif tentative:\n\t\treturn False\n\traise IncompatibleClass(candidate, iface)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not tentative:\n\t\t# Get the class hierarchy for the candidate.\n\t\t# (1) If the candidate does not define a metaclass, it is not a class, and\n\t\t#     therefore cannot be a provider.\n\t\t# (2) If the candidate defines a metaclass, it is a class.\n\t\t# (3) If the candidate is a class, it might provide the interface.\n\t\tif not hasattr(candidate, \"__bases__\"):\n\t\t\treturn False\n\t\tbases = candidate.__bases__\n\t\t# (4) If the metaclass does not define an interfacemeta attribute, it is\n\t\t#     an ordinary class.\n\t\t# (5) If the metaclass defines an interfacemeta attribute, it is an\n\t\t#     interface metaclass.\n\t\t# (6) If the metaclass is an interface metaclass, it might provide the\n\t\t#     interface.\n\t\t# (7) If the metaclass is an ordinary class, it is not a provider.\n\t\tif not hasattr(candidate.__class__, \"interfacemeta\"):\n\t\t\treturn False\n\t\tmetaclass = candidate.__class__\n\t\tif hasattr(metaclass, \"", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif isinstance(iface, str):\n\t\tiface = getClass(iface)\n\n\tif not isinstance(iface, type):\n\t\traise TypeError(\"iface must be a class or a string referencing a class\")\n\n\tif isinstance(candidate, str):\n\t\tcandidate = getClass(candidate)\n\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"candidate must be a class or a string referencing a class\")\n\n\tif tentative:\n\t\treturn issubclass(candidate, iface)\n\telse:\n\t\treturn isinstance(candidate, iface)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not tentative:\n\t\tiface = iface.interfaces()[0]\n\t\tif not iface.isSubtypeOf(candidate.interfaces()[0]):\n\t\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\t\tif iface.implements() and iface.implements() != candidate.implements():\n\t\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\treturn candidate\n\n\nclass _EmptyClass(object):\n\t\"\"\"\n\tA class that implements nothing.\n\t\"\"\"\n\t__implements__ = ()\n\t__provides__ = ()\n\t\n\tdef __new__(cls, *args, **kwds):\n\t\traise NotImplementedError\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# check if we already have a class defined\n\tif iface in _classes:\n\t\t# check if it is a tentative implementation\n\t\tif tentative:\n\t\t\t# check if it is a tentative implementation of the same class\n\t\t\tif _classes[iface] is candidate:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\t# candidate is a different implementation of the same class\n\t\t\t\traise TypeError(\"%s is a different implementation of %s\" % (candidate, iface))\n\t\telse:\n\t\t\t# check if it is an implementation of the same class\n\t\t\tif _classes[iface] is candidate:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\t# candidate is a different implementation of the same class\n\t\t\t\traise TypeError(\"%s is a different implementation of %s\" % (candidate, iface))\n\telse:\n\t\t# check if it is an implementation of the same class\n\t\tif _classes[iface] is candidate:\n\t\t\treturn True\n\t\telse:\n\t\t\t# candidate is a different implementation of the same class\n\t\t\traise TypeError(\"%s is a different implementation of %s\" % (candidate, iface))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif iface is None and candidate is None:\n\t\treturn True\n\tif iface is None:\n\t\treturn False\n\tif candidate is None:\n\t\treturn False\n\tif isinstance(candidate, iface):\n\t\treturn True\n\tif issubclass(candidate, iface):\n\t\treturn True\n\tif not tentative:\n\t\tcandidate = type(candidate)\n\t\tif candidate in iface.__mro__:\n\t\t\treturn True\n\t\tif iface in candidate.__mro__:\n\t\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\telse:\n\t\treturn type(bases[0])\n\nclass DynClass(type):\n\t\"\"\"\n\tDynamic subclassing metaclass.\n\n\tThis metaclass provides the following features:\n\n\t\t- Dynamic subclassing\n\n\t\t- Dynamic method override\n\n\t\t- Dynamically add new methods\n\n\t\t- Dynamically add new attributes\n\n\t\t- Dynamically add new class attributes\n\n\t\t- Dynamically add new class methods\n\n\t\t- Dynamically add new class attributes\n\n\t\t- Dynamically add new class methods\n\n\t\t- Dynamically add/remove/replace new methods\n\n\t\t- Dynamically add/remove/replace new attributes\n\n\t\t- Dynamically add/remove/replace new class attributes\n\n\t\t- Dynamically add/remove/replace new class methods\n\n\t\t- Dynamically add/remove/replace new class attributes\n\n\t\t- Dynamically add/remove/replace new class methods\n\n\t\t- Dynamically add/remove/replace new methods\n\n\t\t- Dynamically add/remove/replace new attributes\n\n\t\t- Dynamically add/remove/replace new class attributes\n\n\t\t- Dynamically add/remove/replace new class methods\n\n\t\t- Dynamically add/remove", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tassert bases\n\tmetaclass = explicit_mc\n\tfor base in bases:\n\t\tif isinstance(base, type):\n\t\t\tif metaclass is None:\n\t\t\t\tmetaclass = base\n\t\t\telif not issubclass(base, metaclass):\n\t\t\t\traise TypeError(\"metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\")\n\tif metaclass is None:\n\t\tmetaclass = type\n\treturn metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\telse:\n\t\treturn determineMetaclass(bases[0].__bases__)\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc:\n\t\treturn explicit_mc\n\tfor base in bases:\n\t\tif type(base) is type:\n\t\t\treturn base\n\treturn type\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc:\n\t\treturn explicit_mc\n\n\tif bases:\n\t\treturn bases[0].__class__\n\telse:\n\t\treturn type\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# if explicit_mc is not None:\n\t#     return explicit_mc\n\t# if len(bases) == 0:\n\t#     return type\n\t# if len(bases) == 1:\n\t#     return bases[0]\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t# all b", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# Find metaclass.\n\tfor base in bases:\n\t\tif hasattr(base, \"__metaclass__\"):\n\t\t\tmc = base.__metaclass__\n\t\t\tbreak\n\telse:\n\t\tmc = type\n\n\t# Set explicit metaclass if given.\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\n\treturn mc\n\n\nclass Meta(type):\n\t\"\"\"\n\tMetaclass for all classes in this module.\n\t\"\"\"\n\n\tdef __new__(cls, clsname, bases, dct):\n\t\t\"\"\"\n\t\tCreate class object.\n\t\t\"\"\"\n\t\tmc = determineMetaclass(bases, dct.get(\"__metaclass__\"))\n\n\t\tif mc is not Meta:\n\t\t\t# Use normal metaclass processing.\n\t\t\treturn super(Meta, cls).__new__(cls, clsname, bases, dct)\n\t\telse:\n\t\t\t# Use special metaclass processing.\n\t\t\t# This is a bit hacky.\n\t\t\t# We need to make sure the __init__ is set to None\n\t\t\t# so that the superclass's __init__ is called.\n\t\t\t# Otherwise", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is None:\n\t\texplicit_mc = []\n\tmc = None\n\tfor base in bases:\n\t\tif hasattr(base, '__metaclass__'):\n\t\t\tif mc is not None:\n\t\t\t\traise TypeError('%s has multiple __metaclass__' % (base,))\n\t\t\tmc = base.__metaclass__\n\t\tif hasattr(base, '__bases__'):\n\t\t\texplicit_mc.extend(base.__bases__)\n\tif mc is None:\n\t\tif explicit_mc:\n\t\t\tmc = determineMetaclass(explicit_mc)\n\t\telse:\n\t\t\tmc = type\n\treturn mc\n\n\n#\n#   class metaclass(type):\n#\nclass metaclass(type):\n\t\"\"\"\n\tMetaclass that supports multiple inheritance and a __bases__\n\tattribute.\n\n\tThis metaclass is based on the implementation from\n\thttp://www.python.org/download/releases/2.2.3/descrintro/#metaclasses\n\t\"\"\"\n\n\tdef __init__(cls, name, bases, dict):\n\t\tif not hasattr(cls, '__bases__'):\n\t\t\tcls", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif len(bases) == 1:\n\t\tbase = bases[0]\n\n\t\tif explicit_mc:\n\t\t\treturn explicit_mc\n\t\telse:\n\t\t\treturn getattr(base, '__metaclass__', type)\n\telse:\n\t\treturn type\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tif not bases:\n\t\treturn type\n\t# if len(bases) == 1:\n\t# \treturn type(bases[0])\n\t# elif len(bases) == 2:\n\t# \tif issubclass(bases[0], bases[1]):\n\t# \t\treturn bases[0]\n\t# \telse:\n\t# \t\treturn bases[1]\n\treturn bases[0]\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\telse:\n\t\treturn type(bases[0])\n\nclass DynClass(type):\n\t\"\"\"\n\tDynamic subclassing metaclass.\n\n\tThis metaclass provides the following features:\n\n\t\t- Dynamic subclassing\n\n\t\t- Dynamic method override\n\n\t\t- Dynamically add new methods\n\n\t\t- Dynamically add new attributes\n\n\t\t- Dynamically add new class attributes\n\n\t\t- Dynamically add new class methods\n\n\t\t- Dynamically add new class attributes\n\n\t\t- Dynamically add new class methods\n\n\t\t- Dynamically add/remove/replace new methods\n\n\t\t- Dynamically add/remove/replace new attributes\n\n\t\t- Dynamically add/remove/replace new class attributes\n\n\t\t- Dynamically add/remove/replace new class methods\n\n\t\t- Dynamically add/remove/replace new class attributes\n\n\t\t- Dynamically add/remove/replace new class methods\n\n\t\t- Dynamically add/remove/replace new methods\n\n\t\t- Dynamically add/remove/replace new attributes\n\n\t\t- Dynamically add/remove/replace new class attributes\n\n\t\t- Dynamically add/remove/replace new class methods\n\n\t\t- Dynamically add/remove", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tassert bases\n\tmetaclass = explicit_mc\n\tfor base in bases:\n\t\tif isinstance(base, type):\n\t\t\tif metaclass is None:\n\t\t\t\tmetaclass = base\n\t\t\telif not issubclass(base, metaclass):\n\t\t\t\traise TypeError(\"metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\")\n\tif metaclass is None:\n\t\tmetaclass = type\n\treturn metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\telse:\n\t\treturn determineMetaclass(bases[0].__bases__)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc:\n\t\treturn explicit_mc\n\tfor base in bases:\n\t\tif type(base) is type:\n\t\t\treturn base\n\treturn type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc:\n\t\treturn explicit_mc\n\n\tif bases:\n\t\treturn bases[0].__class__\n\telse:\n\t\treturn type\n", "is_pass": true, "return_code": 0}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# if explicit_mc is not None:\n\t#     return explicit_mc\n\t# if len(bases) == 0:\n\t#     return type\n\t# if len(bases) == 1:\n\t#     return bases[0]\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t# all b", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# Find metaclass.\n\tfor base in bases:\n\t\tif hasattr(base, \"__metaclass__\"):\n\t\t\tmc = base.__metaclass__\n\t\t\tbreak\n\telse:\n\t\tmc = type\n\n\t# Set explicit metaclass if given.\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\n\treturn mc\n\n\nclass Meta(type):\n\t\"\"\"\n\tMetaclass for all classes in this module.\n\t\"\"\"\n\n\tdef __new__(cls, clsname, bases, dct):\n\t\t\"\"\"\n\t\tCreate class object.\n\t\t\"\"\"\n\t\tmc = determineMetaclass(bases, dct.get(\"__metaclass__\"))\n\n\t\tif mc is not Meta:\n\t\t\t# Use normal metaclass processing.\n\t\t\treturn super(Meta, cls).__new__(cls, clsname, bases, dct)\n\t\telse:\n\t\t\t# Use special metaclass processing.\n\t\t\t# This is a bit hacky.\n\t\t\t# We need to make sure the __init__ is set to None\n\t\t\t# so that the superclass's __init__ is called.\n\t\t\t# Otherwise", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is None:\n\t\texplicit_mc = []\n\tmc = None\n\tfor base in bases:\n\t\tif hasattr(base, '__metaclass__'):\n\t\t\tif mc is not None:\n\t\t\t\traise TypeError('%s has multiple __metaclass__' % (base,))\n\t\t\tmc = base.__metaclass__\n\t\tif hasattr(base, '__bases__'):\n\t\t\texplicit_mc.extend(base.__bases__)\n\tif mc is None:\n\t\tif explicit_mc:\n\t\t\tmc = determineMetaclass(explicit_mc)\n\t\telse:\n\t\t\tmc = type\n\treturn mc\n\n\n#\n#   class metaclass(type):\n#\nclass metaclass(type):\n\t\"\"\"\n\tMetaclass that supports multiple inheritance and a __bases__\n\tattribute.\n\n\tThis metaclass is based on the implementation from\n\thttp://www.python.org/download/releases/2.2.3/descrintro/#metaclasses\n\t\"\"\"\n\n\tdef __init__(cls, name, bases, dict):\n\t\tif not hasattr(cls, '__bases__'):\n\t\t\tcls", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif len(bases) == 1:\n\t\tbase = bases[0]\n\n\t\tif explicit_mc:\n\t\t\treturn explicit_mc\n\t\telse:\n\t\t\treturn getattr(base, '__metaclass__', type)\n\telse:\n\t\treturn type\n", "is_pass": true, "return_code": 0}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tif not bases:\n\t\treturn type\n\t# if len(bases) == 1:\n\t# \treturn type(bases[0])\n\t# elif len(bases) == 2:\n\t# \tif issubclass(bases[0], bases[1]):\n\t# \t\treturn bases[0]\n\t# \telse:\n\t# \t\treturn bases[1]\n\treturn bases[0]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self.data:\n\t\tdata = self.data[key]\n\t\tdel self.data[key]\n\t\treturn data\n\telif default is not __marker:\n\t\treturn default\n\telse:\n\t\traise KeyError(\"Key not found in Dictionary\")\n\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\twith self.lock:\n\t\ttry:\n\t\t\tv = self.data[key]\n\t\texcept KeyError:\n\t\t\tif default is not __marker:\n\t\t\t\treturn default\n\t\t\traise\n\t\tdel self.data[key]\n\t\tif self._expire and key in self._expire:\n\t\t\tdel self._expire[key]\n\t\treturn v\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tv = self.get(key, default)\n\ttry:\n\t\tdel self[key]\n\texcept KeyError:\n\t\tpass\n\treturn v\n\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\n# \tdef setdefault(self, key, default=None):\n# \t\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\"\"\"\n# \t\ttry:\n# \t\t\treturn self[key]\n# \t\texcept KeyError:\n# \t\t\tself[key] = default\n# \t\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tif default is __marker:\n\t\t\tdefault = None\n\t\treturn self[key], self.__delitem(key)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key not in self:\n\t\tif default is self.__marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\ttmp = self[key]\n\tdel self[key]\n\treturn tmp\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self.data:\n\t\tdata = self.data[key]\n\t\tdel self.data[key]\n\t\treturn data\n\telif default is not __marker:\n\t\treturn default\n\telse:\n\t\traise KeyError(\"Key not found in Dictionary\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\twith self.lock:\n\t\ttry:\n\t\t\tv = self.data[key]\n\t\texcept KeyError:\n\t\t\tif default is not __marker:\n\t\t\t\treturn default\n\t\t\traise\n\t\tdel self.data[key]\n\t\tif self._expire and key in self._expire:\n\t\t\tdel self._expire[key]\n\t\treturn v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tv = self.get(key, default)\n\ttry:\n\t\tdel self[key]\n\texcept KeyError:\n\t\tpass\n\treturn v\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\n# \tdef setdefault(self, key, default=None):\n# \t\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\"\"\"\n# \t\ttry:\n# \t\t\treturn self[key]\n# \t\texcept KeyError:\n# \t\t\tself[key] = default\n# \t\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tif default is __marker:\n\t\t\tdefault = None\n\t\treturn self[key], self.__delitem(key)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key not in self:\n\t\tif default is self.__marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\ttmp = self[key]\n\tdel self[key]\n\treturn tmp\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not len(self):\n\t\traise KeyError('dictionary is empty')\n\td = self.data\n\tself._check_size()\n\tpq = d.queue\n\tqsize = len(pq)\n\tif qsize == 1:\n\t\t# Handle edge case of one item in the queue.\n\t\treturn pq.popleft()\n\t# Remove and return the item with minimum frequency.\n\t# Use linear search to find the minimum frequency.\n\tminfreq = pq[0][2]\n\tminidx = 0\n\tfor i in range(1, qsize):\n\t\tif pq[i][2] < minfreq:\n\t\t\tminfreq, minidx = pq[i][2], i\n\titem = pq.popleft()\n\titem[2] = minfreq + 1\n\tpq.appendleft(item)\n\t# Remove the corresponding link from the doubly linked list.\n\tif minidx:\n\t\tprev = pq[minidx - 1]\n\t\tnext = pq[minidx + 1]\n\t\tprev[3] = next\n\t\tnext[2] = prev\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\twhile self.by_key[self.least_used_key].next is not None:\n\t\tself.least_used_key = self.by_key[self.least_used_key].next\n\tkey = self.least_used_key\n\tvalue = self.by_key[key].value\n\tdel self.by_key[key]\n\tdel self.by_value[value]\n\tself.size -= 1\n\tself.least_used_key = self.by_key[self.least_used_key].prev\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.n == 0:\n\t\traise KeyError('popitem(): cache is empty')\n\n\t# This pops the last item in the list, which is the oldest\n\tkey, value = self.cache.popitem(last=False)\n\tself.n -= 1\n\tdel self.count[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self._map:\n\t\traise KeyError('popitem(): map is empty')\n\tself._check_size()\n\toldest = self._order[0]\n\tdel self._map[oldest]\n\tdel self._order[0]\n\tself._size -= 1\n\treturn oldest, self._counts[oldest]\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn _OrderedDict.popitem(self)\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self._mapping:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\tresult = self._mapping.popitem()\n\tdel self._counts[result[1]]\n\treturn result\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.root is None or self.root.val is None:\n\t\treturn None\n\tself._remove(self.root.val)\n\treturn self.root.val\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('popitem(): lru cache is empty')\n\tkey, link = self.root\n\tdel self.root[key]\n\tself.root = link\n\tself.size -= 1\n\treturn key, link\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# TODO: implement this\n\tpass\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# If the cache is empty, return a KeyError to be consistent with dict.popitem\n\tif not self._cache:\n\t\traise KeyError\n\t# Remove the oldest item from the cache\n\tkey = self._cache.pop(0)\n\tvalue = self._dict.pop(key)\n\t# Remove the key from the list of most recently used items\n\tself._lru.remove(key)\n\t# Add the key to the end of the list of most recently used items\n\tself._lru.append(key)\n\treturn key, value\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not len(self):\n\t\traise KeyError('dictionary is empty')\n\td = self.data\n\tself._check_size()\n\tpq = d.queue\n\tqsize = len(pq)\n\tif qsize == 1:\n\t\t# Handle edge case of one item in the queue.\n\t\treturn pq.popleft()\n\t# Remove and return the item with minimum frequency.\n\t# Use linear search to find the minimum frequency.\n\tminfreq = pq[0][2]\n\tminidx = 0\n\tfor i in range(1, qsize):\n\t\tif pq[i][2] < minfreq:\n\t\t\tminfreq, minidx = pq[i][2], i\n\titem = pq.popleft()\n\titem[2] = minfreq + 1\n\tpq.appendleft(item)\n\t# Remove the corresponding link from the doubly linked list.\n\tif minidx:\n\t\tprev = pq[minidx - 1]\n\t\tnext = pq[minidx + 1]\n\t\tprev[3] = next\n\t\tnext[2] = prev\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\twhile self.by_key[self.least_used_key].next is not None:\n\t\tself.least_used_key = self.by_key[self.least_used_key].next\n\tkey = self.least_used_key\n\tvalue = self.by_key[key].value\n\tdel self.by_key[key]\n\tdel self.by_value[value]\n\tself.size -= 1\n\tself.least_used_key = self.by_key[self.least_used_key].prev\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.n == 0:\n\t\traise KeyError('popitem(): cache is empty')\n\n\t# This pops the last item in the list, which is the oldest\n\tkey, value = self.cache.popitem(last=False)\n\tself.n -= 1\n\tdel self.count[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self._map:\n\t\traise KeyError('popitem(): map is empty')\n\tself._check_size()\n\toldest = self._order[0]\n\tdel self._map[oldest]\n\tdel self._order[0]\n\tself._size -= 1\n\treturn oldest, self._counts[oldest]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn _OrderedDict.popitem(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self._mapping:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\tresult = self._mapping.popitem()\n\tdel self._counts[result[1]]\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.root is None or self.root.val is None:\n\t\treturn None\n\tself._remove(self.root.val)\n\treturn self.root.val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('popitem(): lru cache is empty')\n\tkey, link = self.root\n\tdel self.root[key]\n\tself.root = link\n\tself.size -= 1\n\treturn key, link\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# TODO: implement this\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# If the cache is empty, return a KeyError to be consistent with dict.popitem\n\tif not self._cache:\n\t\traise KeyError\n\t# Remove the oldest item from the cache\n\tkey = self._cache.pop(0)\n\tvalue = self._dict.pop(key)\n\t# Remove the key from the list of most recently used items\n\tself._lru.remove(key)\n\t# Add the key to the end of the list of most recently used items\n\tself._lru.append(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# Remove the oldest item.\n\tkey = self.queue.pop()\n\t# Remove it from the dictionary.\n\tdel self.cache[key]\n\treturn key, self.cache[key]\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.root is None:\n\t\traise KeyError\n\tnode = self.root\n\tkey, value = node.key, node.value\n\tnode.key, node.value = None, None\n\tself.root.left = self._remove(node)\n\tself.root = self.root.left\n\tself.size -= 1\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('cache is empty')\n\twhile True:\n\t\tlink = self.root\n\t\twhile link.prev is not None:\n\t\t\tlink = link.prev\n\t\tif link.next is None:\n\t\t\tself.root = link.prev\n\t\t\tlink.prev = None\n\t\telse:\n\t\t\tlink.prev.next = link.next\n\t\t\tlink.next.prev = link.prev\n\t\t\tself.root = link\n\t\t\tlink.prev = None\n\t\t\tlink.next = None\n\t\tkey = link.key\n\t\tif key in self.cache:\n\t\t\tdel self.cache[key]\n\t\t\treturn link.key, link.value\n\t\tlink.key = None\n\t\tlink.value = None\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.getlen() == 0:\n\t\traise KeyError('%s is empty' % repr(self))\n\tpqremove(self, self.lru[0], self.lru[1], self.lru[2])\n\tkey = self.lru[0]\n\tif self.lru[2] is not self.lru[0]:\n\t\tpqremove(self, self.lru[2], self.lru[3], self.lru[4])\n\t\tinsert(self, self.lru[2], self.lru[3], self.lru[4])\n\tdel self.store[key]\n\treturn key, self.store[key]\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self.root:\n\t\traise KeyError('LRU cache is empty.')\n\n\tvictim = self.root\n\tself.root = victim.next\n\tself.root.prev = None\n\tself._unlink(victim)\n\tdel self.data[victim.key]\n\treturn (victim.key, victim.value)\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tmin_pair = self._order_to_pair[self._min_order]\n\tdel self._order_to_pair[self._min_order]\n\tdel self._pair_to_order[min_pair]\n\tself._min_order += 1\n\treturn min_pair\n\n###\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self._len == 0:\n\t\traise KeyError(\"popitem(): cache is empty\")\n\tself._len -= 1\n\tkey, value = self._cache.popitem(last=False)\n\tdel self._cache[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# remove the least recently used key\n\tkey = self.queue.pop()\n\t# remove the pair from the dictionary\n\tdel self.data[key]\n\treturn key\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self._head is None:\n\t\traise KeyError('The LRU cache is empty')\n\tnode = self._head\n\tkey = node.key\n\tvalue = node.value\n\tself._remove(node)\n\tself._add(key, value)\n\treturn key, value\n\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# pylint: disable=E1101\n\tif len(self.items) == 0:\n\t\traise KeyError(\"popitem(): cache is empty\")\n\n\tkey, value = self.items.popitem(last=False)\n\tdel self.data[key]\n\treturn key, value\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# Remove the oldest item.\n\tkey = self.queue.pop()\n\t# Remove it from the dictionary.\n\tdel self.cache[key]\n\treturn key, self.cache[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.root is None:\n\t\traise KeyError\n\tnode = self.root\n\tkey, value = node.key, node.value\n\tnode.key, node.value = None, None\n\tself.root.left = self._remove(node)\n\tself.root = self.root.left\n\tself.size -= 1\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('cache is empty')\n\twhile True:\n\t\tlink = self.root\n\t\twhile link.prev is not None:\n\t\t\tlink = link.prev\n\t\tif link.next is None:\n\t\t\tself.root = link.prev\n\t\t\tlink.prev = None\n\t\telse:\n\t\t\tlink.prev.next = link.next\n\t\t\tlink.next.prev = link.prev\n\t\t\tself.root = link\n\t\t\tlink.prev = None\n\t\t\tlink.next = None\n\t\tkey = link.key\n\t\tif key in self.cache:\n\t\t\tdel self.cache[key]\n\t\t\treturn link.key, link.value\n\t\tlink.key = None\n\t\tlink.value = None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.getlen() == 0:\n\t\traise KeyError('%s is empty' % repr(self))\n\tpqremove(self, self.lru[0], self.lru[1], self.lru[2])\n\tkey = self.lru[0]\n\tif self.lru[2] is not self.lru[0]:\n\t\tpqremove(self, self.lru[2], self.lru[3], self.lru[4])\n\t\tinsert(self, self.lru[2], self.lru[3], self.lru[4])\n\tdel self.store[key]\n\treturn key, self.store[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self.root:\n\t\traise KeyError('LRU cache is empty.')\n\n\tvictim = self.root\n\tself.root = victim.next\n\tself.root.prev = None\n\tself._unlink(victim)\n\tdel self.data[victim.key]\n\treturn (victim.key, victim.value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tmin_pair = self._order_to_pair[self._min_order]\n\tdel self._order_to_pair[self._min_order]\n\tdel self._pair_to_order[min_pair]\n\tself._min_order += 1\n\treturn min_pair\n\n###\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self._len == 0:\n\t\traise KeyError(\"popitem(): cache is empty\")\n\tself._len -= 1\n\tkey, value = self._cache.popitem(last=False)\n\tdel self._cache[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# remove the least recently used key\n\tkey = self.queue.pop()\n\t# remove the pair from the dictionary\n\tdel self.data[key]\n\treturn key\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self._head is None:\n\t\traise KeyError('The LRU cache is empty')\n\tnode = self._head\n\tkey = node.key\n\tvalue = node.value\n\tself._remove(node)\n\tself._add(key, value)\n\treturn key, value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# pylint: disable=E1101\n\tif len(self.items) == 0:\n\t\traise KeyError(\"popitem(): cache is empty\")\n\n\tkey, value = self.items.popitem(last=False)\n\tdel self.data[key]\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif len(self.queue)==0:\n\t\traise KeyError('popitem(): cache is empty')\n\toldest = self.queue[0]\n\tself.queue.remove(oldest)\n\tdel self.mapping[oldest]\n\treturn oldest\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif self.header.prev is self.header:\n\t\traise KeyError('dictionary is empty')\n\tlink = self.header.prev\n\tself.header.prev = link.prev\n\tlink.prev.next = self.header\n\tlink.prev = self.header.prev\n\tlink.next = self.header\n\tself.header.prev = link\n\tself.n -= 1\n\treturn link.key, link.value\n\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\twhile True:\n\t\tkey = self.cache.popitem(last=False)[0]\n\t\ttry:\n\t\t\tvalue = self.cache.popitem(last=False)[1]\n\t\texcept KeyError:\n\t\t\tcontinue\n\t\telse:\n\t\t\tbreak\n\tself.cache[key] = value\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif self.current:\n\t\t# Return current item and update LRU list\n\t\titem = (self.current.key, self.current.value)\n\t\tself.current = self.current.prev\n\t\tif self.current:\n\t\t\tself.current.next = None\n\t\telse:\n\t\t\tself.head = None\n\t\tself.length -= 1\n\t\treturn item\n\telse:\n\t\t# No items in cache\n\t\traise KeyError('popitem(): cache is empty')\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tassert self.size()\n\titem = self.head\n\tself.head = item.next\n\tif not self.head:\n\t\tself.tail = None\n\titem.next = None\n\tself.size -= 1\n\treturn item.key, item.value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif not self.root.left:\n\t\traise KeyError('popitem(): cache is empty')\n\treturn self.__remove_node(self.root.left, None)\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t# If the list is empty, stop.\n\tif self.n == 0:\n\t\traise KeyError('cache is empty')\n\t# Remove the last item.\n\tkey = self.queue.pop()\n\tvalue = self.dictionary.pop(key)\n\t# Reduce the size of the cache.\n\tself.n -= 1\n\t# Return the popped item.\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif self.used_count == 0:\n\t\traise KeyError('Popitem from empty OrderedDict')\n\tself.used_count -= 1\n\tkey = self.keys()[self.used_count]\n\tvalue = self.get(key)\n\tdel self[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('cache is empty')\n\tfor i in range(len(self.hist)):\n\t\tif self.hist[-i - 1] in self.cache:\n\t\t\tself.hist.pop()\n\t\t\tself.hist.append(self.hist[-i - 1])\n\t\t\treturn self.cache.pop(self.hist[-i - 1])\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey, value = self.queue.pop(0)\n\t\tif self.maxsize and self.currsize > self.maxsize:\n\t\t\tself.currsize -= 1\n\t\treturn key, value\n\texcept IndexError:\n\t\traise KeyError('popitem(): cache is empty')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif len(self.queue)==0:\n\t\traise KeyError('popitem(): cache is empty')\n\toldest = self.queue[0]\n\tself.queue.remove(oldest)\n\tdel self.mapping[oldest]\n\treturn oldest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif self.header.prev is self.header:\n\t\traise KeyError('dictionary is empty')\n\tlink = self.header.prev\n\tself.header.prev = link.prev\n\tlink.prev.next = self.header\n\tlink.prev = self.header.prev\n\tlink.next = self.header\n\tself.header.prev = link\n\tself.n -= 1\n\treturn link.key, link.value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\twhile True:\n\t\tkey = self.cache.popitem(last=False)[0]\n\t\ttry:\n\t\t\tvalue = self.cache.popitem(last=False)[1]\n\t\texcept KeyError:\n\t\t\tcontinue\n\t\telse:\n\t\t\tbreak\n\tself.cache[key] = value\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif self.current:\n\t\t# Return current item and update LRU list\n\t\titem = (self.current.key, self.current.value)\n\t\tself.current = self.current.prev\n\t\tif self.current:\n\t\t\tself.current.next = None\n\t\telse:\n\t\t\tself.head = None\n\t\tself.length -= 1\n\t\treturn item\n\telse:\n\t\t# No items in cache\n\t\traise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tassert self.size()\n\titem = self.head\n\tself.head = item.next\n\tif not self.head:\n\t\tself.tail = None\n\titem.next = None\n\tself.size -= 1\n\treturn item.key, item.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif not self.root.left:\n\t\traise KeyError('popitem(): cache is empty')\n\treturn self.__remove_node(self.root.left, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t# If the list is empty, stop.\n\tif self.n == 0:\n\t\traise KeyError('cache is empty')\n\t# Remove the last item.\n\tkey = self.queue.pop()\n\tvalue = self.dictionary.pop(key)\n\t# Reduce the size of the cache.\n\tself.n -= 1\n\t# Return the popped item.\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif self.used_count == 0:\n\t\traise KeyError('Popitem from empty OrderedDict')\n\tself.used_count -= 1\n\tkey = self.keys()[self.used_count]\n\tvalue = self.get(key)\n\tdel self[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('cache is empty')\n\tfor i in range(len(self.hist)):\n\t\tif self.hist[-i - 1] in self.cache:\n\t\t\tself.hist.pop()\n\t\t\tself.hist.append(self.hist[-i - 1])\n\t\t\treturn self.cache.pop(self.hist[-i - 1])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey, value = self.queue.pop(0)\n\t\tif self.maxsize and self.currsize > self.maxsize:\n\t\t\tself.currsize -= 1\n\t\treturn key, value\n\texcept IndexError:\n\t\traise KeyError('popitem(): cache is empty')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t# return a random (key, value) pair\n\treturn self.__choice()\n\n\n\t\ndef __choice(self):\n\t\"\"\"\n\tChoose a random item from the list\n\t\"\"\"\n\t# get a random index\n\tindex = random.randint(0, self.__len-1)\n\t# get the item in the list\n\titem = self.__list[index]\n\t# remove the item from the list\n\tself.__list.pop(index)\n\t# update the length\n\tself.__len -= 1\n\t# return the item\n\treturn item\n\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.__choice()\n\t\ndef pop(self, key, *args):\n\t\"\"\"\n\tFind, remove and return a value from the dictionary via __choice in the class\n\t\"\"\"\n\treturn self.__choice(key, *args)\n\t\ndef update(self, *args, **kwargs):\n\t\"\"\"\n\tUpdate the dictionary by adding or replacing key-value pairs.\n\n\t:param args:\n\t\tSee `dict.update`.\n\n\t:param kwargs:\n\t\tSee `dict.update`.\n\t\"\"\"\n\tself.__choice(*args, **kwargs)\n\t\ndef clear(self):\n\t\"\"\"\n\tRemove all items from the dictionary.\n\t\"\"\"\n\tself.__choice()\n\t\ndef get(self, key, default=None):\n\t\"\"\"\n\tReturn the value of the dictionary entry for the key.\n\n\t:param key:\n\t\tThe key that is looked up.\n\n\t:param default:\n\t\tThe default value if the key is not found.\n\t\"\"\"\n\treturn self.__choice(key, default)\n\t\ndef setdefault(self, key, default=None):\n\t\"\"\"\n\tSet `default` as the value of the dictionary entry for `key`.\n\n\t:param key:\n\t\tThe key that is", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\treturn self.pop(self.__choice(self))\n\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.pop(self.__choice())\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.pop(self.__choice())\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.__choice()\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\treturn key, self[key]\n\n\nclass MutableSet(MutableMapping, Set):\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tInitialize the class, Set and MutableMapping\n\t\t\"\"\"\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself.keys = dict_keys(self)\n\t\tself.values = dict_values(self)\n\t\tself.items = dict_items(self)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tIterate over the class\n\t\t\"\"\"\n\t\treturn self.keys()\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn the length of the class\n\t\t\"\"\"\n\t\treturn len(self.keys())\n\n\tdef __contains__(self, item):\n\t\t\"\"\"\n\t\tCheck if the class contains the item\n\t\t\"\"\"\n\t\treturn item in self.keys()\n\n\tdef add(self, key):\n\t\t\"\"\"\n\t\tAdd the key to the class\n\t\t\"\"\"\n\t\tself[key] = None\n\n\tdef discard(self, key):\n\t\t\"\"\"\n\t\tRemove the key from the class if it exists\n\t", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tk = random.choice(self.__choice)\n\treturn k, self.__delitem__(k)\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tif len(self) == 0:\n\t\traise KeyError\n\telse:\n\t\titem = self.__choice()\n\t\tdel self.keys[item[0]]\n\t\tdel self.values[item[1]]\n\t\treturn item\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.__choice()\n\n\t\ndef pop(self, key, default=None):\n\t\"\"\"\n\tFind, remove and return a value if the key is in the class\n\t\"\"\"\"\n\tif key in self.d:\n\t\treturn self.d.pop(key)\n\telse:\n\t\treturn default\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t# return a random (key, value) pair\n\treturn self.__choice()\n\n\n\t\ndef __choice(self):\n\t\"\"\"\n\tChoose a random item from the list\n\t\"\"\"\n\t# get a random index\n\tindex = random.randint(0, self.__len-1)\n\t# get the item in the list\n\titem = self.__list[index]\n\t# remove the item from the list\n\tself.__list.pop(index)\n\t# update the length\n\tself.__len -= 1\n\t# return the item\n\treturn item\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.__choice()\n\t\ndef pop(self, key, *args):\n\t\"\"\"\n\tFind, remove and return a value from the dictionary via __choice in the class\n\t\"\"\"\n\treturn self.__choice(key, *args)\n\t\ndef update(self, *args, **kwargs):\n\t\"\"\"\n\tUpdate the dictionary by adding or replacing key-value pairs.\n\n\t:param args:\n\t\tSee `dict.update`.\n\n\t:param kwargs:\n\t\tSee `dict.update`.\n\t\"\"\"\n\tself.__choice(*args, **kwargs)\n\t\ndef clear(self):\n\t\"\"\"\n\tRemove all items from the dictionary.\n\t\"\"\"\n\tself.__choice()\n\t\ndef get(self, key, default=None):\n\t\"\"\"\n\tReturn the value of the dictionary entry for the key.\n\n\t:param key:\n\t\tThe key that is looked up.\n\n\t:param default:\n\t\tThe default value if the key is not found.\n\t\"\"\"\n\treturn self.__choice(key, default)\n\t\ndef setdefault(self, key, default=None):\n\t\"\"\"\n\tSet `default` as the value of the dictionary entry for `key`.\n\n\t:param key:\n\t\tThe key that is", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\treturn self.pop(self.__choice(self))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.pop(self.__choice())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.pop(self.__choice())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.__choice()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey = self.__choice()\n\treturn key, self[key]\n\n\nclass MutableSet(MutableMapping, Set):\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tInitialize the class, Set and MutableMapping\n\t\t\"\"\"\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself.keys = dict_keys(self)\n\t\tself.values = dict_values(self)\n\t\tself.items = dict_items(self)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tIterate over the class\n\t\t\"\"\"\n\t\treturn self.keys()\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn the length of the class\n\t\t\"\"\"\n\t\treturn len(self.keys())\n\n\tdef __contains__(self, item):\n\t\t\"\"\"\n\t\tCheck if the class contains the item\n\t\t\"\"\"\n\t\treturn item in self.keys()\n\n\tdef add(self, key):\n\t\t\"\"\"\n\t\tAdd the key to the class\n\t\t\"\"\"\n\t\tself[key] = None\n\n\tdef discard(self, key):\n\t\t\"\"\"\n\t\tRemove the key from the class if it exists\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tk = random.choice(self.__choice)\n\treturn k, self.__delitem__(k)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tif len(self) == 0:\n\t\traise KeyError\n\telse:\n\t\titem = self.__choice()\n\t\tdel self.keys[item[0]]\n\t\tdel self.values[item[1]]\n\t\treturn item\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\treturn self.__choice()\n\n\t\ndef pop(self, key, default=None):\n\t\"\"\"\n\tFind, remove and return a value if the key is in the class\n\t\"\"\"\"\n\tif key in self.d:\n\t\treturn self.d.pop(key)\n\telse:\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r'(?<!\\\\)\\$[a-zA-Z_][a-zA-Z0-9_]*')\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r'{([a-zA-Z0-9-_]+)}')\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"\\s*(?P<name>\\w+)\\s+in\\s+(?P<values>.+)\")\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(\n\t\tr'(?:^| )([\\w-]+)(?::([\\w-]+))?(?: +(in|out))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(\n\t\tr\"\"\"\n\t\t\t(?P<prefix>\n\t\t\t\t(?:%s)?\t\t\t\t\t# Prefix.\n\t\t\t)\\s+\t\t\t\t\t\t# Whitespace.\n\t\t\t(?P<parameter>\t\t\t\t# Parameter.\n\t\t\t\t(?:%s)\t\t\t\t\t# Parameter name.\n\t\t\t\t(?:\\s+(?P<operator>%s))?\t# Operator.\n\t\t\t\t(?:\\s+(?P<value>%s))?\t# Value.\n\t\t\t)\n\t\t\"\"\" % (\n\t\t\tre.escape(self.prefix),\n\t\t\tre.escape(self.name),\n\t\t\tre.escape(self.operator),\n\t\t\tre.escape(self.value)\n\t\t), re.VERBOSE | re.IGNORECASE\n\t)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"(?:in|to)(\\w+)\")\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r'(?:<(?P<in>in)>?)(?::(?P<in_key>.*))?'\n\treturn re.compile(regex)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"^(?:in|iN|In|IN|I|I.N.|I.n.|i.n.|i.N)(\\s+.*)$\")\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\treturn re.compile(r\"(?:^|(?<=\\s))(?:(?<=in)(?:\\s++(?P<in_param>\\w+))|(?<=\\.)\\w+)\\s*(?:\\((?P<in_body>.*?)\\))?(?=$|(?<=\\s))\", re.I)\n\n\n#\n# Create the in-style parameter regular expression.\n#\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r'\\s*(?:in|within)\\s*(?:the\\s*)?\\(?\\s*(?P<min>-?\\d+(?:\\.\\d+)?)\\s*(?:,\\s*(?P<max>-?\\d+(?:\\.\\d+)?)\\s*)?\\)')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r'(?<!\\\\)\\$[a-zA-Z_][a-zA-Z0-9_]*')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r'{([a-zA-Z0-9-_]+)}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"\\s*(?P<name>\\w+)\\s+in\\s+(?P<values>.+)\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(\n\t\tr'(?:^| )([\\w-]+)(?::([\\w-]+))?(?: +(in|out))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +(.*))?(?: +(in|out))?(?: +(.*))?(?: +([\\w-]+))?(?: +", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(\n\t\tr\"\"\"\n\t\t\t(?P<prefix>\n\t\t\t\t(?:%s)?\t\t\t\t\t# Prefix.\n\t\t\t)\\s+\t\t\t\t\t\t# Whitespace.\n\t\t\t(?P<parameter>\t\t\t\t# Parameter.\n\t\t\t\t(?:%s)\t\t\t\t\t# Parameter name.\n\t\t\t\t(?:\\s+(?P<operator>%s))?\t# Operator.\n\t\t\t\t(?:\\s+(?P<value>%s))?\t# Value.\n\t\t\t)\n\t\t\"\"\" % (\n\t\t\tre.escape(self.prefix),\n\t\t\tre.escape(self.name),\n\t\t\tre.escape(self.operator),\n\t\t\tre.escape(self.value)\n\t\t), re.VERBOSE | re.IGNORECASE\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"(?:in|to)(\\w+)\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r'(?:<(?P<in>in)>?)(?::(?P<in_key>.*))?'\n\treturn re.compile(regex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"^(?:in|iN|In|IN|I|I.N.|I.n.|i.n.|i.N)(\\s+.*)$\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\treturn re.compile(r\"(?:^|(?<=\\s))(?:(?<=in)(?:\\s++(?P<in_param>\\w+))|(?<=\\.)\\w+)\\s*(?:\\((?P<in_body>.*?)\\))?(?=$|(?<=\\s))\", re.I)\n\n\n#\n# Create the in-style parameter regular expression.\n#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r'\\s*(?:in|within)\\s*(?:the\\s*)?\\(?\\s*(?P<min>-?\\d+(?:\\.\\d+)?)\\s*(?:,\\s*(?P<max>-?\\d+(?:\\.\\d+)?)\\s*)?\\)')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(\n\t\t\tself,\n\t\t\tself._in_type,\n\t\t\tself._out_type,\n\t\t\tself._converter,\n\t\t\tself._type_checker,\n\t\t\tself._missing_type_checker,\n\t\t\tself._missing_converter,\n\t\t\tself._validation,\n\t\t\tself._coercer,\n\t\t\tself._validator,\n\t\t\tself._default_factory,\n\t\t\tself._ignore_extra,\n\t\t\tself._ignore_missing,\n\t\t\tself._ignore_invalid,\n\t\t)\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._converter\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(\n\t\t\tself._style,\n\t\t\tself._styles,\n\t\t\tself._converter,\n\t\t\tself._converters,\n\t\t\tself._converter_style,\n\t\t\tself._converter_styles,\n\t\t\tself._converter_type,\n\t\t\tself._converter_types,\n\t\t\tself._converter_default,\n\t\t\tself._converter_defaults,\n\t\t\tself._converter_name,\n\t\t\tself._converter_names,\n\t\t\tself._converter_parameter,\n\t\t\tself._converter_parameters,\n\t\t\tself._converter_required,\n\t\t\tself._converter_requireds,\n\t\t\tself._converter_optional,\n\t\t\tself._converter_optionals,\n\t\t\tself._converter_required_optional,\n\t\t\tself._converter_required_optionals,\n\t\t\tself._converter_positional,\n\t\t\tself._converter_positionals,\n\t\t\tself._converter_required_positional,\n\t\t\tself._converter_required_positionals,\n\t\t\tself._converter_optional_positional,\n\t\t\tself._con", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self, convert_to_snake_case=self.convert_to_snake_case)\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif self._converter is None:\n\t\tself._converter = _converting._Converter(self)\n\treturn self._converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self)\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn converting.Converter(self)\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(\n\t\t\tself._converters,\n\t\t\tself._converter_kwargs,\n\t\t\tself._converter_reuse,\n\t\t\tself._converter_kwargs_reuse,\n\t\t\tself._converter_kwargs_default,\n\t\t\tself._converter_kwargs_default_reuse,\n\t\t\tself._converter_kwargs_default_is_callable,\n\t\t\tself._converter_kwargs_default_is_callable_reuse,\n\t\t\tself._converter_kwargs_default_reuse_is_callable,\n\t\t\tself._converter_kwargs_default_reuse_is_callable_reuse,\n\t\t\tself._converter_is_callable,\n\t\t\tself._converter_reuse_is_callable,\n\t\t\tself._converter_kwargs_is_callable,\n\t\t\tself._converter_kwargs_reuse_is_callable,\n\t\t\tself._converter_kwargs_default_is_callable_reuse,\n\t\t\tself._converter_kwargs_default_reuse_is_callable_reuse,\n\t\t\tself._converter_kwargs_default_reuse_is_callable,", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self._param_style)\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(self)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(\n\t\t\tself,\n\t\t\tself._in_type,\n\t\t\tself._out_type,\n\t\t\tself._converter,\n\t\t\tself._type_checker,\n\t\t\tself._missing_type_checker,\n\t\t\tself._missing_converter,\n\t\t\tself._validation,\n\t\t\tself._coercer,\n\t\t\tself._validator,\n\t\t\tself._default_factory,\n\t\t\tself._ignore_extra,\n\t\t\tself._ignore_missing,\n\t\t\tself._ignore_invalid,\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._converter\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(\n\t\t\tself._style,\n\t\t\tself._styles,\n\t\t\tself._converter,\n\t\t\tself._converters,\n\t\t\tself._converter_style,\n\t\t\tself._converter_styles,\n\t\t\tself._converter_type,\n\t\t\tself._converter_types,\n\t\t\tself._converter_default,\n\t\t\tself._converter_defaults,\n\t\t\tself._converter_name,\n\t\t\tself._converter_names,\n\t\t\tself._converter_parameter,\n\t\t\tself._converter_parameters,\n\t\t\tself._converter_required,\n\t\t\tself._converter_requireds,\n\t\t\tself._converter_optional,\n\t\t\tself._converter_optionals,\n\t\t\tself._converter_required_optional,\n\t\t\tself._converter_required_optionals,\n\t\t\tself._converter_positional,\n\t\t\tself._converter_positionals,\n\t\t\tself._converter_required_positional,\n\t\t\tself._converter_required_positionals,\n\t\t\tself._converter_optional_positional,\n\t\t\tself._con", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self, convert_to_snake_case=self.convert_to_snake_case)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif self._converter is None:\n\t\tself._converter = _converting._Converter(self)\n\treturn self._converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn converting.Converter(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(\n\t\t\tself._converters,\n\t\t\tself._converter_kwargs,\n\t\t\tself._converter_reuse,\n\t\t\tself._converter_kwargs_reuse,\n\t\t\tself._converter_kwargs_default,\n\t\t\tself._converter_kwargs_default_reuse,\n\t\t\tself._converter_kwargs_default_is_callable,\n\t\t\tself._converter_kwargs_default_is_callable_reuse,\n\t\t\tself._converter_kwargs_default_reuse_is_callable,\n\t\t\tself._converter_kwargs_default_reuse_is_callable_reuse,\n\t\t\tself._converter_is_callable,\n\t\t\tself._converter_reuse_is_callable,\n\t\t\tself._converter_kwargs_is_callable,\n\t\t\tself._converter_kwargs_reuse_is_callable,\n\t\t\tself._converter_kwargs_default_is_callable_reuse,\n\t\t\tself._converter_kwargs_default_reuse_is_callable_reuse,\n\t\t\tself._converter_kwargs_default_reuse_is_callable,", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self._param_style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(self)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn iso_parse(dt_str, self)\n\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn isoparse_implementation(self, dt_str)\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoparse_datetime(dt_str, self.datetime_cls)\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn isoparse_impl(dt_str, self)\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoparse_datetime(dt_str)\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\tif isinstance(dt_str, six.string_types):\n\t\treturn self.isoparse_str(dt_str)\n\telse:\n\t\treturn self.isoparse_file(dt_str)\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tself.strict = False\n\tself.datetime_formats = ['%Y', '%Y-%m', '%Y-%m-%d',\n\t\t\t\t\t\t\t'%Y%m%d', '%Y-%m-W%W',\n\t\t\t\t\t\t\t'%Y-%m-%w', '%Y%m%w']\n\tself.time_formats = ['%H', '%H%M', '%H%M%S',\n\t\t\t\t\t\t '%H%M%S.%f', '%H%M%S,%f',\n\t\t\t\t\t\t '%H%M%S.%F', '%H%M%S,%F',\n\t\t\t\t\t\t '%H%M%S.%j', '%H%M%S,%j',\n\t\t\t\t\t\t '%H%M%S.%U', '%H%M%S,%U',\n\t\t\t\t\t\t '%H%M%S.%W', '%H%M%S,%W',\n\t\t\t\t\t\t '%H%M%S.%u', '%H%M%S,%u',\n\t\t\t\t\t\t '%H%M%S.%V', '%H%M%S,%V',\n\t\t\t\t\t\t '%H%M%S.", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoparse_datetime(dt_str)\n\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn _isodate.parse_datetime(dt_str)\n\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn parse(dt_str, self.DEFAULT_ISO_FORMAT)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn iso_parse(dt_str, self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn isoparse_implementation(self, dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoparse_datetime(dt_str, self.datetime_cls)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn isoparse_impl(dt_str, self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoparse_datetime(dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\tif isinstance(dt_str, six.string_types):\n\t\treturn self.isoparse_str(dt_str)\n\telse:\n\t\treturn self.isoparse_file(dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tself.strict = False\n\tself.datetime_formats = ['%Y', '%Y-%m', '%Y-%m-%d',\n\t\t\t\t\t\t\t'%Y%m%d', '%Y-%m-W%W',\n\t\t\t\t\t\t\t'%Y-%m-%w', '%Y%m%w']\n\tself.time_formats = ['%H', '%H%M', '%H%M%S',\n\t\t\t\t\t\t '%H%M%S.%f', '%H%M%S,%f',\n\t\t\t\t\t\t '%H%M%S.%F', '%H%M%S,%F',\n\t\t\t\t\t\t '%H%M%S.%j', '%H%M%S,%j',\n\t\t\t\t\t\t '%H%M%S.%U', '%H%M%S,%U',\n\t\t\t\t\t\t '%H%M%S.%W', '%H%M%S,%W',\n\t\t\t\t\t\t '%H%M%S.%u', '%H%M%S,%u',\n\t\t\t\t\t\t '%H%M%S.%V', '%H%M%S,%V',\n\t\t\t\t\t\t '%H%M%S.", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn self.isoparse_datetime(dt_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn _isodate.parse_datetime(dt_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn parse(dt_str, self.DEFAULT_ISO_FORMAT)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif default and ignoretz:\n\t\traise ValueError(\"ignoretz and default arguments are mutually \"\n\t\t\t\t\"exclusive\")\n\tif isinstance(timestr, (str, bytes)):\n\t\treturn self._get_date_time(timestr, default, ignoretz,\n\t\t\t\ttzinfos, **kwargs)\n\telse:\n\t\traise TypeError(\"Unsupported type %s\" % type(timestr))\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"A string or character stream is required\")\n\n\tif default is not None:\n\t\tdefault = _get_default(default)\n\n\tresult = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n\t# If fuzzy_with_tokens is True, then result is actually a tuple,\n\t# with the first element being the datetime object, and the second\n\t# element being a list of fuzzy tokens.  If fuzzy_with_tokens is\n\t# False, then result is the datetime object.\n\tif self.fuzzy_with_tokens:\n\t\treturn result\n\telse:\n\t\treturn result[0]\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tresult = self._parse(timestr, default, ignoretz, tzinfos,\n                          **kwargs)\n\tif self.fuzzy_with_tokens:\n\t\treturn result\n\treturn result.date_time\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif isinstance(timestr, datetime.datetime):\n\t\tif default is not None:\n\t\t\treturn timestr.replace(microsecond=default.microsecond)\n\t\telse:\n\t\t\treturn timestr\n\telif isinstance(timestr, datetime.date):\n\t\tif default is not None:\n\t\t\treturn datetime.datetime.combine(timestr, default.time())\n\t\telse:\n\t\t\treturn datetime.datetime.combine(timestr, datetime.time.min)\n\telif not isinstance(timestr, basestring):\n\t\traise TypeError(\"A datetime string is required\")\n\telse:\n\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError('Non-string or character stream input.')\n\tif default is not None and not isinstance(default, datetime):\n\t\traise TypeError('default must be None or a datetime instance.')\n\tif ignoretz and tzinfos:\n\t\traise ValueError(\"Can't use ignoretz with tzinfos.\")\n\tif default is None:\n\t\tdefault = datetime.now()\n\tif tzinfos:\n\t\tif isinstance(tzinfos, dict):\n\t\t\tself.timezone_infos.update(tzinfos)\n\t\telse:\n\t\t\tself.timezone_infos[tzinfos.__name__] = tzinfos\n\treturn self._parse(timestr, default, ignoretz, **kwargs)\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif default is not None and not isinstance(default, datetime.datetime):\n\t\traise TypeError(\"If provided, 'default' must be a datetime.datetime object\")\n\tif isinstance(timestr, unicode):\n\t\ttimestr = timestr.encode(\"utf-8\")\n\tif ignoretz:\n\t\tdefault = default.replace(tzinfo=None) if default else None\n\telif default is not None and default.tzinfo:\n\t\tif default.tzinfo.utcoffset(default) is None:\n\t\t\tdefault = default.replace(tzinfo=None)\n\treturn _dateparser._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\treturn _parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tparser = self.parserclass(timestr, **kwargs)\n\tparser.tzinfos = tzinfos\n\tparser.ignoretz = ignoretz\n\treturn parser.get_datetime(default)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif default is not None:\n\t\tif not isinstance(default, datetime):\n\t\t\traise TypeError(\"default must be a datetime object\")\n\t\tif default.tzinfo is not None:\n\t\t\tif ignoretz:\n\t\t\t\twarnings.warn(\"Ignoring time zone info in \"\n\t\t\t\t\t\t\t  \"default value\",\n\t\t\t\t\t\t\t  UserWarning)\n\t\t\telse:\n\t\t\t\tif not self.fixed_offset:\n\t\t\t\t\ttzinfos = {}\n\t\t\t\ttzinfos[default.tzinfo.tzname(default)] = default.tzinfo\n\n\t# Parse the string\n\tresult = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n\tif self.fuzzy_with_tokens:\n\t\t# Return a tuple\n\t\treturn result[0], result[1]\n\telse:\n\t\t# Return the parsed datetime object\n\t\treturn result\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif default and ignoretz:\n\t\traise ValueError(\"ignoretz and default arguments are mutually \"\n\t\t\t\t\"exclusive\")\n\tif isinstance(timestr, (str, bytes)):\n\t\treturn self._get_date_time(timestr, default, ignoretz,\n\t\t\t\ttzinfos, **kwargs)\n\telse:\n\t\traise TypeError(\"Unsupported type %s\" % type(timestr))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"A string or character stream is required\")\n\n\tif default is not None:\n\t\tdefault = _get_default(default)\n\n\tresult = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n\t# If fuzzy_with_tokens is True, then result is actually a tuple,\n\t# with the first element being the datetime object, and the second\n\t# element being a list of fuzzy tokens.  If fuzzy_with_tokens is\n\t# False, then result is the datetime object.\n\tif self.fuzzy_with_tokens:\n\t\treturn result\n\telse:\n\t\treturn result[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tresult = self._parse(timestr, default, ignoretz, tzinfos,\n                          **kwargs)\n\tif self.fuzzy_with_tokens:\n\t\treturn result\n\treturn result.date_time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif isinstance(timestr, datetime.datetime):\n\t\tif default is not None:\n\t\t\treturn timestr.replace(microsecond=default.microsecond)\n\t\telse:\n\t\t\treturn timestr\n\telif isinstance(timestr, datetime.date):\n\t\tif default is not None:\n\t\t\treturn datetime.datetime.combine(timestr, default.time())\n\t\telse:\n\t\t\treturn datetime.datetime.combine(timestr, datetime.time.min)\n\telif not isinstance(timestr, basestring):\n\t\traise TypeError(\"A datetime string is required\")\n\telse:\n\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError('Non-string or character stream input.')\n\tif default is not None and not isinstance(default, datetime):\n\t\traise TypeError('default must be None or a datetime instance.')\n\tif ignoretz and tzinfos:\n\t\traise ValueError(\"Can't use ignoretz with tzinfos.\")\n\tif default is None:\n\t\tdefault = datetime.now()\n\tif tzinfos:\n\t\tif isinstance(tzinfos, dict):\n\t\t\tself.timezone_infos.update(tzinfos)\n\t\telse:\n\t\t\tself.timezone_infos[tzinfos.__name__] = tzinfos\n\treturn self._parse(timestr, default, ignoretz, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif default is not None and not isinstance(default, datetime.datetime):\n\t\traise TypeError(\"If provided, 'default' must be a datetime.datetime object\")\n\tif isinstance(timestr, unicode):\n\t\ttimestr = timestr.encode(\"utf-8\")\n\tif ignoretz:\n\t\tdefault = default.replace(tzinfo=None) if default else None\n\telif default is not None and default.tzinfo:\n\t\tif default.tzinfo.utcoffset(default) is None:\n\t\t\tdefault = default.replace(tzinfo=None)\n\treturn _dateparser._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\treturn _parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tparser = self.parserclass(timestr, **kwargs)\n\tparser.tzinfos = tzinfos\n\tparser.ignoretz = ignoretz\n\treturn parser.get_datetime(default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tif default is not None:\n\t\tif not isinstance(default, datetime):\n\t\t\traise TypeError(\"default must be a datetime object\")\n\t\tif default.tzinfo is not None:\n\t\t\tif ignoretz:\n\t\t\t\twarnings.warn(\"Ignoring time zone info in \"\n\t\t\t\t\t\t\t  \"default value\",\n\t\t\t\t\t\t\t  UserWarning)\n\t\t\telse:\n\t\t\t\tif not self.fixed_offset:\n\t\t\t\t\ttzinfos = {}\n\t\t\t\ttzinfos[default.tzinfo.tzname(default)] = default.tzinfo\n\n\t# Parse the string\n\tresult = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n\tif self.fuzzy_with_tokens:\n\t\t# Return a tuple\n\t\treturn result[0], result[1]\n\telse:\n\t\t# Return the parsed datetime object\n\t\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, datetime.datetime):\n\t\traise TypeError(\"datetime must be a datetime.datetime object\")\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\tif dt.tzinfo._utcoffset is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\tif dt.tzinfo._dst is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\tif dt.tzinfo._dst is not None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\t# Get the utc offset for the specified datetime and convert it to a\n\t# timedelta\n\toffset = dt.utcoffset()\n\tif offset is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\toffset = datetime.timedelta(microseconds=offset.microseconds,\n\t\t\t\t\t\t\t\tseconds=offset.seconds,\n\t\t\t\t\t\t\t\tminutes=offset.minutes,\n\t\t\t\t\t\t\t\thours=offset.hours)\n\t# Get the dst difference for the specified datetime and convert it to a\n\t# timedelta\n\tdst = dt.dst()\n\tif dst is None:\n\t\traise ValueError(\"", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"fromutc requires a timezone-aware datetime\")\n\tif dt.tzinfo is self:\n\t\treturn dt\n\tif self._dst_start_time and self._dst_end_time:\n\t\tdst_start_datetime = pytzinfo_datetime(self._dst_start_time)\n\t\tdst_end_datetime = pytzinfo_datetime(self._dst_end_time)\n\t\tif dst_start_datetime <= dt < dst_end_datetime:\n\t\t\tif dt.hour < self._dst_start_time.hour:\n\t\t\t\tif self._std_start_time.hour == dt.hour:\n\t\t\t\t\t# This is the first occurrence of the ambiguous\n\t\t\t\t\t# datetime, so return the datetime in the \"fold\"\n\t\t\t\t\t# forward.\n\t\t\t\t\treturn dt.replace(\n\t\t\t\t\t\ttzinfo=self,\n\t\t\t\t\t\thour=dt.hour + 1,\n\t\t\t\t\t\tminute=0,\n\t\t\t\t\t\tsecond=0,\n\t\t\t\t\t\tmicrosecond=0,\n\t\t\t\t\t)\n\t\t\telif dt.hour > self._", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzinfo is None:\n\t\traise ValueError('Cannot convert naive datetime %r to '\n\t\t\t\t\t\t 'timezone-aware' % dt)\n\t\n\t# If we're in a fold, our offset may be different than it\n\t# appears, so we must calculate the offset from UTC, rather\n\t# than using the offset attribute of the tzinfo.\n\toffset = self._utcoffset(dt)\n\t\n\t# The datetime we return must be in the same fold, so if we're\n\t# in a fold, we need to determine how to adjust our datetime\n\t# to be in the same fold as the datetime we're converting from.\n\t#\n\t# This will raise ValueError if we're in an ambiguous datetime\n\t# that is not in the fold of the datetime we're converting from.\n\tif offset is not None:\n\t\t# Convert to UTC.\n\t\tdt = dt.astimezone(utc)\n\t\t\n\t\t# Adjust to be in the appropriate fold.\n\t\t#\n\t\t# This will raise ValueError if we're in an ambiguous datetime\n\t\t# that is not in the fold of the datetime we're converting from.\n\t\tdt = self._coer", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# For this to work, dt must be a datetime with a tzinfo\n\tassert dt.tzinfo is not None\n\t# Determine the first occurrence of this datetime in the given\n\t# timezone.\n\tfirst = tz.normalize_to_first(dt, self.tz)\n\t# See if it's ambiguous.\n\tif first != dt:\n\t\t# If it is, see if we're in a fold state.\n\t\tif self.is_fold(dt):\n\t\t\t# If we are, then we're in the fold state *before* the\n\t\t\t# first occurrence, not after.\n\t\t\tdt = first\n\t\telse:\n\t\t\t# If we're not in a fold state, then we're in the fold\n\t\t\t# state *after* the first occurrence, not before.\n\t\t\tdt = tz.normalize_to_after(dt, self.tz)\n\t# Now, convert to the given timezone.\n\tdt = self.tz.normalize(dt.astimezone(self.to_tz))\n\treturn dt\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise NonTimezoneAware(dt)\n\tutc_dt = dt.astimezone(pytz.utc)\n\tresult = self.localize(utc_dt, is_dst=None)\n\tif result is None:\n\t\traise AmbiguousTime(dt)\n\tif result.utcoffset() != utc_dt.utcoffset():\n\t\traise FoldTime(dt)\n\treturn result\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tutcoffset = dt.utcoffset()\n\n\t# If we're given a naive datetime, assume that it's in UTC.\n\tif utcoffset is None:\n\t\tutcoffset = datetime.timedelta(0)\n\n\t# We'll need to adjust this by the specified offset\n\toffset = utcoffset + self.offset\n\n\t# Create a naive datetime object (i.e. without a tzinfo member)\n\t# representing midnight on the given date.\n\tmidnight = datetime.datetime(dt.year, dt.month, dt.day)\n\n\t# Add the offset to the datetime and return the result.\n\treturn midnight + offset\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"dt must be a naive datetime\")\n\t\n\tutc = dt.astimezone(UTC)\n\tutc_as_local = utc.astimezone(self)\n\t\n\t# Check for ambiguous datetime, fold if necessary\n\tif utc_as_local != utc:\n\t\t# Find the first occurrence (i.e. the one that's not ambiguous)\n\t\tambiguous_deltas = [utc_as_local - utc, utc - utc_as_local]\n\t\t\n\t\t# Find the first occurrence (i.e. the one that's not ambiguous)\n\t\tambiguous_delta = min(ambiguous_deltas)\n\t\t\n\t\tif ambiguous_delta == ambiguous_deltas[0]:\n\t\t\tfold = True\n\t\telse:\n\t\t\tfold = False\n\t\t\n\t\t# Return a folded datetime\n\t\treturn utc + (ambiguous_delta if fold else 0)\n\t\n\t# Return an unambiguous datetime\n\treturn utc_as_local\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# If the datetime is naive, it's probably a 7-hour-ahead\n\t# time.  We should really try to get the tzinfo from the\n\t# timezone, but that's not always going to be available (and\n\t# even if it is, it's not always going to be a\n\t# pytz.FixedOffset object - sometimes it's a datetime.tzinfo\n\t# that's actually a pytz.FixedOffset object, and sometimes it's\n\t# something else).  So, we'll just assume that the datetime is\n\t# 7 hours ahead.\n\tif dt.tzinfo is None:\n\t\toffset = timedelta(hours=7)\n\telse:\n\t\toffset = dt.tzinfo.utcoffset(dt)\n\treturn dt.replace(tzinfo=None) + offset\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"fromutc() requires a tz-aware datetime\")\n\tif dt.tzinfo.utcoffset(dt) is None:\n\t\traise ValueError(\"fromutc() requires a tzinfo with a UTC offset\")\n\tif dt.tzinfo == self:\n\t\treturn dt\n\t# We're in a different timezone.\n\tutcoffset = dt.tzinfo.utcoffset(dt)\n\t# If this is a fold, we need to return the first occurrence of the\n\t# ambiguous datetime.\n\tif utcoffset is not None:\n\t\t# If we're in a fold, this is the first occurrence.\n\t\tif utcoffset != self._transition_info[0][0]:\n\t\t\treturn dt\n\t# We're in a non-fold, non-DST situation.\n\tif utcoffset is None:\n\t\t# If we're in a fold, this is the first occurrence.\n\t\tif dt.hour < self._transition_info[0][0].hour or \\\n\t\t\t(dt.hour == self._transition_info[0][0].hour and\n\t\t\tdt.minute < self._transition_info[0][0].", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# If the datetime is in UTC, then we're done\n\tif dt.tzinfo == self._utc:\n\t\treturn dt\n\n\t# If the datetime is naive, convert it to UTC and return it\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=self._utc)\n\t\treturn self.fromutc(dt)\n\n\t# If the tzinfo is not a DstTzInfo instance, then we have no idea what\n\t# we're dealing with, so let the default implementation handle that.\n\tif not isinstance(dt.tzinfo, DstTzInfo):\n\t\treturn super(AwareTimezone, self).fromutc(dt)\n\n\t# If the datetime is not in a fold state, then we can just do the\n\t# conversion; no special handling required.\n\tif not dt.tzinfo._isdst(dt):\n\t\treturn super(AwareTimezone, self).fromutc(dt)\n\n\t# If the datetime is in a fold state, then we need to determine whether\n\t# the dt is in the folded or non-folded state. We do this by comparing\n\t# the offset before and after the fold. If the offset is the same, then\n\t# we're in"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, datetime.datetime):\n\t\traise TypeError(\"datetime must be a datetime.datetime object\")\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\tif dt.tzinfo._utcoffset is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\tif dt.tzinfo._dst is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\tif dt.tzinfo._dst is not None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\t# Get the utc offset for the specified datetime and convert it to a\n\t# timedelta\n\toffset = dt.utcoffset()\n\tif offset is None:\n\t\traise ValueError(\"datetime must be timezone-aware\")\n\toffset = datetime.timedelta(microseconds=offset.microseconds,\n\t\t\t\t\t\t\t\tseconds=offset.seconds,\n\t\t\t\t\t\t\t\tminutes=offset.minutes,\n\t\t\t\t\t\t\t\thours=offset.hours)\n\t# Get the dst difference for the specified datetime and convert it to a\n\t# timedelta\n\tdst = dt.dst()\n\tif dst is None:\n\t\traise ValueError(\"", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"fromutc requires a timezone-aware datetime\")\n\tif dt.tzinfo is self:\n\t\treturn dt\n\tif self._dst_start_time and self._dst_end_time:\n\t\tdst_start_datetime = pytzinfo_datetime(self._dst_start_time)\n\t\tdst_end_datetime = pytzinfo_datetime(self._dst_end_time)\n\t\tif dst_start_datetime <= dt < dst_end_datetime:\n\t\t\tif dt.hour < self._dst_start_time.hour:\n\t\t\t\tif self._std_start_time.hour == dt.hour:\n\t\t\t\t\t# This is the first occurrence of the ambiguous\n\t\t\t\t\t# datetime, so return the datetime in the \"fold\"\n\t\t\t\t\t# forward.\n\t\t\t\t\treturn dt.replace(\n\t\t\t\t\t\ttzinfo=self,\n\t\t\t\t\t\thour=dt.hour + 1,\n\t\t\t\t\t\tminute=0,\n\t\t\t\t\t\tsecond=0,\n\t\t\t\t\t\tmicrosecond=0,\n\t\t\t\t\t)\n\t\t\telif dt.hour > self._", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzinfo is None:\n\t\traise ValueError('Cannot convert naive datetime %r to '\n\t\t\t\t\t\t 'timezone-aware' % dt)\n\t\n\t# If we're in a fold, our offset may be different than it\n\t# appears, so we must calculate the offset from UTC, rather\n\t# than using the offset attribute of the tzinfo.\n\toffset = self._utcoffset(dt)\n\t\n\t# The datetime we return must be in the same fold, so if we're\n\t# in a fold, we need to determine how to adjust our datetime\n\t# to be in the same fold as the datetime we're converting from.\n\t#\n\t# This will raise ValueError if we're in an ambiguous datetime\n\t# that is not in the fold of the datetime we're converting from.\n\tif offset is not None:\n\t\t# Convert to UTC.\n\t\tdt = dt.astimezone(utc)\n\t\t\n\t\t# Adjust to be in the appropriate fold.\n\t\t#\n\t\t# This will raise ValueError if we're in an ambiguous datetime\n\t\t# that is not in the fold of the datetime we're converting from.\n\t\tdt = self._coer", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# For this to work, dt must be a datetime with a tzinfo\n\tassert dt.tzinfo is not None\n\t# Determine the first occurrence of this datetime in the given\n\t# timezone.\n\tfirst = tz.normalize_to_first(dt, self.tz)\n\t# See if it's ambiguous.\n\tif first != dt:\n\t\t# If it is, see if we're in a fold state.\n\t\tif self.is_fold(dt):\n\t\t\t# If we are, then we're in the fold state *before* the\n\t\t\t# first occurrence, not after.\n\t\t\tdt = first\n\t\telse:\n\t\t\t# If we're not in a fold state, then we're in the fold\n\t\t\t# state *after* the first occurrence, not before.\n\t\t\tdt = tz.normalize_to_after(dt, self.tz)\n\t# Now, convert to the given timezone.\n\tdt = self.tz.normalize(dt.astimezone(self.to_tz))\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise NonTimezoneAware(dt)\n\tutc_dt = dt.astimezone(pytz.utc)\n\tresult = self.localize(utc_dt, is_dst=None)\n\tif result is None:\n\t\traise AmbiguousTime(dt)\n\tif result.utcoffset() != utc_dt.utcoffset():\n\t\traise FoldTime(dt)\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tutcoffset = dt.utcoffset()\n\n\t# If we're given a naive datetime, assume that it's in UTC.\n\tif utcoffset is None:\n\t\tutcoffset = datetime.timedelta(0)\n\n\t# We'll need to adjust this by the specified offset\n\toffset = utcoffset + self.offset\n\n\t# Create a naive datetime object (i.e. without a tzinfo member)\n\t# representing midnight on the given date.\n\tmidnight = datetime.datetime(dt.year, dt.month, dt.day)\n\n\t# Add the offset to the datetime and return the result.\n\treturn midnight + offset\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"dt must be a naive datetime\")\n\t\n\tutc = dt.astimezone(UTC)\n\tutc_as_local = utc.astimezone(self)\n\t\n\t# Check for ambiguous datetime, fold if necessary\n\tif utc_as_local != utc:\n\t\t# Find the first occurrence (i.e. the one that's not ambiguous)\n\t\tambiguous_deltas = [utc_as_local - utc, utc - utc_as_local]\n\t\t\n\t\t# Find the first occurrence (i.e. the one that's not ambiguous)\n\t\tambiguous_delta = min(ambiguous_deltas)\n\t\t\n\t\tif ambiguous_delta == ambiguous_deltas[0]:\n\t\t\tfold = True\n\t\telse:\n\t\t\tfold = False\n\t\t\n\t\t# Return a folded datetime\n\t\treturn utc + (ambiguous_delta if fold else 0)\n\t\n\t# Return an unambiguous datetime\n\treturn utc_as_local\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# If the datetime is naive, it's probably a 7-hour-ahead\n\t# time.  We should really try to get the tzinfo from the\n\t# timezone, but that's not always going to be available (and\n\t# even if it is, it's not always going to be a\n\t# pytz.FixedOffset object - sometimes it's a datetime.tzinfo\n\t# that's actually a pytz.FixedOffset object, and sometimes it's\n\t# something else).  So, we'll just assume that the datetime is\n\t# 7 hours ahead.\n\tif dt.tzinfo is None:\n\t\toffset = timedelta(hours=7)\n\telse:\n\t\toffset = dt.tzinfo.utcoffset(dt)\n\treturn dt.replace(tzinfo=None) + offset\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"fromutc() requires a tz-aware datetime\")\n\tif dt.tzinfo.utcoffset(dt) is None:\n\t\traise ValueError(\"fromutc() requires a tzinfo with a UTC offset\")\n\tif dt.tzinfo == self:\n\t\treturn dt\n\t# We're in a different timezone.\n\tutcoffset = dt.tzinfo.utcoffset(dt)\n\t# If this is a fold, we need to return the first occurrence of the\n\t# ambiguous datetime.\n\tif utcoffset is not None:\n\t\t# If we're in a fold, this is the first occurrence.\n\t\tif utcoffset != self._transition_info[0][0]:\n\t\t\treturn dt\n\t# We're in a non-fold, non-DST situation.\n\tif utcoffset is None:\n\t\t# If we're in a fold, this is the first occurrence.\n\t\tif dt.hour < self._transition_info[0][0].hour or \\\n\t\t\t(dt.hour == self._transition_info[0][0].hour and\n\t\t\tdt.minute < self._transition_info[0][0].", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# If the datetime is in UTC, then we're done\n\tif dt.tzinfo == self._utc:\n\t\treturn dt\n\n\t# If the datetime is naive, convert it to UTC and return it\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=self._utc)\n\t\treturn self.fromutc(dt)\n\n\t# If the tzinfo is not a DstTzInfo instance, then we have no idea what\n\t# we're dealing with, so let the default implementation handle that.\n\tif not isinstance(dt.tzinfo, DstTzInfo):\n\t\treturn super(AwareTimezone, self).fromutc(dt)\n\n\t# If the datetime is not in a fold state, then we can just do the\n\t# conversion; no special handling required.\n\tif not dt.tzinfo._isdst(dt):\n\t\treturn super(AwareTimezone, self).fromutc(dt)\n\n\t# If the datetime is in a fold state, then we need to determine whether\n\t# the dt is in the folded or non-folded state. We do this by comparing\n\t# the offset before and after the fold. If the offset is the same, then\n\t# we're in", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# If the datetime is naive, we set the ``tzinfo`` parameter\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# If the datetime is naive, we set the ``tzinfo`` parameter\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_before = before\n\tself.cut_after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before_cut = before\n\tself.after_cut = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._cut_chars_before = before\n\tself._cut_chars_after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before_cut = before\n\tself.after_cut = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t...\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_before = before\n\tself.cut_after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before_cut = before\n\tself.after_cut = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._cut_chars_before = before\n\tself._cut_chars_after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before_cut = before\n\tself.after_cut = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t...\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, Request):\n\t\tif request.json and request.json.get('events'):\n\t\t\treturn True\n\t\telif request.get_text(parse_xml=True).tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(request, str):\n\t\tif request.startswith('{'):\n\t\t\ttry:\n\t\t\t\trequest = json.loads(request)\n\t\t\t\tif request.get('events'):\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.exception(e)\n\t\t\t\treturn False\n\t\telse:\n\t\t\ttry:\n\t\t\t\trequest = xml.etree.ElementTree.fromstring(request)\n\t\t\t\tif request.tag == Magic_ENV_TAG:\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.exception(e)\n\t\t\t\treturn False\n\telse:\n\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif request.content_type == 'application/json' and 'events' in request.json:\n\t\treturn True\n\telse:\n\t\tif request.content_type == 'application/xml' and request.get_data(as_text=True).startswith('<{}'.format(Magic_ENV_TAG)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif request.json and 'events' in request.json:\n\t\treturn True\n\telif request.form and request.form.get('tag') == Magic_ENV_TAG:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\nclass API:\n\n\tdef __init__(self, server: ServerType, app: Flask, name: str, version: str, description: str):\n\t\t\"\"\"\n\t\tInitialize the API.\n\t\t\"\"\"\n\t\tself.server = server\n\t\tself.app = app\n\t\tself.name = name\n\t\tself.version = version\n\t\tself.description = description\n\t\tself.endpoints = []\n\t\tself.env_tag = Magic_ENV_TAG\n\n\tdef add_endpoint(self, endpoint: Endpoint):\n\t\t\"\"\"\n\t\tAdd an endpoint to this API.\n\t\t\"\"\"\n\t\tendpoint.api = self\n\t\tself.endpoints.append(endpoint)\n\n\tdef get_request_body(self, request: RequestType) -> str:\n\t\t\"\"\"\n\t\tGet the request body from a Request object.\n\t\t\"\"\"\n\t\tif request.json and 'events' in request.json:\n\t\t\treturn json.dumps(request.json)\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tcontent_type = request.headers.get('Content-Type')\n\tif content_type:\n\t\tif content_type == 'application/json':\n\t\t\tif 'events' in request.json:\n\t\t\t\treturn True\n\t\telif content_type.startswith('text/xml'):\n\t\t\tif request.data.decode('utf-8').startswith('<' + Magic_ENV_TAG + '>'):\n\t\t\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request.json, dict):\n\t\tif 'events' in request.json:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\tif request.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif request.json:\n\t\tif request.json.get(\"events\", None) is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\ttry:\n\t\t\troot = minidom.parseString(request.data).getElementsByTagName(Magic_ENV_TAG)[0]\n\t\t\tif root:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tdata = request.get_data(as_text=True)\n\tif data:\n\t\ttry:\n\t\t\tjson_data = json.loads(data)\n\t\t\tif 'events' in json_data:\n\t\t\t\treturn True\n\t\texcept Exception:\n\t\t\tpass\n\tif 'Magic_ENV_TAG' in data:\n\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, dict):\n\t\tif 'events' in request:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\tif request.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, dict):\n\t\tif 'events' in request:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(request, str):\n\t\tif '<' in request and '>' in request and '</' in request:\n\t\t\tif request.find('<' + Magic_ENV_TAG + '>') != -1 and request.find('</' + Magic_ENV_TAG + '>') != -1:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\telse:\n\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, dict):\n\t\treturn True\n\telif isinstance(request, str):\n\t\tif is_xml(request):\n\t\t\tif \"<\" + Magic_ENV_TAG + \">\" in request:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, Request):\n\t\tif request.json and request.json.get('events'):\n\t\t\treturn True\n\t\telif request.get_text(parse_xml=True).tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(request, str):\n\t\tif request.startswith('{'):\n\t\t\ttry:\n\t\t\t\trequest = json.loads(request)\n\t\t\t\tif request.get('events'):\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.exception(e)\n\t\t\t\treturn False\n\t\telse:\n\t\t\ttry:\n\t\t\t\trequest = xml.etree.ElementTree.fromstring(request)\n\t\t\t\tif request.tag == Magic_ENV_TAG:\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.exception(e)\n\t\t\t\treturn False\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif request.content_type == 'application/json' and 'events' in request.json:\n\t\treturn True\n\telse:\n\t\tif request.content_type == 'application/xml' and request.get_data(as_text=True).startswith('<{}'.format(Magic_ENV_TAG)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif request.json and 'events' in request.json:\n\t\treturn True\n\telif request.form and request.form.get('tag') == Magic_ENV_TAG:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\nclass API:\n\n\tdef __init__(self, server: ServerType, app: Flask, name: str, version: str, description: str):\n\t\t\"\"\"\n\t\tInitialize the API.\n\t\t\"\"\"\n\t\tself.server = server\n\t\tself.app = app\n\t\tself.name = name\n\t\tself.version = version\n\t\tself.description = description\n\t\tself.endpoints = []\n\t\tself.env_tag = Magic_ENV_TAG\n\n\tdef add_endpoint(self, endpoint: Endpoint):\n\t\t\"\"\"\n\t\tAdd an endpoint to this API.\n\t\t\"\"\"\n\t\tendpoint.api = self\n\t\tself.endpoints.append(endpoint)\n\n\tdef get_request_body(self, request: RequestType) -> str:\n\t\t\"\"\"\n\t\tGet the request body from a Request object.\n\t\t\"\"\"\n\t\tif request.json and 'events' in request.json:\n\t\t\treturn json.dumps(request.json)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tcontent_type = request.headers.get('Content-Type')\n\tif content_type:\n\t\tif content_type == 'application/json':\n\t\t\tif 'events' in request.json:\n\t\t\t\treturn True\n\t\telif content_type.startswith('text/xml'):\n\t\t\tif request.data.decode('utf-8').startswith('<' + Magic_ENV_TAG + '>'):\n\t\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request.json, dict):\n\t\tif 'events' in request.json:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\tif request.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif request.json:\n\t\tif request.json.get(\"events\", None) is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\ttry:\n\t\t\troot = minidom.parseString(request.data).getElementsByTagName(Magic_ENV_TAG)[0]\n\t\t\tif root:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tdata = request.get_data(as_text=True)\n\tif data:\n\t\ttry:\n\t\t\tjson_data = json.loads(data)\n\t\t\tif 'events' in json_data:\n\t\t\t\treturn True\n\t\texcept Exception:\n\t\t\tpass\n\tif 'Magic_ENV_TAG' in data:\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, dict):\n\t\tif 'events' in request:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\tif request.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, dict):\n\t\tif 'events' in request:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(request, str):\n\t\tif '<' in request and '>' in request and '</' in request:\n\t\t\tif request.find('<' + Magic_ENV_TAG + '>') != -1 and request.find('</' + Magic_ENV_TAG + '>') != -1:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\tif isinstance(request, dict):\n\t\treturn True\n\telif isinstance(request, str):\n\t\tif is_xml(request):\n\t\t\tif \"<\" + Magic_ENV_TAG + \">\" in request:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\treturn request.json and \"events\" in request.json\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tjson_request = request.get_json().get('events', False)\n\t\treturn bool(json_request)\n\texcept Exception:\n\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tbody = request.json\n\t\tif body['events']:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.get_json() is None:\n\t\treturn False\n\telse:\n\t\treturn True\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.body:\n\t\ttry:\n\t\t\trequest_body = request.json\n\t\texcept ValueError:\n\t\t\treturn False\n\t\telse:\n\t\t\tif isinstance(request_body, list):\n\t\t\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.json and 'events' in request.json:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\nclass Request(RequestType):\n\t\"\"\"\n\tClass to extend RequestType with additional functionality.\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself.__events = []\n\t\tself.__errors = []\n\t\tself.__event_list_count = 0\n\t\tself.__event_count = 0\n\n\t@property\n\tdef event_list_count(self) -> int:\n\t\t\"\"\"\n\t\tGet the number of event lists.\n\t\t:return: The number of event lists.\n\t\t\"\"\"\n\t\treturn self.__event_list_count\n\n\t@property\n\tdef event_count(self) -> int:\n\t\t\"\"\"\n\t\tGet the number of events.\n\t\t:return: The number of events.\n\t\t\"\"\"\n\t\treturn self.__event_count\n\n\t@property\n\tdef events(self) -> List[EventType]:\n\t\t\"\"\"\n\t\tGet the events.\n\t\t:return: The list of events.\n\t\t\"\"\"\n\t\treturn self.__events\n\n\t@property", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif not isinstance(request, Request):\n\t\traise TypeError(f\"Expected type Request, but got {type(request)} instead\")\n\n\ttry:\n\t\tbody = request.json()\n\texcept (TypeError, ValueError):\n\t\traise InvalidRequestBody(\n\t\t\trequest,\n\t\t\t\"Invalid request body. Request body must be valid JSON string.\"\n\t\t)\n\n\tif isinstance(body, str):\n\t\ttry:\n\t\t\tbody = json.loads(body)\n\t\texcept ValueError:\n\t\t\traise InvalidRequestBody(\n\t\t\t\trequest,\n\t\t\t\t\"Invalid request body. Request body must be valid JSON string.\"\n\t\t\t)\n\n\tif isinstance(body, dict):\n\t\tif \"events\" in body:\n\t\t\treturn True\n\n\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_json = request.json\n\texcept:\n\t\treturn False\n\tif \"events\" in request_json:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\treturn \"events\" in request.json\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.method != \"POST\":\n\t\treturn False\n\n\tif request.content_type != \"application/json\":\n\t\treturn False\n\n\ttry:\n\t\tbody = json.loads(request.body)\n\t\tif not isinstance(body, dict):\n\t\t\treturn False\n\texcept json.JSONDecodeError:\n\t\treturn False\n\n\tif \"events\" not in body:\n\t\treturn False\n\n\tif not isinstance(body[\"events\"], list):\n\t\treturn False\n\n\treturn True\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\treturn request.json and \"events\" in request.json\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tjson_request = request.get_json().get('events', False)\n\t\treturn bool(json_request)\n\texcept Exception:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tbody = request.json\n\t\tif body['events']:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.get_json() is None:\n\t\treturn False\n\telse:\n\t\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.body:\n\t\ttry:\n\t\t\trequest_body = request.json\n\t\texcept ValueError:\n\t\t\treturn False\n\t\telse:\n\t\t\tif isinstance(request_body, list):\n\t\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.json and 'events' in request.json:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\nclass Request(RequestType):\n\t\"\"\"\n\tClass to extend RequestType with additional functionality.\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself.__events = []\n\t\tself.__errors = []\n\t\tself.__event_list_count = 0\n\t\tself.__event_count = 0\n\n\t@property\n\tdef event_list_count(self) -> int:\n\t\t\"\"\"\n\t\tGet the number of event lists.\n\t\t:return: The number of event lists.\n\t\t\"\"\"\n\t\treturn self.__event_list_count\n\n\t@property\n\tdef event_count(self) -> int:\n\t\t\"\"\"\n\t\tGet the number of events.\n\t\t:return: The number of events.\n\t\t\"\"\"\n\t\treturn self.__event_count\n\n\t@property\n\tdef events(self) -> List[EventType]:\n\t\t\"\"\"\n\t\tGet the events.\n\t\t:return: The list of events.\n\t\t\"\"\"\n\t\treturn self.__events\n\n\t@property", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif not isinstance(request, Request):\n\t\traise TypeError(f\"Expected type Request, but got {type(request)} instead\")\n\n\ttry:\n\t\tbody = request.json()\n\texcept (TypeError, ValueError):\n\t\traise InvalidRequestBody(\n\t\t\trequest,\n\t\t\t\"Invalid request body. Request body must be valid JSON string.\"\n\t\t)\n\n\tif isinstance(body, str):\n\t\ttry:\n\t\t\tbody = json.loads(body)\n\t\texcept ValueError:\n\t\t\traise InvalidRequestBody(\n\t\t\t\trequest,\n\t\t\t\t\"Invalid request body. Request body must be valid JSON string.\"\n\t\t\t)\n\n\tif isinstance(body, dict):\n\t\tif \"events\" in body:\n\t\t\treturn True\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_json = request.json\n\texcept:\n\t\treturn False\n\tif \"events\" in request_json:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\treturn \"events\" in request.json\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tif request.method != \"POST\":\n\t\treturn False\n\n\tif request.content_type != \"application/json\":\n\t\treturn False\n\n\ttry:\n\t\tbody = json.loads(request.body)\n\t\tif not isinstance(body, dict):\n\t\t\treturn False\n\texcept json.JSONDecodeError:\n\t\treturn False\n\n\tif \"events\" not in body:\n\t\treturn False\n\n\tif not isinstance(body[\"events\"], list):\n\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).isoformat()\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M:%S\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime('%H:%M')\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime('%H:%M')\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).time().strftime(\"%I:%M %p\")\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime('%H:%M')\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn dt.strftime('%I:%M %p')\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).isoformat()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M:%S\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime('%H:%M')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime('%H:%M')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).time().strftime(\"%I:%M %p\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn ensure_timezone(dt).strftime('%H:%M')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\treturn dt.strftime('%I:%M %p')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace('[[', '[').replace(']]', ']')\n\n\tif replacer:\n\t\ttext = replacer(text)\n\n\twhile '[' in text:\n\t\ttry:\n\t\t\ttagidx = text.index('[')\n\t\texcept ValueError:\n\t\t\tbreak\n\t\telse:\n\t\t\ttag = text[tagidx+1:text.index(']', tagidx)]\n\t\t\ttags.add(tag)\n\t\t\ttext = text[:tagidx] + text[text.index(']', tagidx)+1:]\n\n\treturn tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif not replacer:\n\t\treplacer = lambda t: t\n\t\n\ttags = set()\n\t\n\tin_code_block = False\n\t\n\toriginal_text = text\n\ttext = text.replace('\\r', '')\n\t\n\tfor i in range(len(text)):\n\t\tc = text[i]\n\t\t\n\t\tif c == '`' and not in_code_block:\n\t\t\tin_code_block = True\n\t\telif c == '`' and in_code_block:\n\t\t\tin_code_block = False\n\t\telif not in_code_block and c == '#':\n\t\t\tj = i\n\t\t\twhile j < len(text) and text[j] != ' ' and text[j] != '\\n':\n\t\t\t\tj += 1\n\t\t\t\n\t\t\ttag_word = text[i + 1:j].lower()\n\t\t\ttags.add(tag_word)\n\t\t\t\n\t\t\tif replacer:\n\t\t\t\ttext = text[:i] + replacer(tag_word) + text[j:]\n\t\t\t\ti += len(replacer(tag_word)) - 1\n\t\t\n\tif replacer:\n\t\treturn tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tnewtext = text\n\tin_code = False\n\tfor match in re.finditer(r'(\\`{3,})', newtext):\n\t\tin_code = not in_code\n\t\tnewtext = newtext[:match.start()] + '#' * len(match.group(1)) + newtext[match.end():]\n\n\tfor tag in re.findall(r'<\\/?(\\w+)\\b[^>]*>', newtext):\n\t\tif tag not in tags:\n\t\t\ttags.add(tag)\n\t\t\tif replacer:\n\t\t\t\tnewtext = newtext.replace(tag, replacer(tag))\n\n\treturn tags, newtext\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace(\"\\\\[\", \"\u22c6\")\n\ttext = text.replace(\"]\", \"\u22c6\")\n\twhile \"\u22c6\" in text:\n\t\ttag = text.split(\"\u22c6\", 1)[0]\n\t\tif not tag[0] == \"[\" and not tag[-1] == \"]\":\n\t\t\ttag = tag[1:-1]\n\t\t\tif tag in tags:\n\t\t\t\ttag = tag + str(len(tags))\n\t\t\ttags.add(tag)\n\t\t\tif replacer is not None:\n\t\t\t\ttag = replacer(tag)\n\t\t\ttext = text[len(tag) + 2 :]\n\t\telse:\n\t\t\ttext = text[1:]\n\ttext = text.replace(\"\u22c6\", \"]\")\n\tif replacer is not None:\n\t\tfor t in tags:\n\t\t\ttext = text.replace(t, replacer(t))\n\treturn tags, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace(\"\\r\", \"\")\n\n\tfor tag in _TAG_LIST:\n\t\ttag_regex = r\"(?<![\\\\])\\$\" + tag + r\"\\$\"\n\t\tfor match in re.finditer(tag_regex, text):\n\t\t\tif match.group(0) in tags:\n\t\t\t\tprint(f\"DUPLICATE TAG: {match.group(0)}\")\n\t\t\t\tcontinue\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(match.group(0), replacer(match.group(0)))\n\t\t\ttags.add(match.group(0))\n\n\treturn (tags, text)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tis_code_block = False\n\tfor line in text.split('\\n'):\n\t\tif is_code_block:\n\t\t\tif line.startswith('```'):\n\t\t\t\tis_code_block = False\n\t\t\telse:\n\t\t\t\tcontinue\n\t\tif line.startswith('```'):\n\t\t\tis_code_block = True\n\t\t\tcontinue\n\t\tfor tag in TAG_REGEX.findall(line):\n\t\t\tif replacer:\n\t\t\t\ttag = replacer(tag)\n\t\t\ttags.add(tag)\n\treturn tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace('``', '`')\n\ttext = text.replace('`', '``')\n\tif replacer:\n\t\ttext = text.replace('`', replacer)\n\tfor tag in ALL_TAGS:\n\t\tif '`' in tag:\n\t\t\tcontinue\n\t\ttag = tag.replace('`', '')\n\t\ttext = text.replace(tag, f'`{tag}`')\n\ttext = text.replace('`', '')\n\tfor tag in ALL_TAGS:\n\t\tif '`' in tag:\n\t\t\tcontinue\n\t\ttag = tag.replace('`', '')\n\t\tif '`' in tag:\n\t\t\tprint(tag)\n\t\tstart = 0\n\t\twhile True:\n\t\t\tstart = text.find(tag, start)\n\t\t\tif start == -1:\n\t\t\t\tbreak\n\t\t\ttags.add(tag)\n\t\t\tstart += len(tag)\n\tif replacer:\n\t\treturn tags, text\n\treturn tags, text.replace('`', '``')\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tcurrent_tag = \"\"\n\tin_code_block = False\n\toutput = \"\"\n\tfor ch in text:\n\t\tif ch == \"`\":\n\t\t\tin_code_block = not in_code_block\n\t\telif ch == \"[\":\n\t\t\tcurrent_tag = \"\"\n\t\telif ch == \"]\":\n\t\t\tif current_tag and not in_code_block:\n\t\t\t\ttags.add(current_tag)\n\t\t\t\tif replacer:\n\t\t\t\t\toutput += replacer(current_tag)\n\t\t\t\telse:\n\t\t\t\t\toutput += current_tag\n\t\t\t\tcurrent_tag = \"\"\n\t\telif in_code_block:\n\t\t\toutput += ch\n\t\telse:\n\t\t\tcurrent_tag += ch\n\t\t\toutput += ch\n\tif current_tag:\n\t\tif replacer:\n\t\t\toutput += replacer(current_tag)\n\t\telse:\n\t\t\toutput += current_tag\n\treturn tags, output\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.strip()\n\tif not text:\n\t\treturn tags, text\n\tin_code = False\n\tfor line in text.splitlines():\n\t\tif line.startswith(\"```\"):\n\t\t\tin_code = not in_code\n\t\t\tcontinue\n\t\tif in_code:\n\t\t\tcontinue\n\t\tfor word in line.split():\n\t\t\tif word.startswith(\"#\"):\n\t\t\t\ttags.add(word.lstrip(\"#\"))\n\tif replacer is not None:\n\t\ttext = replacer(text)\n\treturn tags, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tnew_text = text\n\tcode_block = False\n\tcode_block_open = False\n\tcode_block_close = False\n\tcode_block_open_line = 0\n\tfor line_index, line in enumerate(text.split(\"\\n\")):\n\t\tif line.strip() == \"```\":\n\t\t\t# This is a new code block.\n\t\t\t# Close the previous one, if any.\n\t\t\tif code_block:\n\t\t\t\tnew_text = new_text.replace(line, \"###\", 1)\n\t\t\t\tcode_block = False\n\t\t\t\tcode_block_open = False\n\t\t\t\tcode_block_close = False\n\t\t\telse:\n\t\t\t\t# This is a new code block.\n\t\t\t\tcode_block = True\n\t\t\t\tcode_block_open = True\n\t\t\t\tcode_block_open_line = line_index\n\t\tif code_block:\n\t\t\t# We're in a code block.\n\t\t\t# Just move to the next line.\n\t\t\tcontinue\n\t\t# We're not in a code block.\n\t\t# We need to check for tags.\n\t\tif code_block_open:\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace('[[', '[').replace(']]', ']')\n\n\tif replacer:\n\t\ttext = replacer(text)\n\n\twhile '[' in text:\n\t\ttry:\n\t\t\ttagidx = text.index('[')\n\t\texcept ValueError:\n\t\t\tbreak\n\t\telse:\n\t\t\ttag = text[tagidx+1:text.index(']', tagidx)]\n\t\t\ttags.add(tag)\n\t\t\ttext = text[:tagidx] + text[text.index(']', tagidx)+1:]\n\n\treturn tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif not replacer:\n\t\treplacer = lambda t: t\n\t\n\ttags = set()\n\t\n\tin_code_block = False\n\t\n\toriginal_text = text\n\ttext = text.replace('\\r', '')\n\t\n\tfor i in range(len(text)):\n\t\tc = text[i]\n\t\t\n\t\tif c == '`' and not in_code_block:\n\t\t\tin_code_block = True\n\t\telif c == '`' and in_code_block:\n\t\t\tin_code_block = False\n\t\telif not in_code_block and c == '#':\n\t\t\tj = i\n\t\t\twhile j < len(text) and text[j] != ' ' and text[j] != '\\n':\n\t\t\t\tj += 1\n\t\t\t\n\t\t\ttag_word = text[i + 1:j].lower()\n\t\t\ttags.add(tag_word)\n\t\t\t\n\t\t\tif replacer:\n\t\t\t\ttext = text[:i] + replacer(tag_word) + text[j:]\n\t\t\t\ti += len(replacer(tag_word)) - 1\n\t\t\n\tif replacer:\n\t\treturn tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tnewtext = text\n\tin_code = False\n\tfor match in re.finditer(r'(\\`{3,})', newtext):\n\t\tin_code = not in_code\n\t\tnewtext = newtext[:match.start()] + '#' * len(match.group(1)) + newtext[match.end():]\n\n\tfor tag in re.findall(r'<\\/?(\\w+)\\b[^>]*>', newtext):\n\t\tif tag not in tags:\n\t\t\ttags.add(tag)\n\t\t\tif replacer:\n\t\t\t\tnewtext = newtext.replace(tag, replacer(tag))\n\n\treturn tags, newtext\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace(\"\\\\[\", \"\u22c6\")\n\ttext = text.replace(\"]\", \"\u22c6\")\n\twhile \"\u22c6\" in text:\n\t\ttag = text.split(\"\u22c6\", 1)[0]\n\t\tif not tag[0] == \"[\" and not tag[-1] == \"]\":\n\t\t\ttag = tag[1:-1]\n\t\t\tif tag in tags:\n\t\t\t\ttag = tag + str(len(tags))\n\t\t\ttags.add(tag)\n\t\t\tif replacer is not None:\n\t\t\t\ttag = replacer(tag)\n\t\t\ttext = text[len(tag) + 2 :]\n\t\telse:\n\t\t\ttext = text[1:]\n\ttext = text.replace(\"\u22c6\", \"]\")\n\tif replacer is not None:\n\t\tfor t in tags:\n\t\t\ttext = text.replace(t, replacer(t))\n\treturn tags, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace(\"\\r\", \"\")\n\n\tfor tag in _TAG_LIST:\n\t\ttag_regex = r\"(?<![\\\\])\\$\" + tag + r\"\\$\"\n\t\tfor match in re.finditer(tag_regex, text):\n\t\t\tif match.group(0) in tags:\n\t\t\t\tprint(f\"DUPLICATE TAG: {match.group(0)}\")\n\t\t\t\tcontinue\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(match.group(0), replacer(match.group(0)))\n\t\t\ttags.add(match.group(0))\n\n\treturn (tags, text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tis_code_block = False\n\tfor line in text.split('\\n'):\n\t\tif is_code_block:\n\t\t\tif line.startswith('```'):\n\t\t\t\tis_code_block = False\n\t\t\telse:\n\t\t\t\tcontinue\n\t\tif line.startswith('```'):\n\t\t\tis_code_block = True\n\t\t\tcontinue\n\t\tfor tag in TAG_REGEX.findall(line):\n\t\t\tif replacer:\n\t\t\t\ttag = replacer(tag)\n\t\t\ttags.add(tag)\n\treturn tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.replace('``', '`')\n\ttext = text.replace('`', '``')\n\tif replacer:\n\t\ttext = text.replace('`', replacer)\n\tfor tag in ALL_TAGS:\n\t\tif '`' in tag:\n\t\t\tcontinue\n\t\ttag = tag.replace('`', '')\n\t\ttext = text.replace(tag, f'`{tag}`')\n\ttext = text.replace('`', '')\n\tfor tag in ALL_TAGS:\n\t\tif '`' in tag:\n\t\t\tcontinue\n\t\ttag = tag.replace('`', '')\n\t\tif '`' in tag:\n\t\t\tprint(tag)\n\t\tstart = 0\n\t\twhile True:\n\t\t\tstart = text.find(tag, start)\n\t\t\tif start == -1:\n\t\t\t\tbreak\n\t\t\ttags.add(tag)\n\t\t\tstart += len(tag)\n\tif replacer:\n\t\treturn tags, text\n\treturn tags, text.replace('`', '``')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tcurrent_tag = \"\"\n\tin_code_block = False\n\toutput = \"\"\n\tfor ch in text:\n\t\tif ch == \"`\":\n\t\t\tin_code_block = not in_code_block\n\t\telif ch == \"[\":\n\t\t\tcurrent_tag = \"\"\n\t\telif ch == \"]\":\n\t\t\tif current_tag and not in_code_block:\n\t\t\t\ttags.add(current_tag)\n\t\t\t\tif replacer:\n\t\t\t\t\toutput += replacer(current_tag)\n\t\t\t\telse:\n\t\t\t\t\toutput += current_tag\n\t\t\t\tcurrent_tag = \"\"\n\t\telif in_code_block:\n\t\t\toutput += ch\n\t\telse:\n\t\t\tcurrent_tag += ch\n\t\t\toutput += ch\n\tif current_tag:\n\t\tif replacer:\n\t\t\toutput += replacer(current_tag)\n\t\telse:\n\t\t\toutput += current_tag\n\treturn tags, output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\ttext = text.strip()\n\tif not text:\n\t\treturn tags, text\n\tin_code = False\n\tfor line in text.splitlines():\n\t\tif line.startswith(\"```\"):\n\t\t\tin_code = not in_code\n\t\t\tcontinue\n\t\tif in_code:\n\t\t\tcontinue\n\t\tfor word in line.split():\n\t\t\tif word.startswith(\"#\"):\n\t\t\t\ttags.add(word.lstrip(\"#\"))\n\tif replacer is not None:\n\t\ttext = replacer(text)\n\treturn tags, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = set()\n\tnew_text = text\n\tcode_block = False\n\tcode_block_open = False\n\tcode_block_close = False\n\tcode_block_open_line = 0\n\tfor line_index, line in enumerate(text.split(\"\\n\")):\n\t\tif line.strip() == \"```\":\n\t\t\t# This is a new code block.\n\t\t\t# Close the previous one, if any.\n\t\t\tif code_block:\n\t\t\t\tnew_text = new_text.replace(line, \"###\", 1)\n\t\t\t\tcode_block = False\n\t\t\t\tcode_block_open = False\n\t\t\t\tcode_block_close = False\n\t\t\telse:\n\t\t\t\t# This is a new code block.\n\t\t\t\tcode_block = True\n\t\t\t\tcode_block_open = True\n\t\t\t\tcode_block_open_line = line_index\n\t\tif code_block:\n\t\t\t# We're in a code block.\n\t\t\t# Just move to the next line.\n\t\t\tcontinue\n\t\t# We're not in a code block.\n\t\t# We need to check for tags.\n\t\tif code_block_open:\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Linkify textual links\n\ttext = linkify(text)\n\n\t# Add target blank to links\n\ttext = re.sub(r\"<a\\s.*href=\\\"(.*?)\\\".*>(.*?)</a>\", r'<a href=\"\\1\" target=\"_blank\">\\2</a>', text)\n\n\t# Add rel=\"noopener noreferrer\" to links\n\ttext = re.sub(r\"<a\\s.*href=\\\"(.*?)\\\".*>\", r'<a href=\"\\1\" rel=\"noopener noreferrer\">', text)\n\n\t# Add class=\"text-link\" to links\n\ttext = re.sub(r\"<a\\s.*href=\\\"(.*?)\\\".*>\", r'<a href=\"\\1\" class=\"text-link\">', text)\n\n\treturn text\n\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# We only need to do this for non-empty text\n\tif text:\n\t\t# Linkify text links like \"http://www.example.com\"\n\t\ttext = linkify(text)\n\n\t\t# Add some attributes to links\n\t\ttext = add_link_attributes(text)\n\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\tfrom urllib.parse import urljoin\n\tfrom . import settings\n\n\t# Find links and add 'href' and 'target' attributes\n\tlinks = []\n\tfor m in re.finditer(r'\\[((?:[^\\[\\]]|(?:\\[\\[.*?\\]))+?)\\]\\(((?:https?|ftp|file):[^\\s\\)']+)\\)', text):\n\t\ttry:\n\t\t\turl = m.group(2)\n\t\t\tlink = m.group(1).strip()\n\t\t\tif settings.url_prefix:\n\t\t\t\turl = urljoin(settings.url_prefix, url)\n\t\t\tlinks.append('<a href=\"%s\" target=\"%s\">%s</a>' % (url, settings.target, link))\n\t\texcept IndexError:\n\t\t\tlinks.append('<a target=\"%s\">%s</a>' % (settings.target, m.group(1).strip()))\n\ttext = re.sub(r'\\[((?:[^\\[\\]]|(?:\\[\\[.*?\\]))+?)\\]\\(((?:https?|ftp|file):[^\\s\\)']+)\\)'\n\t\t\t\t\tr'(?:\\s+title=\"([^\"]+)\")?\\)',\n\t\t", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Linkify all links in text\n\tlinks = re.findall(link_regex, text)\n\tfor link in links:\n\t\tif not link[1]:\n\t\t\ttext = text.replace(link[0], linkify(link[0]))\n\t\telif not link[2]:\n\t\t\ttext = text.replace(link[0], linkify(link[0], link[1]))\n\t\telse:\n\t\t\ttext = text.replace(link[0], linkify(link[0], link[1], link[2]))\n\n\t# Add link attributes to paragraphs\n\tparagraphs = text.split('\\n\\n')\n\tfor paragraph in paragraphs:\n\t\tif paragraph.startswith('<p>'):\n\t\t\tparagraph = paragraph.replace('<p>', '<p class=\"external\">', 1)\n\t\telif paragraph.startswith('<p class=\"internal\"'):\n\t\t\tparagraph = paragraph.replace('<p class=\"internal\"', '<p class=\"internal external\"', 1)\n\t\telse:\n\t\t\tparagraph = '<p class=\"external\">' + paragraph\n\t\ttext = text.replace(paragraph, paragraph.rstrip(), 1)\n\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace(\"[URL]\", \"\")\n\ttext = text.replace(\"[/URL]\", \"\")\n\ttext = text.replace(\"[IMG]\", \"\")\n\ttext = text.replace(\"[/IMG]\", \"\")\n\ttext = text.replace(\"<br>\", \"\\n\")\n\ttext = text.replace(\"[B]\", \"<b>\")\n\ttext = text.replace(\"[/B]\", \"</b>\")\n\ttext = text.replace(\"[I]\", \"<i>\")\n\ttext = text.replace(\"[/I]\", \"</i>\")\n\ttext = text.replace(\"[U]\", \"<u>\")\n\ttext = text.replace(\"[/U]\", \"</u>\")\n\ttext = text.replace(\"[LIST]\", \"<ul><li>\")\n\ttext = text.replace(\"[/LIST]\", \"</li></ul>\")\n\ttext = text.replace(\"[LISTITEM]\", \"<li>\")\n\ttext = text.replace(\"[/LISTITEM]\", \"</li>\")\n\ttext = text.replace(\"[NEWLINE]\", \"<br>\")\n\ttext = text.replace(\"[SPACER]\", \"\")\n\ttext = text.replace(\"[SIZE=30]\", \"<font size=\\\"3\\\">\")\n\ttext = text", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\t# Find links\n\tlinks = re.findall('\\[([^\\]]+)\\]\\(([^\\)]+)\\)', text)\n\t\n\t# Replace links with a linkified version\n\tfor link in links:\n\t\ttext = text.replace('[%s](%s)' % tuple(link), '<a href=\"%s\">%s</a>' % tuple(link))\n\t\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# add a few attributes\n\ttext.add_attributes(\n\t\t'data-foo': 'bar',\n\t\t'data-baz': 'quux',\n\t)\n\t# linkify some textual links\n\ttext.linkify('[link text](http://example.com)')\n\treturn text\n\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif text:\n\t\t# Add title attribute to links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"', r'<a href=\"\\1\" title=\"\\1\"', text)\n\t\t# Add target attribute to links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"', r'<a href=\"\\1\" target=\"_blank\"', text)\n\t\t# Linkify textual links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"', r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n\t\t# Remove empty links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\".+\"\\s*>\\s*</a>', '', text)\n\t\t# Remove empty links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"\\s*>\\s*</a>', r'\\1', text)\n\t\t# Remove empty links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Make sure we have a list.\n\tif not isinstance(text, list):\n\t\ttext = [text]\n\n\tfor i, t in enumerate(text):\n\t\t# Split into lines.\n\t\tlines = t.split('\\n')\n\n\t\t# Process each line.\n\t\tfor j, line in enumerate(lines):\n\t\t\t# Replace textual links with links.\n\t\t\tlines[j] = re.sub(r'([A-Za-z]+://[A-Za-z0-9./\\?=_~%:-]+)', r'<a href=\"\\1\">\\1</a>', line)\n\n\t\t# Join lines back into text.\n\t\ttext[i] = '\\n'.join(lines)\n\n\treturn text\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Linkify textual links\n\ttext = linkify(text)\n\n\t# Add target blank to links\n\ttext = re.sub(r\"<a\\s.*href=\\\"(.*?)\\\".*>(.*?)</a>\", r'<a href=\"\\1\" target=\"_blank\">\\2</a>', text)\n\n\t# Add rel=\"noopener noreferrer\" to links\n\ttext = re.sub(r\"<a\\s.*href=\\\"(.*?)\\\".*>\", r'<a href=\"\\1\" rel=\"noopener noreferrer\">', text)\n\n\t# Add class=\"text-link\" to links\n\ttext = re.sub(r\"<a\\s.*href=\\\"(.*?)\\\".*>\", r'<a href=\"\\1\" class=\"text-link\">', text)\n\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# We only need to do this for non-empty text\n\tif text:\n\t\t# Linkify text links like \"http://www.example.com\"\n\t\ttext = linkify(text)\n\n\t\t# Add some attributes to links\n\t\ttext = add_link_attributes(text)\n\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class=\"link\" href=\"')\n\ttext = text.replace('<a href=\"', '<a class", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\tfrom urllib.parse import urljoin\n\tfrom . import settings\n\n\t# Find links and add 'href' and 'target' attributes\n\tlinks = []\n\tfor m in re.finditer(r'\\[((?:[^\\[\\]]|(?:\\[\\[.*?\\]))+?)\\]\\(((?:https?|ftp|file):[^\\s\\)']+)\\)', text):\n\t\ttry:\n\t\t\turl = m.group(2)\n\t\t\tlink = m.group(1).strip()\n\t\t\tif settings.url_prefix:\n\t\t\t\turl = urljoin(settings.url_prefix, url)\n\t\t\tlinks.append('<a href=\"%s\" target=\"%s\">%s</a>' % (url, settings.target, link))\n\t\texcept IndexError:\n\t\t\tlinks.append('<a target=\"%s\">%s</a>' % (settings.target, m.group(1).strip()))\n\ttext = re.sub(r'\\[((?:[^\\[\\]]|(?:\\[\\[.*?\\]))+?)\\]\\(((?:https?|ftp|file):[^\\s\\)']+)\\)'\n\t\t\t\t\tr'(?:\\s+title=\"([^\"]+)\")?\\)',\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Linkify all links in text\n\tlinks = re.findall(link_regex, text)\n\tfor link in links:\n\t\tif not link[1]:\n\t\t\ttext = text.replace(link[0], linkify(link[0]))\n\t\telif not link[2]:\n\t\t\ttext = text.replace(link[0], linkify(link[0], link[1]))\n\t\telse:\n\t\t\ttext = text.replace(link[0], linkify(link[0], link[1], link[2]))\n\n\t# Add link attributes to paragraphs\n\tparagraphs = text.split('\\n\\n')\n\tfor paragraph in paragraphs:\n\t\tif paragraph.startswith('<p>'):\n\t\t\tparagraph = paragraph.replace('<p>', '<p class=\"external\">', 1)\n\t\telif paragraph.startswith('<p class=\"internal\"'):\n\t\t\tparagraph = paragraph.replace('<p class=\"internal\"', '<p class=\"internal external\"', 1)\n\t\telse:\n\t\t\tparagraph = '<p class=\"external\">' + paragraph\n\t\ttext = text.replace(paragraph, paragraph.rstrip(), 1)\n\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace(\"[URL]\", \"\")\n\ttext = text.replace(\"[/URL]\", \"\")\n\ttext = text.replace(\"[IMG]\", \"\")\n\ttext = text.replace(\"[/IMG]\", \"\")\n\ttext = text.replace(\"<br>\", \"\\n\")\n\ttext = text.replace(\"[B]\", \"<b>\")\n\ttext = text.replace(\"[/B]\", \"</b>\")\n\ttext = text.replace(\"[I]\", \"<i>\")\n\ttext = text.replace(\"[/I]\", \"</i>\")\n\ttext = text.replace(\"[U]\", \"<u>\")\n\ttext = text.replace(\"[/U]\", \"</u>\")\n\ttext = text.replace(\"[LIST]\", \"<ul><li>\")\n\ttext = text.replace(\"[/LIST]\", \"</li></ul>\")\n\ttext = text.replace(\"[LISTITEM]\", \"<li>\")\n\ttext = text.replace(\"[/LISTITEM]\", \"</li>\")\n\ttext = text.replace(\"[NEWLINE]\", \"<br>\")\n\ttext = text.replace(\"[SPACER]\", \"\")\n\ttext = text.replace(\"[SIZE=30]\", \"<font size=\\\"3\\\">\")\n\ttext = text", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\t# Find links\n\tlinks = re.findall('\\[([^\\]]+)\\]\\(([^\\)]+)\\)', text)\n\t\n\t# Replace links with a linkified version\n\tfor link in links:\n\t\ttext = text.replace('[%s](%s)' % tuple(link), '<a href=\"%s\">%s</a>' % tuple(link))\n\t\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# add a few attributes\n\ttext.add_attributes(\n\t\t'data-foo': 'bar',\n\t\t'data-baz': 'quux',\n\t)\n\t# linkify some textual links\n\ttext.linkify('[link text](http://example.com)')\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif text:\n\t\t# Add title attribute to links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"', r'<a href=\"\\1\" title=\"\\1\"', text)\n\t\t# Add target attribute to links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"', r'<a href=\"\\1\" target=\"_blank\"', text)\n\t\t# Linkify textual links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"', r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n\t\t# Remove empty links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\".+\"\\s*>\\s*</a>', '', text)\n\t\t# Remove empty links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([^\"]+)\"\\s*>\\s*</a>', r'\\1', text)\n\t\t# Remove empty links\n\t\ttext = re.sub(r'<a\\s+href\\s*=\\s*\"([", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Make sure we have a list.\n\tif not isinstance(text, list):\n\t\ttext = [text]\n\n\tfor i, t in enumerate(text):\n\t\t# Split into lines.\n\t\tlines = t.split('\\n')\n\n\t\t# Process each line.\n\t\tfor j, line in enumerate(lines):\n\t\t\t# Replace textual links with links.\n\t\t\tlines[j] = re.sub(r'([A-Za-z]+://[A-Za-z0-9./\\?=_~%:-]+)', r'<a href=\"\\1\">\\1</a>', line)\n\n\t\t# Join lines back into text.\n\t\ttext[i] = '\\n'.join(lines)\n\n\treturn text\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(url, headers={'User-Agent': USER_AGENT})\n\t\treturn response.headers.get('content-type', None)\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(e)\n\t\treturn None\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\theaders = {'user-agent': USER_AGENT}\n\t\trequest = urllib.request.Request(url=url, headers=headers)\n\t\tresponse = urllib.request.urlopen(request)\n\t\tcontent_type = response.headers.get_content_type()\n\t\treturn content_type\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code == 200:\n\t\t\treturn None\n\t\telse:\n\t\t\traise\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t# TODO: use try/except\n\treq = urllib.request.Request(url=url, headers={'User-Agent': USER_AGENT})\n\ttry:\n\t\twith urllib.request.urlopen(req, timeout=TIMEOUT) as response:\n\t\t\treturn response.getheader(\"Content-Type\")\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code == 404:\n\t\t\treturn None\n\t\traise e\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, headers={\"User-Agent\": USER_AGENT})\n\t\tresponse = urllib.request.urlopen(request)\n\t\treturn response.info().get_content_type()\n\texcept:\n\t\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(url, headers=HEADERS)\n\t\tresponse.raise_for_status()\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(f\"[ERROR] {e}\")\n\t\treturn None\n\n\treturn response.headers.get('content-type')\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, None, {'User-Agent': USER_AGENT})\n\t\tresponse = urllib.request.urlopen(request)\n\t\tcontent_type = response.getheader(\n\t\t\t'Content-Type').split(';')[0].strip()\n\t\treturn content_type\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code == 404:\n\t\t\treturn None\n\t\telse:\n\t\t\traise e\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treq = urllib.request.Request(url, headers={ 'User-Agent': USER_AGENT })\n\t\tres = urllib.request.urlopen(req)\n\t\treturn res.getheader('Content-Type')\n\texcept:\n\t\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treturn requests.head(url, headers=HEADERS).headers['content-type']\n\texcept:\n\t\treturn None\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(\n\t\t\turl=url,\n\t\t\theaders={'User-Agent': USER_AGENT},\n\t\t\tallow_redirects=True,\n\t\t\ttimeout=5\n\t\t)\n\t\tcontent_type = response.headers.get('Content-Type')\n\t\tif content_type:\n\t\t\t", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(url, headers={\"User-Agent\": USER_AGENT})\n\t\treturn response.headers.get(\"Content-Type\")\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(url, headers={'User-Agent': USER_AGENT})\n\t\treturn response.headers.get('content-type', None)\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(e)\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\theaders = {'user-agent': USER_AGENT}\n\t\trequest = urllib.request.Request(url=url, headers=headers)\n\t\tresponse = urllib.request.urlopen(request)\n\t\tcontent_type = response.headers.get_content_type()\n\t\treturn content_type\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code == 200:\n\t\t\treturn None\n\t\telse:\n\t\t\traise\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t# TODO: use try/except\n\treq = urllib.request.Request(url=url, headers={'User-Agent': USER_AGENT})\n\ttry:\n\t\twith urllib.request.urlopen(req, timeout=TIMEOUT) as response:\n\t\t\treturn response.getheader(\"Content-Type\")\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code == 404:\n\t\t\treturn None\n\t\traise e\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, headers={\"User-Agent\": USER_AGENT})\n\t\tresponse = urllib.request.urlopen(request)\n\t\treturn response.info().get_content_type()\n\texcept:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(url, headers=HEADERS)\n\t\tresponse.raise_for_status()\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(f\"[ERROR] {e}\")\n\t\treturn None\n\n\treturn response.headers.get('content-type')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, None, {'User-Agent': USER_AGENT})\n\t\tresponse = urllib.request.urlopen(request)\n\t\tcontent_type = response.getheader(\n\t\t\t'Content-Type').split(';')[0].strip()\n\t\treturn content_type\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code == 404:\n\t\t\treturn None\n\t\telse:\n\t\t\traise e\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treq = urllib.request.Request(url, headers={ 'User-Agent': USER_AGENT })\n\t\tres = urllib.request.urlopen(req)\n\t\treturn res.getheader('Content-Type')\n\texcept:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treturn requests.head(url, headers=HEADERS).headers['content-type']\n\texcept:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(\n\t\t\turl=url,\n\t\t\theaders={'User-Agent': USER_AGENT},\n\t\t\tallow_redirects=True,\n\t\t\ttimeout=5\n\t\t)\n\t\tcontent_type = response.headers.get('Content-Type')\n\t\tif content_type:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.head(url, headers={\"User-Agent\": USER_AGENT})\n\t\treturn response.headers.get(\"Content-Type\")\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\treturn all([c in LEEGAL_TAG_CHARS for c in tag])\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\treturn all(char in LEEGAL_TAG_CHARS for char in tag)\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\treturn all(character in LEEGAL_TAG_CHARS for character in tag)\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\telse:\n\t\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\treturn all([c in LEEGAL_TAG_CHARS for c in tag])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\treturn all(char in LEEGAL_TAG_CHARS for char in tag)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\treturn all(character in LEEGAL_TAG_CHARS for character in tag)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\telse:\n\t\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict((child.tag, child) for child in node.getchildren())\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = node.findall('*')\n\tchildren_dict = {}\n\tfor child in children:\n\t\tchildren_dict[child.tag] = child\n\treturn children_dict\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn { child.tag : child.text for child in node.getchildren() }\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict([(child.tag, child) for child in node])\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\ttag_name = child.tag\n\t\tchildren[tag_name] = child\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\ttag = child.tag\n\t\tif tag in children:\n\t\t\tchildren[tag] = [children[tag], child]\n\t\telse:\n\t\t\tchildren[tag] = child\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = []\n\tfor child in node.getchildren():\n\t\tchildren.append(child)\n\tchildren = sorted(children, key=lambda x: x.tag)\n\tchildren = dict([(child.tag, child) for child in children])\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tchildren[child.tag] = child\n\treturn children\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict((child.tag, child) for child in node.getchildren())\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict([(child.tag, child) for child in node.getchildren()])\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict((child.tag, child) for child in node.getchildren())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = node.findall('*')\n\tchildren_dict = {}\n\tfor child in children:\n\t\tchildren_dict[child.tag] = child\n\treturn children_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn { child.tag : child.text for child in node.getchildren() }\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict([(child.tag, child) for child in node])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\ttag_name = child.tag\n\t\tchildren[tag_name] = child\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\ttag = child.tag\n\t\tif tag in children:\n\t\t\tchildren[tag] = [children[tag], child]\n\t\telse:\n\t\t\tchildren[tag] = child\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = []\n\tfor child in node.getchildren():\n\t\tchildren.append(child)\n\tchildren = sorted(children, key=lambda x: x.tag)\n\tchildren = dict([(child.tag, child) for child in children])\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tchildren[child.tag] = child\n\treturn children\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict((child.tag, child) for child in node.getchildren())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\treturn dict([(child.tag, child) for child in node.getchildren()])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif entity_handle == sender_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\treturn str(sender_handle) == str(entity_handle)\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\treturn sender_handle == entity_handle\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif entity_handle == sender_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\treturn str(sender_handle) == str(entity_handle)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\treturn sender_handle == entity_handle\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnode_info = {}\n\tnode_info['url'] = url\n\tnode_info['document_path'] = document_path\n\treturn node_info\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\treturn nodeinfo_well_known_document\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\n\tif url.endswith('.well-known/webfinger'):\n\t\tnodeinfo_dict = {\n\t\t\t'url': url,\n\t\t\t'document_path': document_path,\n\t\t\t'type': 'webfinger',\n\t\t}\n\telif url.endswith('.well-known/nodeinfo'):\n\t\tnodeinfo_dict = {\n\t\t\t'url': url,\n\t\t\t'document_path': document_path,\n\t\t\t'type': 'nodeinfo',\n\t\t}\n\telse:\n\t\tnodeinfo_dict = {}\n\treturn nodeinfo_dict\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tdata = {}\n\tdata['url'] = url\n\tif document_path:\n\t\tdata['document_path'] = document_path\n\treturn data\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tresult = get_nodeinfo(url, document_path=document_path)\n\tif result is None:\n\t\treturn None\n\telse:\n\t\tresult['url'] = url\n\t\tresult['document_path'] = document_path\n\t\treturn result\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo = {\n\t\t'url': url,\n\t\t'document_path': document_path\n\t}\n\treturn nodeinfo\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\nclass NodeInfoWellKnownDocument(object):\n\t\"\"\"\n\tThis class is a NodeInfo document that can be used as a base class for other NodeInfo documents,\n\tor as a standalone NodeInfo document.\n\n\tThe well_known_document is a dictionary that can be used to store other information.\n\t\"\"\"\n\tdef __init__(self, well_known_document=None):\n\t\tif well_known_document is None:\n\t\t\twell_known_document = {}\n\t\tself.well_known_document = well_known_document\n\t\tself.well_known_document['@context'] = 'https://www.w3.org/ns/did/v1'\n\n\tdef get_well_known_document(self):\n\t\t\"\"\"\n\t\tReturns the well_known_document.\n\t\t\"\"\"\n\t\treturn self.well_known_document\n\n\tdef set_well_known_document(self, value):\n\t\t\"\"\"\n\t\tSets the well_known_document.\n\t\t\"\"\"\n\t\tself.well_known_document ="], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnode_info = {}\n\tnode_info['url'] = url\n\tnode_info['document_path'] = document_path\n\treturn node_info\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\treturn nodeinfo_well_known_document\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\n\tif url.endswith('.well-known/webfinger'):\n\t\tnodeinfo_dict = {\n\t\t\t'url': url,\n\t\t\t'document_path': document_path,\n\t\t\t'type': 'webfinger',\n\t\t}\n\telif url.endswith('.well-known/nodeinfo'):\n\t\tnodeinfo_dict = {\n\t\t\t'url': url,\n\t\t\t'document_path': document_path,\n\t\t\t'type': 'nodeinfo',\n\t\t}\n\telse:\n\t\tnodeinfo_dict = {}\n\treturn nodeinfo_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tdata = {}\n\tdata['url'] = url\n\tif document_path:\n\t\tdata['document_path'] = document_path\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tresult = get_nodeinfo(url, document_path=document_path)\n\tif result is None:\n\t\treturn None\n\telse:\n\t\tresult['url'] = url\n\t\tresult['document_path'] = document_path\n\t\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo = {\n\t\t'url': url,\n\t\t'document_path': document_path\n\t}\n\treturn nodeinfo\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n\nclass NodeInfoWellKnownDocument(object):\n\t\"\"\"\n\tThis class is a NodeInfo document that can be used as a base class for other NodeInfo documents,\n\tor as a standalone NodeInfo document.\n\n\tThe well_known_document is a dictionary that can be used to store other information.\n\t\"\"\"\n\tdef __init__(self, well_known_document=None):\n\t\tif well_known_document is None:\n\t\t\twell_known_document = {}\n\t\tself.well_known_document = well_known_document\n\t\tself.well_known_document['@context'] = 'https://www.w3.org/ns/did/v1'\n\n\tdef get_well_known_document(self):\n\t\t\"\"\"\n\t\tReturns the well_known_document.\n\t\t\"\"\"\n\t\treturn self.well_known_document\n\n\tdef set_well_known_document(self, value):\n\t\t\"\"\"\n\t\tSets the well_known_document.\n\t\t\"\"\"\n\t\tself.well_known_document =", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tsigned_elements = [\n\t\t'{%s}Sender' % ns.env,\n\t\t'{%s}RelayState' % ns.samlp,\n\t\t'{%s}Assertion' % ns.saml,\n\t\t'{%s}Response' % ns.samlp\n\t]\n\t\n\tif doc.tag not in signed_elements:\n\t\traise ValueError('Document is not a valid signed element')\n\t\n\t# Get the signature algorithm\n\talg = doc.find(\n\t\t'./{%s}Signature/{%s}SignedInfo/{%s}SignatureMethod/{%s}Algorithm' % (\n\t\t\tns.dsig, ns.dsig, ns.dsig, ns.dsig)\n\t).text\n\t\n\t# Get the digest algorithm\n\tdigest_alg = doc.find(\n\t\t'./{%s}Signature/{%s}SignedInfo/{%s}Reference/{%s}DigestMethod/{%s}Algorithm' % (\n\t\t\tns.dsig, ns.dsig, ns.dsig, ns.dsig, ns.dsig)\n\t).text\n\t\n\t# Get the canonicalization algorithm\n\tcanon_alg = doc.find(\n\t\t'./{%s}Sign", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tsignature_element = doc.getroot().find(\"./{}Signature\".format(ns_ds))\n\tsigned_info_element = signature_element.find(\"./{}SignedInfo\".format(ns_ds))\n\tsigned_info_element.remove(signed_info_element.find(\"./{}CanonicalizationMethod\".format(ns_ds)))\n\tsigned_info_element.remove(signed_info_element.find(\"./{}SignatureMethod\".format(ns_ds)))\n\tsigned_info_element.remove(signed_info_element.find(\"./{}Reference\".format(ns_ds)))\n\n\tsigned_info_element.append(etree.Element(\"{}CanonicalizationMethod\".format(ns_ds),\n\t\t\t\t\t\t\t\t\t\t\t nsmap={None: ns_ds},\n\t\t\t\t\t\t\t\t\t\t\t attrib={\"Algorithm\": canonicalization}))\n\tsigned_info_element.append(etree.Element(\"{}SignatureMethod\".format(ns_ds),\n\t\t\t\t\t\t\t\t\t\t\t nsmap={None: ns_ds},\n\t\t\t\t\t\t\t\t\t\t\t attrib={\"Algorithm\": signature_algorithm}))\n\n\treference_element = etree.Element(\"{}Reference\".format(ns_", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t# TODO: Not implemented\n\tpass\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tsigning_method = public_key.get_signing_method()\n\tif signing_method == 'http://www.w3.org/2001/04/xmldsig-more#rsa-sha256':\n\t\tsigning_method = SHA256\n\telif signing_method == 'http://www.w3.org/2001/04/xmldsig-more#rsa-sha512':\n\t\tsigning_method = SHA512\n\telse:\n\t\traise Exception('Unsupported signing method %s' % signing_method)\n\n\t# Verify the signature\n\tverifier = Signature", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key = RSA.importKey(public_key)\n\txml_sig = ds.Signature.fromSignedInfo(doc.documentElement.firstChild)\n\n\tif xml_sig.verify(public_key, signature):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tverifier = DSS.new(public_key, 'SHA-1')\n\txmldoc = minidom.parseString(doc)\n\troot = xmldoc.documentElement\n\tif root.tagName != 'samlp:Response':\n\t\treturn False\n\tel = root.getElementsByTagName('saml:Issuer')[0]\n\tif el is None:\n\t\treturn False\n\tissuer = el.firstChild.data\n\tif issuer != 'https://login.microsoftonline.com/extSTS.srf':\n\t\treturn False\n\tel = root.getElementsByTagName('samlp:Status')[0]\n\tif el is None:\n\t\treturn False\n\tel = el.getElementsByTagName('samlp:StatusCode')[0]\n\tif el is None:\n\t\treturn False\n\tel = el.getElementsByTagName('samlp:StatusCode')[0]\n\tif el is None:\n\t\treturn False\n\tif el.getAttribute('Value') != 'urn:oasis:names:tc:SAML:2.0:status:Success':\n\t\treturn False\n\tel = root.getElementsByTagName('saml:Assertion')[0]\n\tif el is None:\n\t\treturn False\n\tel = el.getElementsByTagName('saml:", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tif not doc.documentElement.tagName == 'Envelope':\n\t\traise Exception('Invalid document - root node is not Envelope')\n\telse:\n\t\tenv = doc.documentElement\n\n\t# Verify the signature of the header\n\theader = env.getElementsByTagName('Header')[0]\n\theader_sig = header.getElementsByTagName('Signature')[0]\n\tsignature_value = header_sig.getElementsByTagName('SignatureValue')[0]\n\tsignature_value_content = signature_value.firstChild.data\n\tsignature_method_alg = header_sig.getElementsByTagName('SignatureMethod')[0].getAttribute('Algorithm')\n\n\t# Verify the public key and signature value\n\tpublic_key.verify(util.decode_base64(signature_value_content), util.decode_base64(signature), signature_method_alg)\n\n\t# Verify the signature of the body\n\tbody = env.getElementsByTagName('Body')[0]\n\tbody_sig = body.getElementsByTagName('Signature')[0]\n\tsignature_value = body_sig.getElementsByTagName('SignatureValue')[0]\n\tsignature_value_content = signature_value.firstChild.data\n\tsignature_method_alg = body_sig.getElementsByTagName('SignatureMethod')[0].get", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key_obj = PublicKey(public_key)\n\ttry:\n\t\tpublic_key_obj.verify(doc, signature)\n\texcept BadSignature:\n\t\traise UnverifiableSignature(\"Bad Signature\")\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t# Construct the document we will verify\n\tdoc_to_verify = minidom.parseString(doc)\n\troot = doc_to_verify.documentElement\n\troot.setAttribute('xmlns:ds', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:dsig', 'urn:oasis:names:tc:dss:1.0:core:schema')\n\troot.setAttribute('xmlns:ns2', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns3', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns4', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns5', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns6', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns7', 'http://www.w3.org/2", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key = rsa.PublicKey.load_pkcs1(public_key)\n\treturn rsa.verify(doc, signature, public_key)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tsigned_elements = [\n\t\t'{%s}Sender' % ns.env,\n\t\t'{%s}RelayState' % ns.samlp,\n\t\t'{%s}Assertion' % ns.saml,\n\t\t'{%s}Response' % ns.samlp\n\t]\n\t\n\tif doc.tag not in signed_elements:\n\t\traise ValueError('Document is not a valid signed element')\n\t\n\t# Get the signature algorithm\n\talg = doc.find(\n\t\t'./{%s}Signature/{%s}SignedInfo/{%s}SignatureMethod/{%s}Algorithm' % (\n\t\t\tns.dsig, ns.dsig, ns.dsig, ns.dsig)\n\t).text\n\t\n\t# Get the digest algorithm\n\tdigest_alg = doc.find(\n\t\t'./{%s}Signature/{%s}SignedInfo/{%s}Reference/{%s}DigestMethod/{%s}Algorithm' % (\n\t\t\tns.dsig, ns.dsig, ns.dsig, ns.dsig, ns.dsig)\n\t).text\n\t\n\t# Get the canonicalization algorithm\n\tcanon_alg = doc.find(\n\t\t'./{%s}Sign", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tsignature_element = doc.getroot().find(\"./{}Signature\".format(ns_ds))\n\tsigned_info_element = signature_element.find(\"./{}SignedInfo\".format(ns_ds))\n\tsigned_info_element.remove(signed_info_element.find(\"./{}CanonicalizationMethod\".format(ns_ds)))\n\tsigned_info_element.remove(signed_info_element.find(\"./{}SignatureMethod\".format(ns_ds)))\n\tsigned_info_element.remove(signed_info_element.find(\"./{}Reference\".format(ns_ds)))\n\n\tsigned_info_element.append(etree.Element(\"{}CanonicalizationMethod\".format(ns_ds),\n\t\t\t\t\t\t\t\t\t\t\t nsmap={None: ns_ds},\n\t\t\t\t\t\t\t\t\t\t\t attrib={\"Algorithm\": canonicalization}))\n\tsigned_info_element.append(etree.Element(\"{}SignatureMethod\".format(ns_ds),\n\t\t\t\t\t\t\t\t\t\t\t nsmap={None: ns_ds},\n\t\t\t\t\t\t\t\t\t\t\t attrib={\"Algorithm\": signature_algorithm}))\n\n\treference_element = etree.Element(\"{}Reference\".format(ns_", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t# TODO: Not implemented\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tsigning_method = public_key.get_signing_method()\n\tif signing_method == 'http://www.w3.org/2001/04/xmldsig-more#rsa-sha256':\n\t\tsigning_method = SHA256\n\telif signing_method == 'http://www.w3.org/2001/04/xmldsig-more#rsa-sha512':\n\t\tsigning_method = SHA512\n\telse:\n\t\traise Exception('Unsupported signing method %s' % signing_method)\n\n\t# Verify the signature\n\tverifier = Signature", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key = RSA.importKey(public_key)\n\txml_sig = ds.Signature.fromSignedInfo(doc.documentElement.firstChild)\n\n\tif xml_sig.verify(public_key, signature):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tverifier = DSS.new(public_key, 'SHA-1')\n\txmldoc = minidom.parseString(doc)\n\troot = xmldoc.documentElement\n\tif root.tagName != 'samlp:Response':\n\t\treturn False\n\tel = root.getElementsByTagName('saml:Issuer')[0]\n\tif el is None:\n\t\treturn False\n\tissuer = el.firstChild.data\n\tif issuer != 'https://login.microsoftonline.com/extSTS.srf':\n\t\treturn False\n\tel = root.getElementsByTagName('samlp:Status')[0]\n\tif el is None:\n\t\treturn False\n\tel = el.getElementsByTagName('samlp:StatusCode')[0]\n\tif el is None:\n\t\treturn False\n\tel = el.getElementsByTagName('samlp:StatusCode')[0]\n\tif el is None:\n\t\treturn False\n\tif el.getAttribute('Value') != 'urn:oasis:names:tc:SAML:2.0:status:Success':\n\t\treturn False\n\tel = root.getElementsByTagName('saml:Assertion')[0]\n\tif el is None:\n\t\treturn False\n\tel = el.getElementsByTagName('saml:", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tif not doc.documentElement.tagName == 'Envelope':\n\t\traise Exception('Invalid document - root node is not Envelope')\n\telse:\n\t\tenv = doc.documentElement\n\n\t# Verify the signature of the header\n\theader = env.getElementsByTagName('Header')[0]\n\theader_sig = header.getElementsByTagName('Signature')[0]\n\tsignature_value = header_sig.getElementsByTagName('SignatureValue')[0]\n\tsignature_value_content = signature_value.firstChild.data\n\tsignature_method_alg = header_sig.getElementsByTagName('SignatureMethod')[0].getAttribute('Algorithm')\n\n\t# Verify the public key and signature value\n\tpublic_key.verify(util.decode_base64(signature_value_content), util.decode_base64(signature), signature_method_alg)\n\n\t# Verify the signature of the body\n\tbody = env.getElementsByTagName('Body')[0]\n\tbody_sig = body.getElementsByTagName('Signature')[0]\n\tsignature_value = body_sig.getElementsByTagName('SignatureValue')[0]\n\tsignature_value_content = signature_value.firstChild.data\n\tsignature_method_alg = body_sig.getElementsByTagName('SignatureMethod')[0].get", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key_obj = PublicKey(public_key)\n\ttry:\n\t\tpublic_key_obj.verify(doc, signature)\n\texcept BadSignature:\n\t\traise UnverifiableSignature(\"Bad Signature\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t# Construct the document we will verify\n\tdoc_to_verify = minidom.parseString(doc)\n\troot = doc_to_verify.documentElement\n\troot.setAttribute('xmlns:ds', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:dsig', 'urn:oasis:names:tc:dss:1.0:core:schema')\n\troot.setAttribute('xmlns:ns2', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns3', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns4', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns5', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns6', 'http://www.w3.org/2000/09/xmldsig#')\n\troot.setAttribute('xmlns:ns7', 'http://www.w3.org/2", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key = rsa.PublicKey.load_pkcs1(public_key)\n\treturn rsa.verify(doc, signature, public_key)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdata = json.loads(document)\n\treturn {\"hcard_url\": data[\"links\"][0][\"href\"]}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\treturn json.loads(document)['links']['hcard_url']\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn webfinger\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn {\"hcard_url\": webfinger[\"links\"][0][\"href\"]}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\treturn document['links'][0]['href']\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdata = json.loads(document)\n\tif 'links' in data:\n\t\tfor link in data['links']:\n\t\t\tif 'rel' in link:\n\t\t\t\tif link['rel'] == 'hcard':\n\t\t\t\t\treturn {'webfinger_url': data['subject'], 'hcard_url': link['href']}\n\treturn {'webfinger_url': data['subject']}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn {\n\t\t\"diaspora_id\": webfinger[\"hcard_url\"]\n\t}\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\treturn json.loads(document)[\"links\"][0][\"href\"]\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger: Dict = json.loads(document)\n\treturn {\n\t\t\"diaspora_id\": webfinger[\"links\"][0][\"href\"]\n\t}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn webfinger\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdata = json.loads(document)\n\treturn {\"hcard_url\": data[\"links\"][0][\"href\"]}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\treturn json.loads(document)['links']['hcard_url']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn webfinger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn {\"hcard_url\": webfinger[\"links\"][0][\"href\"]}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\treturn document['links'][0]['href']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdata = json.loads(document)\n\tif 'links' in data:\n\t\tfor link in data['links']:\n\t\t\tif 'rel' in link:\n\t\t\t\tif link['rel'] == 'hcard':\n\t\t\t\t\treturn {'webfinger_url': data['subject'], 'hcard_url': link['href']}\n\treturn {'webfinger_url': data['subject']}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn {\n\t\t\"diaspora_id\": webfinger[\"hcard_url\"]\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\treturn json.loads(document)[\"links\"][0][\"href\"]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger: Dict = json.loads(document)\n\treturn {\n\t\t\"diaspora_id\": webfinger[\"links\"][0][\"href\"]\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn webfinger\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\twebfinger_doc = retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\telse:\n\t\treturn webfinger_doc\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tlogger.warning(\"Failed to retrieve WebFinger document for: %s\", handle)\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tprint(\"Error retrieving webfinger document: %s\" % e)\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\timport requests\n\timport urllib.parse\n\timport json\n\n\ttry:\n\t\twebfinger = requests.get(f\"https://{handle}/.webfinger\", timeout=10)\n\t\twebfinger.raise_for_status()\n\t\twebfinger_json = webfinger.json()\n\n\t\tif webfinger_json.get(\"properties\"):\n\t\t\treturn webfinger_json[\"properties\"].get(\"http://webfinger.net/rel/profile-page\")\n\texcept Exception:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turl = \"https://webfinger.net/u/\" + handle\n\t\tr = requests.get(url)\n\t\tif r.status_code == 200:\n\t\t\treturn r.text\n\texcept Exception:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tlogger.debug(f\"Trying to retrieve webfinger document for {handle}\")\n\ttry:\n\t\twebfinger_url = f'https://webfinger.net/{handle}'\n\t\twebfinger_doc = requests.get(webfinger_url).text\n\t\treturn webfinger_doc\n\texcept Exception as e:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document for {handle}: {e}\")\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = f\"https://{handle}/.well-known/webfinger?resource={handle}\"\n\ttry:\n\t\tr = requests.get(url, verify=False)\n\t\tif r.status_code", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = 'https://%s/.well-known/webfinger?resource=acct:%s' % (handle, handle)\n\ttry:\n\t\treq = requests.get(url)\n\t\tif req.status_code != 200:\n\t\t\treturn None\n\t\treturn req.text\n\texcept Exception as e:\n\t\tprint('Failed to retrieve webfinger document for handle %s: %s' % (handle, e))\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn try_retrieve_document(f\"https://webfinger.net/{handle}\")\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\twebfinger_doc = retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\telse:\n\t\treturn webfinger_doc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tlogger.warning(\"Failed to retrieve WebFinger document for: %s\", handle)\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tprint(\"Error retrieving webfinger document: %s\" % e)\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\timport requests\n\timport urllib.parse\n\timport json\n\n\ttry:\n\t\twebfinger = requests.get(f\"https://{handle}/.webfinger\", timeout=10)\n\t\twebfinger.raise_for_status()\n\t\twebfinger_json = webfinger.json()\n\n\t\tif webfinger_json.get(\"properties\"):\n\t\t\treturn webfinger_json[\"properties\"].get(\"http://webfinger.net/rel/profile-page\")\n\texcept Exception:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turl = \"https://webfinger.net/u/\" + handle\n\t\tr = requests.get(url)\n\t\tif r.status_code == 200:\n\t\t\treturn r.text\n\texcept Exception:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tlogger.debug(f\"Trying to retrieve webfinger document for {handle}\")\n\ttry:\n\t\twebfinger_url = f'https://webfinger.net/{handle}'\n\t\twebfinger_doc = requests.get(webfinger_url).text\n\t\treturn webfinger_doc\n\texcept Exception as e:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document for {handle}: {e}\")\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = f\"https://{handle}/.well-known/webfinger?resource={handle}\"\n\ttry:\n\t\tr = requests.get(url, verify=False)\n\t\tif r.status_code", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = 'https://%s/.well-known/webfinger?resource=acct:%s' % (handle, handle)\n\ttry:\n\t\treq = requests.get(url)\n\t\tif req.status_code != 200:\n\t\t\treturn None\n\t\treturn req.text\n\texcept Exception as e:\n\t\tprint('Failed to retrieve webfinger document for handle %s: %s' % (handle, e))\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn try_retrieve_document(f\"https://webfinger.net/{handle}\")\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = retrieve_remote_webfinger(handle)\n\tif webfinger is None:\n\t\treturn None\n\t\n\treturn parse_diaspora_webfinger(webfinger)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'http://%s/.well-known/webfinger' % (handle)\n\tdata = urllib.urlopen(url).read()\n\treturn parse_diaspora_webfinger(data)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tif handle[0] == \"@\" and handle[-1] == \"@\":\n\t\tdiaspora_url = \"https://%s.diasporafoundation.org/.well-known/webfinger?resource=%s\" % (handle.split(\"@\")[1], handle)\n\telse:\n\t\tdiaspora_url = \"https://%s/.well-known/webfinger?resource=%s\" % (handle, handle)\n\tr = requests.get(diaspora_url)\n\treturn parse_diaspora_webfinger(r.text)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\treturn webfinger(handle, \"application/atom+xml;type=entry\")\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = retrieve_diaspora_webfinger(handle)\n\tif webfinger:\n\t\treturn parse_diaspora_webfinger(webfinger)\n\treturn None\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom diaspy.utils import json_decode\n\tfrom diaspy.utils import url_parse\n\tfrom diaspy.utils import url_encode\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\t", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\ttry:\n\t\tdata = urllib2.urlopen(WEBFINGER_URI_TEMPLATE % handle)\n\texcept urllib2.URLError:\n\t\traise DiabookError('Could not retrieve diaspora webfinger document')\n\n\tcontent = data.read()\n\tif content.startswith('<html>'):\n\t\traise DiabookError('Could not retrieve diaspora webfinger document')\n\n\treturn parse_diaspora_webfinger(content)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'http://%s/.well-known/webfinger?resource=acct:%s' % (handle, handle)\n\tr = requests.get(url)\n\tif r.status_code != 200:\n\t\traise DiasporaWe", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = webfinger_request(handle)\n\treturn _parse_diaspora_webfinger(webfinger)\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\treturn parse_diaspora_webfinger(urllib2.urlopen(DIASPORA_WEBFINGER_URL.format(handle=handle)))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = retrieve_remote_webfinger(handle)\n\tif webfinger is None:\n\t\treturn None\n\t\n\treturn parse_diaspora_webfinger(webfinger)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'http://%s/.well-known/webfinger' % (handle)\n\tdata = urllib.urlopen(url).read()\n\treturn parse_diaspora_webfinger(data)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tif handle[0] == \"@\" and handle[-1] == \"@\":\n\t\tdiaspora_url = \"https://%s.diasporafoundation.org/.well-known/webfinger?resource=%s\" % (handle.split(\"@\")[1], handle)\n\telse:\n\t\tdiaspora_url = \"https://%s/.well-known/webfinger?resource=%s\" % (handle, handle)\n\tr = requests.get(diaspora_url)\n\treturn parse_diaspora_webfinger(r.text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\treturn webfinger(handle, \"application/atom+xml;type=entry\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = retrieve_diaspora_webfinger(handle)\n\tif webfinger:\n\t\treturn parse_diaspora_webfinger(webfinger)\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tfrom diaspy.utils import json_decode\n\tfrom diaspy.utils import url_parse\n\tfrom diaspy.utils import url_encode\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\tfrom diaspy.utils import url_join\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\ttry:\n\t\tdata = urllib2.urlopen(WEBFINGER_URI_TEMPLATE % handle)\n\texcept urllib2.URLError:\n\t\traise DiabookError('Could not retrieve diaspora webfinger document')\n\n\tcontent = data.read()\n\tif content.startswith('<html>'):\n\t\traise DiabookError('Could not retrieve diaspora webfinger document')\n\n\treturn parse_diaspora_webfinger(content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'http://%s/.well-known/webfinger?resource=acct:%s' % (handle, handle)\n\tr = requests.get(url)\n\tif r.status_code != 200:\n\t\traise DiasporaWe", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = webfinger_request(handle)\n\treturn _parse_diaspora_webfinger(webfinger)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\treturn parse_diaspora_webfinger(urllib2.urlopen(DIASPORA_WEBFINGER_URL.format(handle=handle)))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\turl = 'https://%s/.well-known/host-meta.json' % host\n\ttry:\n\t\tresponse = urllib.request.urlopen(url)\n\texcept urllib.error.URLError:\n\t\traise errors.HostNotFound\n\telse:\n\t\tresponse_url = response.geturl()\n\t\tif not response_url.startswith('https://' + host):\n\t\t\traise errors.HostNotFound\n\t\treturn XRD.from_string(response.read().decode('utf-8'))\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tif host.endswith('/'):\n\t\thost = host[:-1]\n\n\treturn XRD.from_string(urllib2.urlopen('%s/.well-known/host-meta' % (host)).read())\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = requests.get(\"https://{0}/.well-known/host-meta\".format(host))\n\tif host_meta.status_code == 200:\n\t\txrd = XRD.from_xml(host_meta.text)\n\t\txrd.host = host\n\t\txrd.url = host_meta.url\n\t\treturn xrd\n\telse:\n\t\traise Exception(\"Unable to retrieve host-meta from {0}\".format(host))\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treturn retrieve_xrd(host, '/host-meta')\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\ttry:\n\t\tresponse = urllib2.urlopen(host)\n\texcept urllib2.URLError, e:\n\t\traise DiasporaError(str(e))\n\n\treturn parse_host_meta(response.read())\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost = host.strip()\n\tif host.startswith(\"http://\") or host.startswith(\"https://\"):\n\t\thost = host[len(\"http://\"):]\n\t\thost = host[len(\"https://\"):]\n\treturn xrd.retrieve_host_meta(host)\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treturn XRD.from_url(host + \"/.well-known/host-meta.json\")\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treturn XRD.from_url(urlparse.urljoin(host, 'host-meta.xml'))\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t# Host-meta should point to a XRD document\n\txrd = XRD(host.get_url(\"/host-meta.xrd\"))\n\tif xrd.type != \"XRD\":\n\t\traise Exception(\"Invalid host-meta.\")\n\n\t# Validate XRD document\n\tif not xrd.links:\n\t\traise Exception(\"Invalid host-meta.\")\n\n\t# Validate links\n\tvalid_links = [\n\t\t\"diaspora\",\n\t\t\"diaspora/user\",\n\t\t\"diaspora/streams\",\n\t\t\"diaspora/aspects\",\n\t\t\"diaspora/public\",\n\t\t\"diaspora/private\",\n\t\t\"diaspora/private_post\",\n\t\t\"diaspora/contacts\",\n\t\t\"diaspora/contacts/authorize\",\n\t\t\"diaspora/contacts/authorize/accept\",\n\t\t\"diaspora/contacts/authorize/deny\",\n\t\t\"diaspora/contacts/authorize/reject\",\n\t\t\"diaspora/contacts/authorize/unauthorize\",\n\t\t\"diaspora/contacts/authorize/cancel\",\n\t", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tassert(isinstance(host, str))\n\thost = urlparse(host)\n\thost = host.netloc\n\tif host[-1] == '/':\n\t\thost = host[:-1]\n\treturn retrieve_host_meta(host)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\turl = 'https://%s/.well-known/host-meta.json' % host\n\ttry:\n\t\tresponse = urllib.request.urlopen(url)\n\texcept urllib.error.URLError:\n\t\traise errors.HostNotFound\n\telse:\n\t\tresponse_url = response.geturl()\n\t\tif not response_url.startswith('https://' + host):\n\t\t\traise errors.HostNotFound\n\t\treturn XRD.from_string(response.read().decode('utf-8'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tif host.endswith('/'):\n\t\thost = host[:-1]\n\n\treturn XRD.from_string(urllib2.urlopen('%s/.well-known/host-meta' % (host)).read())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = requests.get(\"https://{0}/.well-known/host-meta\".format(host))\n\tif host_meta.status_code == 200:\n\t\txrd = XRD.from_xml(host_meta.text)\n\t\txrd.host = host\n\t\txrd.url = host_meta.url\n\t\treturn xrd\n\telse:\n\t\traise Exception(\"Unable to retrieve host-meta from {0}\".format(host))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treturn retrieve_xrd(host, '/host-meta')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\ttry:\n\t\tresponse = urllib2.urlopen(host)\n\texcept urllib2.URLError, e:\n\t\traise DiasporaError(str(e))\n\n\treturn parse_host_meta(response.read())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost = host.strip()\n\tif host.startswith(\"http://\") or host.startswith(\"https://\"):\n\t\thost = host[len(\"http://\"):]\n\t\thost = host[len(\"https://\"):]\n\treturn xrd.retrieve_host_meta(host)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treturn XRD.from_url(host + \"/.well-known/host-meta.json\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treturn XRD.from_url(urlparse.urljoin(host, 'host-meta.xml'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t# Host-meta should point to a XRD document\n\txrd = XRD(host.get_url(\"/host-meta.xrd\"))\n\tif xrd.type != \"XRD\":\n\t\traise Exception(\"Invalid host-meta.\")\n\n\t# Validate XRD document\n\tif not xrd.links:\n\t\traise Exception(\"Invalid host-meta.\")\n\n\t# Validate links\n\tvalid_links = [\n\t\t\"diaspora\",\n\t\t\"diaspora/user\",\n\t\t\"diaspora/streams\",\n\t\t\"diaspora/aspects\",\n\t\t\"diaspora/public\",\n\t\t\"diaspora/private\",\n\t\t\"diaspora/private_post\",\n\t\t\"diaspora/contacts\",\n\t\t\"diaspora/contacts/authorize\",\n\t\t\"diaspora/contacts/authorize/accept\",\n\t\t\"diaspora/contacts/authorize/deny\",\n\t\t\"diaspora/contacts/authorize/reject\",\n\t\t\"diaspora/contacts/authorize/unauthorize\",\n\t\t\"diaspora/contacts/authorize/cancel\",\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tassert(isinstance(host, str))\n\thost = urlparse(host)\n\thost = host.netloc\n\tif host[-1] == '/':\n\t\thost = host[:-1]\n\treturn retrieve_host_meta(host)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\ttry:\n\t\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\treturn response\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(e)\n\t\treturn False\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\tfrom requests.exceptions import Timeout\n\tfrom requests.exceptions import ConnectionError\n\n\theaders = {\n\t\t'Content-Type': 'application/json'\n\t}\n\n\ttry:\n\t\tresponse = requests.post(\n\t\t\turl,\n\t\t\tdata=data,\n\t\t\theaders=headers,\n\t\t\ttimeout=timeout,\n\t\t\t*args,\n\t\t\t**kwargs\n\t\t)\n\t\treturn response\n\texcept Timeout:\n\t\tprint('Request timed out')\n\t\treturn None\n\texcept ConnectionError:\n\t\tprint('Failed to connect to server')\n\t\treturn None\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\timport json\n\theaders = {'Content-Type': 'application/json'}\n\tdata = json.dumps({'data': data})\n\n\ttry:\n\t\tresponse = requests.post(url, data, headers=headers, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\tprint(\"Timeout\")\n\t\tresponse = None\n\t\t\n\tif response:\n\t\tif response.status_code == 200:\n\t\t\tprint(\"Successful\")\n\t\telse:\n\t\t\tprint(\"Unsuccessful\")\n\t\t\tprint(response.status_code)\n\t\t\tprint(response.text)\n\telse:\n\t\tprint(\"No response from server\")\n\t\t\n\treturn response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\treturn send_request(url, data, timeout=timeout, method=method, *args, **kwargs)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n\texcept Exception as e:\n\t\t", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\treturn send_request(url, data, method, timeout, *args, **kwargs)\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tresp = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\tif resp.status_code != 200:\n\t\traise Exception(f\"Error sending document. Response code {resp.status_code}: {resp.text}\")\n\treturn resp\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\traise\n\texcept requests.exceptions.ConnectionError:\n\t\traise\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tif method == \"post\":\n\t\treturn requests.post(url, data, timeout=timeout, *args, **kwargs)\n\telif method == \"get\":\n\t\treturn requests.get(url, data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\traise ValueError(\"Invalid method.\")\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\treturn send_response(url, data, timeout, method, *args, **kwargs)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\ttry:\n\t\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\treturn response\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(e)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\tfrom requests.exceptions import Timeout\n\tfrom requests.exceptions import ConnectionError\n\n\theaders = {\n\t\t'Content-Type': 'application/json'\n\t}\n\n\ttry:\n\t\tresponse = requests.post(\n\t\t\turl,\n\t\t\tdata=data,\n\t\t\theaders=headers,\n\t\t\ttimeout=timeout,\n\t\t\t*args,\n\t\t\t**kwargs\n\t\t)\n\t\treturn response\n\texcept Timeout:\n\t\tprint('Request timed out')\n\t\treturn None\n\texcept ConnectionError:\n\t\tprint('Failed to connect to server')\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport requests\n\timport json\n\theaders = {'Content-Type': 'application/json'}\n\tdata = json.dumps({'data': data})\n\n\ttry:\n\t\tresponse = requests.post(url, data, headers=headers, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\tprint(\"Timeout\")\n\t\tresponse = None\n\t\t\n\tif response:\n\t\tif response.status_code == 200:\n\t\t\tprint(\"Successful\")\n\t\telse:\n\t\t\tprint(\"Unsuccessful\")\n\t\t\tprint(response.status_code)\n\t\t\tprint(response.text)\n\telse:\n\t\tprint(\"No response from server\")\n\t\t\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\treturn send_request(url, data, timeout=timeout, method=method, *args, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n\texcept Exception as e:\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\treturn send_request(url, data, method, timeout, *args, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tresp = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\tif resp.status_code != 200:\n\t\traise Exception(f\"Error sending document. Response code {resp.status_code}: {resp.text}\")\n\treturn resp\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\traise\n\texcept requests.exceptions.ConnectionError:\n\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tif method == \"post\":\n\t\treturn requests.post(url, data, timeout=timeout, *args, **kwargs)\n\telif method == \"get\":\n\t\treturn requests.get(url, data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\traise ValueError(\"Invalid method.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\treturn send_response(url, data, timeout, method, *args, **kwargs)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor k in keys[:-1]:\n\t\tdic[k] = dic.get(k, {})\n\t\tdic = dic[k]\n\tdic[keys[-1]] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) > 1:\n\t\tif not key in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[1], *keys[1:])\n\telse:\n\t\tdic[key] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not dic:\n\t\tdic = {}\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not key:\n\t\traise ValueError('key cannot be empty')\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif not dic.get(key):\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t# if the key is not set, create it\n\tif key not in dic:\n\t\tdic[key] = {}\n\t# if there are more keys in the chain,\n\t# insert value to the next\n\tif len(keys) > 0:\n\t\tdict_insert(dic[key], val, *keys)\n\t# if the list of keys is empty, insert the value\n\telse:\n\t\tdic[key] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor k in keys:\n\t\tdic = dic.setdefault(k, {})\n\tdic[key] = val\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor k in keys[:-1]:\n\t\tdic[k] = dic.get(k, {})\n\t\tdic = dic[k]\n\tdic[keys[-1]] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) > 1:\n\t\tif not key in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[1], *keys[1:])\n\telse:\n\t\tdic[key] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not dic:\n\t\tdic = {}\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not key:\n\t\traise ValueError('key cannot be empty')\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif not dic.get(key):\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t# if the key is not set, create it\n\tif key not in dic:\n\t\tdic[key] = {}\n\t# if there are more keys in the chain,\n\t# insert value to the next\n\tif len(keys) > 0:\n\t\tdict_insert(dic[key], val, *keys)\n\t# if the list of keys is empty, insert the value\n\telse:\n\t\tdic[key] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor k in keys:\n\t\tdic = dic.setdefault(k, {})\n\tdic[key] = val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tlist_of_file_names = IniType.ListOfFileNames()\n\tfor dir_name in settings_dirs:\n\t\tif dir_name.endswith(os.sep):\n\t\t\tdir_name = dir_name[:-1]\n\t\tlist_dir = os.path.abspath(dir_name)\n\t\tif os.path.exists(list_dir):\n\t\t\tfor file_name in os.listdir(list_dir):\n\t\t\t\tif file_name.endswith(\".ini\"):\n\t\t\t\t\tlist_of_file_names.add_item(os.path.join(list_dir, file_name))\n\t\telse:\n\t\t\tprint(\"Error: The file '{0}' does not exist\".format(list_dir))\n\t\t\tsys.exit(1)\n\tif spec_option:\n\t\tlist_of_file_names.spec_option = spec_option\n\treturn list_of_file_names\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tparser = cli.Parser()\n\tparser.add_argument('-s', '--settings', nargs='*',\n\t\t\t\t\t\thelp=\"The directory(s) to search for settings files in\",\n\t\t\t\t\t\tdefault=settings_dirs)\n\tparser.add_argument('-t', '--type', nargs='?',\n\t\t\t\t\t\thelp=\"The type of settings file to look for\",\n\t\t\t\t\t\tchoices=['file', 'folder'], default='file')\n\tparser.add_argument('-a', '--all', action='store_true', required=False,\n\t\t\t\t\t\thelp=\"Display all settings files found\", default=False)\n\tparser.add_argument('-i', '--include', nargs='*',\n\t\t\t\t\t\thelp=\"List of settings files to include\",\n\t\t\t\t\t\tdefault=[])\n\tparser.add_argument('-e', '--exclude', nargs='*',\n\t\t\t\t\t\thelp=\"List of settings files to exclude\",\n\t\t\t\t\t\tdefault=[])\n\targs = parser.parse_args(spec_option)\n\tparser.process_args(args)\n\treturn parser.IniType(args.settings, args.type, args.all", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tlist_of_file_names = cli.ListOfFileNames(settings_dirs, spec_option)\n\treturn list_of_file_names\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(\n\t\tspec_option,\n\t\tsettings_dirs,\n\t\t\"Settings\",\n\t\t\"settings\",\n\t\t\"settings.ini\",\n\t\t\"settings.json\",\n\t\t\"settings.yaml\",\n\t\t\"settings.yml\",\n\t\t\"settings.toml\",\n\t\t\"settings.properties\",\n\t\t\"settings.prop\",\n\t\t\"settings.xml\",\n\t\t\"settings.proper\",\n\t\t\"settings.config\",\n\t\t\"settings.conf\",\n\t\t\"settings.cnf\",\n\t\t\"settings.settings\",\n\t\t\"settings.props\",\n\t\t\"settings.prop\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t#\n\t# The list_of_file_names is a list of tuples.\n\t# The first value is the string of the file name.\n\t# The second value is the string of the file's directory.\n\t#\n\tlist_of_file_names = []\n\t#\n\t# Loop through each of the settings_dirs and build the list_of_file_names.\n\t#\n\tfor settings_dir in settings_dirs:\n\t\t#\n\t\t# Get the INI file names from the settings_dir\n\t\t#\n\t\t# The spec_option is either 'list' or 'list_long' and is used to determine which of\n\t\t# the two functions to call.\n\t\t#\n\t\t# The list_of_file_names is a list of tuples.\n\t\t# The first value is the string of the file name.\n\t\t# The second value is the string of the file's directory.\n\t\t#\n\t\tlist_of_file_names += cli.ListOfFileNames(settings_dir, spec_option)\n\t#\n\t# Return the list_of_file_names.\n\t#\n\treturn list_of_file_names\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t_list_of_file_names = cli.ListOfFileNames()\n\tif settings_dirs:\n\t\t_list_of_file_names.set_dirs(settings_dirs)\n\tif spec_option:\n\t\t_list_of_file_names.set_spec_option(spec_option)\n\treturn _list_of_file_names\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tlist_of_file_names = IniType.ListOfFileNames()\n\tfor dir_name in settings_dirs:\n\t\tif dir_name.endswith(os.sep):\n\t\t\tdir_name = dir_name[:-1]\n\t\tlist_dir = os.path.abspath(dir_name)\n\t\tif os.path.exists(list_dir):\n\t\t\tfor file_name in os.listdir(list_dir):\n\t\t\t\tif file_name.endswith(\".ini\"):\n\t\t\t\t\tlist_of_file_names.add_item(os.path.join(list_dir, file_name))\n\t\telse:\n\t\t\tprint(\"Error: The file '{0}' does not exist\".format(list_dir))\n\t\t\tsys.exit(1)\n\tif spec_option:\n\t\tlist_of_file_names.spec_option = spec_option\n\treturn list_of_file_names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tparser = cli.Parser()\n\tparser.add_argument('-s', '--settings', nargs='*',\n\t\t\t\t\t\thelp=\"The directory(s) to search for settings files in\",\n\t\t\t\t\t\tdefault=settings_dirs)\n\tparser.add_argument('-t', '--type', nargs='?',\n\t\t\t\t\t\thelp=\"The type of settings file to look for\",\n\t\t\t\t\t\tchoices=['file', 'folder'], default='file')\n\tparser.add_argument('-a', '--all', action='store_true', required=False,\n\t\t\t\t\t\thelp=\"Display all settings files found\", default=False)\n\tparser.add_argument('-i', '--include', nargs='*',\n\t\t\t\t\t\thelp=\"List of settings files to include\",\n\t\t\t\t\t\tdefault=[])\n\tparser.add_argument('-e', '--exclude', nargs='*',\n\t\t\t\t\t\thelp=\"List of settings files to exclude\",\n\t\t\t\t\t\tdefault=[])\n\targs = parser.parse_args(spec_option)\n\tparser.process_args(args)\n\treturn parser.IniType(args.settings, args.type, args.all", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tlist_of_file_names = cli.ListOfFileNames(settings_dirs, spec_option)\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(\n\t\tspec_option,\n\t\tsettings_dirs,\n\t\t\"Settings\",\n\t\t\"settings\",\n\t\t\"settings.ini\",\n\t\t\"settings.json\",\n\t\t\"settings.yaml\",\n\t\t\"settings.yml\",\n\t\t\"settings.toml\",\n\t\t\"settings.properties\",\n\t\t\"settings.prop\",\n\t\t\"settings.xml\",\n\t\t\"settings.proper\",\n\t\t\"settings.config\",\n\t\t\"settings.conf\",\n\t\t\"settings.cnf\",\n\t\t\"settings.settings\",\n\t\t\"settings.props\",\n\t\t\"settings.prop\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.props\",\n\t\t\"settings.", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t#\n\t# The list_of_file_names is a list of tuples.\n\t# The first value is the string of the file name.\n\t# The second value is the string of the file's directory.\n\t#\n\tlist_of_file_names = []\n\t#\n\t# Loop through each of the settings_dirs and build the list_of_file_names.\n\t#\n\tfor settings_dir in settings_dirs:\n\t\t#\n\t\t# Get the INI file names from the settings_dir\n\t\t#\n\t\t# The spec_option is either 'list' or 'list_long' and is used to determine which of\n\t\t# the two functions to call.\n\t\t#\n\t\t# The list_of_file_names is a list of tuples.\n\t\t# The first value is the string of the file name.\n\t\t# The second value is the string of the file's directory.\n\t\t#\n\t\tlist_of_file_names += cli.ListOfFileNames(settings_dir, spec_option)\n\t#\n\t# Return the list_of_file_names.\n\t#\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t_list_of_file_names = cli.ListOfFileNames()\n\tif settings_dirs:\n\t\t_list_of_file_names.set_dirs(settings_dirs)\n\tif spec_option:\n\t\t_list_of_file_names.set_spec_option(spec_option)\n\treturn _list_of_file_names\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().ANSIBLE_CONFIG_MANAGER\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tif not getattr(cls, 'service_name', None):\n\t\treturn None\n\tservice = cls._get_service()\n\tif not service:\n\t\treturn None\n\treturn service.get_ansible_config_manager()\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER.value)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.ANSIBLE_CONFIG_MANAGER)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.ANSIBLE_CONFIG_MANAGER)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().ANSIBLE_CONFIG_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tif not getattr(cls, 'service_name', None):\n\t\treturn None\n\tservice = cls._get_service()\n\tif not service:\n\t\treturn None\n\treturn service.get_ansible_config_manager()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER.value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().WORKSPACE_MANAGER\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom ..WorkspaceManager import WorkspaceManager\n\treturn WorkspaceManager\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif WORKSPACE_MANAGER in service:\n\t\treturn service[WORKSPACE_MANAGER]\n\treturn None\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().WORKSPACE_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom ..WorkspaceManager import WorkspaceManager\n\treturn WorkspaceManager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif WORKSPACE_MANAGER in service:\n\t\treturn service[WORKSPACE_MANAGER]\n\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service('PLUGINS_MANAGER')\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t@property\n\tdef plugins_manager(self):\n\t\treturn self._get_service(self.__class__.PLUGINS_MANAGER)\n\treturn plugins_manager\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service('PLUGINS_MANAGER')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t@property\n\tdef plugins_manager(self):\n\t\treturn self._get_service(self.__class__.PLUGINS_MANAGER)\n\treturn plugins_manager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\treturn cls._get_service().PLUGINS_MANAGER\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Spec content can't be None\")\n\n\tif not spec_content.get(\"version\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'version' field\")\n\n\tif not spec_content.get(\"metadata\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"name\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.name' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"version\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.version' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"description\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.description' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"maintainers\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.maintainers' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"maintainers\", []):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.m", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = cls.load_spec_content(spec_content)\n\tcls.validate_required_fields(spec_data)\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = {}\n\tif not spec_content:\n\t\traise IRValidatorException(\"No spec content\")\n\n\t# convert YAML to json\n\ttry:\n\t\tspec = yaml.load(spec_content, Loader=yaml.FullLoader)\n\texcept Exception as e:\n\t\traise IRValidatorException(\"Invalid YAML spec file: \" + str(e))\n\n\tif not spec.get(\"title\"):\n\t\traise IRValidatorException(\"No title in spec file\")\n\n\tif not spec.get(\"version\"):\n\t\traise IRValidatorException(\"No version in spec file\")\n\n\tif not spec.get(\"description\"):\n\t\traise IRValidatorException(\"No description in spec file\")\n\n\tif not spec.get(\"license\"):\n\t\traise IRValidatorException(\"No license in spec file\")\n\n\tif not spec.get(\"provider\").get(\"name\"):\n\t\traise IRValidatorException(\"No provider name in spec file\")\n\n\tif not spec.get(\"provider\").get(\"display_name\"):\n\t\traise IRValidatorException(\"No provider display_name in spec file\")\n\n\tif not spec.get(\"provider\").get(\"url\"):\n\t\traise IRValidatorException(\"No provider url in spec file\")\n\n\tif not spec.get(\"support\"):\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_content = spec_content\n\tif not spec_content:\n\t\traise IRValidatorException('Invalid Data')\n\telse:\n\t\tspec_content = spec_content.replace('\\n', '')\n\t\tspec_content = spec_content.replace('\\r', '')\n\t\ttry:\n\t\t\tspec_data = yaml.safe_load(spec_content)\n\t\t\tif not spec_data:\n\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\telse:\n\t\t\t\tfor key in cls.MANDATORY_FIELDS:\n\t\t\t\t\tif key not in spec_data:\n\t\t\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\t\t\telse:\n\t\t\t\t\t\tif key == 'name':\n\t\t\t\t\t\t\tif not spec_data[key]:\n\t\t\t\t\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\t\t\t\tif key == 'version':\n\t\t\t\t\t\t\tif not spec_data[key]:\n\t\t\t\t\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\t\t\t\tif key == 'description':\n\t\t\t\t\t\t\tif not spec_data[key]:\n\t\t\t\t\t\t", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec = yaml.load(spec_content)\n\t\tspec = spec.get('spec', {})\n\texcept:\n\t\traise IRValidatorException('Invalid spec file format.')\n\n\tcontent_type = spec.get('content_type')\n\tif not content_type:\n\t\traise IRValidatorException('\"content_type\" is a mandatory field.')\n\n\tcontent = spec.get('content')\n\tif not content:\n\t\traise IRValidatorException('\"content\" is a mandatory field.')\n\n\treturn spec\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = yaml.safe_load(spec_content)\n\tif not spec_data:\n\t\traise IRValidatorException(\"The spec file is empty\")\n\tif 'schema' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'schema' key\")\n\tif 'version' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'version' key\")\n\tif 'description' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'description' key\")\n\tif 'component' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'component' key\")\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException('Content of the spec (YAML) file is empty')\n\ttry:\n\t\tspec_dict = yaml.safe_load(spec_content)\n\t\tif not isinstance(spec_dict, dict):\n\t\t\traise IRValidatorException('Content of the spec (YAML) file is not a YAML dictionary')\n\n\t\trequired_fields = ['name', 'version', 'description', 'type', 'spec_version', 'metadata', 'data']\n\t\tfor field in required_fields:\n\t\t\tif field not in spec_dict:\n\t\t\t\traise IRValidatorException('YAML file is missing required field: %s' % field)\n\n\t\tif spec_dict['type'] != cls.IR_TYPE:\n\t\t\traise IRValidatorException('YAML file type is not %s' % cls.IR_TYPE)\n\n\t\tif spec_dict['spec_version'] != cls.IR_VERSION:\n\t\t\traise IRValidatorException('YAML file spec_version is not %s' % cls.IR_VERSION)\n\n\t\tif not isinstance(spec_dict['data'], dict):\n\t\t\traise IRValidatorException('YAML file data is not a YAML dictionary')\n\n\t\t", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec = yaml.load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Error parsing spec file: {}\".format(exc))\n\n\tif not spec:\n\t\traise IRValidatorException(\"Spec file is empty\")\n\n\tif not isinstance(spec, dict):\n\t\traise IRValidatorException(\"Invalid format. Dictionary required\")\n\n\tif \"provider\" not in spec:\n\t\traise IRValidatorException(\"Key 'provider' is missing in specification file\")\n\n\tif isinstance(spec[\"provider\"], dict):\n\t\tif \"name\" not in spec[\"provider\"]:\n\t\t\traise IRValidatorException(\"Key 'provider.name' is missing in specification file\")\n\t\tif not isinstance(spec[\"provider\"][\"name\"], str):\n\t\t\traise IRValidatorException(\"Key 'provider.name' must be a string\")\n\n\tif not isinstance(spec[\"provider\"][\"name\"], str):\n\t\traise IRValidatorException(\"Key 'provider.name' must be a string\")\n\n\tif \"service\" not in spec:\n\t\traise IRValidatorException(\"Key 'service' is missing in specification file\")\n\n\tif not isinstance(spec[\"service\"], dict):\n\t\traise IRValidatorException(\"Key 'service'", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\t\tif spec_data is None:\n\t\t\traise IRValidatorException(\"spec_data is None\")\n\t\t# TODO: validate it is a dictionary\n\t\tif 'metadata' not in spec_data:\n\t\t\traise IRValidatorException('metadata is missing from spec')\n\t\tif 'name' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.name is missing from spec')\n\t\tif 'version' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.version is missing from spec')\n\t\tif 'schema_version' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.schema_version is missing from spec')\n\t\tif 'description' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.description is missing from spec')\n\t\tif 'license' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.license is missing from spec')\n\t\tif 'valid_versions' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.valid_versions is missing from spec')\n\t\tif 'valid_from", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t# parse content to dictionary\n\tspec_dict = yaml.safe_load(spec_content)\n\t# check mandatory fields\n\tfor field in cls.MANDATORY_FIELDS:\n\t\tif field not in spec_dict or not spec_dict[field]:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Mandatory field '%s' is missing in spec file.\" % field)\n\tif not spec_dict.get(\"name\"):\n\t\tspec_dict[\"name\"] = os.path.splitext(os.path.basename(spec_content))[0]\n\t# check \"version\" field\n\tif spec_dict.get(\"version\"):\n\t\ttry:\n\t\t\tspec_dict[\"version\"] = Version.coerce(spec_dict[\"version\"])\n\t\texcept ValueError as e:\n\t\t\traise IRValidatorException(\"'version' field is not valid version: \"\n\t\t\t\t\t\t\t\t\t  \"%s\" % e)\n\t# check if \"metadata\" field is present\n\tif not spec_dict.get(\"metadata\"):\n\t\traise IRValidatorException(\"'metadata' field is missing in spec file.\")\n\t# check if \"metadata.name\" field is present\n\tif not spec_dict[\"metadata\"].get(\""], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Spec content can't be None\")\n\n\tif not spec_content.get(\"version\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'version' field\")\n\n\tif not spec_content.get(\"metadata\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"name\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.name' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"version\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.version' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"description\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.description' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"maintainers\", False):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.maintainers' field\")\n\n\tif not spec_content.get(\"metadata\", {}).get(\"maintainers\", []):\n\t\traise IRValidatorException(\"Spec content missing mandatory 'metadata.m", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = cls.load_spec_content(spec_content)\n\tcls.validate_required_fields(spec_data)\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = {}\n\tif not spec_content:\n\t\traise IRValidatorException(\"No spec content\")\n\n\t# convert YAML to json\n\ttry:\n\t\tspec = yaml.load(spec_content, Loader=yaml.FullLoader)\n\texcept Exception as e:\n\t\traise IRValidatorException(\"Invalid YAML spec file: \" + str(e))\n\n\tif not spec.get(\"title\"):\n\t\traise IRValidatorException(\"No title in spec file\")\n\n\tif not spec.get(\"version\"):\n\t\traise IRValidatorException(\"No version in spec file\")\n\n\tif not spec.get(\"description\"):\n\t\traise IRValidatorException(\"No description in spec file\")\n\n\tif not spec.get(\"license\"):\n\t\traise IRValidatorException(\"No license in spec file\")\n\n\tif not spec.get(\"provider\").get(\"name\"):\n\t\traise IRValidatorException(\"No provider name in spec file\")\n\n\tif not spec.get(\"provider\").get(\"display_name\"):\n\t\traise IRValidatorException(\"No provider display_name in spec file\")\n\n\tif not spec.get(\"provider\").get(\"url\"):\n\t\traise IRValidatorException(\"No provider url in spec file\")\n\n\tif not spec.get(\"support\"):\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_content = spec_content\n\tif not spec_content:\n\t\traise IRValidatorException('Invalid Data')\n\telse:\n\t\tspec_content = spec_content.replace('\\n', '')\n\t\tspec_content = spec_content.replace('\\r', '')\n\t\ttry:\n\t\t\tspec_data = yaml.safe_load(spec_content)\n\t\t\tif not spec_data:\n\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\telse:\n\t\t\t\tfor key in cls.MANDATORY_FIELDS:\n\t\t\t\t\tif key not in spec_data:\n\t\t\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\t\t\telse:\n\t\t\t\t\t\tif key == 'name':\n\t\t\t\t\t\t\tif not spec_data[key]:\n\t\t\t\t\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\t\t\t\tif key == 'version':\n\t\t\t\t\t\t\tif not spec_data[key]:\n\t\t\t\t\t\t\t\traise IRValidatorException('Invalid Data')\n\t\t\t\t\t\tif key == 'description':\n\t\t\t\t\t\t\tif not spec_data[key]:\n\t\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec = yaml.load(spec_content)\n\t\tspec = spec.get('spec', {})\n\texcept:\n\t\traise IRValidatorException('Invalid spec file format.')\n\n\tcontent_type = spec.get('content_type')\n\tif not content_type:\n\t\traise IRValidatorException('\"content_type\" is a mandatory field.')\n\n\tcontent = spec.get('content')\n\tif not content:\n\t\traise IRValidatorException('\"content\" is a mandatory field.')\n\n\treturn spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = yaml.safe_load(spec_content)\n\tif not spec_data:\n\t\traise IRValidatorException(\"The spec file is empty\")\n\tif 'schema' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'schema' key\")\n\tif 'version' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'version' key\")\n\tif 'description' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'description' key\")\n\tif 'component' not in spec_data:\n\t\traise IRValidatorException(\"The spec file is missing 'component' key\")\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException('Content of the spec (YAML) file is empty')\n\ttry:\n\t\tspec_dict = yaml.safe_load(spec_content)\n\t\tif not isinstance(spec_dict, dict):\n\t\t\traise IRValidatorException('Content of the spec (YAML) file is not a YAML dictionary')\n\n\t\trequired_fields = ['name', 'version', 'description', 'type', 'spec_version', 'metadata', 'data']\n\t\tfor field in required_fields:\n\t\t\tif field not in spec_dict:\n\t\t\t\traise IRValidatorException('YAML file is missing required field: %s' % field)\n\n\t\tif spec_dict['type'] != cls.IR_TYPE:\n\t\t\traise IRValidatorException('YAML file type is not %s' % cls.IR_TYPE)\n\n\t\tif spec_dict['spec_version'] != cls.IR_VERSION:\n\t\t\traise IRValidatorException('YAML file spec_version is not %s' % cls.IR_VERSION)\n\n\t\tif not isinstance(spec_dict['data'], dict):\n\t\t\traise IRValidatorException('YAML file data is not a YAML dictionary')\n\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec = yaml.load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Error parsing spec file: {}\".format(exc))\n\n\tif not spec:\n\t\traise IRValidatorException(\"Spec file is empty\")\n\n\tif not isinstance(spec, dict):\n\t\traise IRValidatorException(\"Invalid format. Dictionary required\")\n\n\tif \"provider\" not in spec:\n\t\traise IRValidatorException(\"Key 'provider' is missing in specification file\")\n\n\tif isinstance(spec[\"provider\"], dict):\n\t\tif \"name\" not in spec[\"provider\"]:\n\t\t\traise IRValidatorException(\"Key 'provider.name' is missing in specification file\")\n\t\tif not isinstance(spec[\"provider\"][\"name\"], str):\n\t\t\traise IRValidatorException(\"Key 'provider.name' must be a string\")\n\n\tif not isinstance(spec[\"provider\"][\"name\"], str):\n\t\traise IRValidatorException(\"Key 'provider.name' must be a string\")\n\n\tif \"service\" not in spec:\n\t\traise IRValidatorException(\"Key 'service' is missing in specification file\")\n\n\tif not isinstance(spec[\"service\"], dict):\n\t\traise IRValidatorException(\"Key 'service'", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\t\tif spec_data is None:\n\t\t\traise IRValidatorException(\"spec_data is None\")\n\t\t# TODO: validate it is a dictionary\n\t\tif 'metadata' not in spec_data:\n\t\t\traise IRValidatorException('metadata is missing from spec')\n\t\tif 'name' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.name is missing from spec')\n\t\tif 'version' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.version is missing from spec')\n\t\tif 'schema_version' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.schema_version is missing from spec')\n\t\tif 'description' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.description is missing from spec')\n\t\tif 'license' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.license is missing from spec')\n\t\tif 'valid_versions' not in spec_data['metadata']:\n\t\t\traise IRValidatorException('metadata.valid_versions is missing from spec')\n\t\tif 'valid_from", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t# parse content to dictionary\n\tspec_dict = yaml.safe_load(spec_content)\n\t# check mandatory fields\n\tfor field in cls.MANDATORY_FIELDS:\n\t\tif field not in spec_dict or not spec_dict[field]:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Mandatory field '%s' is missing in spec file.\" % field)\n\tif not spec_dict.get(\"name\"):\n\t\tspec_dict[\"name\"] = os.path.splitext(os.path.basename(spec_content))[0]\n\t# check \"version\" field\n\tif spec_dict.get(\"version\"):\n\t\ttry:\n\t\t\tspec_dict[\"version\"] = Version.coerce(spec_dict[\"version\"])\n\t\texcept ValueError as e:\n\t\t\traise IRValidatorException(\"'version' field is not valid version: \"\n\t\t\t\t\t\t\t\t\t  \"%s\" % e)\n\t# check if \"metadata\" field is present\n\tif not spec_dict.get(\"metadata\"):\n\t\traise IRValidatorException(\"'metadata' field is missing in spec file.\")\n\t# check if \"metadata.name\" field is present\n\tif not spec_dict[\"metadata\"].get(\"", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as file:\n\t\tdata_file = yaml.load(file, Loader=yaml.FullLoader)\n\t\tdata_file = cls.validate(data_file)\n\treturn data_file\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t# Load data from YAML file\n\tyaml_data = cls.load_from_file(yaml_file)\n\n\t# Validate data\n\tcls.validate(yaml_data)\n\n\treturn yaml_data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\tyaml_file = cls.DEFAULT_FILE\n\n\ttry:\n\t\twith open(yaml_file) as f:\n\t\t\tyaml_data = yaml.load(f)\n\texcept yaml.YAMLError, exc:\n\t\tprint(exc)\n\t\traise IRValidatorException(\n\t\t\t\"Unable to load YAML file {0}\".format(yaml_file)\n\t\t)\n\n\ttry:\n\t\tyaml_data = cls.validate(yaml_data, yaml_file)\n\texcept IRValidatorException, exc:\n\t\traise IRValidatorException(\n\t\t\t\"Unable to validate YAML file {0}. Error: {1}\".format(\n\t\t\t\tyaml_file,\n\t\t\t\texc.message\n\t\t\t)\n\t\t)\n\n\treturn yaml_data\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\tyaml_file = cls.yaml_file\n\twith open(yaml_file, 'r') as yaml_stream:\n\t\tyaml_data = yaml.load(yaml_stream)\n\n\tcls.validate_from_dict(yaml_data)\n\treturn yaml_data\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException('File not found: %s' % yaml_file)\n\n\twith open(yaml_file, 'rb') as f:\n\t\tyaml_data = yaml.load(f)\n\n\tfor key in cls.required_keys:\n\t\tif key not in yaml_data:\n\t\t\traise IRValidatorException('Required key \"%s\" not found in %s' % (key, yaml_file))\n\n\treturn yaml_data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file) as file:\n\t\tdata = yaml.load(file)\n\t\t\n\t\t# Check if all mandatory fields are present\n\t\tfor field in cls.REQUIRED_FIELDS:\n\t\t\tif field not in data:\n\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\"Could not find required field '{}' in YAML file.\".format(\n\t\t\t\t\t\tfield\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\n\t\t# Validate each field\n\t\tfor field in data:\n\t\t\tif field not in cls.REQUIRED_FIELDS:\n\t\t\t\tif field not in cls.OPTIONAL_FIELDS:\n\t\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\t\"Could not find field '{}' in YAML file.\".format(\n\t\t\t\t\t\t\tfield\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tcls.validate_optional_field(field, data[field])\n\t\t\telse:\n\t\t\t\tcls.validate_required_field(field, data[field])\n\t\t\n\t\treturn data\n\ndef", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tyaml_data = load_yaml(yml_file=yaml_file)\n\tif not yaml_data:\n\t\traise IRValidatorException(\"Empty YAML file\")\n\n\ttry:\n\t\tinstance = cls()\n\t\tinstance.validate(yaml_data)\n\t\treturn yaml_data\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(e)\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\traise IRValidatorException('YAML file not specified')\n\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException('YAML file does not exist')\n\n\ttry:\n\t\twith open(yaml_file, \"r\") as stream:\n\t\t\tdata = yaml.load(stream)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(exc)\n\n\tvalidation_data = cls.validate(data)\n\treturn validation_data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file) as f:\n\t\ttry:\n\t\t\tdata = yaml.safe_load(f)\n\t\texcept yaml.YAMLError as e:\n\t\t\tprint(e)\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Failed to parse YAML file. Please check if the file is valid YAML.\")\n\t\tif not data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"File is empty. Please add data to the file.\")\n\t\tif 'inputs' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'inputs' section to the file. Inputs section is mandatory.\")\n\t\tif 'outputs' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'outputs' section to the file. Outputs section is mandatory.\")\n\t\tif 'parameters' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'parameters' section to the file. Parameters section is mandatory.\")\n\t\tif 'components' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'components' section to the file. Components section is mandatory.\")\n\t\tif 'graph'", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise IRValidatorException('YAML file is missing')\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_config = yaml.load(f, Loader=yaml.FullLoader)\n\texcept IOError:\n\t\traise IRValidatorException('YAML file not found')\n\texcept yaml.YAMLError:\n\t\traise IRValidatorException('YAML file is not valid')\n\n\t# Check if it is a valid config by checking required fields\n\tif not yaml_config.get('name'):\n\t\traise IRValidatorException('name is missing')\n\tif not yaml_config.get('type'):\n\t\traise IRValidatorException('type is missing')\n\tif not yaml_config.get('version'):\n\t\traise IRValidatorException('version is missing')\n\tif not yaml_config.get('spec'):\n\t\tyaml_config['spec'] = {}\n\tif not yaml_config.get('metadata'):\n\t\tyaml_config['metadata'] = {}\n\n\tif yaml_config['type'] == 'cluster':\n\t\tif not yaml_config.get('spec').get('cluster_name'):\n\t\t\tyaml_config['spec']['cluster_name'] = yaml"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as file:\n\t\tdata_file = yaml.load(file, Loader=yaml.FullLoader)\n\t\tdata_file = cls.validate(data_file)\n\treturn data_file\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t# Load data from YAML file\n\tyaml_data = cls.load_from_file(yaml_file)\n\n\t# Validate data\n\tcls.validate(yaml_data)\n\n\treturn yaml_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\tyaml_file = cls.DEFAULT_FILE\n\n\ttry:\n\t\twith open(yaml_file) as f:\n\t\t\tyaml_data = yaml.load(f)\n\texcept yaml.YAMLError, exc:\n\t\tprint(exc)\n\t\traise IRValidatorException(\n\t\t\t\"Unable to load YAML file {0}\".format(yaml_file)\n\t\t)\n\n\ttry:\n\t\tyaml_data = cls.validate(yaml_data, yaml_file)\n\texcept IRValidatorException, exc:\n\t\traise IRValidatorException(\n\t\t\t\"Unable to validate YAML file {0}. Error: {1}\".format(\n\t\t\t\tyaml_file,\n\t\t\t\texc.message\n\t\t\t)\n\t\t)\n\n\treturn yaml_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\tyaml_file = cls.yaml_file\n\twith open(yaml_file, 'r') as yaml_stream:\n\t\tyaml_data = yaml.load(yaml_stream)\n\n\tcls.validate_from_dict(yaml_data)\n\treturn yaml_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException('File not found: %s' % yaml_file)\n\n\twith open(yaml_file, 'rb') as f:\n\t\tyaml_data = yaml.load(f)\n\n\tfor key in cls.required_keys:\n\t\tif key not in yaml_data:\n\t\t\traise IRValidatorException('Required key \"%s\" not found in %s' % (key, yaml_file))\n\n\treturn yaml_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file) as file:\n\t\tdata = yaml.load(file)\n\t\t\n\t\t# Check if all mandatory fields are present\n\t\tfor field in cls.REQUIRED_FIELDS:\n\t\t\tif field not in data:\n\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\"Could not find required field '{}' in YAML file.\".format(\n\t\t\t\t\t\tfield\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\n\t\t# Validate each field\n\t\tfor field in data:\n\t\t\tif field not in cls.REQUIRED_FIELDS:\n\t\t\t\tif field not in cls.OPTIONAL_FIELDS:\n\t\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\t\"Could not find field '{}' in YAML file.\".format(\n\t\t\t\t\t\t\tfield\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tcls.validate_optional_field(field, data[field])\n\t\t\telse:\n\t\t\t\tcls.validate_required_field(field, data[field])\n\t\t\n\t\treturn data\n\ndef", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tyaml_data = load_yaml(yml_file=yaml_file)\n\tif not yaml_data:\n\t\traise IRValidatorException(\"Empty YAML file\")\n\n\ttry:\n\t\tinstance = cls()\n\t\tinstance.validate(yaml_data)\n\t\treturn yaml_data\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(e)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\traise IRValidatorException('YAML file not specified')\n\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException('YAML file does not exist')\n\n\ttry:\n\t\twith open(yaml_file, \"r\") as stream:\n\t\t\tdata = yaml.load(stream)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(exc)\n\n\tvalidation_data = cls.validate(data)\n\treturn validation_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file) as f:\n\t\ttry:\n\t\t\tdata = yaml.safe_load(f)\n\t\texcept yaml.YAMLError as e:\n\t\t\tprint(e)\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Failed to parse YAML file. Please check if the file is valid YAML.\")\n\t\tif not data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"File is empty. Please add data to the file.\")\n\t\tif 'inputs' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'inputs' section to the file. Inputs section is mandatory.\")\n\t\tif 'outputs' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'outputs' section to the file. Outputs section is mandatory.\")\n\t\tif 'parameters' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'parameters' section to the file. Parameters section is mandatory.\")\n\t\tif 'components' not in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Please add 'components' section to the file. Components section is mandatory.\")\n\t\tif 'graph'", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise IRValidatorException('YAML file is missing')\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_config = yaml.load(f, Loader=yaml.FullLoader)\n\texcept IOError:\n\t\traise IRValidatorException('YAML file not found')\n\texcept yaml.YAMLError:\n\t\traise IRValidatorException('YAML file is not valid')\n\n\t# Check if it is a valid config by checking required fields\n\tif not yaml_config.get('name'):\n\t\traise IRValidatorException('name is missing')\n\tif not yaml_config.get('type'):\n\t\traise IRValidatorException('type is missing')\n\tif not yaml_config.get('version'):\n\t\traise IRValidatorException('version is missing')\n\tif not yaml_config.get('spec'):\n\t\tyaml_config['spec'] = {}\n\tif not yaml_config.get('metadata'):\n\t\tyaml_config['metadata'] = {}\n\n\tif yaml_config['type'] == 'cluster':\n\t\tif not yaml_config.get('spec').get('cluster_name'):\n\t\t\tyaml_config['spec']['cluster_name'] = yaml", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor group_name, group_content in parser_dict.items():\n\t\tif group_name == 'include':\n\t\t\tfor group_file_name in group_content:\n\t\t\t\tgroup_file_path = os.path.join(self.spec_path, group_file_name)\n\t\t\t\tif os.path.isfile(group_file_path):\n\t\t\t\t\tgroup_file_str = open(group_file_path).read()\n\t\t\t\t\tgroup_file_dict = yaml.load(group_file_str)\n\t\t\t\t\tif group_file_dict:\n\t\t\t\t\t\tself._include_groups(group_file_dict)\n\t\t\t\t\t\tself._parse_groups(group_file_dict)\n\t\t\t\telse:\n\t\t\t\t\traise Exception(\"{} can't be found.\".format(group_file_path))\n\t\telse:\n\t\t\tif group_name not in self.groups:\n\t\t\t\tself.groups[group_name] = {}\n\t\t\tparser_dict[group_name] = self._resolve_references(group_name, group_content)\n\t\t\tself._parse_groups(parser_dict[group_name])\n\n\t", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif 'groups' in parser_dict:\n\t\tfor group in parser_dict['groups']:\n\t\t\tif type(group) == str:\n\t\t\t\tgroup = self.parse_file(group)\n\t\t\tfor directive in group['directives']:\n\t\t\t\tif directive not in parser_dict['directives']:\n\t\t\t\t\tparser_dict['directives'][directive] = {}\n\t\t\t\tparser_dict['directives'][directive].update(group['directives'][directive])\n\t\t\tfor directive in group['directives']:\n\t\t\t\tif directive not in parser_dict['directives']:\n\t\t\t\t\tparser_dict['directives'][directive] = {}\n\t\t\t\tparser_dict['directives'][directive].update(group['directives'][directive])\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif not self.include_groups:\n\t\treturn\n\n\tfor group in self.include_groups:\n\t\tparser_dict[group] = []\n\n\tfor group in self.include_groups:\n\t\tfor spec_file in self.include_dict[group]:\n\t\t\tspec = Spec.from_file(spec_file)\n\t\t\tparser_dict[group] += spec.parser_dict[group]\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get('include', None)\n\tif include_groups is not None:\n\t\tfor group_name in include_groups:\n\t\t\tif group_name in self._groups:\n\t\t\t\tparser_dict.update(self._groups[group_name])\n\t\t\telse:\n\t\t\t\traise MissingIncludeGroupException(group_name)\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdef _resolve_include_groups(include_groups):\n\t\t\"\"\"\n\t\tResolves each include group.\n\t\t\"\"\"\n\t\tfor group_name, group_dict in include_groups.items():\n\t\t\tif not group_name.startswith('_'):\n\t\t\t\tgroup_dict['name'] = group_name\n\t\t\t\tgroup_dict['type'] = 'include'\n\t\t\t\tgroup_dict['subgroups'] = {}\n\t\t\t\tgroup_dict['includes'] = []\n\t\t\t\t# Check for includes.\n\t\t\t\tif 'includes' in group_dict:\n\t\t\t\t\tfor include_group in group_dict['includes']:\n\t\t\t\t\t\tif include_group in parser_dict:\n\t\t\t\t\t\t\t# Ensure that it is a group and not a spec.\n\t\t\t\t\t\t\tif 'type' in parser_dict[include_group]:\n\t\t\t\t\t\t\t\tif parser_dict[include_group]['type'] == 'group':\n\t\t\t\t\t\t\t\t\tgroup_dict['subgroups'][include_group] = parser_dict[include_group]\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\traise ValueError", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor key in parser_dict:\n\t\tif key == 'include':\n\t\t\tfor k in parser_dict[key]:\n\t\t\t\tparser_dict[k] = parser_dict[key][k]\n\treturn parser_dict\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor group in self.groups:\n\t\tinclude_dict = group.get(\"include\", None)\n\t\tif include_dict:\n\t\t\tparser_dict.update(include_dict)\n\n\t\tinclude_groups = group.get(\"include_groups\", [])\n\t\tif include_groups:\n\t\t\tfor group_name in include_groups:\n\t\t\t\tif group_name not in self.parser_dict:\n\t\t\t\t\traise SpecParserException(\"include_groups: group %s not found in the spec file\" % group_name)\n\t\t\t\tparser_dict.update(self.parser_dict[group_name])\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.logger.info(\"Including groups\")\n\tfor group in parser_dict:\n\t\tgroup_dict = parser_dict[group]\n\t\tgroup_dict['name'] = group\n\t\tself.logger.info(\"Processing group {}\".format(group))\n\t\tself.logger.info(\"Group dict {}\".format(group_dict))\n\t\tself.process_group(group_dict)\n\t\tself.logger.info(\"Group processed\")\n\n\tself.logger.info(\"Groups included\")\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor name, value in parser_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif 'include' in value and isinstance(value['include'], dict):\n\t\t\t\tself._resolve_groups_include(parser_dict, name, value['include'])\n\n\t\telif isinstance(value, list):\n\t\t\tfor item in value:\n\t\t\t\tif isinstance(item, dict):\n\t\t\t\t\tif 'include' in item and isinstance(item['include'], dict):\n\t\t\t\t\t\tself._resolve_groups_include(parser_dict, name, item['include'])\n\n\t\telif isinstance(value, str):\n\t\t\tif value == 'include':\n\t\t\t\traise ValueError(\"Include directives require a subdict\")\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser_dict = parser_dict or {}\n\tself.groups = {}\n\tfor group_name, group_dict in parser_dict.get(\"groups\", {}).items():\n\t\tgroup = self.groups.get(group_name)\n\t\tif group:\n\t\t\tgroup.update(group_dict)\n\t\telse:\n\t\t\tself.groups[group_name] = group_dict\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor group_name, group_content in parser_dict.items():\n\t\tif group_name == 'include':\n\t\t\tfor group_file_name in group_content:\n\t\t\t\tgroup_file_path = os.path.join(self.spec_path, group_file_name)\n\t\t\t\tif os.path.isfile(group_file_path):\n\t\t\t\t\tgroup_file_str = open(group_file_path).read()\n\t\t\t\t\tgroup_file_dict = yaml.load(group_file_str)\n\t\t\t\t\tif group_file_dict:\n\t\t\t\t\t\tself._include_groups(group_file_dict)\n\t\t\t\t\t\tself._parse_groups(group_file_dict)\n\t\t\t\telse:\n\t\t\t\t\traise Exception(\"{} can't be found.\".format(group_file_path))\n\t\telse:\n\t\t\tif group_name not in self.groups:\n\t\t\t\tself.groups[group_name] = {}\n\t\t\tparser_dict[group_name] = self._resolve_references(group_name, group_content)\n\t\t\tself._parse_groups(parser_dict[group_name])\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif 'groups' in parser_dict:\n\t\tfor group in parser_dict['groups']:\n\t\t\tif type(group) == str:\n\t\t\t\tgroup = self.parse_file(group)\n\t\t\tfor directive in group['directives']:\n\t\t\t\tif directive not in parser_dict['directives']:\n\t\t\t\t\tparser_dict['directives'][directive] = {}\n\t\t\t\tparser_dict['directives'][directive].update(group['directives'][directive])\n\t\t\tfor directive in group['directives']:\n\t\t\t\tif directive not in parser_dict['directives']:\n\t\t\t\t\tparser_dict['directives'][directive] = {}\n\t\t\t\tparser_dict['directives'][directive].update(group['directives'][directive])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif not self.include_groups:\n\t\treturn\n\n\tfor group in self.include_groups:\n\t\tparser_dict[group] = []\n\n\tfor group in self.include_groups:\n\t\tfor spec_file in self.include_dict[group]:\n\t\t\tspec = Spec.from_file(spec_file)\n\t\t\tparser_dict[group] += spec.parser_dict[group]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get('include', None)\n\tif include_groups is not None:\n\t\tfor group_name in include_groups:\n\t\t\tif group_name in self._groups:\n\t\t\t\tparser_dict.update(self._groups[group_name])\n\t\t\telse:\n\t\t\t\traise MissingIncludeGroupException(group_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdef _resolve_include_groups(include_groups):\n\t\t\"\"\"\n\t\tResolves each include group.\n\t\t\"\"\"\n\t\tfor group_name, group_dict in include_groups.items():\n\t\t\tif not group_name.startswith('_'):\n\t\t\t\tgroup_dict['name'] = group_name\n\t\t\t\tgroup_dict['type'] = 'include'\n\t\t\t\tgroup_dict['subgroups'] = {}\n\t\t\t\tgroup_dict['includes'] = []\n\t\t\t\t# Check for includes.\n\t\t\t\tif 'includes' in group_dict:\n\t\t\t\t\tfor include_group in group_dict['includes']:\n\t\t\t\t\t\tif include_group in parser_dict:\n\t\t\t\t\t\t\t# Ensure that it is a group and not a spec.\n\t\t\t\t\t\t\tif 'type' in parser_dict[include_group]:\n\t\t\t\t\t\t\t\tif parser_dict[include_group]['type'] == 'group':\n\t\t\t\t\t\t\t\t\tgroup_dict['subgroups'][include_group] = parser_dict[include_group]\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\traise ValueError", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor key in parser_dict:\n\t\tif key == 'include':\n\t\t\tfor k in parser_dict[key]:\n\t\t\t\tparser_dict[k] = parser_dict[key][k]\n\treturn parser_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor group in self.groups:\n\t\tinclude_dict = group.get(\"include\", None)\n\t\tif include_dict:\n\t\t\tparser_dict.update(include_dict)\n\n\t\tinclude_groups = group.get(\"include_groups\", [])\n\t\tif include_groups:\n\t\t\tfor group_name in include_groups:\n\t\t\t\tif group_name not in self.parser_dict:\n\t\t\t\t\traise SpecParserException(\"include_groups: group %s not found in the spec file\" % group_name)\n\t\t\t\tparser_dict.update(self.parser_dict[group_name])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.logger.info(\"Including groups\")\n\tfor group in parser_dict:\n\t\tgroup_dict = parser_dict[group]\n\t\tgroup_dict['name'] = group\n\t\tself.logger.info(\"Processing group {}\".format(group))\n\t\tself.logger.info(\"Group dict {}\".format(group_dict))\n\t\tself.process_group(group_dict)\n\t\tself.logger.info(\"Group processed\")\n\n\tself.logger.info(\"Groups included\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor name, value in parser_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif 'include' in value and isinstance(value['include'], dict):\n\t\t\t\tself._resolve_groups_include(parser_dict, name, value['include'])\n\n\t\telif isinstance(value, list):\n\t\t\tfor item in value:\n\t\t\t\tif isinstance(item, dict):\n\t\t\t\t\tif 'include' in item and isinstance(item['include'], dict):\n\t\t\t\t\t\tself._resolve_groups_include(parser_dict, name, item['include'])\n\n\t\telif isinstance(value, str):\n\t\t\tif value == 'include':\n\t\t\t\traise ValueError(\"Include directives require a subdict\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser_dict = parser_dict or {}\n\tself.groups = {}\n\tfor group_name, group_dict in parser_dict.get(\"groups\", {}).items():\n\t\tgroup = self.groups.get(group_name)\n\t\tif group:\n\t\t\tgroup.update(group_dict)\n\t\telse:\n\t\t\tself.groups[group_name] = group_dict\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\targs = {}\n\tfor a in self.arguments:\n\t\targs[a.name] = self._get_defaults(a.name)\n\treturn args\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.default = defaults[arg.name]\n\t\telse:\n\t\t\targ.default = None\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\t# resolve argument values from spec\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\t# resolve argument values from defaults\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\n\n\t# set default value for options\n\tfor option in self.options:\n\t\toption.default = option.default()\n\n\t# resolve option values from spec\n\tif self.spec:\n\t\tfor option in self.options:\n\t\t\toption.value = self.spec.get(option.name)\n\t# resolve option values from defaults\n\tfor option in self.options:\n\t\toption.value = option.default()\n\n\t# resolve argument values from spec\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\n\t# resolve argument values from defaults\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\n\t# resolve argument values from spec\n\tfor arg in self.arguments:\n\t\tif", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t# get_defaults() is a private method.\n\t# pylint: disable=protected-access\n\treturn self._get_defaults(self.spec)\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tif not callable(spec):\n\t\treturn\n\tspec_defaults = self._get_defaults(spec)\n\tfor k, v in spec_defaults.items():\n\t\tif k not in self.args:\n\t\t\tself.args[k] = v\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor arg in self.arguments:\n\t\tdefaults[arg.name] = self._get_default(arg)\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tif self.spec is None:\n\t\treturn\n\n\tfor name, arg in self.spec.arguments.items():\n\t\tif arg.default is None:\n\t\t\tcontinue\n\n\t\tif arg.default is not None:\n\t\t\tif arg.default is not None and arg.default.startswith('${'):\n\t\t\t\tdflt = self._get_default(arg.default.strip('${}'), name)\n\t\t\telse:\n\t\t\t\tdflt = arg.default\n\t\t\tif arg.type == 'list':\n\t\t\t\tdflt = dflt.split()\n\t\t\tif arg.type == 'dict':\n\t\t\t\tdflt = dict(dflt.split(':', 1))\n\t\t\tif arg.type == 'bool':\n\t\t\t\tif dflt.lower() in ['true', 'yes', '1']:\n\t\t\t\t\tdflt = True\n\t\t\t\telse:\n\t\t\t\t\tdflt = False\n\t\t\tif arg.type == 'int':\n\t\t\t\tdflt = int(dflt)\n\n\t\t\tself.args[name] = dflt\n\n\tif self.spec.defaults is not None:", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tif self.spec is None:\n\t\treturn self._get_defaults()\n\telse:\n\t\targs = {}\n\t\tfor key, value in self.spec.items():\n\t\t\tif key in self.args:\n\t\t\t\targs[key] = self.args[key]\n\t\t\telse:\n\t\t\t\targs[key] = self._get_defaults(key, value)\n\t\treturn args\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tif self._spec is None:\n\t\treturn\n\tfor arg, value in self._spec.items():\n\t\tif isinstance(value, (str, int, float, bool, bytes)):\n\t\t\tself[arg] = value\n\t\telif isinstance(value, dict):\n\t\t\targ_value = self._get_defaults(arg, value)\n\t\t\tif arg_value is not None:\n\t\t\t\tself[arg] = arg_value\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec_args = self.get_spec_args()\n\tfor k, v in spec_args.items():\n\t\ttry:\n\t\t\tvalue = self._get_defaults(k)\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Failed to resolve spec argument '%s': %s\" % (k, e))\n\t\tif value is not None:\n\t\t\tself.set_value(k, value)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\targs = {}\n\tfor a in self.arguments:\n\t\targs[a.name] = self._get_defaults(a.name)\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.default = defaults[arg.name]\n\t\telse:\n\t\t\targ.default = None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\t# resolve argument values from spec\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\t# resolve argument values from defaults\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\n\n\t# set default value for options\n\tfor option in self.options:\n\t\toption.default = option.default()\n\n\t# resolve option values from spec\n\tif self.spec:\n\t\tfor option in self.options:\n\t\t\toption.value = self.spec.get(option.name)\n\t# resolve option values from defaults\n\tfor option in self.options:\n\t\toption.value = option.default()\n\n\t# resolve argument values from spec\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\n\t# resolve argument values from defaults\n\tfor arg in self.arguments:\n\t\tif arg.name in defaults:\n\t\t\targ.value = defaults[arg.name]\n\n\t# resolve argument values from spec\n\tfor arg in self.arguments:\n\t\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t# get_defaults() is a private method.\n\t# pylint: disable=protected-access\n\treturn self._get_defaults(self.spec)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tif not callable(spec):\n\t\treturn\n\tspec_defaults = self._get_defaults(spec)\n\tfor k, v in spec_defaults.items():\n\t\tif k not in self.args:\n\t\t\tself.args[k] = v\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\tfor arg in self.arguments:\n\t\tdefaults[arg.name] = self._get_default(arg)\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tif self.spec is None:\n\t\treturn\n\n\tfor name, arg in self.spec.arguments.items():\n\t\tif arg.default is None:\n\t\t\tcontinue\n\n\t\tif arg.default is not None:\n\t\t\tif arg.default is not None and arg.default.startswith('${'):\n\t\t\t\tdflt = self._get_default(arg.default.strip('${}'), name)\n\t\t\telse:\n\t\t\t\tdflt = arg.default\n\t\t\tif arg.type == 'list':\n\t\t\t\tdflt = dflt.split()\n\t\t\tif arg.type == 'dict':\n\t\t\t\tdflt = dict(dflt.split(':', 1))\n\t\t\tif arg.type == 'bool':\n\t\t\t\tif dflt.lower() in ['true', 'yes', '1']:\n\t\t\t\t\tdflt = True\n\t\t\t\telse:\n\t\t\t\t\tdflt = False\n\t\t\tif arg.type == 'int':\n\t\t\t\tdflt = int(dflt)\n\n\t\t\tself.args[name] = dflt\n\n\tif self.spec.defaults is not None:", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tif self.spec is None:\n\t\treturn self._get_defaults()\n\telse:\n\t\targs = {}\n\t\tfor key, value in self.spec.items():\n\t\t\tif key in self.args:\n\t\t\t\targs[key] = self.args[key]\n\t\t\telse:\n\t\t\t\targs[key] = self._get_defaults(key, value)\n\t\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tif self._spec is None:\n\t\treturn\n\tfor arg, value in self._spec.items():\n\t\tif isinstance(value, (str, int, float, bool, bytes)):\n\t\t\tself[arg] = value\n\t\telif isinstance(value, dict):\n\t\t\targ_value = self._get_defaults(arg, value)\n\t\t\tif arg_value is not None:\n\t\t\t\tself[arg] = arg_value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec_args = self.get_spec_args()\n\tfor k, v in spec_args.items():\n\t\ttry:\n\t\t\tvalue = self._get_defaults(k)\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Failed to resolve spec argument '%s': %s\" % (k, e))\n\t\tif value is not None:\n\t\t\tself.set_value(k, value)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.deprecated_for:\n\t\t\tdeprecated_args[option_spec.deprecated_for] = option_spec\n\treturn deprecated_args\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn {\n\t\t\"doctype\": \"Deprecated Arguments\",\n\t\t\"fieldname\": \"argument\",\n\t\t\"transitions\": {\n\t\t\t\"active\": {\n\t\t\t\t\"new_state\": \"deprecated\",\n\t\t\t\t\"conditions\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"fieldname\": \"deprecated_by\",\n\t\t\t\t\t\t\"value\": \"\"\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"deprecated\": {\n\t\t\t\t\"new_state\": \"deprecated\",\n\t\t\t\t\"conditions\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"fieldname\": \"deprecated_by\",\n\t\t\t\t\t\t\"value\": \"\"\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t},\n\t\t\"states\": {\n\t\t\t\"deprecated\": {\n\t\t\t\t\"color\": \"#999\",\n\t\t\t\t\"icon\": \"fa fa-ban\"\n\t\t\t},\n\t\t\t\"active\": {\n\t\t\t\t\"color\": \"#000\",\n\t\t\t\t\"icon\": \"fa fa-cog\"", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif \"deprecate_option\" in spec and spec[\"deprecate_option\"]:\n\t\t\tdeprecated_args[spec[\"deprecate_option\"]] = spec[\"name\"]\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn {}\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn dict()\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn {}\n\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t# TODO: move to specs helper\n\tdeprecated = {\n\t\t'options': {},\n\t\t'actions': {}\n\t}\n\tfor option_name, option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.get('deprecated'):\n\t\t\tdeprecated['options'][option_name] = option_spec.get('deprecated')\n\tfor action_name, action_spec in self.spec_helper.iterate_action_specs():\n\t\tif action_spec.get('deprecated'):\n\t\t\tdeprecated['actions'][action_name] = action_spec.get('deprecated')\n\treturn deprecated\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option[1].get('deprecated_in_version') and option[1].get('deprecated_in_version') != self.version:\n\t\t\tdeprecated_args[option[0]] = option[1]['deprecated_in_version']\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\topts = {}\n\tfor opts_spec in self.spec_helper.iterate_option_specs():\n\t\tfor dep_opt in opts_spec.get('deprecate_option_names', []):\n\t\t\tif dep_opt in opts:\n\t\t\t\traise ValueError('Option {} can not be used together with {}'.format(opts_spec.get('name'), dep_opt))\n\t\t\topts[dep_opt] = opts_spec\n\n\treturn opts\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor arg in self.spec_helper.iterate_option_specs():\n\t\tif arg in self.args and arg in self.deprecated_args:\n\t\t\tif self.deprecated_args[arg] in deprecated_args:\n\t\t\t\tdeprecated_args[self.deprecated_args[arg]].append(self.args[arg])\n\t\t\telse:\n\t\t\t\tdeprecated_args[self.deprecated_args[arg]] = [self.args[arg]]\n\treturn deprecated_args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.deprecated_for:\n\t\t\tdeprecated_args[option_spec.deprecated_for] = option_spec\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn {\n\t\t\"doctype\": \"Deprecated Arguments\",\n\t\t\"fieldname\": \"argument\",\n\t\t\"transitions\": {\n\t\t\t\"active\": {\n\t\t\t\t\"new_state\": \"deprecated\",\n\t\t\t\t\"conditions\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"fieldname\": \"deprecated_by\",\n\t\t\t\t\t\t\"value\": \"\"\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"deprecated\": {\n\t\t\t\t\"new_state\": \"deprecated\",\n\t\t\t\t\"conditions\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"fieldname\": \"deprecated_by\",\n\t\t\t\t\t\t\"value\": \"\"\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t},\n\t\t\"states\": {\n\t\t\t\"deprecated\": {\n\t\t\t\t\"color\": \"#999\",\n\t\t\t\t\"icon\": \"fa fa-ban\"\n\t\t\t},\n\t\t\t\"active\": {\n\t\t\t\t\"color\": \"#000\",\n\t\t\t\t\"icon\": \"fa fa-cog\"", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif \"deprecate_option\" in spec and spec[\"deprecate_option\"]:\n\t\t\tdeprecated_args[spec[\"deprecate_option\"]] = spec[\"name\"]\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn dict()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\treturn {}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t# TODO: move to specs helper\n\tdeprecated = {\n\t\t'options': {},\n\t\t'actions': {}\n\t}\n\tfor option_name, option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.get('deprecated'):\n\t\t\tdeprecated['options'][option_name] = option_spec.get('deprecated')\n\tfor action_name, action_spec in self.spec_helper.iterate_action_specs():\n\t\tif action_spec.get('deprecated'):\n\t\t\tdeprecated['actions'][action_name] = action_spec.get('deprecated')\n\treturn deprecated\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option[1].get('deprecated_in_version') and option[1].get('deprecated_in_version') != self.version:\n\t\t\tdeprecated_args[option[0]] = option[1]['deprecated_in_version']\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\topts = {}\n\tfor opts_spec in self.spec_helper.iterate_option_specs():\n\t\tfor dep_opt in opts_spec.get('deprecate_option_names', []):\n\t\t\tif dep_opt in opts:\n\t\t\t\traise ValueError('Option {} can not be used together with {}'.format(opts_spec.get('name'), dep_opt))\n\t\t\topts[dep_opt] = opts_spec\n\n\treturn opts\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor arg in self.spec_helper.iterate_option_specs():\n\t\tif arg in self.args and arg in self.deprecated_args:\n\t\t\tif self.deprecated_args[arg] in deprecated_args:\n\t\t\t\tdeprecated_args[self.deprecated_args[arg]].append(self.args[arg])\n\t\t\telse:\n\t\t\t\tdeprecated_args[self.deprecated_args[arg]] = [self.args[arg]]\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"WARNING: Some arguments are deprecated and will be removed in future versions\")\n\tprint(\"\\t-c and -p are deprecated, please use --config and --profile instead\")\n\tprint(\"\\t-u is deprecated, please use -r instead\")\n\tprint(\"\\t-e is deprecated, please use -p instead\")\n\tfor option in [\"-c\", \"-p\", \"-u\", \"-e\"]:\n\t\tif option in cli_args:\n\t\t\tprint(\"Deprecated option: {0}\".format(option))\n\t\tif option in answer_file_args:\n\t\t\tprint(\"Deprecated option: {0}\".format(option))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor arg in cli_args:\n\t\tif arg in answer_file_args:\n\t\t\tdeprecated_args[arg] = answer_file_args[arg]\n\tif deprecated_args:\n\t\tprint_info_msg(\"Deprecated arguments:\")\n\t\tprint_info_msg(\"---------------------\")\n\t\tfor arg, value in deprecated_args.items():\n\t\t\tprint_info_msg(\n\t\t\t\t\"{0}={1}\".format(arg, value))\n\tif deprecated_args:\n\t\tprint_warning_msg(\"Deprecated arguments will be ignored\")\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = []\n\tfor key in cli_args:\n\t\tif key in answer_file_args and cli_args[key] != answer_file_args[key]:\n\t\t\tself.logger.info('Argument {0} has been deprecated. Its value in the answer file is {1}.'.format(key, answer_file_args[key]))\n\t\t\tdeprecated_args.append(key)\n\tif deprecated_args:\n\t\tself.logger.info('Deprecated arguments: {0}'.format(deprecated_args))\n\t\tself.logger.info('Please check the documentation for the latest usage.')\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t\"--disable-telemetry\": \"The telemetry is now enabled by default. To disable it please specify --enable-telemetry=false\",\n\t\t\"--disable-upgrade-check\": \"Upgrade check is now enabled by default. To disable it please specify --enable-upgrade-check=false\",\n\t\t\"--enable-telemetry\": \"The telemetry is now enabled by default. To disable it please specify --enable-telemetry=false\",\n\t\t\"--enable-upgrade-check\": \"Upgrade check is now enabled by default. To disable it please specify --enable-upgrade-check=false\",\n\t\t\"--disable-analytics\": \"The analytics is now enabled by default. To disable it please specify --enable-analytics=false\",\n\t\t\"--enable-analytics\": \"The analytics is now enabled by default. To disable it please specify --enable-analytics=false\"\n\t}\n\n\tfor deprecated_arg in deprecated_args:\n\t\tif deprecated_arg in cli_args or deprecated_arg in answer_file_args:\n\t\t\tprint(deprecated_args[deprecated_arg])\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif answer_file_args is None:\n\t\tanswer_file_args = {}\n\n\tdeprecated_args = {\n\t\t\"--networkInterface\": \"networkInterfaces\",\n\t\t\"--networkInterfaceFile\": \"networkInterfaces\",\n\t\t\"--networkInterfaceList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = [\n\t\t'--cluster-size', '--master-size', '--worker-size', '--bastion-size',\n\t\t'--worker-count', '--bastion-count', '--worker-min-size', '--worker-max-size',\n\t\t'--bastion-min-size', '--bastion-max-size',\n\t]\n\n\tfor arg in deprecated_args:\n\t\tif cli_args.get(arg) or answer_file_args.get(arg):\n\t\t\twarnings.warn(\n\t\t\t\t'\\n\\tThe ' + arg + ' argument is deprecated and will be removed in a future version. ' +\n\t\t\t\t'Please remove it from your command line and answer file.',\n\t\t\t\tDeprecationWarning\n\t\t\t)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef print_arg_deprecation(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead.\".format(arg, replacement_arg_str))\n\n\tdef print_default_arg_deprecation(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead (default value is '{2}').\".format(arg, replacement_arg_str, cli_args[arg]))\n\n\tdef print_default_arg_deprecation_no_value(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead (default value is '').\".format(arg, replacement_arg_str))\n\n\tdef print_default_arg_deprecation_no_value_no_args(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead.\".format(arg, replacement_arg_str))\n\n\tdef print_default_arg_deprecation_no_value_no_args_no_value(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead (default value is '').\".format(arg", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = ['--force', '--no-recommendations']\n\n\tfor arg in deprecated_args:\n\t\t# Check if the argument is in the cli_args\n\t\tif arg in cli_args:\n\t\t\t# If it is, print the deprecation\n\t\t\tprint('Warning: %s is deprecated and will be removed in future versions.' % arg)\n\n\t\t# Check if the argument is in the answer_file_args\n\t\tif arg in answer_file_args:\n\t\t\t# If it is, print the deprecation\n\t\t\tprint('Warning: %s is deprecated and will be removed in future versions.' % arg)\n\n\t# If the user passed both --force and --no-recommendations, print an error\n\tif '--force' in cli_args and '--no-recommendations' in cli_args:\n\t\tprint('Error: --force and --no-recommendations cannot be used together.')\n\t\tsys.exit(1)\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif cli_args['args_deprecation_dict']:\n\t\tself.print_deprecated_args(cli_args['args_deprecation_dict'], cli_args['args_deprecation_dict_reason'])\n\n\tif answer_file_args['args_deprecation_dict']:\n\t\tself.print_deprecated_args(answer_file_args['args_deprecation_dict'], answer_file_args['args_deprecation_dict_reason'])\n\n\treturn True\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor arg in cli_args:\n\t\tif arg in answer_file_args:\n\t\t\tif cli_args[arg] != answer_file_args[arg]:\n\t\t\t\tself.print_warning(arg, cli_args[arg], answer_file_args[arg])\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"WARNING: Some arguments are deprecated and will be removed in future versions\")\n\tprint(\"\\t-c and -p are deprecated, please use --config and --profile instead\")\n\tprint(\"\\t-u is deprecated, please use -r instead\")\n\tprint(\"\\t-e is deprecated, please use -p instead\")\n\tfor option in [\"-c\", \"-p\", \"-u\", \"-e\"]:\n\t\tif option in cli_args:\n\t\t\tprint(\"Deprecated option: {0}\".format(option))\n\t\tif option in answer_file_args:\n\t\t\tprint(\"Deprecated option: {0}\".format(option))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor arg in cli_args:\n\t\tif arg in answer_file_args:\n\t\t\tdeprecated_args[arg] = answer_file_args[arg]\n\tif deprecated_args:\n\t\tprint_info_msg(\"Deprecated arguments:\")\n\t\tprint_info_msg(\"---------------------\")\n\t\tfor arg, value in deprecated_args.items():\n\t\t\tprint_info_msg(\n\t\t\t\t\"{0}={1}\".format(arg, value))\n\tif deprecated_args:\n\t\tprint_warning_msg(\"Deprecated arguments will be ignored\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = []\n\tfor key in cli_args:\n\t\tif key in answer_file_args and cli_args[key] != answer_file_args[key]:\n\t\t\tself.logger.info('Argument {0} has been deprecated. Its value in the answer file is {1}.'.format(key, answer_file_args[key]))\n\t\t\tdeprecated_args.append(key)\n\tif deprecated_args:\n\t\tself.logger.info('Deprecated arguments: {0}'.format(deprecated_args))\n\t\tself.logger.info('Please check the documentation for the latest usage.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = {\n\t\t\"--disable-telemetry\": \"The telemetry is now enabled by default. To disable it please specify --enable-telemetry=false\",\n\t\t\"--disable-upgrade-check\": \"Upgrade check is now enabled by default. To disable it please specify --enable-upgrade-check=false\",\n\t\t\"--enable-telemetry\": \"The telemetry is now enabled by default. To disable it please specify --enable-telemetry=false\",\n\t\t\"--enable-upgrade-check\": \"Upgrade check is now enabled by default. To disable it please specify --enable-upgrade-check=false\",\n\t\t\"--disable-analytics\": \"The analytics is now enabled by default. To disable it please specify --enable-analytics=false\",\n\t\t\"--enable-analytics\": \"The analytics is now enabled by default. To disable it please specify --enable-analytics=false\"\n\t}\n\n\tfor deprecated_arg in deprecated_args:\n\t\tif deprecated_arg in cli_args or deprecated_arg in answer_file_args:\n\t\t\tprint(deprecated_args[deprecated_arg])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif answer_file_args is None:\n\t\tanswer_file_args = {}\n\n\tdeprecated_args = {\n\t\t\"--networkInterface\": \"networkInterfaces\",\n\t\t\"--networkInterfaceFile\": \"networkInterfaces\",\n\t\t\"--networkInterfaceList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"networkInterfaces\",\n\t\t\"--networkInterfacesFile\": \"networkInterfaces\",\n\t\t\"--networkInterfacesList\": \"networkInterfaces\",\n\t\t\"--networkInterfaces\": \"", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = [\n\t\t'--cluster-size', '--master-size', '--worker-size', '--bastion-size',\n\t\t'--worker-count', '--bastion-count', '--worker-min-size', '--worker-max-size',\n\t\t'--bastion-min-size', '--bastion-max-size',\n\t]\n\n\tfor arg in deprecated_args:\n\t\tif cli_args.get(arg) or answer_file_args.get(arg):\n\t\t\twarnings.warn(\n\t\t\t\t'\\n\\tThe ' + arg + ' argument is deprecated and will be removed in a future version. ' +\n\t\t\t\t'Please remove it from your command line and answer file.',\n\t\t\t\tDeprecationWarning\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef print_arg_deprecation(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead.\".format(arg, replacement_arg_str))\n\n\tdef print_default_arg_deprecation(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead (default value is '{2}').\".format(arg, replacement_arg_str, cli_args[arg]))\n\n\tdef print_default_arg_deprecation_no_value(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead (default value is '').\".format(arg, replacement_arg_str))\n\n\tdef print_default_arg_deprecation_no_value_no_args(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead.\".format(arg, replacement_arg_str))\n\n\tdef print_default_arg_deprecation_no_value_no_args_no_value(arg, replacement_arg_str):\n\t\tprint(\"argument --{0} is deprecated. Use --{1} instead (default value is '').\".format(arg", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = ['--force', '--no-recommendations']\n\n\tfor arg in deprecated_args:\n\t\t# Check if the argument is in the cli_args\n\t\tif arg in cli_args:\n\t\t\t# If it is, print the deprecation\n\t\t\tprint('Warning: %s is deprecated and will be removed in future versions.' % arg)\n\n\t\t# Check if the argument is in the answer_file_args\n\t\tif arg in answer_file_args:\n\t\t\t# If it is, print the deprecation\n\t\t\tprint('Warning: %s is deprecated and will be removed in future versions.' % arg)\n\n\t# If the user passed both --force and --no-recommendations, print an error\n\tif '--force' in cli_args and '--no-recommendations' in cli_args:\n\t\tprint('Error: --force and --no-recommendations cannot be used together.')\n\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif cli_args['args_deprecation_dict']:\n\t\tself.print_deprecated_args(cli_args['args_deprecation_dict'], cli_args['args_deprecation_dict_reason'])\n\n\tif answer_file_args['args_deprecation_dict']:\n\t\tself.print_deprecated_args(answer_file_args['args_deprecation_dict'], answer_file_args['args_deprecation_dict_reason'])\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor arg in cli_args:\n\t\tif arg in answer_file_args:\n\t\t\tif cli_args[arg] != answer_file_args[arg]:\n\t\t\t\tself.print_warning(arg, cli_args[arg], answer_file_args[arg])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = []\n\ttry:\n\t\tparser_option_specs = self._parser_option_specs[command_name]\n\texcept KeyError as e:\n\t\tself.log.debug(\"The command %s does not have any option\" % command_name)\n\treturn parser_option_specs\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = []\n\tcommand_class = getattr(self.parser.virsh, command_name)\n\tfor option_spec in command_class.option_list:\n\t\tparser_option_specs.append(option_spec)\n\n\treturn parser_option_specs\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tcommand_options = []\n\tif command_name in self.commands:\n\t\tcommand = self.commands[command_name]\n\t\tcommand_options = command['options']\n\treturn command_options\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tcommand = self.get_command(command_name)\n\treturn command.option_specs\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\treturn self.parser.get_option_specs(command_name)\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\treturn self.parser.get_option_specs(command_name)\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\treturn self.parser.get_option_specs(command_name)\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\toptions = []\n\tfor attr in dir(self.parser):\n\t\tif attr[0] != '_':\n\t\t\toptions.append(attr)\n\tif command_name in options:\n\t\treturn self.parser.options(command_name)\n\treturn []\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tcommand_spec = self.get_command_spec(command_name)\n\n\tif command_spec is None:\n\t\treturn []\n\n\treturn command_spec.get_option_specs()\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_options = []\n\tcommand = self.parser.get_command(command_name)\n\tfor action in command.option_list:\n\t\tif action.option_strings:\n\t\t\toption_info = {\n\t\t\t\t\"option\": action.option_strings[0],\n\t\t\t\t\"help\": action.help,\n\t\t\t\t\"value\": action.default,\n\t\t\t\t\"default\": action.default\n\t\t\t}\n\t\t\tparser_options.append(option_info)\n\treturn parser_options\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = []\n\ttry:\n\t\tparser_option_specs = self._parser_option_specs[command_name]\n\texcept KeyError as e:\n\t\tself.log.debug(\"The command %s does not have any option\" % command_name)\n\treturn parser_option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = []\n\tcommand_class = getattr(self.parser.virsh, command_name)\n\tfor option_spec in command_class.option_list:\n\t\tparser_option_specs.append(option_spec)\n\n\treturn parser_option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tcommand_options = []\n\tif command_name in self.commands:\n\t\tcommand = self.commands[command_name]\n\t\tcommand_options = command['options']\n\treturn command_options\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tcommand = self.get_command(command_name)\n\treturn command.option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\treturn self.parser.get_option_specs(command_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\treturn self.parser.get_option_specs(command_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\treturn self.parser.get_option_specs(command_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\toptions = []\n\tfor attr in dir(self.parser):\n\t\tif attr[0] != '_':\n\t\t\toptions.append(attr)\n\tif command_name in options:\n\t\treturn self.parser.options(command_name)\n\treturn []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tcommand_spec = self.get_command_spec(command_name)\n\n\tif command_spec is None:\n\t\treturn []\n\n\treturn command_spec.get_option_specs()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_options = []\n\tcommand = self.parser.get_command(command_name)\n\tfor action in command.option_list:\n\t\tif action.option_strings:\n\t\t\toption_info = {\n\t\t\t\t\"option\": action.option_strings[0],\n\t\t\t\t\"help\": action.help,\n\t\t\t\t\"value\": action.default,\n\t\t\t\t\"default\": action.default\n\t\t\t}\n\t\t\tparser_options.append(option_info)\n\treturn parser_options\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tif command_name not in self.parser_option_specs:\n\t\traise ValueError(\"Unknown command_name: {}\".format(command_name))\n\tif argument_name not in self.parser_option_specs[command_name]:\n\t\traise ValueError(\"Unknown argument_name: {}\".format(argument_name))\n\treturn self.parser_option_specs[command_name][argument_name]\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.argument_name == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.get_name() == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toption_specs = self.get_parser_option_specs(command_name)\n\tif not option_specs:\n\t\treturn None\n\tfor spec in option_specs:\n\t\tif spec[0] == argument_name:\n\t\t\treturn spec\n\treturn None\n\t\ndef get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the specification for each option of the given command_name.\n\tThe option_spec is a list of (name, help, required, type, default)\n\t\"\"\"\n\tparser_specs = self.get_parser_specs()\n\tcommand_specs = self.get_command_specs()\n\tif not command_name in parser_specs:\n\t\treturn []\n\tif not command_name in command_specs:\n\t\treturn []\n\tcommand_spec = command_specs[command_name]\n\treturn command_spec[0]\n\t\ndef get_command_specs(self):\n\t\"\"\"\n\tGets the specification for each command.\n\tThe command_spec is a list of (option_spec, help, callback, name)\n\t\"\"\"\n\treturn self._command_specs\n\t\ndef get_parser_specs(self):\n\t\"\"", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspecs = self.get_parser_option_specs(command_name)\n\tfor spec in specs:\n\t\tif spec.name == argument_name:\n\t\t\treturn spec\n\t\n\treturn None\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tif command_name not in self.command_option_specs:\n\t\traise ValueError(\"Invalid command name: %s\" % command_name)\n\toption_specs = self.command_option_specs[command_name]\n\tif argument_name not in option_specs:\n\t\traise ValueError(\"Invalid argument name: %s\" % argument_name)\n\treturn option_specs[argument_name]\n\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.get_option_name() == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toption_specs = self.get_parser_option_specs(command_name)\n\treturn option_specs.get(argument_name)\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.get_argument_name() == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.name == argument_name:\n\t\t\treturn option_spec\n\treturn None\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tif command_name not in self.parser_option_specs:\n\t\traise ValueError(\"Unknown command_name: {}\".format(command_name))\n\tif argument_name not in self.parser_option_specs[command_name]:\n\t\traise ValueError(\"Unknown argument_name: {}\".format(argument_name))\n\treturn self.parser_option_specs[command_name][argument_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.argument_name == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.get_name() == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toption_specs = self.get_parser_option_specs(command_name)\n\tif not option_specs:\n\t\treturn None\n\tfor spec in option_specs:\n\t\tif spec[0] == argument_name:\n\t\t\treturn spec\n\treturn None\n\t\ndef get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the specification for each option of the given command_name.\n\tThe option_spec is a list of (name, help, required, type, default)\n\t\"\"\"\n\tparser_specs = self.get_parser_specs()\n\tcommand_specs = self.get_command_specs()\n\tif not command_name in parser_specs:\n\t\treturn []\n\tif not command_name in command_specs:\n\t\treturn []\n\tcommand_spec = command_specs[command_name]\n\treturn command_spec[0]\n\t\ndef get_command_specs(self):\n\t\"\"\"\n\tGets the specification for each command.\n\tThe command_spec is a list of (option_spec, help, callback, name)\n\t\"\"\"\n\treturn self._command_specs\n\t\ndef get_parser_specs(self):\n\t\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspecs = self.get_parser_option_specs(command_name)\n\tfor spec in specs:\n\t\tif spec.name == argument_name:\n\t\t\treturn spec\n\t\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tif command_name not in self.command_option_specs:\n\t\traise ValueError(\"Invalid command name: %s\" % command_name)\n\toption_specs = self.command_option_specs[command_name]\n\tif argument_name not in option_specs:\n\t\traise ValueError(\"Invalid argument name: %s\" % argument_name)\n\treturn option_specs[argument_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.get_option_name() == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toption_specs = self.get_parser_option_specs(command_name)\n\treturn option_specs.get(argument_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.get_argument_name() == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tfor option_spec in self.get_parser_option_specs(command_name):\n\t\tif option_spec.name == argument_name:\n\t\t\treturn option_spec\n\treturn None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tsilent_args_tmp = []\n\tfor arg in args:\n\t\tif arg.startswith('--silent-'):\n\t\t\tsilent_args_tmp.append(arg[10:])\n\t\t\tsilent_args.append(arg)\n\t\telif arg.startswith('-silent-'):\n\t\t\tsilent_args_tmp.append(arg[8:])\n\t\t\tsilent_args.append(arg)\n\tif len(silent_args_tmp) > 0:\n\t\tself.log.warning('Silent argument {0} not recognized'.format(', '.join(silent_args_tmp)))\n\treturn silent_args\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith(\"--silent\") or arg.startswith(\"-silent\"):\n\t\t\tsilent_args.append(arg.split(\"=\")[1])\n\treturn silent_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith(\"-\"):\n\t\t\tsilent_args.append(arg)\n\t\telif arg.startswith(\"--\"):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg_name in self.silent_arguments:\n\t\tif arg_name in args:\n\t\t\tsilent_args.append(arg_name)\n\treturn silent_args\n\n\t\ndef get_unknown_args(self, args):\n\t\"\"\"\n\tlist of unknown argument\n\n\t:param args: The received arguments.\n\t:return: list, unknown argument names\n\t\"\"\"\n\tunknown_args = []\n\tfor arg_name in args:\n\t\tif arg_name not in self.silent_arguments and arg_name not in self.known_arguments:\n\t\t\tunknown_args.append(arg_name)\n\treturn unknown_args\n\n\t\ndef get_unknown_args_description(self, args):\n\t\"\"\"\n\tlist of unknown argument\n\n\t:param args: The received arguments.\n\t:return: list, unknown argument names\n\t\"\"\"\n\tunknown_args = []\n\tfor arg_name in args:\n\t\tif arg_name not in self.silent_arguments and arg_name not in self.known_arguments:\n\t\t\tunknown_args.append(self.get_argument_description(arg_name))\n\treturn unknown_args\n\n\t\ndef get_argument_description(self,", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\treturn [key for key in args if key.startswith('_')]\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tif args.silent:\n\t\tsilent_args = args.silent\n\treturn silent_args\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-') and args[arg] == '':\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\treturn [a for a in args if a.startswith('--')]\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t# Get list of silenced arguments\n\tsilent_args = []\n\tsilent_args_names = []\n\tsilent_args_values = []\n\n\tif \"silent_args\" in args:\n\t\tsilent_args = args[\"silent_args\"]\n\t\tsilent_args = silent_args.split(\",\")\n\t\tsilent_args_names = [x.split(\":\")[0] for x in silent_args]\n\t\tsilent_args_values = [x.split(\":\")[1] for x in silent_args]\n\n\treturn silent_args, silent_args_names, silent_args_values\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('--silent'):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tsilent_args_tmp = []\n\tfor arg in args:\n\t\tif arg.startswith('--silent-'):\n\t\t\tsilent_args_tmp.append(arg[10:])\n\t\t\tsilent_args.append(arg)\n\t\telif arg.startswith('-silent-'):\n\t\t\tsilent_args_tmp.append(arg[8:])\n\t\t\tsilent_args.append(arg)\n\tif len(silent_args_tmp) > 0:\n\t\tself.log.warning('Silent argument {0} not recognized'.format(', '.join(silent_args_tmp)))\n\treturn silent_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith(\"--silent\") or arg.startswith(\"-silent\"):\n\t\t\tsilent_args.append(arg.split(\"=\")[1])\n\treturn silent_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith(\"-\"):\n\t\t\tsilent_args.append(arg)\n\t\telif arg.startswith(\"--\"):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg_name in self.silent_arguments:\n\t\tif arg_name in args:\n\t\t\tsilent_args.append(arg_name)\n\treturn silent_args\n\n\t\ndef get_unknown_args(self, args):\n\t\"\"\"\n\tlist of unknown argument\n\n\t:param args: The received arguments.\n\t:return: list, unknown argument names\n\t\"\"\"\n\tunknown_args = []\n\tfor arg_name in args:\n\t\tif arg_name not in self.silent_arguments and arg_name not in self.known_arguments:\n\t\t\tunknown_args.append(arg_name)\n\treturn unknown_args\n\n\t\ndef get_unknown_args_description(self, args):\n\t\"\"\"\n\tlist of unknown argument\n\n\t:param args: The received arguments.\n\t:return: list, unknown argument names\n\t\"\"\"\n\tunknown_args = []\n\tfor arg_name in args:\n\t\tif arg_name not in self.silent_arguments and arg_name not in self.known_arguments:\n\t\t\tunknown_args.append(self.get_argument_description(arg_name))\n\treturn unknown_args\n\n\t\ndef get_argument_description(self,", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\treturn [key for key in args if key.startswith('_')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tif args.silent:\n\t\tsilent_args = args.silent\n\treturn silent_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-') and args[arg] == '':\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\treturn [a for a in args if a.startswith('--')]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t# Get list of silenced arguments\n\tsilent_args = []\n\tsilent_args_names = []\n\tsilent_args_values = []\n\n\tif \"silent_args\" in args:\n\t\tsilent_args = args[\"silent_args\"]\n\t\tsilent_args = silent_args.split(\",\")\n\t\tsilent_args_names = [x.split(\":\")[0] for x in silent_args]\n\t\tsilent_args_values = [x.split(\":\")[1] for x in silent_args]\n\n\treturn silent_args, silent_args_names, silent_args_values\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('--silent'):\n\t\t\tsilent_args.append(arg)\n\treturn silent_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t# Check if all required arguments have been provided\n\tfor arg in self.required_args:\n\t\tif not getattr(args, arg, None):\n\t\t\traise Exception(f\"Required argument {arg} not provided.\")\n\n\t# Check if the input file exists\n\tif os.path.isfile(args.input_file):\n\t\tlogging.info(f\"Input file provided: {args.input_file}\")\n\telse:\n\t\traise Exception(f\"Input file not found: {args.input_file}\")\n\n\t# Check if the output directory exists\n\tif os.path.isdir(args.output_dir):\n\t\tlogging.info(f\"Output directory provided: {args.output_dir}\")\n\telse:\n\t\tos.makedirs(args.output_dir)\n\n\t# Check if the output file exists\n\tif args.output_file:\n\t\tif os.path.isfile(args.output_file):\n\t\t\tlogging.info(f\"Output file provided: {args.output_file}\")\n\t\telse:\n\t\t\traise Exception(f\"Output file not found: {args.output_file}\")\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.required_args\n\t# Check if the required arguments have been provided\n\tfor required_arg in required_args:\n\t\tif required_arg not in args:\n\t\t\traise ValueError('Required argument ' + required_arg + ' not provided')\n\n\t# Check if arguments are of the correct type\n\tfor arg in args:\n\t\tif arg in required_args:\n\t\t\tif not isinstance(args[arg], required_args[arg]):\n\t\t\t\traise ValueError('Argument ' + arg + ' is not of type ' + str(required_args[arg]))\n\n\t# Check if the arguments are within the correct range\n\tfor arg in args:\n\t\tif arg in required_args:\n\t\t\tif isinstance(required_args[arg], tuple):\n\t\t\t\tif not required_args[arg][0] <= args[arg] <= required_args[arg][1]:\n\t\t\t\t\traise ValueError('Argument ' + arg + ' is not within range ' + str(required_args[arg]))\n\n\t# Check if the arguments are an allowed value\n\tfor arg in args:\n\t\tif arg in required_args:\n\t\t\tif isinstance(required_args[arg], list):\n\t\t\t\tif args[arg]", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args.mode == 'train':\n\t\tif (not args.train_files) and (not args.train_dir):\n\t\t\traise ValueError('No training file was provided.')\n\t\telse:\n\t\t\tif args.train_files:\n\t\t\t\tif not os.path.exists(args.train_files):\n\t\t\t\t\traise ValueError('The training file path was not valid.')\n\t\t\tif args.train_dir:\n\t\t\t\tif not os.path.exists(args.train_dir):\n\t\t\t\t\traise ValueError('The training directory path was not valid.')\n\tif args.mode == 'convert':\n\t\tif not args.input_file:\n\t\t\traise ValueError('No input file was provided.')\n\t\telse:\n\t\t\tif not os.path.exists(args.input_file):\n\t\t\t\traise ValueError('The input file path was not valid.')\n\t\tif not args.output_file:\n\t\t\traise ValueError('No output file was provided.')\n\tif args.mode == 'predict':\n\t\tif not args.input_file:\n\t\t\traise ValueError('No input file was provided.')\n\t\telse:\n\t\t\tif not os.path", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing = []\n\tfor flag, attr in self.flags.items():\n\t\tif attr['required'] and not getattr(args, flag, None):\n\t\t\tmissing.append(flag)\n\tif missing:\n\t\traise ValueError('Missing required arguments: %s' % missing)\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.requires_args:\n\t\tif arg not in args:\n\t\t\traise Exception(\"Required argument '{}' not found.\".format(arg))\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tpass\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = ['host', 'port', 'username', 'password']\n\tfor arg in required:\n\t\tif not getattr(args, arg):\n\t\t\traise argparse.ArgumentError(None, f'Please provide the {arg} argument')\n\t\t\treturn False\n\treturn True\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.REQUIRED_ARGS:\n\t\tif arg not in args.keys():\n\t\t\traise ValueError('Missing required argument: {0}'.format(arg))\n\n\treturn True\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args[\"input\"]:\n\t\traise argparse.ArgumentTypeError(\"Input not specified.\")\n\t\n\tif not args[\"output\"]:\n\t\traise argparse.ArgumentTypeError(\"Output directory not specified.\")\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args.input_file:\n\t\tprint('Error: no input file provided')\n\t\tprint('')\n\t\tself.parser.print_help()\n\t\texit(1)\n\n\tif not args.output_file:\n\t\tprint('Error: no output file provided')\n\t\tprint('')\n\t\tself.parser.print_help()\n\t\texit(1)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t# Check if all required arguments have been provided\n\tfor arg in self.required_args:\n\t\tif not getattr(args, arg, None):\n\t\t\traise Exception(f\"Required argument {arg} not provided.\")\n\n\t# Check if the input file exists\n\tif os.path.isfile(args.input_file):\n\t\tlogging.info(f\"Input file provided: {args.input_file}\")\n\telse:\n\t\traise Exception(f\"Input file not found: {args.input_file}\")\n\n\t# Check if the output directory exists\n\tif os.path.isdir(args.output_dir):\n\t\tlogging.info(f\"Output directory provided: {args.output_dir}\")\n\telse:\n\t\tos.makedirs(args.output_dir)\n\n\t# Check if the output file exists\n\tif args.output_file:\n\t\tif os.path.isfile(args.output_file):\n\t\t\tlogging.info(f\"Output file provided: {args.output_file}\")\n\t\telse:\n\t\t\traise Exception(f\"Output file not found: {args.output_file}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.required_args\n\t# Check if the required arguments have been provided\n\tfor required_arg in required_args:\n\t\tif required_arg not in args:\n\t\t\traise ValueError('Required argument ' + required_arg + ' not provided')\n\n\t# Check if arguments are of the correct type\n\tfor arg in args:\n\t\tif arg in required_args:\n\t\t\tif not isinstance(args[arg], required_args[arg]):\n\t\t\t\traise ValueError('Argument ' + arg + ' is not of type ' + str(required_args[arg]))\n\n\t# Check if the arguments are within the correct range\n\tfor arg in args:\n\t\tif arg in required_args:\n\t\t\tif isinstance(required_args[arg], tuple):\n\t\t\t\tif not required_args[arg][0] <= args[arg] <= required_args[arg][1]:\n\t\t\t\t\traise ValueError('Argument ' + arg + ' is not within range ' + str(required_args[arg]))\n\n\t# Check if the arguments are an allowed value\n\tfor arg in args:\n\t\tif arg in required_args:\n\t\t\tif isinstance(required_args[arg], list):\n\t\t\t\tif args[arg]", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args.mode == 'train':\n\t\tif (not args.train_files) and (not args.train_dir):\n\t\t\traise ValueError('No training file was provided.')\n\t\telse:\n\t\t\tif args.train_files:\n\t\t\t\tif not os.path.exists(args.train_files):\n\t\t\t\t\traise ValueError('The training file path was not valid.')\n\t\t\tif args.train_dir:\n\t\t\t\tif not os.path.exists(args.train_dir):\n\t\t\t\t\traise ValueError('The training directory path was not valid.')\n\tif args.mode == 'convert':\n\t\tif not args.input_file:\n\t\t\traise ValueError('No input file was provided.')\n\t\telse:\n\t\t\tif not os.path.exists(args.input_file):\n\t\t\t\traise ValueError('The input file path was not valid.')\n\t\tif not args.output_file:\n\t\t\traise ValueError('No output file was provided.')\n\tif args.mode == 'predict':\n\t\tif not args.input_file:\n\t\t\traise ValueError('No input file was provided.')\n\t\telse:\n\t\t\tif not os.path", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing = []\n\tfor flag, attr in self.flags.items():\n\t\tif attr['required'] and not getattr(args, flag, None):\n\t\t\tmissing.append(flag)\n\tif missing:\n\t\traise ValueError('Missing required arguments: %s' % missing)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.requires_args:\n\t\tif arg not in args:\n\t\t\traise Exception(\"Required argument '{}' not found.\".format(arg))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = ['host', 'port', 'username', 'password']\n\tfor arg in required:\n\t\tif not getattr(args, arg):\n\t\t\traise argparse.ArgumentError(None, f'Please provide the {arg} argument')\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.REQUIRED_ARGS:\n\t\tif arg not in args.keys():\n\t\t\traise ValueError('Missing required argument: {0}'.format(arg))\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args[\"input\"]:\n\t\traise argparse.ArgumentTypeError(\"Input not specified.\")\n\t\n\tif not args[\"output\"]:\n\t\traise argparse.ArgumentTypeError(\"Output directory not specified.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif not args.input_file:\n\t\tprint('Error: no input file provided')\n\t\tprint('')\n\t\tself.parser.print_help()\n\t\texit(1)\n\n\tif not args.output_file:\n\t\tprint('Error: no output file provided')\n\t\tprint('')\n\t\tself.parser.print_help()\n\t\texit(1)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t# check for required_when conditions\n\trequired_args = []\n\tfor arg_name, arg_def in options_spec.items():\n\t\trequired_when = arg_def.get('required_when', None)\n\t\tif required_when:\n\t\t\t# required_when can be a single condition or a list of conditions\n\t\t\tif isinstance(required_when, str):\n\t\t\t\trequired_when = [required_when]\n\t\t\tfor condition in required_when:\n\t\t\t\tcondition_matched = False\n\t\t\t\t# check if condition is a command name\n\t\t\t\tif condition == command_name:\n\t\t\t\t\tcondition_matched = True\n\t\t\t\telse:\n\t\t\t\t\t# check if condition is a command option\n\t\t\t\t\t# check if condition is a command option\n\t\t\t\t\tif condition in options_spec:\n\t\t\t\t\t\targ_spec = options_spec[condition]\n\t\t\t\t\t\tif arg_spec.get('required', False) and \\\n\t\t\t\t\t\t\t\targs.get(condition, False):\n\t\t\t\t\t\t\tcondition_matched = True\n\t\t\t\tif condition_matched:\n\t\t\t\t\trequired_args.", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor (arg_name, arg) in options_spec.items():\n\t\tif arg.get('required_when'):\n\t\t\tmatched = all(arg_name in arg_list for arg_list in args)\n\t\t\tif matched:\n\t\t\t\tresult.append(arg_name)\n\treturn result\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired = []\n\tfor name, spec in options_spec.items():\n\t\tif 'required_when' in spec and args[command_name] in spec['required_when']:\n\t\t\trequired.append(name)\n\treturn required\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\n\tfor (option, value) in options_spec:\n\t\tif not isinstance(option, tuple):\n\t\t\tcontinue\n\t\tif not isinstance(option[1], dict):\n\t\t\tcontinue\n\t\tif not 'required_when' in option[1]:\n\t\t\tcontinue\n\t\tif not option[0] in args:\n\t\t\tcontinue\n\t\tif not args[option[0]] in option[1]['required_when']:\n\t\t\tcontinue\n\t\trequired_args.append(option[0])\n\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor option in options_spec:\n\t\tif option['required_when'] and option['required_when'][0] == command_name:\n\t\t\tfor arg in option['required_when'][1]:\n\t\t\t\tif arg in args:\n\t\t\t\t\tresult.append(option['dest'])\n\treturn result\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\treturn [name for name, value in args.items()\n\t        for required, required_when in options_spec[name]['required_when']\n\t        if required and required_when in args.keys()\n\t        and args[required_when] == value]\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tif command_name not in self._conditionally_required_args:\n\t\tself._conditionally_required_args[command_name] = {}\n\t\tfor option_name, option_spec in options_spec.items():\n\t\t\tfor option_spec_key, option_spec_value in option_spec.items():\n\t\t\t\tif option_spec_key == 'required_when' and \\\n\t\t\t\t\t\t\t\tisinstance(option_spec_value, list):\n\t\t\t\t\tfor required_when_condition in option_spec_value:\n\t\t\t\t\t\tif required_when_condition in args:\n\t\t\t\t\t\t\tself._conditionally_required_args[command_name][option_name] = True\n\t\t\t\t\t\t\tbreak\n\n\trequired_arguments = []\n\tfor argument_name, argument_spec in args.items():\n\t\tif argument_name in self._conditionally_required_args[command_name] and \\\n\t\t\t\t\t\t\t\t\t\t\t\t\targument_spec.get('required') is False:\n\t\t\trequired_arguments.append(argument_name)\n\n\treturn required_arguments\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg in args:\n\t\tif arg in options_spec:\n\t\t\trequired_args = options_spec[arg].get('required_when', [])\n\t\t\tif not required_args:\n\t\t\t\tcontinue\n\t\t\tfor cond_arg in required_args:\n\t\t\t\tif cond_arg in args:\n\t\t\t\t\trequired_args.append(arg)\n\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg_name, arg_spec in options_spec.items():\n\t\tif arg_name in args:\n\t\t\tcontinue\n\t\trequired_when = arg_spec.get('required_when', None)\n\t\tif required_when:\n\t\t\tfor key, value in required_when:\n\t\t\t\tif key in args and args[key] == value:\n\t\t\t\t\trequired_args.append(arg_name)\n\t\t\t\t\tbreak\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_args = []\n\trequired_args = []\n\t# required_when_args_dict = {}\n\n\tif options_spec.get('options'):\n\t\tfor opt_name, opt_spec in options_spec.get('options').items():\n\t\t\tif opt_spec.get('required_when'):\n\t\t\t\tfor required_when_args_item in opt_spec.get(\n\t\t\t\t\t\t'required_when'):\n\t\t\t\t\tif required_when_args_item in args:\n\t\t\t\t\t\trequired_when_args.append(opt_name)\n\t\t\t\t\t\tbreak\n\t\t\tif opt_spec.get('required_when') and not required_when_args:\n\t\t\t\trequired_args.append(opt_name)\n\t\t\t\t# required_when_args_dict[opt_name] = opt_spec.get(\n\t\t\t\t#     'required_when')\n\n\tif options_spec.get('arguments'):\n\t\tfor arg_name, arg_spec in options_spec.get('arguments').items():\n\t\t\tif arg_spec.get('required_when'):\n\t\t\t\tfor required_when_args_item in arg_spec.get(\n\t\t\t\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t# check for required_when conditions\n\trequired_args = []\n\tfor arg_name, arg_def in options_spec.items():\n\t\trequired_when = arg_def.get('required_when', None)\n\t\tif required_when:\n\t\t\t# required_when can be a single condition or a list of conditions\n\t\t\tif isinstance(required_when, str):\n\t\t\t\trequired_when = [required_when]\n\t\t\tfor condition in required_when:\n\t\t\t\tcondition_matched = False\n\t\t\t\t# check if condition is a command name\n\t\t\t\tif condition == command_name:\n\t\t\t\t\tcondition_matched = True\n\t\t\t\telse:\n\t\t\t\t\t# check if condition is a command option\n\t\t\t\t\t# check if condition is a command option\n\t\t\t\t\tif condition in options_spec:\n\t\t\t\t\t\targ_spec = options_spec[condition]\n\t\t\t\t\t\tif arg_spec.get('required', False) and \\\n\t\t\t\t\t\t\t\targs.get(condition, False):\n\t\t\t\t\t\t\tcondition_matched = True\n\t\t\t\tif condition_matched:\n\t\t\t\t\trequired_args.", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor (arg_name, arg) in options_spec.items():\n\t\tif arg.get('required_when'):\n\t\t\tmatched = all(arg_name in arg_list for arg_list in args)\n\t\t\tif matched:\n\t\t\t\tresult.append(arg_name)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired = []\n\tfor name, spec in options_spec.items():\n\t\tif 'required_when' in spec and args[command_name] in spec['required_when']:\n\t\t\trequired.append(name)\n\treturn required\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\n\tfor (option, value) in options_spec:\n\t\tif not isinstance(option, tuple):\n\t\t\tcontinue\n\t\tif not isinstance(option[1], dict):\n\t\t\tcontinue\n\t\tif not 'required_when' in option[1]:\n\t\t\tcontinue\n\t\tif not option[0] in args:\n\t\t\tcontinue\n\t\tif not args[option[0]] in option[1]['required_when']:\n\t\t\tcontinue\n\t\trequired_args.append(option[0])\n\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\tfor option in options_spec:\n\t\tif option['required_when'] and option['required_when'][0] == command_name:\n\t\t\tfor arg in option['required_when'][1]:\n\t\t\t\tif arg in args:\n\t\t\t\t\tresult.append(option['dest'])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\treturn [name for name, value in args.items()\n\t        for required, required_when in options_spec[name]['required_when']\n\t        if required and required_when in args.keys()\n\t        and args[required_when] == value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tif command_name not in self._conditionally_required_args:\n\t\tself._conditionally_required_args[command_name] = {}\n\t\tfor option_name, option_spec in options_spec.items():\n\t\t\tfor option_spec_key, option_spec_value in option_spec.items():\n\t\t\t\tif option_spec_key == 'required_when' and \\\n\t\t\t\t\t\t\t\tisinstance(option_spec_value, list):\n\t\t\t\t\tfor required_when_condition in option_spec_value:\n\t\t\t\t\t\tif required_when_condition in args:\n\t\t\t\t\t\t\tself._conditionally_required_args[command_name][option_name] = True\n\t\t\t\t\t\t\tbreak\n\n\trequired_arguments = []\n\tfor argument_name, argument_spec in args.items():\n\t\tif argument_name in self._conditionally_required_args[command_name] and \\\n\t\t\t\t\t\t\t\t\t\t\t\t\targument_spec.get('required') is False:\n\t\t\trequired_arguments.append(argument_name)\n\n\treturn required_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg in args:\n\t\tif arg in options_spec:\n\t\t\trequired_args = options_spec[arg].get('required_when', [])\n\t\t\tif not required_args:\n\t\t\t\tcontinue\n\t\t\tfor cond_arg in required_args:\n\t\t\t\tif cond_arg in args:\n\t\t\t\t\trequired_args.append(arg)\n\n\treturn required_args\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg_name, arg_spec in options_spec.items():\n\t\tif arg_name in args:\n\t\t\tcontinue\n\t\trequired_when = arg_spec.get('required_when', None)\n\t\tif required_when:\n\t\t\tfor key, value in required_when:\n\t\t\t\tif key in args and args[key] == value:\n\t\t\t\t\trequired_args.append(arg_name)\n\t\t\t\t\tbreak\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_args = []\n\trequired_args = []\n\t# required_when_args_dict = {}\n\n\tif options_spec.get('options'):\n\t\tfor opt_name, opt_spec in options_spec.get('options').items():\n\t\t\tif opt_spec.get('required_when'):\n\t\t\t\tfor required_when_args_item in opt_spec.get(\n\t\t\t\t\t\t'required_when'):\n\t\t\t\t\tif required_when_args_item in args:\n\t\t\t\t\t\trequired_when_args.append(opt_name)\n\t\t\t\t\t\tbreak\n\t\t\tif opt_spec.get('required_when') and not required_when_args:\n\t\t\t\trequired_args.append(opt_name)\n\t\t\t\t# required_when_args_dict[opt_name] = opt_spec.get(\n\t\t\t\t#     'required_when')\n\n\tif options_spec.get('arguments'):\n\t\tfor arg_name, arg_spec in options_spec.get('arguments').items():\n\t\t\tif arg_spec.get('required_when'):\n\t\t\t\tfor required_when_args_item in arg_spec.get(\n\t\t\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor key, value in args.items():\n\t\tif key not in self.length_args:\n\t\t\tcontinue\n\t\tif len(value) > self.length_args[key]:\n\t\t\traise ValueError(\n\t\t\t\t\"Value of the argument '{}' should be no longer than {}. Received: '{}'\".format(\n\t\t\t\t\tkey, self.length_args[key], value\n\t\t\t\t)\n\t\t\t)\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.length:\n\t\t\traise ValueError(f\"{type(arg).__name__} - length of the string is longer than {self.length}\")\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(str(args[arg])) > self.length:\n\t\t\traise ValueError(f\"{arg} cannot be longer than {self.length} characters.\")\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = int(args[0])\n\tfor arg in args[1:]:\n\t\tif len(arg) > length:\n\t\t\traise ValueError(\n\t\t\t\tf\"Value '{arg}' is too long. Maximum length is {length}\")\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args.length <= 0:\n\t\traise ValueError(\"Length cannot be 0 or less.\")\n\telif args.length > 2048:\n\t\traise ValueError(\"Length cannot be greater than 2048.\")\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(str(arg)) > 255:\n\t\t\traise ValueError(\"The length of the given argument is longer than 255 characters.\")\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.length:\n\t\t\traise ValueError(\"The given value '%s' is longer than the specified length '%s'.\" % (arg, self.length))\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor key in args:\n\t\tif not isinstance(args[key], str):\n\t\t\traise ValueError('The value of {} is not a string.'.format(key))\n\t\tif len(args[key]) > self.length:\n\t\t\traise ValueError('The length of {} is longer than {}.'.format(key, self.length))\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.__length:\n\t\t\traise ValueError(\"The value of {} is longer than the length specified.\".format(arg))\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.max_length:\n\t\t\tself.error_handler.raise_value_too_long()\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor key, value in args.items():\n\t\tif key not in self.length_args:\n\t\t\tcontinue\n\t\tif len(value) > self.length_args[key]:\n\t\t\traise ValueError(\n\t\t\t\t\"Value of the argument '{}' should be no longer than {}. Received: '{}'\".format(\n\t\t\t\t\tkey, self.length_args[key], value\n\t\t\t\t)\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.length:\n\t\t\traise ValueError(f\"{type(arg).__name__} - length of the string is longer than {self.length}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(str(args[arg])) > self.length:\n\t\t\traise ValueError(f\"{arg} cannot be longer than {self.length} characters.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = int(args[0])\n\tfor arg in args[1:]:\n\t\tif len(arg) > length:\n\t\t\traise ValueError(\n\t\t\t\tf\"Value '{arg}' is too long. Maximum length is {length}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args.length <= 0:\n\t\traise ValueError(\"Length cannot be 0 or less.\")\n\telif args.length > 2048:\n\t\traise ValueError(\"Length cannot be greater than 2048.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(str(arg)) > 255:\n\t\t\traise ValueError(\"The length of the given argument is longer than 255 characters.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.length:\n\t\t\traise ValueError(\"The given value '%s' is longer than the specified length '%s'.\" % (arg, self.length))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor key in args:\n\t\tif not isinstance(args[key], str):\n\t\t\traise ValueError('The value of {} is not a string.'.format(key))\n\t\tif len(args[key]) > self.length:\n\t\t\traise ValueError('The length of {} is longer than {}.'.format(key, self.length))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.__length:\n\t\t\traise ValueError(\"The value of {} is longer than the length specified.\".format(arg))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > self.max_length:\n\t\t\tself.error_handler.raise_value_too_long()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif not isinstance(args, dict):\n\t\traise TypeError(\"Invalid argument type!\")\n\tif not 'value' in args:\n\t\traise KeyError(\"'value' is missing from argument dictionary!\")\n\tif not 'choices' in args:\n\t\traise KeyError(\"'choices' is missing from argument dictionary!\")\n\tif not isinstance(args['choices'], list):\n\t\traise TypeError(\"'choices' must be a list!\")\n\tif not isinstance(args['value'], str):\n\t\traise TypeError(\"'value' must be a string!\")\n\tif not args['value'] in args['choices']:\n\t\traise ValueError(\"'value' is not one of the available choices!\")\n\treturn args\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args in self.choices:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif not arg in self.args['choices']:\n\t\t\tprint('Invalid argument: ' + arg + '. Please try again.')\n\t\t\tself.choose_args()\n\t\telse:\n\t\t\treturn arg\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif not args.get('value') in args.get('choices'):\n\t\traise argparse.ArgumentTypeError(\n\t\t\t'Value %s is not one of the available choices: %s' % (\n\t\t\t\targs['value'],\n\t\t\t\t' '.join(args['choices'])\n\t\t\t)\n\t\t)\n\treturn args['value']\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args not in self.args.keys():\n\t\traise ArgumentError(self.name, args, \"The argument is not a valid choice.\")\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = args.get(\"choices\")\n\tvalue = args.get(\"value\")\n\tself.error(\n\t\t'Value of the argument \"{}\" must be one of the following choices: {}. Given value: {}'.format(args.get(\"name\"), choices, value),\n\t\tvalue not in choices\n\t)\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args.choices:\n\t\tif args.value not in args.choices:\n\t\t\traise ValueError(f'Invalid value for {args.name}')\n\tif args.choices_file:\n\t\tif args.value not in args.choices_file:\n\t\t\traise ValueError(f'Invalid value for {args.name}')\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif not args[0] in self.choices:\n\t\traise ValueError(\"'%s' is not a valid choice\" % args[0])\n\treturn args\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args in self.args:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in [*args]:\n\t\tif args[arg] not in self.choices:\n\t\t\traise ValueError(f\"Value of {arg} must be one of the following: {self.choices}\")\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif not isinstance(args, dict):\n\t\traise TypeError(\"Invalid argument type!\")\n\tif not 'value' in args:\n\t\traise KeyError(\"'value' is missing from argument dictionary!\")\n\tif not 'choices' in args:\n\t\traise KeyError(\"'choices' is missing from argument dictionary!\")\n\tif not isinstance(args['choices'], list):\n\t\traise TypeError(\"'choices' must be a list!\")\n\tif not isinstance(args['value'], str):\n\t\traise TypeError(\"'value' must be a string!\")\n\tif not args['value'] in args['choices']:\n\t\traise ValueError(\"'value' is not one of the available choices!\")\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args in self.choices:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif not arg in self.args['choices']:\n\t\t\tprint('Invalid argument: ' + arg + '. Please try again.')\n\t\t\tself.choose_args()\n\t\telse:\n\t\t\treturn arg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif not args.get('value') in args.get('choices'):\n\t\traise argparse.ArgumentTypeError(\n\t\t\t'Value %s is not one of the available choices: %s' % (\n\t\t\t\targs['value'],\n\t\t\t\t' '.join(args['choices'])\n\t\t\t)\n\t\t)\n\treturn args['value']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args not in self.args.keys():\n\t\traise ArgumentError(self.name, args, \"The argument is not a valid choice.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = args.get(\"choices\")\n\tvalue = args.get(\"value\")\n\tself.error(\n\t\t'Value of the argument \"{}\" must be one of the following choices: {}. Given value: {}'.format(args.get(\"name\"), choices, value),\n\t\tvalue not in choices\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args.choices:\n\t\tif args.value not in args.choices:\n\t\t\traise ValueError(f'Invalid value for {args.name}')\n\tif args.choices_file:\n\t\tif args.value not in args.choices_file:\n\t\t\traise ValueError(f'Invalid value for {args.name}')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif not args[0] in self.choices:\n\t\traise ValueError(\"'%s' is not a valid choice\" % args[0])\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tif args in self.args:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tfor arg in [*args]:\n\t\tif args[arg] not in self.choices:\n\t\t\traise ValueError(f\"Value of {arg} must be one of the following: {self.choices}\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg not in self.args:\n\t\t\tcontinue\n\n\t\tvalue = self.args[arg]\n\t\tif value < self.min_args[arg]:\n\t\t\tself.args[arg] = self.min_args[arg]\n\t\telif value > self.max_args[arg]:\n\t\t\tself.args[arg] = self.max_args[arg]\n\n\treturn\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\ttry:\n\t\t\tif int(args[arg]) < self.min_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too small. Minimum value is {}.\".format(self.min_args[arg])\n\t\t\telif int(args[arg]) > self.max_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too big. Maximum value is {}.\".format(self.max_args[arg])\n\t\texcept:\n\t\t\tself.error_args[arg] = \"Value must be integer.\"\n\t\t\treturn self.error_args\n\t\t\t\ndef validate_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\ttry:\n\t\t\tif int(args[arg]) < self.min_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too small. Minimum value is {}.\".format(self.min_args[arg])\n\t\t\telif int(args[arg]) > self.max_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too big.", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg in self.min_args and arg in self.max_args:\n\t\t\tif not (self.min_args[arg] <= args[arg] <= self.max_args[arg]):\n\t\t\t\traise ValueError(\"{} must be between {} and {}\".format(arg, self.min_args[arg], self.max_args[arg]))\n\t\telif arg in self.min_args:\n\t\t\tif not (self.min_args[arg] <= args[arg]):\n\t\t\t\traise ValueError(\"{} must be greater than {}\".format(arg, self.min_args[arg]))\n\t\telif arg in self.max_args:\n\t\t\tif not (self.max_args[arg] >= args[arg]):\n\t\t\t\traise ValueError(\"{} must be less than {}\".format(arg, self.max_args[arg]))\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\terrors = {}\n\tif args['min'] is not None:\n\t\tif not 0 <= args['min']:\n\t\t\terrors['min'] = \"min value must be greater than 0\"\n\t\tif args['max'] is not None and args['min'] > args['max']:\n\t\t\terrors['min'] = \"min value must be less than max value\"\n\tif args['max'] is not None:\n\t\tif not 0 <= args['max']:\n\t\t\terrors['max'] = \"max value must be greater than 0\"\n\t\tif args['min'] is not None and args['min'] > args['max']:\n\t\t\terrors['max'] = \"max value must be greater than min value\"\n\tif errors:\n\t\traise ValueError('Invalid argument value', errors)\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tif args['min'] > args['max']:\n\t\traise ValueError(\"min must be less than max\")\n\n\treturn True\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_val = args.min_val\n\tmax_val = args.max_val\n\tif min_val is None and max_val is None:\n\t\treturn\n\tif min_val is not None and max_val is not None:\n\t\tif min_val > max_val:\n\t\t\traise ValueError(\"Minimum value should be less than maximum value\")\n\tif min_val is not None and args.val < min_val:\n\t\traise ValueError(\"Value should be greater than minimum value\")\n\tif max_val is not None and args.val > max_val:\n\t\traise ValueError(\"Value should be less than maximum value\")\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg < args[\"min\"] or arg > args[\"max\"]:\n\t\t\traise ValueError(f\"Invalid value for {arg}. It must be between {args['min']} and {args['max']}\")\n\t\treturn True\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg.value < arg.min or arg.value > arg.max:\n\t\t\traise ValueError('Invalid value for argument ' + arg.name + ': ' + str(arg.value))\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_val = args['min']\n\tmax_val = args['max']\n\tfor key in args:\n\t\tif key == 'min' or key == 'max' or key == 'type':\n\t\t\tcontinue\n\t\tval = args[key]\n\t\tif not isinstance(val, int):\n\t\t\traise ValueError('{} is not an integer'.format(val))\n\t\tif val < min_val or val > max_val:\n\t\t\traise ValueError('value of {} is not between {} and {}'.format(key, min_val, max_val))\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tmin_value = self.min_args.get(arg)\n\t\tmax_value = self.max_args.get(arg)\n\t\tif min_value and max_value:\n\t\t\tif args[arg] < min_value or args[arg] > max_value:\n\t\t\t\traise ValueError(f\"Minimum value of {arg} is {min_value} and maximum value is {max_value}\")\n\t\telif min_value and not max_value:\n\t\t\tif args[arg] < min_value:\n\t\t\t\traise ValueError(f\"Minimum value of {arg} is {min_value}\")\n\t\telif not min_value and max_value:\n\t\t\tif args[arg] > max_value:\n\t\t\t\traise ValueError(f\"Maximum value of {arg} is {max_value}\")\n\t\telse:\n\t\t\traise ValueError(\"Minimum and maximum values should be given\")\n\t\t\t\ndef validate_args(self, args):\n\t\"\"\"\n\tCheck if all the given args are valid\n\t\"\"\"\n\tself.validate_min_max_args(args)\n\tfor arg in args:\n\t\tif arg in self.min_args.keys():\n\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg not in self.args:\n\t\t\tcontinue\n\n\t\tvalue = self.args[arg]\n\t\tif value < self.min_args[arg]:\n\t\t\tself.args[arg] = self.min_args[arg]\n\t\telif value > self.max_args[arg]:\n\t\t\tself.args[arg] = self.max_args[arg]\n\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\ttry:\n\t\t\tif int(args[arg]) < self.min_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too small. Minimum value is {}.\".format(self.min_args[arg])\n\t\t\telif int(args[arg]) > self.max_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too big. Maximum value is {}.\".format(self.max_args[arg])\n\t\texcept:\n\t\t\tself.error_args[arg] = \"Value must be integer.\"\n\t\t\treturn self.error_args\n\t\t\t\ndef validate_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\ttry:\n\t\t\tif int(args[arg]) < self.min_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too small. Minimum value is {}.\".format(self.min_args[arg])\n\t\t\telif int(args[arg]) > self.max_args[arg]:\n\t\t\t\tself.error_args[arg] = \"Too big.", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg in self.min_args and arg in self.max_args:\n\t\t\tif not (self.min_args[arg] <= args[arg] <= self.max_args[arg]):\n\t\t\t\traise ValueError(\"{} must be between {} and {}\".format(arg, self.min_args[arg], self.max_args[arg]))\n\t\telif arg in self.min_args:\n\t\t\tif not (self.min_args[arg] <= args[arg]):\n\t\t\t\traise ValueError(\"{} must be greater than {}\".format(arg, self.min_args[arg]))\n\t\telif arg in self.max_args:\n\t\t\tif not (self.max_args[arg] >= args[arg]):\n\t\t\t\traise ValueError(\"{} must be less than {}\".format(arg, self.max_args[arg]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\terrors = {}\n\tif args['min'] is not None:\n\t\tif not 0 <= args['min']:\n\t\t\terrors['min'] = \"min value must be greater than 0\"\n\t\tif args['max'] is not None and args['min'] > args['max']:\n\t\t\terrors['min'] = \"min value must be less than max value\"\n\tif args['max'] is not None:\n\t\tif not 0 <= args['max']:\n\t\t\terrors['max'] = \"max value must be greater than 0\"\n\t\tif args['min'] is not None and args['min'] > args['max']:\n\t\t\terrors['max'] = \"max value must be greater than min value\"\n\tif errors:\n\t\traise ValueError('Invalid argument value', errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tif args['min'] > args['max']:\n\t\traise ValueError(\"min must be less than max\")\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_val = args.min_val\n\tmax_val = args.max_val\n\tif min_val is None and max_val is None:\n\t\treturn\n\tif min_val is not None and max_val is not None:\n\t\tif min_val > max_val:\n\t\t\traise ValueError(\"Minimum value should be less than maximum value\")\n\tif min_val is not None and args.val < min_val:\n\t\traise ValueError(\"Value should be greater than minimum value\")\n\tif max_val is not None and args.val > max_val:\n\t\traise ValueError(\"Value should be less than maximum value\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg < args[\"min\"] or arg > args[\"max\"]:\n\t\t\traise ValueError(f\"Invalid value for {arg}. It must be between {args['min']} and {args['max']}\")\n\t\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg.value < arg.min or arg.value > arg.max:\n\t\t\traise ValueError('Invalid value for argument ' + arg.name + ': ' + str(arg.value))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_val = args['min']\n\tmax_val = args['max']\n\tfor key in args:\n\t\tif key == 'min' or key == 'max' or key == 'type':\n\t\t\tcontinue\n\t\tval = args[key]\n\t\tif not isinstance(val, int):\n\t\t\traise ValueError('{} is not an integer'.format(val))\n\t\tif val < min_val or val > max_val:\n\t\t\traise ValueError('value of {} is not between {} and {}'.format(key, min_val, max_val))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tfor arg in args:\n\t\tmin_value = self.min_args.get(arg)\n\t\tmax_value = self.max_args.get(arg)\n\t\tif min_value and max_value:\n\t\t\tif args[arg] < min_value or args[arg] > max_value:\n\t\t\t\traise ValueError(f\"Minimum value of {arg} is {min_value} and maximum value is {max_value}\")\n\t\telif min_value and not max_value:\n\t\t\tif args[arg] < min_value:\n\t\t\t\traise ValueError(f\"Minimum value of {arg} is {min_value}\")\n\t\telif not min_value and max_value:\n\t\t\tif args[arg] > max_value:\n\t\t\t\traise ValueError(f\"Maximum value of {arg} is {max_value}\")\n\t\telse:\n\t\t\traise ValueError(\"Minimum and maximum values should be given\")\n\t\t\t\ndef validate_args(self, args):\n\t\"\"\"\n\tCheck if all the given args are valid\n\t\"\"\"\n\tself.validate_min_max_args(args)\n\tfor arg in args:\n\t\tif arg in self.min_args.keys():\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option, subcommand)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise NotImplementedError('Type {} is not supported'.format(type_name))\n\n\treturn complex_action(self, option_name, subcommand, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == \"bool\":\n\t\treturn self.complex_action(self.bool_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"int\":\n\t\treturn self.complex_action(self.int_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"float\":\n\t\treturn self.complex_action(self.float_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"string\":\n\t\treturn self.complex_action(self.string_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"path\":\n\t\treturn self.complex_action(self.path_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"choice\":\n\t\treturn self.complex_action(self.choice_args, option_name, (self.vars, self.", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tspec_option.set_is_required(True)\n\tspec_option.set_nargs(1)\n\tspec_option.set_type(str)\n\tspec_option.set_default(None)\n\tspec_option.set_help(type_name + ' to be set')\n\tspec_option.set_action(self.complex_action)\n\tspec_option.set_choices(None)\n\tspec_option.set_dest(option_name)\n\n\tspec_option.set_name(option_name)\n\tspec_option.set_short_name(None)\n\tspec_option.set_long_name(option_name.replace('_', '-'))\n\tspec_option.set_aliases(None)\n\tspec_option.set_metavar(None)\n\t", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tself.vars = {}\n\tself.defaults = {}\n\tself.plugin_path = None\n\tcomplex_action = None\n\tfor key, value in COMPLEX_TYPES.items():\n\t\tif key == type_name:\n\t\t\tcomplex_action = COMPLEX_TYPES[key]\n\tif complex_action:\n\t\tcomplex_action(self, subcommand, type_name, option_name, spec_option)\n\t\treturn self.vars, self.defaults, self.plugin_path\n\telse:\n\t\traise NotImplementedError(\n\t\t\t'The type of argument {0} is not implemented in COMPLEX_TYPES'.format(type_name))\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\treturn {\n\t\t'path': 'complex_types.{}.{}({}, {}, {})'.format(\n\t\t\ttype_name, self.complex_action, option_name, self.vars, self.defaults, subcommand, spec_option),\n\t\t'type': 'complex'\n\t}\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == \"path\":\n\t\tif subcommand == \"add\":\n\t\t\treturn self.add_path(option_name, spec_option)\n\t\telif subcommand == \"remove\":\n\t\t\treturn self.remove_path(option_name, spec_option)\n\t\telif subcommand == \"list\":\n\t\t\treturn self.list_paths(option_name, spec_option)\n\t\telse:\n\t\t\traise Exception(\"Invalid subcommand: %s\" % subcommand)\n\telif type_name == \"list\":\n\t\tif subcommand == \"add\":\n\t\t\treturn self.add_list(option_name, spec_option)\n\t\telif subcommand == \"remove\":\n\t\t\treturn self.remove_list(option_name, spec_option)\n\t\telif subcommand == \"list\":\n\t\t\treturn self.list_lists(option_name, spec_option)\n\t\telse:\n\t\t\traise Exception(\"Invalid subcommand: %s\" % subcommand)\n\telif type_name == \"list_file\":\n\t\tif subcommand == \"add\":\n\t\t\treturn self.add_list_file(option_name, spec_option)\n\t\telif subcommand == \"remove\":\n\t\t\treturn self.remove_", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif complex_action is None:\n\t\traise MissingComplexType(type_name)\n\tif spec_option is None:\n\t\traise MissingComplexTypeOption(type_name, option_name)\n\treturn complex_action(option_name, subcommand, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == 'bool':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'str':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'list':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'dict':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'tuple':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'int':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'float':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'callable':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == \"file\":\n\t\tif self.option_is_set(option_name):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[option_name]\n\t\telif self.option_is_set(spec_option):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[spec_option]\n\t\telse:\n\t\t\treturn self.plugin_path[0]\n\telif type_name == \"directory\":\n\t\tif self.option_is_set(option_name):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[option_name]\n\t\telif self.option_is_set(spec_option):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[spec_option]\n\t\telse:\n\t\t\treturn self.plugin_path[0]\n\telif type_name == \"string\":\n\t\tif self.option_is_set(option_name):\n\t\t\treturn self.vars[option_name]\n\t\telif self.option_is_set(spec_option):\n\t\t\treturn self.vars[spec_option]\n\t\telse:\n\t\t\treturn \"\"\n\telif"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option, subcommand)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise NotImplementedError('Type {} is not supported'.format(type_name))\n\n\treturn complex_action(self, option_name, subcommand, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == \"bool\":\n\t\treturn self.complex_action(self.bool_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"int\":\n\t\treturn self.complex_action(self.int_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"float\":\n\t\treturn self.complex_action(self.float_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"string\":\n\t\treturn self.complex_action(self.string_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"path\":\n\t\treturn self.complex_action(self.path_args, option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\telif type_name == \"choice\":\n\t\treturn self.complex_action(self.choice_args, option_name, (self.vars, self.", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tspec_option.set_is_required(True)\n\tspec_option.set_nargs(1)\n\tspec_option.set_type(str)\n\tspec_option.set_default(None)\n\tspec_option.set_help(type_name + ' to be set')\n\tspec_option.set_action(self.complex_action)\n\tspec_option.set_choices(None)\n\tspec_option.set_dest(option_name)\n\n\tspec_option.set_name(option_name)\n\tspec_option.set_short_name(None)\n\tspec_option.set_long_name(option_name.replace('_', '-'))\n\tspec_option.set_aliases(None)\n\tspec_option.set_metavar(None)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tself.vars = {}\n\tself.defaults = {}\n\tself.plugin_path = None\n\tcomplex_action = None\n\tfor key, value in COMPLEX_TYPES.items():\n\t\tif key == type_name:\n\t\t\tcomplex_action = COMPLEX_TYPES[key]\n\tif complex_action:\n\t\tcomplex_action(self, subcommand, type_name, option_name, spec_option)\n\t\treturn self.vars, self.defaults, self.plugin_path\n\telse:\n\t\traise NotImplementedError(\n\t\t\t'The type of argument {0} is not implemented in COMPLEX_TYPES'.format(type_name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\treturn {\n\t\t'path': 'complex_types.{}.{}({}, {}, {})'.format(\n\t\t\ttype_name, self.complex_action, option_name, self.vars, self.defaults, subcommand, spec_option),\n\t\t'type': 'complex'\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == \"path\":\n\t\tif subcommand == \"add\":\n\t\t\treturn self.add_path(option_name, spec_option)\n\t\telif subcommand == \"remove\":\n\t\t\treturn self.remove_path(option_name, spec_option)\n\t\telif subcommand == \"list\":\n\t\t\treturn self.list_paths(option_name, spec_option)\n\t\telse:\n\t\t\traise Exception(\"Invalid subcommand: %s\" % subcommand)\n\telif type_name == \"list\":\n\t\tif subcommand == \"add\":\n\t\t\treturn self.add_list(option_name, spec_option)\n\t\telif subcommand == \"remove\":\n\t\t\treturn self.remove_list(option_name, spec_option)\n\t\telif subcommand == \"list\":\n\t\t\treturn self.list_lists(option_name, spec_option)\n\t\telse:\n\t\t\traise Exception(\"Invalid subcommand: %s\" % subcommand)\n\telif type_name == \"list_file\":\n\t\tif subcommand == \"add\":\n\t\t\treturn self.add_list_file(option_name, spec_option)\n\t\telif subcommand == \"remove\":\n\t\t\treturn self.remove_", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_action = COMPLEX_TYPES.get(type_name)\n\tif complex_action is None:\n\t\traise MissingComplexType(type_name)\n\tif spec_option is None:\n\t\traise MissingComplexTypeOption(type_name, option_name)\n\treturn complex_action(option_name, subcommand, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == 'bool':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'str':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'list':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'dict':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'tuple':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'int':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'float':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n\telif type_name == 'callable':\n\t\treturn COMPLEX_TYPES[type_name](self, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tif type_name == \"file\":\n\t\tif self.option_is_set(option_name):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[option_name]\n\t\telif self.option_is_set(spec_option):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[spec_option]\n\t\telse:\n\t\t\treturn self.plugin_path[0]\n\telif type_name == \"directory\":\n\t\tif self.option_is_set(option_name):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[option_name]\n\t\telif self.option_is_set(spec_option):\n\t\t\treturn self.plugin_path[0] + \"/\" + self.vars[spec_option]\n\t\telse:\n\t\t\treturn self.plugin_path[0]\n\telif type_name == \"string\":\n\t\tif self.option_is_set(option_name):\n\t\t\treturn self.vars[option_name]\n\t\telif self.option_is_set(spec_option):\n\t\t\treturn self.vars[spec_option]\n\t\telse:\n\t\t\treturn \"\"\n\telif", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\n\tfor arg in args:\n\t\t# Split args into custom and control\n\t\tif arg.startswith('-'):\n\t\t\tcontrol_args[arg] = True\n\t\telse:\n\t\t\tnested_args[arg] = True\n\treturn control_args, nested_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t# split arguments\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--control\"):\n\t\t\tcontrol_args[arg.replace(\"--control-\", \"\")] = args[arg]\n\t\telif arg.startswith(\"--nested\"):\n\t\t\tnested_args[arg.replace(\"--nested-\", \"\")] = args[arg]\n\t\telse:\n\t\t\tcustom_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor k, v in args.items():\n\t\tif k.startswith('-'):\n\t\t\tcontrol_args[k] = v\n\t\telif k in self.nested_args:\n\t\t\tnested_args[k] = v\n\t\telse:\n\t\t\tcustom_args[k] = v\n\n\treturn control_args, nested_args, custom_args\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\n\t# Filter out any arguments that are control args\n\tfor arg in args:\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg in self.nested_args:\n\t\t\tnested_args[arg] = args[arg]\n\t\telif arg in self.custom_args:\n\t\t\tcustom_args[arg] = args[arg]\n\n\treturn nested_args, control_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\targ_name = arg[0]\n\t\targ_value = arg[1]\n\t\tif arg_name in self.control_args:\n\t\t\tcontrol_args[arg_name] = arg_value\n\t\telif arg_name in self.nested_args:\n\t\t\tnested_args[arg_name] = arg_value\n\t\telif arg_name in self.custom_args:\n\t\t\tcustom_args[arg_name] = arg_value\n\t\telse:\n\t\t\traise ValueError(\"Unknown argument %s\" % arg_name)\n\n\treturn control_args, nested_args, custom_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif '=' in arg:\n\t\t\targ_name, arg_value = arg.split('=', 1)\n\t\t\tif arg_name.startswith('--'):\n\t\t\t\targ_name = arg_name[2:]\n\t\t\t\t# arg_name = arg_name.replace('-', '_')\n\t\t\t\tif arg_name in self.nested_args:\n\t\t\t\t\tnested_args[arg_name] = arg_value\n\t\t\t\telif arg_name in self.control_args:\n\t\t\t\t\tcontrol_args[arg_name] = arg_value\n\t\t\t\telif arg_name in self.custom_args:\n\t\t\t\t\tcustom_args[arg_name] = arg_value\n\t\t\t\telse:\n\t\t\t\t\traise RuntimeError(\"Unknown argument: {}\".format(arg))\n\t\telse:\n\t\t\traise RuntimeError(\"Invalid argument: {}\".format(arg))\n\n\treturn control_args, nested_args, custom_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = args.copy()\n\n\t# remove custom args from args (since they are not nested)\n\tfor custom_arg in self.custom_args:\n\t\tif custom_arg in custom_args:\n\t\t\tcustom_args.remove(custom_arg)\n\n\t# remove nested args from args (since they are not nested)\n\tfor nested_arg in self.nested_args:\n\t\tif nested_arg in custom_args:\n\t\t\tcustom_args.remove(nested_arg)\n\t\tif nested_arg in args:\n\t\t\tnested_args[nested_arg] = args[nested_arg]\n\t\t\tcustom_args.remove(nested_arg)\n\n\t# remove control args from args (since they are not nested)\n\tfor control_arg in self.control_args:\n\t\tif control_arg in custom_args:\n\t\t\tcustom_args.remove(control_arg)\n\t\tif control_arg in args:\n\t\t\tcontrol_args[control_arg] = args[control_arg]\n\t\t\tcustom_args.remove(control_arg)\n\n\t# remove nested_args that are not in the list\n\tfor nested_arg in list(nested_args.", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif '=' in arg:\n\t\t\t# Custom arg\n\t\t\tcustom_arg = arg.split('=')\n\t\t\tcustom_args[custom_arg[0]] = custom_arg[1]\n\t\t\tcontinue\n\t\tif arg.startswith('--'):\n\t\t\t# Control arg\n\t\t\tcontrol_args[arg] = True\n\t\t\tcontinue\n\t\t# Normal arg\n\t\tnested_args[arg] = True\n\n\treturn control_args, nested_args, custom_args\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tkey = arg.split('=')[0]\n\t\tval = arg.split('=')[1]\n\t\tif key in self.control_args:\n\t\t\tcontrol_args[key] = val\n\t\telif key in self.nested_args:\n\t\t\tnested_args[key] = val\n\t\telif key in self.custom_args:\n\t\t\tcustom_args[key] = val\n\t\telse:\n\t\t\traise ValueError(\"Invalid argument: %s\" % arg)\n\treturn nested_args, control_args, custom_args\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\n\tfor arg in args:\n\t\t# Split args into custom and control\n\t\tif arg.startswith('-'):\n\t\t\tcontrol_args[arg] = True\n\t\telse:\n\t\t\tnested_args[arg] = True\n\treturn control_args, nested_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t# split arguments\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--control\"):\n\t\t\tcontrol_args[arg.replace(\"--control-\", \"\")] = args[arg]\n\t\telif arg.startswith(\"--nested\"):\n\t\t\tnested_args[arg.replace(\"--nested-\", \"\")] = args[arg]\n\t\telse:\n\t\t\tcustom_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor k, v in args.items():\n\t\tif k.startswith('-'):\n\t\t\tcontrol_args[k] = v\n\t\telif k in self.nested_args:\n\t\t\tnested_args[k] = v\n\t\telse:\n\t\t\tcustom_args[k] = v\n\n\treturn control_args, nested_args, custom_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\n\t# Filter out any arguments that are control args\n\tfor arg in args:\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg in self.nested_args:\n\t\t\tnested_args[arg] = args[arg]\n\t\telif arg in self.custom_args:\n\t\t\tcustom_args[arg] = args[arg]\n\n\treturn nested_args, control_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\targ_name = arg[0]\n\t\targ_value = arg[1]\n\t\tif arg_name in self.control_args:\n\t\t\tcontrol_args[arg_name] = arg_value\n\t\telif arg_name in self.nested_args:\n\t\t\tnested_args[arg_name] = arg_value\n\t\telif arg_name in self.custom_args:\n\t\t\tcustom_args[arg_name] = arg_value\n\t\telse:\n\t\t\traise ValueError(\"Unknown argument %s\" % arg_name)\n\n\treturn control_args, nested_args, custom_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif '=' in arg:\n\t\t\targ_name, arg_value = arg.split('=', 1)\n\t\t\tif arg_name.startswith('--'):\n\t\t\t\targ_name = arg_name[2:]\n\t\t\t\t# arg_name = arg_name.replace('-', '_')\n\t\t\t\tif arg_name in self.nested_args:\n\t\t\t\t\tnested_args[arg_name] = arg_value\n\t\t\t\telif arg_name in self.control_args:\n\t\t\t\t\tcontrol_args[arg_name] = arg_value\n\t\t\t\telif arg_name in self.custom_args:\n\t\t\t\t\tcustom_args[arg_name] = arg_value\n\t\t\t\telse:\n\t\t\t\t\traise RuntimeError(\"Unknown argument: {}\".format(arg))\n\t\telse:\n\t\t\traise RuntimeError(\"Invalid argument: {}\".format(arg))\n\n\treturn control_args, nested_args, custom_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = args.copy()\n\n\t# remove custom args from args (since they are not nested)\n\tfor custom_arg in self.custom_args:\n\t\tif custom_arg in custom_args:\n\t\t\tcustom_args.remove(custom_arg)\n\n\t# remove nested args from args (since they are not nested)\n\tfor nested_arg in self.nested_args:\n\t\tif nested_arg in custom_args:\n\t\t\tcustom_args.remove(nested_arg)\n\t\tif nested_arg in args:\n\t\t\tnested_args[nested_arg] = args[nested_arg]\n\t\t\tcustom_args.remove(nested_arg)\n\n\t# remove control args from args (since they are not nested)\n\tfor control_arg in self.control_args:\n\t\tif control_arg in custom_args:\n\t\t\tcustom_args.remove(control_arg)\n\t\tif control_arg in args:\n\t\t\tcontrol_args[control_arg] = args[control_arg]\n\t\t\tcustom_args.remove(control_arg)\n\n\t# remove nested_args that are not in the list\n\tfor nested_arg in list(nested_args.", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif '=' in arg:\n\t\t\t# Custom arg\n\t\t\tcustom_arg = arg.split('=')\n\t\t\tcustom_args[custom_arg[0]] = custom_arg[1]\n\t\t\tcontinue\n\t\tif arg.startswith('--'):\n\t\t\t# Control arg\n\t\t\tcontrol_args[arg] = True\n\t\t\tcontinue\n\t\t# Normal arg\n\t\tnested_args[arg] = True\n\n\treturn control_args, nested_args, custom_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tkey = arg.split('=')[0]\n\t\tval = arg.split('=')[1]\n\t\tif key in self.control_args:\n\t\t\tcontrol_args[key] = val\n\t\telif key in self.nested_args:\n\t\t\tnested_args[key] = val\n\t\telif key in self.custom_args:\n\t\t\tcustom_args[key] = val\n\t\telse:\n\t\t\traise ValueError(\"Invalid argument: %s\" % arg)\n\treturn nested_args, control_args, custom_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor var in extra_vars:\n\t\tif '=' in var:\n\t\t\tkey, value = var.split('=', 1)\n\t\t\tvars_dict[key] = value\n\t\telif var.startswith('@'):\n\t\t\tfile_name = var[1:]\n\t\t\ttry:\n\t\t\t\twith open(file_name, 'r') as f:\n\t\t\t\t\tvars_dict.update(json.load(f))\n\t\t\texcept IOError:\n\t\t\t\tlogger.warning('Failed to load extra-vars from file: %s', file_name)\n\t\telse:\n\t\t\tlogger.warning('Ignoring extra-vars: %s', var)\n\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor item in extra_vars:\n\t\tif \"=\" in item:\n\t\t\tk, v = item.split(\"=\")\n\t\telse:\n\t\t\tk, v = item, True\n\t\tvars_dict[k] = v\n\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is not None:\n\t\tfor var in extra_vars:\n\t\t\tif \"=\" in var:\n\t\t\t\tkey, value = var.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\tvars_dict[var] = True\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn\n\tfor var_name, var_value in extra_vars:\n\t\tif var_name in vars_dict:\n\t\t\t# TODO: Raise error\n\t\t\tpass\n\t\tvars_dict[var_name] = var_value\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tfor var in extra_vars:\n\t\t\tif '=' in var:\n\t\t\t\t(key, value) = var.split('=', 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\tvars_dict[var] = True\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is not None:\n\t\tfor var in extra_vars:\n\t\t\tif var.startswith('--extra-vars'):\n\t\t\t\t# get the value of the variable\n\t\t\t\tstart = len('--extra-vars')\n\t\t\t\tif var[start:start+1] == '=':\n\t\t\t\t\tstart += 1\n\t\t\t\tvalue = var[start:]\n\t\t\t\t# split the key and value\n\t\t\t\tkey = var[start:var.find('=')]\n\t\t\t\t# remove any quotes from the value\n\t\t\t\tif len(value) > 2 and value[0] == value[-1] and value[0] in ('\"',\"'\"):\n\t\t\t\t\tvalue = value[1:-1]\n\t\t\t\tvars_dict[key] = value\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif isinstance(extra_vars, str):\n\t\textra_vars = extra_vars.split()\n\n\tif isinstance(extra_vars, list):\n\t\tfor extra_var in extra_vars:\n\t\t\tif \"=\" not in extra_var:\n\t\t\t\traise AnsibleOptionsError(\n\t\t\t\t\t'Extra vars must be in the form of key=value')\n\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\tif not isinstance(value, str):\n\t\t\t\tvalue = str(value)\n\t\t\tif key in vars_dict:\n\t\t\t\tif isinstance(vars_dict[key], str):\n\t\t\t\t\tvars_dict[key] = [vars_dict[key]]\n\t\t\t\tvars_dict[key].append(value)\n\t\t\telse:\n\t\t\t\tvars_dict[key] = value\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split('=')\n\t\t\tvars_dict[key] = value\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\treturn vars_dict\n\tfor entry in extra_vars:\n\t\tif isinstance(entry, basestring):\n\t\t\tentry = yaml.load(entry)\n\t\tvars_dict.update(entry)\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor var in extra_vars:\n\t\tif '=' not in var:\n\t\t\tcontinue\n\t\tname, value = var.split('=', 1)\n\t\tvars_dict[name] = value\n\treturn vars_dict\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor var in extra_vars:\n\t\tif '=' in var:\n\t\t\tkey, value = var.split('=', 1)\n\t\t\tvars_dict[key] = value\n\t\telif var.startswith('@'):\n\t\t\tfile_name = var[1:]\n\t\t\ttry:\n\t\t\t\twith open(file_name, 'r') as f:\n\t\t\t\t\tvars_dict.update(json.load(f))\n\t\t\texcept IOError:\n\t\t\t\tlogger.warning('Failed to load extra-vars from file: %s', file_name)\n\t\telse:\n\t\t\tlogger.warning('Ignoring extra-vars: %s', var)\n\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor item in extra_vars:\n\t\tif \"=\" in item:\n\t\t\tk, v = item.split(\"=\")\n\t\telse:\n\t\t\tk, v = item, True\n\t\tvars_dict[k] = v\n\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is not None:\n\t\tfor var in extra_vars:\n\t\t\tif \"=\" in var:\n\t\t\t\tkey, value = var.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\tvars_dict[var] = True\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn\n\tfor var_name, var_value in extra_vars:\n\t\tif var_name in vars_dict:\n\t\t\t# TODO: Raise error\n\t\t\tpass\n\t\tvars_dict[var_name] = var_value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tfor var in extra_vars:\n\t\t\tif '=' in var:\n\t\t\t\t(key, value) = var.split('=', 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\tvars_dict[var] = True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is not None:\n\t\tfor var in extra_vars:\n\t\t\tif var.startswith('--extra-vars'):\n\t\t\t\t# get the value of the variable\n\t\t\t\tstart = len('--extra-vars')\n\t\t\t\tif var[start:start+1] == '=':\n\t\t\t\t\tstart += 1\n\t\t\t\tvalue = var[start:]\n\t\t\t\t# split the key and value\n\t\t\t\tkey = var[start:var.find('=')]\n\t\t\t\t# remove any quotes from the value\n\t\t\t\tif len(value) > 2 and value[0] == value[-1] and value[0] in ('\"',\"'\"):\n\t\t\t\t\tvalue = value[1:-1]\n\t\t\t\tvars_dict[key] = value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif isinstance(extra_vars, str):\n\t\textra_vars = extra_vars.split()\n\n\tif isinstance(extra_vars, list):\n\t\tfor extra_var in extra_vars:\n\t\t\tif \"=\" not in extra_var:\n\t\t\t\traise AnsibleOptionsError(\n\t\t\t\t\t'Extra vars must be in the form of key=value')\n\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\tif not isinstance(value, str):\n\t\t\t\tvalue = str(value)\n\t\t\tif key in vars_dict:\n\t\t\t\tif isinstance(vars_dict[key], str):\n\t\t\t\t\tvars_dict[key] = [vars_dict[key]]\n\t\t\t\tvars_dict[key].append(value)\n\t\t\telse:\n\t\t\t\tvars_dict[key] = value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split('=')\n\t\t\tvars_dict[key] = value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\treturn vars_dict\n\tfor entry in extra_vars:\n\t\tif isinstance(entry, basestring):\n\t\t\tentry = yaml.load(entry)\n\t\tvars_dict.update(entry)\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor var in extra_vars:\n\t\tif '=' not in var:\n\t\t\tcontinue\n\t\tname, value = var.split('=', 1)\n\t\tvars_dict[name] = value\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tverbose = verbose or 0\n\textra_vars = extra_vars or {}\n\n\tansible_args['inventory'] = ir_workspace.config.get('ansible', 'inventory')\n\tansible_args['verbose'] = verbose\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['playbook'] = playbook_path\n\n\treturn ir_plugin.ansible_plugin.run_playbook(ansible_args=ansible_args)\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tansible_args['playbook'] = playbook_path\n\n\tif verbose is not None:\n\t\tansible_args['verbosity'] = verbose\n\n\tif extra_vars is not None:\n\t\tansible_args['extra-vars'] = extra_vars\n\n\textra_args = []\n\tfor k, v in ansible_args.items():\n\t\tif v is not None:\n\t\t\textra_args.extend([str(k), str(v)])\n\n\tplaybook_path = os.path.relpath(playbook_path, ir_workspace.project_dir)\n\tcmd = [\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t]\n\tcmd.extend(extra_args)\n\n\twith ir_plugin.spawn_shell(cmd, cwd=ir_workspace.project_dir) as shell:\n\t\tshell.wait()\n\n\t\tif shell.returncode != 0:\n\t\t\traise Exception(f\"Ansible exited with non-zero exit code {shell.returncode}\")\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tir_workspace.logger.debug('Executing ansible-playbook...')\n\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tif not ansible_args.get('inventory'):\n\t\tansible_args['inventory'] = ir_workspace.config.ansible.inventory\n\n\tif not ansible_args.get('connection'):\n\t\tansible_args['connection'] = ir_workspace.config.ansible.connection\n\n\tif not ansible_args.get('ask_sudo_pass'):\n\t\tansible_args['ask_sudo_pass'] = ir_workspace.config.ansible.ask_sudo_pass\n\n\tif not ansible_args.get('ask_vault_pass'):\n\t\tansible_args['ask_vault_pass'] = ir_workspace.config.ansible.ask_vault_pass\n\n\tif not ansible_args.get('verbosity'):\n\t\tansible_args['verbosity'] = verbose\n\n\tif not ansible_args.get('module_path'):\n\t\tansible_args['module_path'] = ir_workspace.config.ansible.module_path", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\n\tconfig = ir_plugin.config\n\tansible_bin = ir_plugin.get_ansible_bin()\n\n\tansible_args = ansible_args or {}\n\t# FIXME: ansible-playbook needs ansible_python_interpreter in ansible_args\n\t# but it breaks ansible-playbook --list-hosts\n\t# ansible_args['ansible_python_interpreter'] = config.get('ansible_python_interpreter')\n\tif not ansible_args.get('become', False):\n\t\tansible_args['become'] = config.get('become', False)\n\tif not ansible_args.get('become_user', False):\n\t\tansible_args['become_user'] = config.get('become_user', False)\n\tif not ansible_args.get('become_method', False):\n\t\tansible_args['become_method'] = config.get('become_method', False)\n\n\tif not ansible_args.get('v', verbose):\n\t\tansible_args['v'] = verbose\n\tif not ansible_args.get('extra_vars', False):\n\t\tansible_args['", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tansible_args.setdefault('become', True)\n\tansible_args.setdefault('become_method', 'sudo')\n\tansible_args.setdefault('become_user', 'root')\n\tansible_args.setdefault('skip_tags', [])\n\treturn ir_workspace.run_ansible_playbook(playbook_path, verbose=verbose,\n                                             extra_vars=extra_vars,\n                                             ansible_args=ansible_args)\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t# NOTE(harlowja): this is a bit of a hack, but we don't have a good way of\n\t# getting the current playbook path, so we'll just use the current\n\t# workspace path as the playbook path.\n\tif playbook_path is None:\n\t\tplaybook_path = ir_workspace.workspace_path\n\n\t# NOTE(harlowja): we don't want to mess with ansible's command line args,\n\t# so we'll just look for any extra vars, if any are found, and add them\n\t# to the ansible command line args.\n\tif extra_vars is not None:\n\t\tansible_args = ansible_args or {}\n\t\t# NOTE(HarlowJA): this will overwrite any existing args, but we don't\n\t\t# really care.\n\t\tansible_args[\"extra_vars\"] = extra_vars\n\n\t# NOTE(HarlowJA): ansible-playbook cli args are passed like so:\n\t#     ansible-playbook ... --some-cli-arg some-cli-arg-value ...\n\t#\n\t#     ansible-playbook ... --some-cli-arg=\"some-cli-arg-value\" ...\n\t#\n\t#     ansible", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tir_workspace.logger.debug(\"Running Ansible playbook: %s\", playbook_path)\n\n\t# create the extra-vars dict if it is None\n\tif extra_vars is None:\n\t\textra_vars = {}\n\n\t# create the ansible-playbook args dict if it is None\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# create a temp file to store the extra-vars\n\textra_vars_file = tempfile.NamedTemporaryFile()\n\textra_vars_file.write(json.dumps(extra_vars))\n\textra_vars_file.flush()\n\n\t# create the ansible-playbook args dict if it is None\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# create the ansible-playbook args dict if it is None\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# run the playbook\n\tplaybook_command = [\n\t\t\"ansible-playbook\",\n\t\tplaybook_path,\n\t\t\"--extra-vars", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom infrared.core.utils import run_command\n\n\tansible_env = os.environ.copy()\n\tansible_env['ANSIBLE_FORCE_COLOR'] = '1'\n\tif ansible_args:\n\t\tif ansible_args.get('become'):\n\t\t\tansible_env['ANSIBLE_BECOME'] = '1'\n\t\tif ansible_args.get('become_user'):\n\t\t\tansible_env['ANSIBLE_BECOME_USER'] = ansible_args['become_user']\n\t\tif ansible_args.get('become_method'):\n\t\t\tansible_env['ANSIBLE_BECOME_METHOD'] = ansible_args['become_method']\n\t\tif ansible_args.get('become_pass'):\n\t\t\tansible_env['ANSIBLE_BECOME_PASS'] = ansible_args['become_pass']\n\t\tif ansible_args.get('limit'):\n\t\t\tansible_env['ANSIBLE_LIMIT'] = ansible_args['limit']\n\t\tif ansible_args.get('extra_vars'):\n\t\t", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_path = ir_workspace.get_ansible_path()\n\tif not ansible_path:\n\t\traise RuntimeError(\n\t\t\t\"Could not find ansible path. Run 'ansible --version' \"\n\t\t\t\"to find out where ansible is installed.\")\n\tplaybook_path = os.path.join(ansible_path, playbook_path)\n\targs = ['ansible-playbook', playbook_path]\n\tif verbose:\n\t\targs.append('--verbose={}'.format(verbose))\n\tif extra_vars:\n\t\targs.append('--extra-vars={}'.format(extra_vars))\n\tif ansible_args:\n\t\tfor arg in ansible_args:\n\t\t\targs.append(arg)\n\tir_workspace.run_command(args, cwd=ansible_path)\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\tlogger.debug(\"Executing ansible-playbook on %s\" % playbook_path)\n\tlogger.debug(\"Ansible verbosity level is set to %s\" % verbose)\n\n\targs = [\n\t\t'ansible-playbook',\n\t\tplaybook_path\n\t]\n\tif ansible_args:\n\t\tfor arg, val in ansible_args.items():\n\t\t\targs.append('--%s=%s' % (arg, val))\n\n\tlogger.debug(\"Calling ansible-playbook with the following args: %s\" %\n\t\t\t\t args)\n\n\tif not verbose:\n\t\tverbose = ir_workspace.get_setting('ansible_verbosity', default=0)\n\tif verbose:\n\t\targs.append('-vvv')\n\tif extra_vars:\n\t\targs.append('-e %s' % json.dumps(extra_vars))\n\tif ir_workspace.get_setting('ansible_check', default=False):\n\t\targs.append('--check')\n\n\tlogger.debug(\"Executing: %s\" % ' '.join(args))\n\tsubprocess.call(args, shell=False)\n\n\ndef"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tverbose = verbose or 0\n\textra_vars = extra_vars or {}\n\n\tansible_args['inventory'] = ir_workspace.config.get('ansible', 'inventory')\n\tansible_args['verbose'] = verbose\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['playbook'] = playbook_path\n\n\treturn ir_plugin.ansible_plugin.run_playbook(ansible_args=ansible_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tansible_args['playbook'] = playbook_path\n\n\tif verbose is not None:\n\t\tansible_args['verbosity'] = verbose\n\n\tif extra_vars is not None:\n\t\tansible_args['extra-vars'] = extra_vars\n\n\textra_args = []\n\tfor k, v in ansible_args.items():\n\t\tif v is not None:\n\t\t\textra_args.extend([str(k), str(v)])\n\n\tplaybook_path = os.path.relpath(playbook_path, ir_workspace.project_dir)\n\tcmd = [\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t]\n\tcmd.extend(extra_args)\n\n\twith ir_plugin.spawn_shell(cmd, cwd=ir_workspace.project_dir) as shell:\n\t\tshell.wait()\n\n\t\tif shell.returncode != 0:\n\t\t\traise Exception(f\"Ansible exited with non-zero exit code {shell.returncode}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tir_workspace.logger.debug('Executing ansible-playbook...')\n\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tif not ansible_args.get('inventory'):\n\t\tansible_args['inventory'] = ir_workspace.config.ansible.inventory\n\n\tif not ansible_args.get('connection'):\n\t\tansible_args['connection'] = ir_workspace.config.ansible.connection\n\n\tif not ansible_args.get('ask_sudo_pass'):\n\t\tansible_args['ask_sudo_pass'] = ir_workspace.config.ansible.ask_sudo_pass\n\n\tif not ansible_args.get('ask_vault_pass'):\n\t\tansible_args['ask_vault_pass'] = ir_workspace.config.ansible.ask_vault_pass\n\n\tif not ansible_args.get('verbosity'):\n\t\tansible_args['verbosity'] = verbose\n\n\tif not ansible_args.get('module_path'):\n\t\tansible_args['module_path'] = ir_workspace.config.ansible.module_path", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\n\tconfig = ir_plugin.config\n\tansible_bin = ir_plugin.get_ansible_bin()\n\n\tansible_args = ansible_args or {}\n\t# FIXME: ansible-playbook needs ansible_python_interpreter in ansible_args\n\t# but it breaks ansible-playbook --list-hosts\n\t# ansible_args['ansible_python_interpreter'] = config.get('ansible_python_interpreter')\n\tif not ansible_args.get('become', False):\n\t\tansible_args['become'] = config.get('become', False)\n\tif not ansible_args.get('become_user', False):\n\t\tansible_args['become_user'] = config.get('become_user', False)\n\tif not ansible_args.get('become_method', False):\n\t\tansible_args['become_method'] = config.get('become_method', False)\n\n\tif not ansible_args.get('v', verbose):\n\t\tansible_args['v'] = verbose\n\tif not ansible_args.get('extra_vars', False):\n\t\tansible_args['", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\tansible_args.setdefault('become', True)\n\tansible_args.setdefault('become_method', 'sudo')\n\tansible_args.setdefault('become_user', 'root')\n\tansible_args.setdefault('skip_tags', [])\n\treturn ir_workspace.run_ansible_playbook(playbook_path, verbose=verbose,\n                                             extra_vars=extra_vars,\n                                             ansible_args=ansible_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t# NOTE(harlowja): this is a bit of a hack, but we don't have a good way of\n\t# getting the current playbook path, so we'll just use the current\n\t# workspace path as the playbook path.\n\tif playbook_path is None:\n\t\tplaybook_path = ir_workspace.workspace_path\n\n\t# NOTE(harlowja): we don't want to mess with ansible's command line args,\n\t# so we'll just look for any extra vars, if any are found, and add them\n\t# to the ansible command line args.\n\tif extra_vars is not None:\n\t\tansible_args = ansible_args or {}\n\t\t# NOTE(HarlowJA): this will overwrite any existing args, but we don't\n\t\t# really care.\n\t\tansible_args[\"extra_vars\"] = extra_vars\n\n\t# NOTE(HarlowJA): ansible-playbook cli args are passed like so:\n\t#     ansible-playbook ... --some-cli-arg some-cli-arg-value ...\n\t#\n\t#     ansible-playbook ... --some-cli-arg=\"some-cli-arg-value\" ...\n\t#\n\t#     ansible", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tir_workspace.logger.debug(\"Running Ansible playbook: %s\", playbook_path)\n\n\t# create the extra-vars dict if it is None\n\tif extra_vars is None:\n\t\textra_vars = {}\n\n\t# create the ansible-playbook args dict if it is None\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# create a temp file to store the extra-vars\n\textra_vars_file = tempfile.NamedTemporaryFile()\n\textra_vars_file.write(json.dumps(extra_vars))\n\textra_vars_file.flush()\n\n\t# create the ansible-playbook args dict if it is None\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# create the ansible-playbook args dict if it is None\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# run the playbook\n\tplaybook_command = [\n\t\t\"ansible-playbook\",\n\t\tplaybook_path,\n\t\t\"--extra-vars", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom infrared.core.utils import run_command\n\n\tansible_env = os.environ.copy()\n\tansible_env['ANSIBLE_FORCE_COLOR'] = '1'\n\tif ansible_args:\n\t\tif ansible_args.get('become'):\n\t\t\tansible_env['ANSIBLE_BECOME'] = '1'\n\t\tif ansible_args.get('become_user'):\n\t\t\tansible_env['ANSIBLE_BECOME_USER'] = ansible_args['become_user']\n\t\tif ansible_args.get('become_method'):\n\t\t\tansible_env['ANSIBLE_BECOME_METHOD'] = ansible_args['become_method']\n\t\tif ansible_args.get('become_pass'):\n\t\t\tansible_env['ANSIBLE_BECOME_PASS'] = ansible_args['become_pass']\n\t\tif ansible_args.get('limit'):\n\t\t\tansible_env['ANSIBLE_LIMIT'] = ansible_args['limit']\n\t\tif ansible_args.get('extra_vars'):\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_path = ir_workspace.get_ansible_path()\n\tif not ansible_path:\n\t\traise RuntimeError(\n\t\t\t\"Could not find ansible path. Run 'ansible --version' \"\n\t\t\t\"to find out where ansible is installed.\")\n\tplaybook_path = os.path.join(ansible_path, playbook_path)\n\targs = ['ansible-playbook', playbook_path]\n\tif verbose:\n\t\targs.append('--verbose={}'.format(verbose))\n\tif extra_vars:\n\t\targs.append('--extra-vars={}'.format(extra_vars))\n\tif ansible_args:\n\t\tfor arg in ansible_args:\n\t\t\targs.append(arg)\n\tir_workspace.run_command(args, cwd=ansible_path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\tlogger.debug(\"Executing ansible-playbook on %s\" % playbook_path)\n\tlogger.debug(\"Ansible verbosity level is set to %s\" % verbose)\n\n\targs = [\n\t\t'ansible-playbook',\n\t\tplaybook_path\n\t]\n\tif ansible_args:\n\t\tfor arg, val in ansible_args.items():\n\t\t\targs.append('--%s=%s' % (arg, val))\n\n\tlogger.debug(\"Calling ansible-playbook with the following args: %s\" %\n\t\t\t\t args)\n\n\tif not verbose:\n\t\tverbose = ir_workspace.get_setting('ansible_verbosity', default=0)\n\tif verbose:\n\t\targs.append('-vvv')\n\tif extra_vars:\n\t\targs.append('-e %s' % json.dumps(extra_vars))\n\tif ir_workspace.get_setting('ansible_check', default=False):\n\t\targs.append('--check')\n\n\tlogger.debug(\"Executing: %s\" % ' '.join(args))\n\tsubprocess.call(args, shell=False)\n\n\ndef", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# Get ansible paths\n\tansible_paths = get_ansible_paths(cli_args, ir_workspace, ir_plugin)\n\n\t# Run ansible playbook\n\tresult = run_ansible_playbook(cli_args, ir_workspace, ir_plugin,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ansible_paths, vars_dict)\n\n\t# Run post playbook hook\n\tpost_playbook_hook(cli_args, ir_workspace, ir_plugin, result)\n\n\treturn result\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# TODO: this should be moved to a common place\n\tlogger = logging.getLogger(__name__)\n\tlogger.info(\"Running Ansible playbook with args {}\".format(cli_args))\n\tif ir_plugin.ansible_config:\n\t\ttry:\n\t\t\twith open(ir_plugin.ansible_config, 'r') as f:\n\t\t\t\tconfig = yaml.safe_load(f)\n\t\t\t\tcli_args.extend([\"-c\", str(config.get(\"config_file\", \"ansible.cfg\"))])\n\t\t\t\tcli_args.extend([\"-e\", str(config.get(\"extra_vars\", \"\"))])\n\t\t\t\tcli_args.extend([\"-m\", str(config.get(\"module_name\", \"\"))])\n\t\t\t\tcli_args.extend([\"-a\", str(config.get(\"module_args\", \"\"))])\n\t\t\t\tcli_args.extend([\"-o\", str(config.get(\"output_file\", \"\"))])\n\t\t\t\tcli_args.extend([\"-t\", str(config.get(\"tags\", \"\"))])\n\t\t\t\tcli_args.extend([\"-f\", str(config.get(\"forks\", \"\"))])\n\t\t", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# get infrared config\n\tconfig = ir_workspace.config.get_plugin_config(ir_plugin.name)\n\tif not config:\n\t\traise Exception(\"Plugin config not found\")\n\n\t# get ansible config\n\tansible_config = config.get('ansible', {})\n\n\t# get ansible playbook\n\tplaybook_path = ansible_config.get('playbook', None)\n\tif not playbook_path:\n\t\traise Exception(\"Playbook path not found\")\n\n\t# get ansible inventory\n\tinventory_path = ansible_config.get('inventory', None)\n\tif not inventory_path:\n\t\traise Exception(\"Inventory path not found\")\n\n\t# get ansible ssh config\n\tssh_config = ansible_config.get('ssh', {})\n\n\t# get ansible ssh config\n\tansible_config = ansible_config.get('ansible', {})\n\n\t# check if a private key is set\n\tprivate_key = ssh_config.get('private_key', None)\n\n\t# check if an ansible password is set\n\tansible_password = ansible_config.get('password', None)\n\n\t# get ansible verbose level\n\tverbose = ansible_config.", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible_runner\n\timport json\n\n\toutput = ansible_runner.run(private_data_dir=ir_workspace.workspace_path,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcommand=cli_args,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tenvvars={\"ANSIBLE_NOCOLOR\": \"1\"},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\troles_path=\"./\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\textra_vars=json.dumps(vars_dict),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvault_password_file=ir_workspace.ansible_vault_pwd_file,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvault_id_file=ir_workspace.ansible_vault_id_file)\n\n\treturn output\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport tempfile\n\timport os\n\timport shutil\n\n\t# Create temp dir to store playbook and vars file\n\ttemp_dir = tempfile.mkdtemp()\n\n\t# Write vars into vars file\n\tvars_file = os.path.join(temp_dir, 'vars.yml')\n\twith open(vars_file, 'w') as f:\n\t\tf.write(vars_dict)\n\n\t# Write playbook into playbook file\n\tplaybook_file = os.path.join(temp_dir, 'playbook.yml')\n\twith open(playbook_file, 'w') as f:\n\t\tf.write(ir_workspace.get_playbook_content())\n\n\t# Run ansible cli with vars file and playbook file\n\targs = cli_args + ['-vvvv', '-i', 'localhost,', '-e', '@' + vars_file, playbook_file]\n\tret = ir_plugin.run_command('ansible-playbook', args)\n\n\t# Remove temp dir\n\tshutil.rmtree(temp_dir)\n\n\treturn ret\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tplaybook_path = cli_args.playbook\n\tplaybook_dir = os.path.dirname(playbook_path)\n\n\t# Create ansible command\n\trun_command = ['ansible-playbook', '-i', 'localhost,', '--connection=local']\n\n\t# Add extra vars\n\tif vars_dict:\n\t\trun_command.append('--extra-vars')\n\t\trun_command.append(json.dumps(vars_dict))\n\n\t# Add playbook\n\trun_command.append(playbook_path)\n\n\t# Execute ansible\n\tansible_results = subprocess.run(\n\t\trun_command,\n\t\tshell=False,\n\t\tcwd=playbook_dir,\n\t\tcapture_output=True,\n\t\ttext=True\n\t)\n\n\t# Output\n\tprint(ansible_results.stdout)\n\tprint(ansible_results.stderr)\n\n\treturn ansible_results\n\n\n@cli.command()\n@click.argument('playbook')\n@click.option('--vars', '-v', default=None,\n\t\t\t  help='Vars for Ansible')\n@click.option('--vars-file', '-f', default=None,\n\t\t\t  help='", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# Run cli with vars\n\tcli_args.extend([\"-e\", \"@%s\" % os.path.abspath(vars_dict.get_file_path())])\n\t# run ansible cli\n\treturn run_cli(cli_args, ir_workspace, ir_plugin)\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# Check if we are running in a workspace\n\tif not ir_workspace:\n\t\traise ValueError(\"No workspace defined, please ensure that \"\n\t\t                 \"you have an active workspace\")\n\n\t# Check that we have a workspace with a playbook\n\tif not ir_workspace.playbook:\n\t\traise ValueError(\"No playbook defined for workspace, please \"\n\t\t                 \"ensure that you have a playbook defined\")\n\n\t# Check if we are running in ansible-playbook\n\tif 'ansible-playbook' not in cli_args:\n\t\traise ValueError(\"Please ensure that you are calling \"\n\t\t                 \"ansible-playbook\")\n\n\t# Check that we have an inventory\n\tif not ir_workspace.inventory:\n\t\traise ValueError(\"Please define an inventory for the workspace\")\n\n\t# Check if we have a credential for the inventory\n\tif not ir_workspace.inventory.credential:\n\t\traise ValueError(\"Please define an inventory credential for the \"\n\t\t                 \"workspace\")\n\n\t# Check that we have a playbook path\n\tif not ir_workspace.playbook.path:\n\t\traise ValueError(\"Please define a playbook path for the workspace\")", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tcli_args = cli_args + ['--extra-vars', json.dumps(vars_dict)]\n\ttry:\n\t\treturn ansible.cli.run(cli_args)\n\texcept Exception as e:\n\t\tir_plugin.get_logger().error(e)\n\t\texit(1)\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport os\n\tfrom ansible_runner import run\n\n\t# ansible-runner expects a dictionary of lists\n\t# https://github.com/ansible/ansible-runner/issues/125\n\tif vars_dict:\n\t\tif isinstance(vars_dict, dict):\n\t\t\tvars_dict = vars_dict.items()\n\n\t\tif not isinstance(vars_dict, list):\n\t\t\traise TypeError('vars_dict must be a dict or a list of tuples')\n\n\t\tfor k, v in vars_dict:\n\t\t\tif not isinstance(v, list):\n\t\t\t\tvars_dict[k] = [v]\n\n\t# Ansible-runner expects the playbook to be in the working directory\n\t# so we create a temporary dir for that\n\ttemp_dir = os.path.join(ir_workspace.get_artifact_dir(), 'ansible')\n\tos.makedirs(temp_dir)\n\n\t# we create a playbook with a single task, and a single include\n\t# statement\n\tplaybook_content = '''\n\t---\n\t- hosts: all\n\t  tasks:\n\t    - include: ansible_playbook.yml\n\t'''\n\tplaybook_path = os.path.join(temp_"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# Get ansible paths\n\tansible_paths = get_ansible_paths(cli_args, ir_workspace, ir_plugin)\n\n\t# Run ansible playbook\n\tresult = run_ansible_playbook(cli_args, ir_workspace, ir_plugin,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ansible_paths, vars_dict)\n\n\t# Run post playbook hook\n\tpost_playbook_hook(cli_args, ir_workspace, ir_plugin, result)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# TODO: this should be moved to a common place\n\tlogger = logging.getLogger(__name__)\n\tlogger.info(\"Running Ansible playbook with args {}\".format(cli_args))\n\tif ir_plugin.ansible_config:\n\t\ttry:\n\t\t\twith open(ir_plugin.ansible_config, 'r') as f:\n\t\t\t\tconfig = yaml.safe_load(f)\n\t\t\t\tcli_args.extend([\"-c\", str(config.get(\"config_file\", \"ansible.cfg\"))])\n\t\t\t\tcli_args.extend([\"-e\", str(config.get(\"extra_vars\", \"\"))])\n\t\t\t\tcli_args.extend([\"-m\", str(config.get(\"module_name\", \"\"))])\n\t\t\t\tcli_args.extend([\"-a\", str(config.get(\"module_args\", \"\"))])\n\t\t\t\tcli_args.extend([\"-o\", str(config.get(\"output_file\", \"\"))])\n\t\t\t\tcli_args.extend([\"-t\", str(config.get(\"tags\", \"\"))])\n\t\t\t\tcli_args.extend([\"-f\", str(config.get(\"forks\", \"\"))])\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# get infrared config\n\tconfig = ir_workspace.config.get_plugin_config(ir_plugin.name)\n\tif not config:\n\t\traise Exception(\"Plugin config not found\")\n\n\t# get ansible config\n\tansible_config = config.get('ansible', {})\n\n\t# get ansible playbook\n\tplaybook_path = ansible_config.get('playbook', None)\n\tif not playbook_path:\n\t\traise Exception(\"Playbook path not found\")\n\n\t# get ansible inventory\n\tinventory_path = ansible_config.get('inventory', None)\n\tif not inventory_path:\n\t\traise Exception(\"Inventory path not found\")\n\n\t# get ansible ssh config\n\tssh_config = ansible_config.get('ssh', {})\n\n\t# get ansible ssh config\n\tansible_config = ansible_config.get('ansible', {})\n\n\t# check if a private key is set\n\tprivate_key = ssh_config.get('private_key', None)\n\n\t# check if an ansible password is set\n\tansible_password = ansible_config.get('password', None)\n\n\t# get ansible verbose level\n\tverbose = ansible_config.", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible_runner\n\timport json\n\n\toutput = ansible_runner.run(private_data_dir=ir_workspace.workspace_path,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcommand=cli_args,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tenvvars={\"ANSIBLE_NOCOLOR\": \"1\"},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\troles_path=\"./\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\textra_vars=json.dumps(vars_dict),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvault_password_file=ir_workspace.ansible_vault_pwd_file,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvault_id_file=ir_workspace.ansible_vault_id_file)\n\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport tempfile\n\timport os\n\timport shutil\n\n\t# Create temp dir to store playbook and vars file\n\ttemp_dir = tempfile.mkdtemp()\n\n\t# Write vars into vars file\n\tvars_file = os.path.join(temp_dir, 'vars.yml')\n\twith open(vars_file, 'w') as f:\n\t\tf.write(vars_dict)\n\n\t# Write playbook into playbook file\n\tplaybook_file = os.path.join(temp_dir, 'playbook.yml')\n\twith open(playbook_file, 'w') as f:\n\t\tf.write(ir_workspace.get_playbook_content())\n\n\t# Run ansible cli with vars file and playbook file\n\targs = cli_args + ['-vvvv', '-i', 'localhost,', '-e', '@' + vars_file, playbook_file]\n\tret = ir_plugin.run_command('ansible-playbook', args)\n\n\t# Remove temp dir\n\tshutil.rmtree(temp_dir)\n\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tplaybook_path = cli_args.playbook\n\tplaybook_dir = os.path.dirname(playbook_path)\n\n\t# Create ansible command\n\trun_command = ['ansible-playbook', '-i', 'localhost,', '--connection=local']\n\n\t# Add extra vars\n\tif vars_dict:\n\t\trun_command.append('--extra-vars')\n\t\trun_command.append(json.dumps(vars_dict))\n\n\t# Add playbook\n\trun_command.append(playbook_path)\n\n\t# Execute ansible\n\tansible_results = subprocess.run(\n\t\trun_command,\n\t\tshell=False,\n\t\tcwd=playbook_dir,\n\t\tcapture_output=True,\n\t\ttext=True\n\t)\n\n\t# Output\n\tprint(ansible_results.stdout)\n\tprint(ansible_results.stderr)\n\n\treturn ansible_results\n\n\n@cli.command()\n@click.argument('playbook')\n@click.option('--vars', '-v', default=None,\n\t\t\t  help='Vars for Ansible')\n@click.option('--vars-file', '-f', default=None,\n\t\t\t  help='", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# Run cli with vars\n\tcli_args.extend([\"-e\", \"@%s\" % os.path.abspath(vars_dict.get_file_path())])\n\t# run ansible cli\n\treturn run_cli(cli_args, ir_workspace, ir_plugin)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t# Check if we are running in a workspace\n\tif not ir_workspace:\n\t\traise ValueError(\"No workspace defined, please ensure that \"\n\t\t                 \"you have an active workspace\")\n\n\t# Check that we have a workspace with a playbook\n\tif not ir_workspace.playbook:\n\t\traise ValueError(\"No playbook defined for workspace, please \"\n\t\t                 \"ensure that you have a playbook defined\")\n\n\t# Check if we are running in ansible-playbook\n\tif 'ansible-playbook' not in cli_args:\n\t\traise ValueError(\"Please ensure that you are calling \"\n\t\t                 \"ansible-playbook\")\n\n\t# Check that we have an inventory\n\tif not ir_workspace.inventory:\n\t\traise ValueError(\"Please define an inventory for the workspace\")\n\n\t# Check if we have a credential for the inventory\n\tif not ir_workspace.inventory.credential:\n\t\traise ValueError(\"Please define an inventory credential for the \"\n\t\t                 \"workspace\")\n\n\t# Check that we have a playbook path\n\tif not ir_workspace.playbook.path:\n\t\traise ValueError(\"Please define a playbook path for the workspace\")", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tcli_args = cli_args + ['--extra-vars', json.dumps(vars_dict)]\n\ttry:\n\t\treturn ansible.cli.run(cli_args)\n\texcept Exception as e:\n\t\tir_plugin.get_logger().error(e)\n\t\texit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport os\n\tfrom ansible_runner import run\n\n\t# ansible-runner expects a dictionary of lists\n\t# https://github.com/ansible/ansible-runner/issues/125\n\tif vars_dict:\n\t\tif isinstance(vars_dict, dict):\n\t\t\tvars_dict = vars_dict.items()\n\n\t\tif not isinstance(vars_dict, list):\n\t\t\traise TypeError('vars_dict must be a dict or a list of tuples')\n\n\t\tfor k, v in vars_dict:\n\t\t\tif not isinstance(v, list):\n\t\t\t\tvars_dict[k] = [v]\n\n\t# Ansible-runner expects the playbook to be in the working directory\n\t# so we create a temporary dir for that\n\ttemp_dir = os.path.join(ir_workspace.get_artifact_dir(), 'ansible')\n\tos.makedirs(temp_dir)\n\n\t# we create a playbook with a single task, and a single include\n\t# statement\n\tplaybook_content = '''\n\t---\n\t- hosts: all\n\t  tasks:\n\t    - include: ansible_playbook.yml\n\t'''\n\tplaybook_path = os.path.join(temp_", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == 'main':\n\t\tif 'vlan' in values_dict:\n\t\t\tvalues_dict['vlan'] = int(values_dict['vlan'])\n\t\tif 'iface' in values_dict:\n\t\t\tvalues_dict['iface'] = str(values_dict['iface'])\n\t\tif 'nic' in values_dict:\n\t\t\tvalues_dict['nic'] = str(values_dict['nic'])\n\t\tif 'ip' in values_dict:\n\t\t\tvalues_dict['ip'] = str(values_dict['ip'])\n\t\tif 'gateway' in values_dict:\n\t\t\tvalues_dict['gateway'] = str(values_dict['gateway'])\n\t\tif 'netmask' in values_dict:\n\t\t\tvalues_dict['netmask'] = str(values_dict['netmask'])\n\t\tif 'dns' in values_dict:\n\t\t\tvalues_dict['dns'] = str(values_dict['dns'])\n\t\tif 'domain' in values_dict:\n\t\t\tvalues_dict['domain'] = str(values_dict['domain'])\n\t\tif 'host' in values_dict:\n\t\t\tvalues_dict['host'] = str(values_dict['", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == 'main':\n\t\t# convert the boolean values\n\t\tfor k in values_dict:\n\t\t\tif k.startswith('--'):\n\t\t\t\tif values_dict[k] in ('True', 'true'):\n\t\t\t\t\tvalues_dict[k] = True\n\t\t\t\telif values_dict[k] in ('False', 'false'):\n\t\t\t\t\tvalues_dict[k] = False\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t# convert the int values\n\t\tfor k in values_dict:\n\t\t\tif k.startswith('--'):\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[k] = int(values_dict[k])\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# pylint: disable=too-many-branches\n\tif parser_name == 'main':\n\t\tif 'config' in values_dict:\n\t\t\tvalues_dict['config'] = os.path.abspath(values_dict['config'])\n\t\tif 'log-level' in values_dict:\n\t\t\tvalues_dict['log_level'] = values_dict['log-level']\n\t\tif 'log-file' in values_dict:\n\t\t\tvalues_dict['log_file'] = values_dict['log-file']\n\t\tif 'log-format' in values_dict:\n\t\t\tvalues_dict['log_format'] = values_dict['log-format']\n\t\tif 'log-timestamp' in values_dict:\n\t\t\tvalues_dict['log_timestamp'] = values_dict['log-timestamp']\n\t\tif 'log-color' in values_dict:\n\t\t\tvalues_dict['log_color'] = bool(values_dict['log-color'])\n\t\tif 'log-output' in values_dict:\n\t\t\tvalues_dict['log_output'] = values_dict['log-output']\n\t\tif 'verbose' in values_dict:\n\t\t\tvalues_dict['verbose'] = bool(", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == \"main\":\n\t\tif \"--version\" in values_dict:\n\t\t\tvalues_dict[\"version\"] = True\n\t\tif \"--help\" in values_dict:\n\t\t\tvalues_dict[\"help\"] = True\n\tif parser_name == \"virsh\":\n\t\tif \"--help\" in values_dict:\n\t\t\tvalues_dict[\"help\"] = True\n\t\tif \"--version\" in values_dict:\n\t\t\tvalues_dict[\"version\"] = True\n\t\tif \"--list\" in values_dict:\n\t\t\tvalues_dict[\"list\"] = True\n\t\tif \"--name\" in values_dict:\n\t\t\tvalues_dict[\"name\"] = values_dict[\"--name\"]\n\t\t\tdel values_dict[\"--name\"]\n\t\tif \"--uuid\" in values_dict:\n\t\t\tvalues_dict[\"uuid\"] = values_dict[\"--uuid\"]\n\t\t\tdel values_dict[\"--uuid\"]\n\t\tif \"--uuidgen\" in values_dict:\n\t\t\tvalues_dict[\"uuid\"] = uuid.uuid4()\n\t\tif \"--command\" in values_dict:\n\t\t\tvalues_dict[\"command\"] = values_dict[\"--command\"]\n\t\t\tdel values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name not in self._ARG_TYPES:\n\t\treturn\n\n\tfor arg_type in self._ARG_TYPES[parser_name]:\n\t\targ_type.convert(values_dict)\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name not in self._non_cli_args_converters:\n\t\treturn\n\n\tfor arg_name in self._non_cli_args_converters[parser_name]:\n\t\targ_value = values_dict.get(arg_name)\n\t\tif arg_value is None:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tvalues_dict[arg_name] = self._non_cli_args_converters[parser_name][arg_name](arg_value)\n\t\texcept Exception:\n\t\t\traise Exception(\"Cannot convert argument '%s' to %s\" % (arg_name, self._non_cli_args_converters[parser_name][arg_name]))\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == 'main':\n\t\tif 'domain-name' in values_dict:\n\t\t\tvalues_dict['domain-name'] = values_dict['domain-name'].split(',')\n\n\t\tif 'xml-path' in values_dict:\n\t\t\tvalues_dict['xml-path'] = values_dict['xml-path'].split(',')\n\n\t\tif 'xml-template' in values_dict:\n\t\t\tvalues_dict['xml-template'] = values_dict['xml-template'].split(',')\n\n\t\tif 'xml' in values_dict:\n\t\t\tvalues_dict['xml'] = values_dict['xml'].split(',')\n\n\t\tif 'key-path' in values_dict:\n\t\t\tvalues_dict['key-path'] = values_dict['key-path'].split(',')\n\n\t\tif 'xml-from-file' in values_dict:\n\t\t\tvalues_dict['xml-from-file'] = values_dict['xml-from-file'].split(',')\n\n\t\tif 'xml-from-string' in values_dict:\n\t\t\tvalues_dict['xml-from-string'] = values_dict['xml-from-string'].split(',')\n\n\t\tif 'template' in values_dict:\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tcli_args = _get_cli_args(parser_name)\n\tfor cli_arg in cli_args:\n\t\tif cli_arg.name in values_dict.keys():\n\t\t\ttry:\n\t\t\t\tif cli_arg.type == 'int':\n\t\t\t\t\tvalues_dict[cli_arg.name] = int(values_dict[cli_arg.name])\n\t\t\t\telif cli_arg.type == 'float':\n\t\t\t\t\tvalues_dict[cli_arg.name] = float(values_dict[cli_arg.name])\n\t\t\t\telif cli_arg.type == 'bool':\n\t\t\t\t\tvalues_dict[cli_arg.name] = True if values_dict[cli_arg.name].lower() == 'true' else False\n\n\t\t\texcept ValueError:\n\t\t\t\tprint('Error: Argument {name} is of type {type} and its value {value} cannot be converted.'.format(\n\t\t\t\t\tname=cli_arg.name,\n\t\t\t\t\ttype=cli_arg.type,\n\t\t\t\t\tvalue=values_dict[cli_arg.name]))\n\t\t\t\tsys.exit(1)\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# Check if this is a cli command\n\tif parser_name in self.cli_commands:\n\t\treturn\n\n\tparser = self.get_parser(parser_name)\n\tfor action in parser._actions:\n\t\t# If the action has a dest attribute, check if the value should be casted\n\t\tif hasattr(action, 'dest'):\n\t\t\t# If the value exists in the values_dict, cast it\n\t\t\tif action.dest in values_dict.keys():\n\t\t\t\t# Check if the value should be casted\n\t\t\t\tif hasattr(action, 'type'):\n\t\t\t\t\t# Cast the value with the action.type\n\t\t\t\t\tvalues_dict[action.dest] = action.type(values_dict[action.dest])\n\n\t\t\t\t\t# Check if the value should be casted as a list\n\t\t\t\t\tif hasattr(action, 'choices'):\n\t\t\t\t\t\t# Cast the value as a list\n\t\t\t\t\t\tvalues_dict[action.dest] = [action.type(x) for x in values_dict[action.dest].split(',')]\n\n\t\t# If the action has an option_strings attribute, check if the value should be casted\n\t\tif hasattr(", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# Converts the values in values_dict to correct types\n\tfor key, value in values_dict.items():\n\t\tif key not in self.arg_info[parser_name]:\n\t\t\tcontinue\n\t\targ_info = self.arg_info[parser_name][key]\n\t\tif not value:\n\t\t\tcontinue\n\t\tif arg_info.typ == 'bool':\n\t\t\tvalues_dict[key] = utils.str2bool(value)\n\t\telif arg_info.typ == 'int':\n\t\t\tvalues_dict[key] = int(value)\n\t\telif arg_info.typ == 'float':\n\t\t\tvalues_dict[key] = float(value)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == 'main':\n\t\tif 'vlan' in values_dict:\n\t\t\tvalues_dict['vlan'] = int(values_dict['vlan'])\n\t\tif 'iface' in values_dict:\n\t\t\tvalues_dict['iface'] = str(values_dict['iface'])\n\t\tif 'nic' in values_dict:\n\t\t\tvalues_dict['nic'] = str(values_dict['nic'])\n\t\tif 'ip' in values_dict:\n\t\t\tvalues_dict['ip'] = str(values_dict['ip'])\n\t\tif 'gateway' in values_dict:\n\t\t\tvalues_dict['gateway'] = str(values_dict['gateway'])\n\t\tif 'netmask' in values_dict:\n\t\t\tvalues_dict['netmask'] = str(values_dict['netmask'])\n\t\tif 'dns' in values_dict:\n\t\t\tvalues_dict['dns'] = str(values_dict['dns'])\n\t\tif 'domain' in values_dict:\n\t\t\tvalues_dict['domain'] = str(values_dict['domain'])\n\t\tif 'host' in values_dict:\n\t\t\tvalues_dict['host'] = str(values_dict['", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == 'main':\n\t\t# convert the boolean values\n\t\tfor k in values_dict:\n\t\t\tif k.startswith('--'):\n\t\t\t\tif values_dict[k] in ('True', 'true'):\n\t\t\t\t\tvalues_dict[k] = True\n\t\t\t\telif values_dict[k] in ('False', 'false'):\n\t\t\t\t\tvalues_dict[k] = False\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t# convert the int values\n\t\tfor k in values_dict:\n\t\t\tif k.startswith('--'):\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[k] = int(values_dict[k])\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# pylint: disable=too-many-branches\n\tif parser_name == 'main':\n\t\tif 'config' in values_dict:\n\t\t\tvalues_dict['config'] = os.path.abspath(values_dict['config'])\n\t\tif 'log-level' in values_dict:\n\t\t\tvalues_dict['log_level'] = values_dict['log-level']\n\t\tif 'log-file' in values_dict:\n\t\t\tvalues_dict['log_file'] = values_dict['log-file']\n\t\tif 'log-format' in values_dict:\n\t\t\tvalues_dict['log_format'] = values_dict['log-format']\n\t\tif 'log-timestamp' in values_dict:\n\t\t\tvalues_dict['log_timestamp'] = values_dict['log-timestamp']\n\t\tif 'log-color' in values_dict:\n\t\t\tvalues_dict['log_color'] = bool(values_dict['log-color'])\n\t\tif 'log-output' in values_dict:\n\t\t\tvalues_dict['log_output'] = values_dict['log-output']\n\t\tif 'verbose' in values_dict:\n\t\t\tvalues_dict['verbose'] = bool(", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == \"main\":\n\t\tif \"--version\" in values_dict:\n\t\t\tvalues_dict[\"version\"] = True\n\t\tif \"--help\" in values_dict:\n\t\t\tvalues_dict[\"help\"] = True\n\tif parser_name == \"virsh\":\n\t\tif \"--help\" in values_dict:\n\t\t\tvalues_dict[\"help\"] = True\n\t\tif \"--version\" in values_dict:\n\t\t\tvalues_dict[\"version\"] = True\n\t\tif \"--list\" in values_dict:\n\t\t\tvalues_dict[\"list\"] = True\n\t\tif \"--name\" in values_dict:\n\t\t\tvalues_dict[\"name\"] = values_dict[\"--name\"]\n\t\t\tdel values_dict[\"--name\"]\n\t\tif \"--uuid\" in values_dict:\n\t\t\tvalues_dict[\"uuid\"] = values_dict[\"--uuid\"]\n\t\t\tdel values_dict[\"--uuid\"]\n\t\tif \"--uuidgen\" in values_dict:\n\t\t\tvalues_dict[\"uuid\"] = uuid.uuid4()\n\t\tif \"--command\" in values_dict:\n\t\t\tvalues_dict[\"command\"] = values_dict[\"--command\"]\n\t\t\tdel values_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name not in self._ARG_TYPES:\n\t\treturn\n\n\tfor arg_type in self._ARG_TYPES[parser_name]:\n\t\targ_type.convert(values_dict)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name not in self._non_cli_args_converters:\n\t\treturn\n\n\tfor arg_name in self._non_cli_args_converters[parser_name]:\n\t\targ_value = values_dict.get(arg_name)\n\t\tif arg_value is None:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tvalues_dict[arg_name] = self._non_cli_args_converters[parser_name][arg_name](arg_value)\n\t\texcept Exception:\n\t\t\traise Exception(\"Cannot convert argument '%s' to %s\" % (arg_name, self._non_cli_args_converters[parser_name][arg_name]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name == 'main':\n\t\tif 'domain-name' in values_dict:\n\t\t\tvalues_dict['domain-name'] = values_dict['domain-name'].split(',')\n\n\t\tif 'xml-path' in values_dict:\n\t\t\tvalues_dict['xml-path'] = values_dict['xml-path'].split(',')\n\n\t\tif 'xml-template' in values_dict:\n\t\t\tvalues_dict['xml-template'] = values_dict['xml-template'].split(',')\n\n\t\tif 'xml' in values_dict:\n\t\t\tvalues_dict['xml'] = values_dict['xml'].split(',')\n\n\t\tif 'key-path' in values_dict:\n\t\t\tvalues_dict['key-path'] = values_dict['key-path'].split(',')\n\n\t\tif 'xml-from-file' in values_dict:\n\t\t\tvalues_dict['xml-from-file'] = values_dict['xml-from-file'].split(',')\n\n\t\tif 'xml-from-string' in values_dict:\n\t\t\tvalues_dict['xml-from-string'] = values_dict['xml-from-string'].split(',')\n\n\t\tif 'template' in values_dict:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tcli_args = _get_cli_args(parser_name)\n\tfor cli_arg in cli_args:\n\t\tif cli_arg.name in values_dict.keys():\n\t\t\ttry:\n\t\t\t\tif cli_arg.type == 'int':\n\t\t\t\t\tvalues_dict[cli_arg.name] = int(values_dict[cli_arg.name])\n\t\t\t\telif cli_arg.type == 'float':\n\t\t\t\t\tvalues_dict[cli_arg.name] = float(values_dict[cli_arg.name])\n\t\t\t\telif cli_arg.type == 'bool':\n\t\t\t\t\tvalues_dict[cli_arg.name] = True if values_dict[cli_arg.name].lower() == 'true' else False\n\n\t\t\texcept ValueError:\n\t\t\t\tprint('Error: Argument {name} is of type {type} and its value {value} cannot be converted.'.format(\n\t\t\t\t\tname=cli_arg.name,\n\t\t\t\t\ttype=cli_arg.type,\n\t\t\t\t\tvalue=values_dict[cli_arg.name]))\n\t\t\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# Check if this is a cli command\n\tif parser_name in self.cli_commands:\n\t\treturn\n\n\tparser = self.get_parser(parser_name)\n\tfor action in parser._actions:\n\t\t# If the action has a dest attribute, check if the value should be casted\n\t\tif hasattr(action, 'dest'):\n\t\t\t# If the value exists in the values_dict, cast it\n\t\t\tif action.dest in values_dict.keys():\n\t\t\t\t# Check if the value should be casted\n\t\t\t\tif hasattr(action, 'type'):\n\t\t\t\t\t# Cast the value with the action.type\n\t\t\t\t\tvalues_dict[action.dest] = action.type(values_dict[action.dest])\n\n\t\t\t\t\t# Check if the value should be casted as a list\n\t\t\t\t\tif hasattr(action, 'choices'):\n\t\t\t\t\t\t# Cast the value as a list\n\t\t\t\t\t\tvalues_dict[action.dest] = [action.type(x) for x in values_dict[action.dest].split(',')]\n\n\t\t# If the action has an option_strings attribute, check if the value should be casted\n\t\tif hasattr(", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# Converts the values in values_dict to correct types\n\tfor key, value in values_dict.items():\n\t\tif key not in self.arg_info[parser_name]:\n\t\t\tcontinue\n\t\targ_info = self.arg_info[parser_name][key]\n\t\tif not value:\n\t\t\tcontinue\n\t\tif arg_info.typ == 'bool':\n\t\t\tvalues_dict[key] = utils.str2bool(value)\n\t\telif arg_info.typ == 'int':\n\t\t\tvalues_dict[key] = int(value)\n\t\telif arg_info.typ == 'float':\n\t\t\tvalues_dict[key] = float(value)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t# TODO: add support for multiple plugin_dirs\n\tplugin_spec = {}\n\tplugin_dir_path = Path(plugin_dir)\n\tplugin_spec_dir = plugin_dir_path / 'plugin.spec'\n\tif plugin_spec_dir.is_dir():\n\t\tplugin_spec['plugin_dir'] = plugin_dir\n\t\tplugin_spec['plugin_spec_dir'] = plugin_spec_dir\n\t\tplugin_spec['plugin_dir_path'] = plugin_dir_path\n\t\tplugin_spec['plugin_name'] = plugin_spec_dir.name\n\t\tplugin_spec['plugin_spec_file'] = plugin_spec_dir / 'plugin.spec.yaml'\n\t\tplugin_spec['plugin_spec'] = yaml.safe_load(plugin_spec['plugin_spec_file'].read_text())\n\t\tplugin_spec['plugin_spec']['plugin_name'] = plugin_spec['plugin_name']\n\t\tplugin_spec['plugin_spec']['plugin_dir'] = plugin_spec['plugin_dir']\n\t\tplugin_spec['plugin_spec']['plugin_spec_dir'] = plugin_spec['plugin_spec_dir']\n\t\tplugin_spec['plugin_spec']['plugin_spec_file'] = plugin_spec['plugin_spec_file']", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t# Find all yaml files in plugin_dir\n\tyaml_files = []\n\tfor root, dirs, files in os.walk(plugin_dir):\n\t\tfor f in files:\n\t\t\tif os.path.splitext(f)[-1][1:] == \"yaml\":\n\t\t\t\tyaml_files.append(os.path.join(root, f))\n\n\t# Read all yaml files and flatten the dictionary\n\tspec_dict = {}\n\tfor yaml_file in yaml_files:\n\t\twith open(yaml_file) as f:\n\t\t\tspec_dict.update(yaml.load(f))\n\treturn spec_dict\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec = {}\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin.yaml')\n\tif os.path.exists(plugin_spec_path):\n\t\twith open(plugin_spec_path, 'r') as f:\n\t\t\tplugin_spec = yaml.safe_load(f)\n\telse:\n\t\tplugin_spec['name'] = os.path.basename(plugin_dir)\n\t\tplugin_spec['enabled'] = True\n\n\treturn plugin_spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tresult = {}\n\tplugin_yaml = os.path.join(plugin_dir, \"plugin.yaml\")\n\tif os.path.exists(plugin_yaml):\n\t\twith open(plugin_yaml, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tplugin_spec = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\t\t\t\texit(1)\n\t\tresult[\"plugin_spec\"] = plugin_spec\n\telse:\n\t\tprint(\"plugin.yaml not found in %s\" % plugin_dir)\n\t\texit(1)\n\n\tplugin_config_yaml = os.path.join(plugin_dir, \"plugin_config.yaml\")\n\tif os.path.exists(plugin_config_yaml):\n\t\twith open(plugin_config_yaml, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tplugin_config = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\t\t\t\texit(1)\n\t\tresult[\"plugin_config\"] = plugin_config\n\telse:\n\t\tprint(\"plugin_config.yaml not found", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tyaml_file_info_dict = dict()\n\tyaml_file_list = [f for f in os.listdir(plugin_dir) if os.path.isfile(os.path.join(plugin_dir, f))]\n\tfor yaml_file in yaml_file_list:\n\t\tplugin_name = yaml_file.split('.')[0]\n\t\tplugin_spec_yaml_file = os.path.join(plugin_dir, yaml_file)\n\t\twith open(plugin_spec_yaml_file) as f:\n\t\t\tyaml_file_info_dict[plugin_name] = yaml.load(f)\n\treturn yaml_file_info_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_dict = {}\n\tplugin_spec_path = plugin_dir + '/plugin.yml'\n\tif os.path.exists(plugin_spec_path):\n\t\twith open(plugin_spec_path, 'r') as f:\n\t\t\ttry:\n\t\t\t\tspec_dict = yaml.load(f, Loader=yaml.FullLoader)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\t\t\t\tsys.exit(1)\n\telse:\n\t\tprint(\"[ERROR] Missing plugin.yml in plugin directory {}\".format(plugin_dir))\n\t\tsys.exit(1)\n\n\tflatten_dict = {\n\t\t'name': spec_dict.get('name'),\n\t\t'display_name': spec_dict.get('display_name'),\n\t\t'description': spec_dict.get('description'),\n\t\t'version': spec_dict.get('version'),\n\t\t'license': spec_dict.get('license'),\n\t\t'tags': spec_dict.get('tags'),\n\t\t'required_parameters': spec_dict.get('required_parameters'),\n\t\t'optional_parameters': spec_dict.get('optional_parameters'),\n\t\t'output_parameters", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_dir = os.path.abspath(plugin_dir)\n\tif not os.path.exists(plugin_dir):\n\t\traise Exception(\"plugin directory not found: {}\".format(plugin_dir))\n\n\tplugin_spec = {}\n\ttry:\n\t\twith open(os.path.join(plugin_dir, \"plugin.yml\"), 'r') as f:\n\t\t\tplugin_spec.update(yaml.load(f))\n\texcept:\n\t\traise Exception(\"Failed to load plugin.yml\")\n\n\ttry:\n\t\twith open(os.path.join(plugin_dir, \"plugin_config.yml\"), 'r') as f:\n\t\t\tplugin_spec.update(yaml.load(f))\n\texcept:\n\t\tpass\n\n\t# flatten the nested dictionary\n\tplugin_spec = flatten_dict(plugin_spec, '.')\n\treturn plugin_spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec_dict = {}\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin.yaml\")\n\tif not os.path.exists(plugin_spec_file):\n\t\tplugin_spec_file = os.path.join(plugin_dir, \"plugin.yml\")\n\t\tif not os.path.exists(plugin_spec_file):\n\t\t\traise FileNotFoundError(f\"Plugin specification file not found in {plugin_dir}.\")\n\twith open(plugin_spec_file, \"r\") as f:\n\t\tplugin_spec_dict = yaml.safe_load(f)\n\treturn plugin_spec_dict\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin_spec.yml')\n\tif os.path.exists(plugin_spec_file):\n\t\twith open(plugin_spec_file, 'r') as f:\n\t\t\tplugin_spec = yaml.safe_load(f)\n\telse:\n\t\tplugin_spec = {}\n\t\n\t# flatten the dict\n\tplugin_spec_flatten = {}\n\tfor key, value in plugin_spec.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor sub_key, sub_value in value.items():\n\t\t\t\tplugin_spec_flatten['%s.%s' % (key, sub_key)] = sub_value\n\t\telse:\n\t\t\tplugin_spec_flatten[key] = value\n\n\treturn plugin_spec_flatten\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_file = os.path.join(plugin_dir, \"plugin.spec\")\n\n\tif not os.path.isfile(spec_file):\n\t\traise ValueError(\"Plugin: {} does not have a plugin.spec file.\".format(plugin_dir))\n\n\twith open(spec_file) as f:\n\t\tplugin_spec_dict = yaml.load(f, Loader=yaml.SafeLoader)\n\n\t# Flatten dictionary\n\tplugin_spec_flatten_dict = {}\n\tfor key, value in plugin_spec_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor subkey, subvalue in value.items():\n\t\t\t\tplugin_spec_flatten_dict[subkey] = subvalue\n\t\telse:\n\t\t\tplugin_spec_flatten_dict[key] = value\n\n\treturn plugin_spec_flatten_dict\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t# TODO: add support for multiple plugin_dirs\n\tplugin_spec = {}\n\tplugin_dir_path = Path(plugin_dir)\n\tplugin_spec_dir = plugin_dir_path / 'plugin.spec'\n\tif plugin_spec_dir.is_dir():\n\t\tplugin_spec['plugin_dir'] = plugin_dir\n\t\tplugin_spec['plugin_spec_dir'] = plugin_spec_dir\n\t\tplugin_spec['plugin_dir_path'] = plugin_dir_path\n\t\tplugin_spec['plugin_name'] = plugin_spec_dir.name\n\t\tplugin_spec['plugin_spec_file'] = plugin_spec_dir / 'plugin.spec.yaml'\n\t\tplugin_spec['plugin_spec'] = yaml.safe_load(plugin_spec['plugin_spec_file'].read_text())\n\t\tplugin_spec['plugin_spec']['plugin_name'] = plugin_spec['plugin_name']\n\t\tplugin_spec['plugin_spec']['plugin_dir'] = plugin_spec['plugin_dir']\n\t\tplugin_spec['plugin_spec']['plugin_spec_dir'] = plugin_spec['plugin_spec_dir']\n\t\tplugin_spec['plugin_spec']['plugin_spec_file'] = plugin_spec['plugin_spec_file']", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t# Find all yaml files in plugin_dir\n\tyaml_files = []\n\tfor root, dirs, files in os.walk(plugin_dir):\n\t\tfor f in files:\n\t\t\tif os.path.splitext(f)[-1][1:] == \"yaml\":\n\t\t\t\tyaml_files.append(os.path.join(root, f))\n\n\t# Read all yaml files and flatten the dictionary\n\tspec_dict = {}\n\tfor yaml_file in yaml_files:\n\t\twith open(yaml_file) as f:\n\t\t\tspec_dict.update(yaml.load(f))\n\treturn spec_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec = {}\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin.yaml')\n\tif os.path.exists(plugin_spec_path):\n\t\twith open(plugin_spec_path, 'r') as f:\n\t\t\tplugin_spec = yaml.safe_load(f)\n\telse:\n\t\tplugin_spec['name'] = os.path.basename(plugin_dir)\n\t\tplugin_spec['enabled'] = True\n\n\treturn plugin_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tresult = {}\n\tplugin_yaml = os.path.join(plugin_dir, \"plugin.yaml\")\n\tif os.path.exists(plugin_yaml):\n\t\twith open(plugin_yaml, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tplugin_spec = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\t\t\t\texit(1)\n\t\tresult[\"plugin_spec\"] = plugin_spec\n\telse:\n\t\tprint(\"plugin.yaml not found in %s\" % plugin_dir)\n\t\texit(1)\n\n\tplugin_config_yaml = os.path.join(plugin_dir, \"plugin_config.yaml\")\n\tif os.path.exists(plugin_config_yaml):\n\t\twith open(plugin_config_yaml, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tplugin_config = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\t\t\t\texit(1)\n\t\tresult[\"plugin_config\"] = plugin_config\n\telse:\n\t\tprint(\"plugin_config.yaml not found", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tyaml_file_info_dict = dict()\n\tyaml_file_list = [f for f in os.listdir(plugin_dir) if os.path.isfile(os.path.join(plugin_dir, f))]\n\tfor yaml_file in yaml_file_list:\n\t\tplugin_name = yaml_file.split('.')[0]\n\t\tplugin_spec_yaml_file = os.path.join(plugin_dir, yaml_file)\n\t\twith open(plugin_spec_yaml_file) as f:\n\t\t\tyaml_file_info_dict[plugin_name] = yaml.load(f)\n\treturn yaml_file_info_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_dict = {}\n\tplugin_spec_path = plugin_dir + '/plugin.yml'\n\tif os.path.exists(plugin_spec_path):\n\t\twith open(plugin_spec_path, 'r') as f:\n\t\t\ttry:\n\t\t\t\tspec_dict = yaml.load(f, Loader=yaml.FullLoader)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\t\t\t\tsys.exit(1)\n\telse:\n\t\tprint(\"[ERROR] Missing plugin.yml in plugin directory {}\".format(plugin_dir))\n\t\tsys.exit(1)\n\n\tflatten_dict = {\n\t\t'name': spec_dict.get('name'),\n\t\t'display_name': spec_dict.get('display_name'),\n\t\t'description': spec_dict.get('description'),\n\t\t'version': spec_dict.get('version'),\n\t\t'license': spec_dict.get('license'),\n\t\t'tags': spec_dict.get('tags'),\n\t\t'required_parameters': spec_dict.get('required_parameters'),\n\t\t'optional_parameters': spec_dict.get('optional_parameters'),\n\t\t'output_parameters", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_dir = os.path.abspath(plugin_dir)\n\tif not os.path.exists(plugin_dir):\n\t\traise Exception(\"plugin directory not found: {}\".format(plugin_dir))\n\n\tplugin_spec = {}\n\ttry:\n\t\twith open(os.path.join(plugin_dir, \"plugin.yml\"), 'r') as f:\n\t\t\tplugin_spec.update(yaml.load(f))\n\texcept:\n\t\traise Exception(\"Failed to load plugin.yml\")\n\n\ttry:\n\t\twith open(os.path.join(plugin_dir, \"plugin_config.yml\"), 'r') as f:\n\t\t\tplugin_spec.update(yaml.load(f))\n\texcept:\n\t\tpass\n\n\t# flatten the nested dictionary\n\tplugin_spec = flatten_dict(plugin_spec, '.')\n\treturn plugin_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec_dict = {}\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin.yaml\")\n\tif not os.path.exists(plugin_spec_file):\n\t\tplugin_spec_file = os.path.join(plugin_dir, \"plugin.yml\")\n\t\tif not os.path.exists(plugin_spec_file):\n\t\t\traise FileNotFoundError(f\"Plugin specification file not found in {plugin_dir}.\")\n\twith open(plugin_spec_file, \"r\") as f:\n\t\tplugin_spec_dict = yaml.safe_load(f)\n\treturn plugin_spec_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin_spec.yml')\n\tif os.path.exists(plugin_spec_file):\n\t\twith open(plugin_spec_file, 'r') as f:\n\t\t\tplugin_spec = yaml.safe_load(f)\n\telse:\n\t\tplugin_spec = {}\n\t\n\t# flatten the dict\n\tplugin_spec_flatten = {}\n\tfor key, value in plugin_spec.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor sub_key, sub_value in value.items():\n\t\t\t\tplugin_spec_flatten['%s.%s' % (key, sub_key)] = sub_value\n\t\telse:\n\t\t\tplugin_spec_flatten[key] = value\n\n\treturn plugin_spec_flatten\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_file = os.path.join(plugin_dir, \"plugin.spec\")\n\n\tif not os.path.isfile(spec_file):\n\t\traise ValueError(\"Plugin: {} does not have a plugin.spec file.\".format(plugin_dir))\n\n\twith open(spec_file) as f:\n\t\tplugin_spec_dict = yaml.load(f, Loader=yaml.SafeLoader)\n\n\t# Flatten dictionary\n\tplugin_spec_flatten_dict = {}\n\tfor key, value in plugin_spec_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor subkey, subvalue in value.items():\n\t\t\t\tplugin_spec_flatten_dict[subkey] = subvalue\n\t\telse:\n\t\t\tplugin_spec_flatten_dict[key] = value\n\n\treturn plugin_spec_flatten_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif not 'ANSIBLE_CONFIG' in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\treturn True\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\t\"\"\"\n\tIf the ANSIBLE_ROLES_PATH property does not exist in os.environ, add self.ansible_roles_path to the list.\n\t\"\"\"\n\tif 'ANSIBLE_ROLES_PATH' not in os.environ:\n\t\tos.environ['ANSIBLE_ROLES_PATH'] = self.ansible_roles_path\n\n\t\"\"\"\n\tIf the ANSIBLE_GALAXY_ROLE_FILE property does not exist in os.environ, set it to self.galaxy_role_file.\n\t\"\"\"\n\tif 'ANSIBLE_GALAXY_ROLE_FILE' not in os.environ:\n\t\tos.environ['ANSIBLE_GALAXY_ROLE_FILE'] = self.galaxy_role_file\n\n\t\"\"\"\n\tIf the ANSIBLE_GALAXY_ROLE_PATH property does not exist in os.environ, set it to self.galaxy_role_path.\n\t\"\"\"\n\tif 'ANSIBLE_GALAXY_ROLE_PATH", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\t# If the ANSIBLE_RETRY_FILES_ENABLED property does not exist in os.environ, set it to \"False\".\n\tif \"ANSIBLE_RETRY_FILES_ENABLED\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_RETRY_FILES_ENABLED\"] = \"False\"\n\n\t# If the ANSIBLE_SSH_ARGS property does not exist in os.environ, set it to \"-o ControlMaster=auto -o ControlPersist=60s\".\n\tif \"ANSIBLE_SSH_ARGS\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_SSH_ARGS\"] = \"-o ControlMaster=auto -o ControlPersist=60s\"\n\n\t# If the ANSIBLE_RETRY_FILES_ENABLED property does not exist in os.environ, set it to \"False\".\n\tif \"ANSIBLE_SSH_RETRIES\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_SSH_RETRI", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif not 'ANSIBLE_CONFIG' in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\t\"\"\"\n\tIf the ANSIBLE_ROLES_PATH property does not exist in os.environ, add self.ansible_roles_path to the list.\n\t\"\"\"\n\tif 'ANSIBLE_ROLES_PATH' not in os.environ:\n\t\tos.environ['ANSIBLE_ROLES_PATH'] = self.ansible_roles_path\n\n\t\"\"\"\n\tIf the ANSIBLE_GALAXY_ROLE_FILE property does not exist in os.environ, set it to self.galaxy_role_file.\n\t\"\"\"\n\tif 'ANSIBLE_GALAXY_ROLE_FILE' not in os.environ:\n\t\tos.environ['ANSIBLE_GALAXY_ROLE_FILE'] = self.galaxy_role_file\n\n\t\"\"\"\n\tIf the ANSIBLE_GALAXY_ROLE_PATH property does not exist in os.environ, set it to self.galaxy_role_path.\n\t\"\"\"\n\tif 'ANSIBLE_GALAXY_ROLE_PATH", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\t# If the ANSIBLE_RETRY_FILES_ENABLED property does not exist in os.environ, set it to \"False\".\n\tif \"ANSIBLE_RETRY_FILES_ENABLED\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_RETRY_FILES_ENABLED\"] = \"False\"\n\n\t# If the ANSIBLE_SSH_ARGS property does not exist in os.environ, set it to \"-o ControlMaster=auto -o ControlPersist=60s\".\n\tif \"ANSIBLE_SSH_ARGS\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_SSH_ARGS\"] = \"-o ControlMaster=auto -o ControlPersist=60s\"\n\n\t# If the ANSIBLE_RETRY_FILES_ENABLED property does not exist in os.environ, set it to \"False\".\n\tif \"ANSIBLE_SSH_RETRIES\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_SSH_RETRI", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='Specify a spec.')\n\tsubparser.set_defaults(func=self.run)\n\tsubparser.add_argument('-o', '--output',\n\t\thelp='Where to save the file. Defaults to the current directory.')\n\tsubparser.add_argument('-v', '--version', default='1.0',\n\t\thelp='The version number of the spec.')\n\tsubparser.add_argument('-t', '--title', required=True, help='The title of the spec.')\n\tsubparser.add_argument('--license', default='Apache-2.0',\n\t\thelp='The license of the spec.')\n\tsubparser.add_argument('--description', required=True,\n\t\thelp='The description of the spec.')\n\tsubparser.add_argument('--contact', required=True,\n\t\thelp='The contact of the spec.')\n\tsubparser.add_argument('--url', required=True,\n\t\thelp='The url of the spec.')\n\tsubparser.add_argument('--email', required=True,\n\t\thelp='The email of the spec.')\n\tsubparser.add_argument('--spec-file', required=", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t# Setup the parser object\n\tparser = root_subparsers.add_parser('spec', help='spec related operations')\n\tparser.set_defaults(func=self.do_command)\n\tsubparsers = parser.add_subparsers()\n\n\t# Spec info parser\n\tinfo_parser = subparsers.add_parser('info', help='view information about a spec')\n\tinfo_parser.add_argument('spec', help='the spec to view information about')\n\tinfo_parser.set_defaults(func=self.do_spec_info)\n\n\t# Spec create parser\n\tcreate_parser = subparsers.add_parser('create', help='create a new spec')\n\tcreate_parser.add_argument('name', help='the name of the new spec')\n\tcreate_parser.add_argument('-t', '--template', help='the template to use for the new spec')\n\tcreate_parser.add_argument('-d', '--default', help='set the new spec as default', action='store_true')\n\tcreate_parser.set_defaults(func=self.do_spec_create)\n\n\t# Spec delete parser\n\tdelete_parser = subparsers.add_parser('delete', help='delete a spec')\n\tdelete_parser.add", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tfrom .. import api\n\n\textend_parser_for_spec(root_subparsers, self.name, self.description, self.spec)\n\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tcli_parser = root_subparsers.add_parser('spec', help='specs help')\n\tcli_parser.set_defaults(func=self.cli_parser_handler)\n\n\tcli_subparsers = cli_parser.add_subparsers(dest='subcommand',\n\t\t\t\t\t\t\t\t\t\t\t  title='available spec subcommands',\n\t\t\t\t\t\t\t\t\t\t\t  description='spec subcommands',\n\t\t\t\t\t\t\t\t\t\t\t  help='spec subcommand help')\n\n\tcli_subparsers.add_parser('list',\n\t\t\t\t\t\t\t  help='list specs')\n\n\tcli_subparsers.add_parser('create',\n\t\t\t\t\t\t\t  help='create a spec')\n\n\tcli_subparsers.add_parser('delete',\n\t\t\t\t\t\t\t  help='delete a spec')\n\n\tcli_subparsers.add_parser('edit',\n\t\t\t\t\t\t\t  help='edit a spec')\n\n\tcli_subparsers.add_parser('show',\n\t\t\t\t\t\t\t  help='show a spec')\n\n\tcli_subparsers.add_parser('validate',\n\t\t\t\t\t\t\t ", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='specification related commands')\n\tsubparser.set_defaults(func=self.execute_subcommand)\n\n\t# Subcommands\n\tsubparsers = subparser.add_subparsers(help='spec subcommands')\n\n\t# List specs\n\tparser = subparsers.add_parser('list', help='list all specifications')\n\tparser.set_defaults(func=self.list_specs)\n\n\t# Create spec\n\tparser = subparsers.add_parser('create', help='creates a new specification')\n\tparser.add_argument('name', help='the name of the specification to create')\n\tparser.add_argument('-f', '--force', action='store_true',\n\t\thelp='overwrite an existing specification with the same name')\n\tparser.set_defaults(func=self.create_spec)\n\n\t# Edit spec\n\tparser = subparsers.add_parser('edit', help='edit an existing specification')\n\tparser.add_argument('name', help='the name of the specification to edit')\n\tparser.add_argument('-f', '--force', action='store_true',\n\t\thelp='overwrite an existing specification with the same name')\n\t", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser('spec', help=\"Specification operations\")\n\n\tsubparsers = parser.add_subparsers(title='spec subcommands')\n\n\t# Get the subcommand\n\tparser_get_spec = subparsers.add_parser('get', help=\"Get the spec\")\n\tparser_get_spec.add_argument('spec_path', nargs='?',\n\t\thelp='Path to the spec to get. If not specified, the spec from the current context is returned.')\n\tparser_get_spec.set_defaults(func=self.get_spec)\n\n\t# Set the subcommand\n\tparser_set_spec = subparsers.add_parser('set', help=\"Set the current spec\")\n\tparser_set_spec.add_argument('spec_path', nargs='?',\n\t\thelp='Path to the spec to set. If not specified, the current context is cleared.')\n\tparser_set_spec.set_defaults(func=self.set_spec)\n\n\t# Add the subcommand\n\tparser_add_spec = subparsers.add_parser('add', help=\"Add a spec\")\n\tparser_add_spec.add_argument('spec_path', nargs='?',\n\t\thelp='Path to the", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tp = root_subparsers.add_parser('spec', help='Runs the spec command.')\n\tp.add_argument('spec_name', help='Name of the spec to run.')\n\tp.set_defaults(func=self.run_spec)\n\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tcli_subparser = root_subparsers.add_parser('spec', help=\"Spec file management\")\n\tcli_subparser.set_defaults(func=self.handle_cli)\n\tcli_subparser.add_argument(\"--config\", \"-c\", type=str, help=\"Specify the config file to use\")\n\tcli_subparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Enable verbose logging\")\n\tcli_subparser.add_argument(\"--logfile\", \"-l\", type=str, help=\"Redirect logging to a file\")\n\tcli_subparser.add_argument(\"--quiet\", \"-q\", action=\"store_true\", help=\"Do not output logging info to stdout\")\n\tcli_subparser.add_argument(\"--no-color\", \"-nc\", action=\"store_true\", help=\"Disable colorized logging\")\n\tcli_subparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", help=\"Enable debug logging\")\n\tcli_subparser.add_argument(\"--dry-run\", \"-n\", action=\"store_true\", help=\"Only run in dry-run mode\")\n\tcli_subparser.add_argument(\"--force\", \"-f\", action=\"store_true\", help=\"Force the execution\")\n\tcli_", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers.add_parser('spec', description='Specs', help='specs related commands')\n\n\tsubparsers.add_parser('list', help='List all specs', description='List all specs')\n\n\tparser = subparsers.add_parser('create', help='Create a new spec', description='Create a new spec')\n\tparser.add_argument('name', help='The name of the spec')\n\tparser.add_argument('--from', dest='from_spec', help='The spec to create from')\n\n\tparser = subparsers.add_parser('delete', help='Delete a spec', description='Delete a spec')\n\tparser.add_argument('name', help='The name of the spec to delete')\n\n\tparser = subparsers.add_parser('edit', help='Edit a spec', description='Edit a spec')\n\tparser.add_argument('name', help='The name of the spec to edit')\n\n\tparser = subparsers.add_parser('show', help='Show a spec', description='Show a spec')\n\tparser.add_argument('name', help='The name of the spec to show')\n\n\tparser = subparsers.add_parser('update', help='Update a spec', description='Update a spec')\n\tparser.add", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\t\n\t\t# get the parser for the \"show\" command\n\t\tparser = root_subparsers.add_parser('tunnel', help='show tunnel info')\n\n\t\t# now we add the arguments\n\t\tparser.add_argument('-n', '--new', action='store_true', help='create new tunnel')\n\t\tparser.add_argument('-l', '--list', action='store_true', help='list all tunnels')\n\t\tparser.add_argument('-r', '--rm', action='store_true', help='remove tunnel')\n\t\tparser.add_argument('-a', '--add', action='store_true', help='add tunnel')\n\t\tparser.add_argument('-e', '--edit', action='store_true', help='edit tunnel')\n\t\tparser.add_argument('-d', '--del', action='store_true', help='delete tunnel')\n\t\tparser.add_argument('-c', '--copy', action='store_true', help='copy tunnel')\n\t\tparser.add_argument('-u', '--up', action='store_true', help='bring up tunnel')\n\t\tparser.add_argument('-s', '--show', action='store_true', help='show tunnel')\n\t\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='Specify a spec.')\n\tsubparser.set_defaults(func=self.run)\n\tsubparser.add_argument('-o', '--output',\n\t\thelp='Where to save the file. Defaults to the current directory.')\n\tsubparser.add_argument('-v', '--version', default='1.0',\n\t\thelp='The version number of the spec.')\n\tsubparser.add_argument('-t', '--title', required=True, help='The title of the spec.')\n\tsubparser.add_argument('--license', default='Apache-2.0',\n\t\thelp='The license of the spec.')\n\tsubparser.add_argument('--description', required=True,\n\t\thelp='The description of the spec.')\n\tsubparser.add_argument('--contact', required=True,\n\t\thelp='The contact of the spec.')\n\tsubparser.add_argument('--url', required=True,\n\t\thelp='The url of the spec.')\n\tsubparser.add_argument('--email', required=True,\n\t\thelp='The email of the spec.')\n\tsubparser.add_argument('--spec-file', required=", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t# Setup the parser object\n\tparser = root_subparsers.add_parser('spec', help='spec related operations')\n\tparser.set_defaults(func=self.do_command)\n\tsubparsers = parser.add_subparsers()\n\n\t# Spec info parser\n\tinfo_parser = subparsers.add_parser('info', help='view information about a spec')\n\tinfo_parser.add_argument('spec', help='the spec to view information about')\n\tinfo_parser.set_defaults(func=self.do_spec_info)\n\n\t# Spec create parser\n\tcreate_parser = subparsers.add_parser('create', help='create a new spec')\n\tcreate_parser.add_argument('name', help='the name of the new spec')\n\tcreate_parser.add_argument('-t', '--template', help='the template to use for the new spec')\n\tcreate_parser.add_argument('-d', '--default', help='set the new spec as default', action='store_true')\n\tcreate_parser.set_defaults(func=self.do_spec_create)\n\n\t# Spec delete parser\n\tdelete_parser = subparsers.add_parser('delete', help='delete a spec')\n\tdelete_parser.add", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tfrom .. import api\n\n\textend_parser_for_spec(root_subparsers, self.name, self.description, self.spec)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tcli_parser = root_subparsers.add_parser('spec', help='specs help')\n\tcli_parser.set_defaults(func=self.cli_parser_handler)\n\n\tcli_subparsers = cli_parser.add_subparsers(dest='subcommand',\n\t\t\t\t\t\t\t\t\t\t\t  title='available spec subcommands',\n\t\t\t\t\t\t\t\t\t\t\t  description='spec subcommands',\n\t\t\t\t\t\t\t\t\t\t\t  help='spec subcommand help')\n\n\tcli_subparsers.add_parser('list',\n\t\t\t\t\t\t\t  help='list specs')\n\n\tcli_subparsers.add_parser('create',\n\t\t\t\t\t\t\t  help='create a spec')\n\n\tcli_subparsers.add_parser('delete',\n\t\t\t\t\t\t\t  help='delete a spec')\n\n\tcli_subparsers.add_parser('edit',\n\t\t\t\t\t\t\t  help='edit a spec')\n\n\tcli_subparsers.add_parser('show',\n\t\t\t\t\t\t\t  help='show a spec')\n\n\tcli_subparsers.add_parser('validate',\n\t\t\t\t\t\t\t ", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='specification related commands')\n\tsubparser.set_defaults(func=self.execute_subcommand)\n\n\t# Subcommands\n\tsubparsers = subparser.add_subparsers(help='spec subcommands')\n\n\t# List specs\n\tparser = subparsers.add_parser('list', help='list all specifications')\n\tparser.set_defaults(func=self.list_specs)\n\n\t# Create spec\n\tparser = subparsers.add_parser('create', help='creates a new specification')\n\tparser.add_argument('name', help='the name of the specification to create')\n\tparser.add_argument('-f', '--force', action='store_true',\n\t\thelp='overwrite an existing specification with the same name')\n\tparser.set_defaults(func=self.create_spec)\n\n\t# Edit spec\n\tparser = subparsers.add_parser('edit', help='edit an existing specification')\n\tparser.add_argument('name', help='the name of the specification to edit')\n\tparser.add_argument('-f', '--force', action='store_true',\n\t\thelp='overwrite an existing specification with the same name')\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser('spec', help=\"Specification operations\")\n\n\tsubparsers = parser.add_subparsers(title='spec subcommands')\n\n\t# Get the subcommand\n\tparser_get_spec = subparsers.add_parser('get', help=\"Get the spec\")\n\tparser_get_spec.add_argument('spec_path', nargs='?',\n\t\thelp='Path to the spec to get. If not specified, the spec from the current context is returned.')\n\tparser_get_spec.set_defaults(func=self.get_spec)\n\n\t# Set the subcommand\n\tparser_set_spec = subparsers.add_parser('set', help=\"Set the current spec\")\n\tparser_set_spec.add_argument('spec_path', nargs='?',\n\t\thelp='Path to the spec to set. If not specified, the current context is cleared.')\n\tparser_set_spec.set_defaults(func=self.set_spec)\n\n\t# Add the subcommand\n\tparser_add_spec = subparsers.add_parser('add', help=\"Add a spec\")\n\tparser_add_spec.add_argument('spec_path', nargs='?',\n\t\thelp='Path to the", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tp = root_subparsers.add_parser('spec', help='Runs the spec command.')\n\tp.add_argument('spec_name', help='Name of the spec to run.')\n\tp.set_defaults(func=self.run_spec)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tcli_subparser = root_subparsers.add_parser('spec', help=\"Spec file management\")\n\tcli_subparser.set_defaults(func=self.handle_cli)\n\tcli_subparser.add_argument(\"--config\", \"-c\", type=str, help=\"Specify the config file to use\")\n\tcli_subparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Enable verbose logging\")\n\tcli_subparser.add_argument(\"--logfile\", \"-l\", type=str, help=\"Redirect logging to a file\")\n\tcli_subparser.add_argument(\"--quiet\", \"-q\", action=\"store_true\", help=\"Do not output logging info to stdout\")\n\tcli_subparser.add_argument(\"--no-color\", \"-nc\", action=\"store_true\", help=\"Disable colorized logging\")\n\tcli_subparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", help=\"Enable debug logging\")\n\tcli_subparser.add_argument(\"--dry-run\", \"-n\", action=\"store_true\", help=\"Only run in dry-run mode\")\n\tcli_subparser.add_argument(\"--force\", \"-f\", action=\"store_true\", help=\"Force the execution\")\n\tcli_", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers.add_parser('spec', description='Specs', help='specs related commands')\n\n\tsubparsers.add_parser('list', help='List all specs', description='List all specs')\n\n\tparser = subparsers.add_parser('create', help='Create a new spec', description='Create a new spec')\n\tparser.add_argument('name', help='The name of the spec')\n\tparser.add_argument('--from', dest='from_spec', help='The spec to create from')\n\n\tparser = subparsers.add_parser('delete', help='Delete a spec', description='Delete a spec')\n\tparser.add_argument('name', help='The name of the spec to delete')\n\n\tparser = subparsers.add_parser('edit', help='Edit a spec', description='Edit a spec')\n\tparser.add_argument('name', help='The name of the spec to edit')\n\n\tparser = subparsers.add_parser('show', help='Show a spec', description='Show a spec')\n\tparser.add_argument('name', help='The name of the spec to show')\n\n\tparser = subparsers.add_parser('update', help='Update a spec', description='Update a spec')\n\tparser.add", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\t\n\t\t# get the parser for the \"show\" command\n\t\tparser = root_subparsers.add_parser('tunnel', help='show tunnel info')\n\n\t\t# now we add the arguments\n\t\tparser.add_argument('-n', '--new', action='store_true', help='create new tunnel')\n\t\tparser.add_argument('-l', '--list', action='store_true', help='list all tunnels')\n\t\tparser.add_argument('-r', '--rm', action='store_true', help='remove tunnel')\n\t\tparser.add_argument('-a', '--add', action='store_true', help='add tunnel')\n\t\tparser.add_argument('-e', '--edit', action='store_true', help='edit tunnel')\n\t\tparser.add_argument('-d', '--del', action='store_true', help='delete tunnel')\n\t\tparser.add_argument('-c', '--copy', action='store_true', help='copy tunnel')\n\t\tparser.add_argument('-u', '--up', action='store_true', help='bring up tunnel')\n\t\tparser.add_argument('-s', '--show', action='store_true', help='show tunnel')\n\t\t", "is_pass": false, "return_code": 1}]}
