{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n#\tif nanoseconds == 0:\n#\t\treturn None\n#\t\treturn '00:00:00'\n\tif nanoseconds is None:\n\t\treturn None\n\treturn time.strftime('%H:%M:%S', time.gmtime(nanoseconds / 1000000000.0))\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t \n\tseconds = float(nanoseconds) / 1000000000.0\n\treturn time.strftime('%H:%M:%S', time.gmtime(seconds))\n\t", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\n\tif tz is None:\n\t\ttz = timezone.utc\n\n\treturn (tz.localize(datetime.datetime.fromtimestamp(nanoseconds / 1e9))).astimezone(tz)\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\n\tif tz is None:\n\t\ttz = pytz.timezone(settings.TIME_ZONE)\n\n\treturn tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds / 1e9)).astimezone(tz)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\t# convert to seconds\n\tseconds = nanoseconds / 1.0e9\n\t# make a datetime\n\tdt = datetime.datetime(1970,1,1) + datetime.timedelta(seconds=seconds)\n\t# convert to a datetime with timezone\n\tif tz:\n\t\tdt = dt.replace(tzinfo=tz)\n\t# return as a string\n\treturn dt.strftime('%H:%M:%S.%f')\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = DEFAULT_TIMEZONE\n\t\tif nanoseconds == 0:\n\t\t\treturn None\n\t\treturn datetime.datetime.fromtimestamp(\n\t\t\tnanoseconds / 1000000000.0, tz=tz).time()\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t \n\tnanos = nanoseconds % 1000000000\n\tseconds = nanoseconds // 1000000000\n\tminutes = seconds // 60\n\tseconds -= (60 * minutes)\n\thours = minutes // 60\n\tminutes -= (60 * hours)\n\tdays = hours // 24\n\thours -= (24 * days)\n\tweeks = days // 7\n\tdays -= (7 * weeks)\n\tmonths = weeks // 4\n\tweeks -= (4 * months)\n\tyears = months // 12\n\tmonths -= (12 * years)\n\treturn '%04d-%02d-%02d %02d:%02d:%02d.%09d' % (\n\t\tyears + 1970, months + 1, days, hours, minutes, seconds, nanos)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n#\tif nanoseconds == 0:\n#\t\treturn '00:00:00.000000'\n\t\n\tif tz:\n\t\tnanoseconds -= tz.utcoffset(datetime.datetime.now()) * 1e9\n\t\n\tseconds = nanoseconds / 1e9\n\tminutes, seconds = divmod(seconds, 60)\n\thours, minutes = divmod(minutes, 60)\n\tdays, hours = divmod(hours, 24)\n\t\n\tif days > 0:\n\t\treturn '{:02d}:{:02d}:{:02d}.{:06d}'.format(days, hours, minutes, seconds)\n\telse:\n\t\treturn '{:02d}:{:02d}:{:02d}.{:06d}'.format(hours, minutes, seconds, nanoseconds)\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n#\tassert nanoseconds >= 0\n\tif nanoseconds == 0:\n\t\treturn '00:00:00'\n\t\n\tseconds = nanoseconds / 1000000000\n\tdays = seconds // (3600 * 24)\n\tseconds -= days * 3600 * 24\n\thours = seconds // 3600\n\tseconds -= hours * 3600\n\tminutes = seconds // 60\n\tseconds -= minutes * 60\n\t\n\tday_part = ''\n\tif days >= 1:\n\t\tday_part = '{}d '.format(days)\n\tif days < 1 and hours >= 1:\n\t\tday_part = '{}h '.format(hours)\n\tif days < 1 and hours < 1 and minutes >= 1:\n\t\tday_part = '{}m '.format(minutes)\n\tif days < 1 and hours < 1 and minutes < 1:\n\t\tday_part = '{}s '.format(seconds)\n\n\treturn day_part\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\treturn time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(nanoseconds / 1e9))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n#\tif nanoseconds == 0:\n#\t\treturn None\n#\t\treturn '00:00:00'\n\tif nanoseconds is None:\n\t\treturn None\n\treturn time.strftime('%H:%M:%S', time.gmtime(nanoseconds / 1000000000.0))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t \n\tseconds = float(nanoseconds) / 1000000000.0\n\treturn time.strftime('%H:%M:%S', time.gmtime(seconds))\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\n\tif tz is None:\n\t\ttz = timezone.utc\n\n\treturn (tz.localize(datetime.datetime.fromtimestamp(nanoseconds / 1e9))).astimezone(tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\n\tif tz is None:\n\t\ttz = pytz.timezone(settings.TIME_ZONE)\n\n\treturn tz.localize(datetime.datetime.utcfromtimestamp(nanoseconds / 1e9)).astimezone(tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\t# convert to seconds\n\tseconds = nanoseconds / 1.0e9\n\t# make a datetime\n\tdt = datetime.datetime(1970,1,1) + datetime.timedelta(seconds=seconds)\n\t# convert to a datetime with timezone\n\tif tz:\n\t\tdt = dt.replace(tzinfo=tz)\n\t# return as a string\n\treturn dt.strftime('%H:%M:%S.%f')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = DEFAULT_TIMEZONE\n\t\tif nanoseconds == 0:\n\t\t\treturn None\n\t\treturn datetime.datetime.fromtimestamp(\n\t\t\tnanoseconds / 1000000000.0, tz=tz).time()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t \n\tnanos = nanoseconds % 1000000000\n\tseconds = nanoseconds // 1000000000\n\tminutes = seconds // 60\n\tseconds -= (60 * minutes)\n\thours = minutes // 60\n\tminutes -= (60 * hours)\n\tdays = hours // 24\n\thours -= (24 * days)\n\tweeks = days // 7\n\tdays -= (7 * weeks)\n\tmonths = weeks // 4\n\tweeks -= (4 * months)\n\tyears = months // 12\n\tmonths -= (12 * years)\n\treturn '%04d-%02d-%02d %02d:%02d:%02d.%09d' % (\n\t\tyears + 1970, months + 1, days, hours, minutes, seconds, nanos)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n#\tif nanoseconds == 0:\n#\t\treturn '00:00:00.000000'\n\t\n\tif tz:\n\t\tnanoseconds -= tz.utcoffset(datetime.datetime.now()) * 1e9\n\t\n\tseconds = nanoseconds / 1e9\n\tminutes, seconds = divmod(seconds, 60)\n\thours, minutes = divmod(minutes, 60)\n\tdays, hours = divmod(hours, 24)\n\t\n\tif days > 0:\n\t\treturn '{:02d}:{:02d}:{:02d}.{:06d}'.format(days, hours, minutes, seconds)\n\telse:\n\t\treturn '{:02d}:{:02d}:{:02d}.{:06d}'.format(hours, minutes, seconds, nanoseconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n#\tassert nanoseconds >= 0\n\tif nanoseconds == 0:\n\t\treturn '00:00:00'\n\t\n\tseconds = nanoseconds / 1000000000\n\tdays = seconds // (3600 * 24)\n\tseconds -= days * 3600 * 24\n\thours = seconds // 3600\n\tseconds -= hours * 3600\n\tminutes = seconds // 60\n\tseconds -= minutes * 60\n\t\n\tday_part = ''\n\tif days >= 1:\n\t\tday_part = '{}d '.format(days)\n\tif days < 1 and hours >= 1:\n\t\tday_part = '{}h '.format(hours)\n\tif days < 1 and hours < 1 and minutes >= 1:\n\t\tday_part = '{}m '.format(minutes)\n\tif days < 1 and hours < 1 and minutes < 1:\n\t\tday_part = '{}s '.format(seconds)\n\n\treturn day_part\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\treturn time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(nanoseconds / 1e9))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\n\n\treturn Structure(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\n\n\treturn Structure(days=value.days, seconds=value.seconds, microseconds=value.microseconds, milliseconds=value.microseconds/1000, minutes=value.seconds/60, hours=value.seconds/3600, weeks=value.days/7)\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\treturn Structure(\n\t\t\tdays=value.days,\n\t\t\tseconds=value.seconds,\n\t\t\tmicroseconds=value.microseconds,\n\t\t\tmilliseconds=value.microseconds/1000,\n\t\t\tminutes=value.seconds/60,\n\t\t\thours=value.seconds/60/60,\n\t\t\tweeks=value.days/7\n\t\t)\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\tif value is None:\n\t\t\treturn None\n\t\tif value.days == 0 and value.seconds == 0:\n\t\t\treturn None\n\t\treturn Structure(days=value.days, hours=value.seconds // 3600, minutes=(value.seconds % 3600) // 60, seconds=value.seconds % 60)\n\t", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\treturn Structure({\n\t\t\t\"days\":value.days,\n\t\t\t\"microseconds\":value.microseconds\n\t\t})\n\t", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t/src/tests/test_models.py\nimport unittest\nfrom datetime import datetime, timedelta\n\nfrom models import *\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\n\n\treturn Structure(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\n\n\treturn Structure(days=value.days, seconds=value.seconds, microseconds=value.microseconds, milliseconds=value.microseconds/1000, minutes=value.seconds/60, hours=value.seconds/3600, weeks=value.days/7)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\treturn Structure(\n\t\t\tdays=value.days,\n\t\t\tseconds=value.seconds,\n\t\t\tmicroseconds=value.microseconds,\n\t\t\tmilliseconds=value.microseconds/1000,\n\t\t\tminutes=value.seconds/60,\n\t\t\thours=value.seconds/60/60,\n\t\t\tweeks=value.days/7\n\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\tif value is None:\n\t\t\treturn None\n\t\tif value.days == 0 and value.seconds == 0:\n\t\t\treturn None\n\t\treturn Structure(days=value.days, hours=value.seconds // 3600, minutes=(value.seconds % 3600) // 60, seconds=value.seconds % 60)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\treturn Structure({\n\t\t\t\"days\":value.days,\n\t\t\t\"microseconds\":value.microseconds\n\t\t})\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t/src/tests/test_models.py\nimport unittest\nfrom datetime import datetime, timedelta\n\nfrom models import *\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn ticks_to_time(value)\n\texcept:\n\t\treturn dehydrate_structure(value)\n\t\t", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\t#TODO: figure out how to get rid of the need to check if the value is\n\t#a string and then handle it\n\tif isinstance(value, str):\n\t\tvalue = Time(value)\n\n\t#if the value has a seconds attribute, it's a Structure\n\tif hasattr(value,'seconds'):\n\t\treturn value\n\n\t#if the value has a ticks attribute, it's a Time\n\tif hasattr(value, 'ticks'):\n\t\treturn Structure(ticks=value.ticks)\n\n\t#if the value has a time attribute, it's a Time\n\tif hasattr(value, 'time'):\n\t\treturn Structure(time=value.time)\n\n\t#if the value has a microseconds attribute, it's a Time\n\tif hasattr(value,'microseconds'):\n\t\treturn Structure(microseconds=value.microseconds)\n\n\t#if the value has a milliseconds attribute, it's a Time\n\tif hasattr(value,'milliseconds'):\n\t\treturn Structure(milliseconds=value.milliseconds)\n\n\t#if the value has a minutes attribute, it's a Time\n\tif hasattr(value,'minutes'):\n\t\treturn Structure(minutes=value.minutes)\n\n\t#if the value has a hours attribute, it's a Time\n\tif hasattr(value, 'hours'):\n\t\treturn Structure(hours=value.hours)\n\n\t#if the", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\t# print(value)\n\tif value.seconds == 0:\n\t\treturn \"00:00:00\"\n\telse:\n\t\ttry:\n\t\t\treturn str(value.ticks).zfill(6)\n\t\texcept AttributeError:\n\t\t\treturn str(value.seconds).zfill(6)\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\tif isinstance(value, datetime.datetime):\n\t\treturn value, value\n\telif isinstance(value, datetime.time):\n\t\treturn datetime.datetime.combine(datetime.date.today(), value), datetime.datetime.combine(datetime.date.today(), value)\n\telif isinstance(value, list):\n\t\treturn dehydrate_time(value[0]), dehydrate_time(value[1])\n\telif isinstance(value, str):\n\t\t# Convert string to time.\n\t\t# TODO: Add support for non-AM/PM time formats\n\t\tvalue = value.lower()\n\t\tif value[-1] == 'p':\n\t\t\tvalue = value.replace('p', '')\n\t\t\ttime = datetime.datetime.strptime(value, '%I:%M').time()\n\t\t\treturn datetime.datetime.combine(datetime.date.today(), time), datetime.datetime.combine(datetime.date.today(), time)\n\t\telif value[-1] == 'a':\n\t\t\tvalue = value.replace('a', '')\n\t\t\ttime = datetime.datetime.strptime(value, '%I:%M').time()\n\t\t\treturn datetime.datetime.combine(datetime.date.today(), time), datetime.datetime.combine(datetime.date.today(), time)\n\t\telse:\n\t\t\treturn None, None\n\telse:\n\t\treturn None, None\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\t# The first value is the current tick\n\t# The second value is the next tick\n\t# The third value is the number of ticks in the current period\n\t# The fourth value is the number of ticks in the next period\n\t# The fifth value is the number of ticks in the new period\n\t# The sixth value is the number of ticks in the previous period\n\t# The seventh value is the number of ticks in the whole period\n\t\n\t# The tick values are stored as integers\n\t# The period values are stored as fractions\n\t# The period values are stored as pairs of integers\n\t# The period values are stored as pairs of fractions\n\t# The period values are stored as pairs of pairs of integers\n\t# The period values are stored as pairs of pairs of fractions\n\t\n\t# The first value is the current tick\n\t# The second value is the next tick\n\t# The third value is the number of ticks in the current period\n\t# The fourth value is the number of ticks in the next period\n\t# The fifth value is the number of ticks in the new period\n\t# The sixth value is the number of ticks in the previous period\n\t# The seventh value is the number of ticks in the whole period\n\t\n\t# The tick values are stored as integers\n\t# The period values are stored as fractions\n\t# The period values", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\treturn value\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\treturn Time(value)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn ticks_to_time(value)\n\texcept:\n\t\treturn dehydrate_structure(value)\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\t#TODO: figure out how to get rid of the need to check if the value is\n\t#a string and then handle it\n\tif isinstance(value, str):\n\t\tvalue = Time(value)\n\n\t#if the value has a seconds attribute, it's a Structure\n\tif hasattr(value,'seconds'):\n\t\treturn value\n\n\t#if the value has a ticks attribute, it's a Time\n\tif hasattr(value, 'ticks'):\n\t\treturn Structure(ticks=value.ticks)\n\n\t#if the value has a time attribute, it's a Time\n\tif hasattr(value, 'time'):\n\t\treturn Structure(time=value.time)\n\n\t#if the value has a microseconds attribute, it's a Time\n\tif hasattr(value,'microseconds'):\n\t\treturn Structure(microseconds=value.microseconds)\n\n\t#if the value has a milliseconds attribute, it's a Time\n\tif hasattr(value,'milliseconds'):\n\t\treturn Structure(milliseconds=value.milliseconds)\n\n\t#if the value has a minutes attribute, it's a Time\n\tif hasattr(value,'minutes'):\n\t\treturn Structure(minutes=value.minutes)\n\n\t#if the value has a hours attribute, it's a Time\n\tif hasattr(value, 'hours'):\n\t\treturn Structure(hours=value.hours)\n\n\t#if the", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\t# print(value)\n\tif value.seconds == 0:\n\t\treturn \"00:00:00\"\n\telse:\n\t\ttry:\n\t\t\treturn str(value.ticks).zfill(6)\n\t\texcept AttributeError:\n\t\t\treturn str(value.seconds).zfill(6)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\tif isinstance(value, datetime.datetime):\n\t\treturn value, value\n\telif isinstance(value, datetime.time):\n\t\treturn datetime.datetime.combine(datetime.date.today(), value), datetime.datetime.combine(datetime.date.today(), value)\n\telif isinstance(value, list):\n\t\treturn dehydrate_time(value[0]), dehydrate_time(value[1])\n\telif isinstance(value, str):\n\t\t# Convert string to time.\n\t\t# TODO: Add support for non-AM/PM time formats\n\t\tvalue = value.lower()\n\t\tif value[-1] == 'p':\n\t\t\tvalue = value.replace('p', '')\n\t\t\ttime = datetime.datetime.strptime(value, '%I:%M').time()\n\t\t\treturn datetime.datetime.combine(datetime.date.today(), time), datetime.datetime.combine(datetime.date.today(), time)\n\t\telif value[-1] == 'a':\n\t\t\tvalue = value.replace('a', '')\n\t\t\ttime = datetime.datetime.strptime(value, '%I:%M').time()\n\t\t\treturn datetime.datetime.combine(datetime.date.today(), time), datetime.datetime.combine(datetime.date.today(), time)\n\t\telse:\n\t\t\treturn None, None\n\telse:\n\t\treturn None, None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\t# The first value is the current tick\n\t# The second value is the next tick\n\t# The third value is the number of ticks in the current period\n\t# The fourth value is the number of ticks in the next period\n\t# The fifth value is the number of ticks in the new period\n\t# The sixth value is the number of ticks in the previous period\n\t# The seventh value is the number of ticks in the whole period\n\t\n\t# The tick values are stored as integers\n\t# The period values are stored as fractions\n\t# The period values are stored as pairs of integers\n\t# The period values are stored as pairs of fractions\n\t# The period values are stored as pairs of pairs of integers\n\t# The period values are stored as pairs of pairs of fractions\n\t\n\t# The first value is the current tick\n\t# The second value is the next tick\n\t# The third value is the number of ticks in the current period\n\t# The fourth value is the number of ticks in the next period\n\t# The fifth value is the number of ticks in the new period\n\t# The sixth value is the number of ticks in the previous period\n\t# The seventh value is the number of ticks in the whole period\n\t\n\t# The tick values are stored as integers\n\t# The period values are stored as fractions\n\t# The period values", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\treturn Time(value)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\tif len(value) is 1:\n\t\t\tpoint = Point(value[0], value[0])\n\t\telif len(value) is 2:\n\t\t\tpoint = Point(value[0], value[1])\n\t\telif len(value) is 3:\n\t\t\tpoint = Point(value[0], value[1], value[2])\n\t\telif len(value) is 4:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3])\n\t\telif len(value) is 5:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4])\n\t\telif len(value) is 6:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4], value[5])\n\t\telif len(value) is 7:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4], value[5], value[6])\n\t\telif len(value) is 8:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7])\n\t\telif len(value) is 9:\n\t\t\tpoint = Point(", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\t# Dehydrate the geometry\n\t\tif isinstance(value, list):\n\t\t\treturn dehydrate_geometry(value)\n\t\telif isinstance(value, dict):\n\t\t\treturn dehydrate_geometry(value)\n\t\telif isinstance(value, tuple):\n\t\t\treturn dehydrate_geometry(value)\n\t\telse:\n\t\t\treturn value\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\tlength = len(value)\n\t\tif length == 1:\n\t\t\treturn value[0]\n\t\telif length == 2:\n\t\t\treturn Point(value[0], value[1])\n\t\telif length == 3:\n\t\t\treturn Point(value[0], value[1], value[2])\n\t\telse:\n\t\t\traise ValueError('Point can not be dehydrated from %s' % value)\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\treturn struct.pack('=i', value)\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\t# value is a list of lists\n\tif isinstance(value, list) and len(value) > 0 and isinstance(value[0], list):\n\t\treturn [Point(x, y) for x, y in value]\n\n\telse:\n\t\treturn Point(value[0], value[1])\n/src/python/tests/test_utils.py\nimport unittest\nimport os\nfrom utils import *\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\tif len(value) == 1:\n\t\t\treturn value[0]\n\t\telif len(value) == 2:\n\t\t\treturn value[0]\n\t\telif len(value) == 3:\n\t\t\treturn value[0]\n\t\telif len(value) == 4:\n\t\t\treturn value[0]\n\t\telif len(value) == 5:\n\t\t\treturn value[0]\n\t\telif len(value) == 6:\n\t\t\treturn value[0]\n\t\telif len(value) == 7:\n\t\t\treturn value[0]\n\t\telif len(value) == 8:\n\t\t\treturn value[0]\n\t\telif len(value) == 9:\n\t\t\treturn value[0]\n\t\telif len(value) == 10:\n\t\t\treturn value[0]\n\t\telif len(value) == 11:\n\t\t\treturn value[0]\n\t\telif len(value) == 12:\n\t\t\treturn value[0]\n\t\telif len(value) == 13:\n\t\t\treturn value[0]\n\t\telif len(value) == 14:\n\t\t\treturn value[0]\n\t\telif len(value) == 15:\n\t\t\treturn value[0]\n\t\telif len(value) == 16:\n\t\t\treturn value[0]\n\t\telif len(value", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\tif len(value) == 8:\n\t\treturn Point(int.from_bytes(value[0:4], 'little', signed=True), int.from_bytes(value[4:8], 'little', signed=True))\n\telif len(value) == 32:\n\t\treturn Point(int.from_bytes(value[0:16], 'little', signed=True), int.from_bytes(value[16:32], 'little', signed=True))\n\telse:\n\t\traise Exception('Invalid Point')\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\tif type(value) is not tuple:\n\t\traise ValueError(\"The value must be a tuple\")\n\n\tif len(value)!= 2:\n\t\traise ValueError(\"The length of the value must be 2\")\n\n\treturn Point(value[0], value[1])\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\tif len(value) == 1:\n\t\treturn dehydrate_value(value[0])\n\telif len(value) == 2:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1])\n\telif len(value) == 3:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2])\n\telif len(value) == 4:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2]), dehydrate_point(value[3])\n\telif len(value) == 5:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2]), dehydrate_point(value[3]), dehydrate_point(value[4])\n\telif len(value) == 6:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2]), dehydrate_point(value[3]), dehydrate_point(value[4]), dehydrate_point(value[5])\n\telif len(value"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\tif len(value) is 1:\n\t\t\tpoint = Point(value[0], value[0])\n\t\telif len(value) is 2:\n\t\t\tpoint = Point(value[0], value[1])\n\t\telif len(value) is 3:\n\t\t\tpoint = Point(value[0], value[1], value[2])\n\t\telif len(value) is 4:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3])\n\t\telif len(value) is 5:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4])\n\t\telif len(value) is 6:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4], value[5])\n\t\telif len(value) is 7:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4], value[5], value[6])\n\t\telif len(value) is 8:\n\t\t\tpoint = Point(value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7])\n\t\telif len(value) is 9:\n\t\t\tpoint = Point(", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\t# Dehydrate the geometry\n\t\tif isinstance(value, list):\n\t\t\treturn dehydrate_geometry(value)\n\t\telif isinstance(value, dict):\n\t\t\treturn dehydrate_geometry(value)\n\t\telif isinstance(value, tuple):\n\t\t\treturn dehydrate_geometry(value)\n\t\telse:\n\t\t\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\tlength = len(value)\n\t\tif length == 1:\n\t\t\treturn value[0]\n\t\telif length == 2:\n\t\t\treturn Point(value[0], value[1])\n\t\telif length == 3:\n\t\t\treturn Point(value[0], value[1], value[2])\n\t\telse:\n\t\t\traise ValueError('Point can not be dehydrated from %s' % value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\treturn struct.pack('=i', value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\t# value is a list of lists\n\tif isinstance(value, list) and len(value) > 0 and isinstance(value[0], list):\n\t\treturn [Point(x, y) for x, y in value]\n\n\telse:\n\t\treturn Point(value[0], value[1])\n/src/python/tests/test_utils.py\nimport unittest\nimport os\nfrom utils import *\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\tif len(value) == 1:\n\t\t\treturn value[0]\n\t\telif len(value) == 2:\n\t\t\treturn value[0]\n\t\telif len(value) == 3:\n\t\t\treturn value[0]\n\t\telif len(value) == 4:\n\t\t\treturn value[0]\n\t\telif len(value) == 5:\n\t\t\treturn value[0]\n\t\telif len(value) == 6:\n\t\t\treturn value[0]\n\t\telif len(value) == 7:\n\t\t\treturn value[0]\n\t\telif len(value) == 8:\n\t\t\treturn value[0]\n\t\telif len(value) == 9:\n\t\t\treturn value[0]\n\t\telif len(value) == 10:\n\t\t\treturn value[0]\n\t\telif len(value) == 11:\n\t\t\treturn value[0]\n\t\telif len(value) == 12:\n\t\t\treturn value[0]\n\t\telif len(value) == 13:\n\t\t\treturn value[0]\n\t\telif len(value) == 14:\n\t\t\treturn value[0]\n\t\telif len(value) == 15:\n\t\t\treturn value[0]\n\t\telif len(value) == 16:\n\t\t\treturn value[0]\n\t\telif len(value", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\tif len(value) == 8:\n\t\treturn Point(int.from_bytes(value[0:4], 'little', signed=True), int.from_bytes(value[4:8], 'little', signed=True))\n\telif len(value) == 32:\n\t\treturn Point(int.from_bytes(value[0:16], 'little', signed=True), int.from_bytes(value[16:32], 'little', signed=True))\n\telse:\n\t\traise Exception('Invalid Point')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\tif type(value) is not tuple:\n\t\traise ValueError(\"The value must be a tuple\")\n\n\tif len(value)!= 2:\n\t\traise ValueError(\"The length of the value must be 2\")\n\n\treturn Point(value[0], value[1])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\n\tif len(value) == 1:\n\t\treturn dehydrate_value(value[0])\n\telif len(value) == 2:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1])\n\telif len(value) == 3:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2])\n\telif len(value) == 4:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2]), dehydrate_point(value[3])\n\telif len(value) == 5:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2]), dehydrate_point(value[3]), dehydrate_point(value[4])\n\telif len(value) == 6:\n\t\treturn dehydrate_point(value[0]), dehydrate_point(value[1]), dehydrate_point(value[2]), dehydrate_point(value[3]), dehydrate_point(value[4]), dehydrate_point(value[5])\n\telif len(value", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns the string representation of the object\n\t\t\"\"\"\n\t\treturn str(self.__dict__)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the __dict__\n\t\t\"\"\"\n\t\treturn len(self.__dict__)\ne = \"4-to_json\")\r\n\t\tself.assertEqual(json_dictionary, js)\r\n\r\n\tdef test_to_json_with_attributes(self):\r\n\t\t\"\"\"\r\n\t\tTesting the to_json method with attributes\r\n\t\t\"\"\"\r\n\r\n\t\tjs = json_dictionary = {'id': 1, 'name': 'John', 'age': 30, 'weight': 300.3, 'first_name': 'John', 'last_name': 'Snow'}\r\n\t\tself.assertEqual(json_dictionary, js)\r\n/tests/test_models/test_base_model.py\n#!/usr/bin/python3\r\n\"\"\"\r\nTest for BaseModel class\r\n\"\"\"\r\n\r\nimport unittest\r\nimport pep8\r\nfrom models.base_model import BaseModel\r\nfrom datetime import datetime\r\nimport uuid\r\n\r\n\r", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn dir(self.__keys)\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n\t\n\tdef values(self):\n\t\"\"\"\n\tReturns the values of a class as a List\n\t\"\"\"\n\t\treturn self.__values\n\t\n\tdef items(self):\n\t\"\"\"\n\tReturns the items of a class as a List\n\t\"\"\"\n\t\treturn self.__items\n'x')\n\t\"\"\"\n\t\treturn self.__x\n\t\n\t@property\n\tdef y(self):\n\t\"\"\"\n\tReturns the __y property of a class\n\t\"\"\"\n\t\treturn self.__y\n\t\n\t@y.setter\n\tdef y(self, value):\n\t\"\"\"\n\tSets the __y property of a class to the value of the argument,\n\twith the condition that the argument must be an integer\n\t\"\"\"\n\t\tif type(value) is not int:\n\t\t\traise TypeError('y must be an integer')\n\t\tself.__y = value\n\t\n\t@property\n\tdef id(self):\n\t\"\"\"\n\tReturns the __id property of a class\n\t\"\"\"\n\t\treturn self.__id\n\t\n\t@property\n\tdef width(self):\n\t\"\"\"\n\tReturns the __width property of a class\n\t\"\"\"\n\t\treturn self.__width\n\t\n\t@width.setter\n\tdef width(self, value):\n\t\"\"\"\n\tSets the __width property of a class to the value of the argument,\n\twith the condition that", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns the string representation of the object\n\t\t\"\"\"\n\t\treturn str(self.__dict__)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the __dict__\n\t\t\"\"\"\n\t\treturn len(self.__dict__)\ne = \"4-to_json\")\r\n\t\tself.assertEqual(json_dictionary, js)\r\n\r\n\tdef test_to_json_with_attributes(self):\r\n\t\t\"\"\"\r\n\t\tTesting the to_json method with attributes\r\n\t\t\"\"\"\r\n\r\n\t\tjs = json_dictionary = {'id': 1, 'name': 'John', 'age': 30, 'weight': 300.3, 'first_name': 'John', 'last_name': 'Snow'}\r\n\t\tself.assertEqual(json_dictionary, js)\r\n/tests/test_models/test_base_model.py\n#!/usr/bin/python3\r\n\"\"\"\r\nTest for BaseModel class\r\n\"\"\"\r\n\r\nimport unittest\r\nimport pep8\r\nfrom models.base_model import BaseModel\r\nfrom datetime import datetime\r\nimport uuid\r\n\r\n\r", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn dir(self.__keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n\t\n\tdef values(self):\n\t\"\"\"\n\tReturns the values of a class as a List\n\t\"\"\"\n\t\treturn self.__values\n\t\n\tdef items(self):\n\t\"\"\"\n\tReturns the items of a class as a List\n\t\"\"\"\n\t\treturn self.__items\n'x')\n\t\"\"\"\n\t\treturn self.__x\n\t\n\t@property\n\tdef y(self):\n\t\"\"\"\n\tReturns the __y property of a class\n\t\"\"\"\n\t\treturn self.__y\n\t\n\t@y.setter\n\tdef y(self, value):\n\t\"\"\"\n\tSets the __y property of a class to the value of the argument,\n\twith the condition that the argument must be an integer\n\t\"\"\"\n\t\tif type(value) is not int:\n\t\t\traise TypeError('y must be an integer')\n\t\tself.__y = value\n\t\n\t@property\n\tdef id(self):\n\t\"\"\"\n\tReturns the __id property of a class\n\t\"\"\"\n\t\treturn self.__id\n\t\n\t@property\n\tdef width(self):\n\t\"\"\"\n\tReturns the __width property of a class\n\t\"\"\"\n\t\treturn self.__width\n\t\n\t@width.setter\n\tdef width(self, value):\n\t\"\"\"\n\tSets the __width property of a class to the value of the argument,\n\twith the condition that", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\treturn self.__keys\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls.bolt_handlers\n\n\t\treturn cls.protocol_handlers.get(protocol_version, cls.bolt_handlers)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\treturn cls.protocol_handlers_11 if protocol_version == 11 else cls.protocol_handlers_10\n\n# ======================================================================================================================\n# Bolt Protocol Handlers\n# ======================================================================================================================", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.protocol_version\n\t\tif protocol_version == 0:\n\t\t\treturn cls.protocol_0_handlers\n\t\telif protocol_version == 1:\n\t\t\treturn cls.protocol_1_handlers\n\t\telif protocol_version == 2:\n\t\t\treturn cls.protocol_2_handlers\n\t\telse:\n\t\t\traise ValueError(f\"Invalid protocol version: {protocol_version}\")\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls.protocol_v1(), cls.protocol_v2()\n\t\telif protocol_version == 1:\n\t\t\treturn cls.protocol_v1()\n\t\telif protocol_version == 2:\n\t\t\treturn cls.protocol_v2()\n\t\telse:\n\t\t\traise ValueError(\"Unknown protocol version.\")\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls._protocol_handlers\n\t\t# Protocol version is specified.\n\t\tif protocol_version in cls._protocol_handlers:\n\t\t\treturn cls._protocol_handlers[protocol_version]\n\t\t# Protocol version is not supported.\n\t\treturn None\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls.protocol_handlers()\n\t\telif protocol_version is 0:\n\t\t\treturn cls.protocol_handlers_0()\n\t\telif protocol_version is 1:\n\t\t\treturn cls.protocol_handlers_1()\n\t\telif protocol_version is 2:\n\t\t\treturn cls.protocol_handlers_2()\n\t\telse:\n\t\t\traise ValueError(\"Unknown protocol version: {}\".format(protocol_version))\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.protocol_version\n\t\tif protocol_version == 1:\n\t\t\treturn {\n\t\t\t\t'bolt': cls.bolt_1,\n\t\t\t\t'bolt_1': cls.bolt_1,\n\t\t\t\t'bolt_0_1': cls.bolt_0_1,\n\t\t\t}\n\t\telif protocol_version == 0:\n\t\t\treturn {\n\t\t\t\t'bolt': cls.bolt_0,\n\t\t\t\t'bolt_1': cls.bolt_0,\n\t\t\t\t'bolt_0_1': cls.bolt_0_1,\n\t\t\t}\n\t\telif protocol_version == 0.1:\n\t\t\treturn {\n\t\t\t\t'bolt': cls.bolt_0_1,\n\t\t\t\t'bolt_1': cls.bolt_0_1,\n\t\t\t\t'bolt_0_1': cls.bolt_0_1,\n\t\t\t}\n\t\telse:\n\t\t\traise ValueError(\"Protocol version %s is not supported\" % protocol_version)\n\n\t@classmethod\n\tdef bolt_0_1(cls, request, protocol_version, **kwargs):\n\t\t\"\"\"\n\t\tThis is the first release of bolt.\n\t\t\"\"\"\n\t\t# Parse the request and make sure it is valid\n\t\tif not isinstance(request, dict):\n\t\t\traise ValueError(\"Request must be a dict\")\n\t\tif '", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls._protocol_handlers\n\t\telse:\n\t\t\treturn cls._protocol_handlers.get(protocol_version, None)\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\n        if protocol_version is None:\n            return BoltProtocolHandler\n        if protocol_version == '1':\n            return BoltProtocolV1Handler\n        if protocol_version == '3':\n            return BoltProtocolV3Handler\n        raise Exception('Protocol version {} is not supported'.format(protocol_version))\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version == 1:\n\t\t\treturn cls._protocol_1_handlers\n\t\telif protocol_version == 2:\n\t\t\treturn cls._protocol_2_handlers\n\t\telse:\n\t\t\treturn cls._protocol_3_handlers\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls.bolt_handlers\n\n\t\treturn cls.protocol_handlers.get(protocol_version, cls.bolt_handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\treturn cls.protocol_handlers_11 if protocol_version == 11 else cls.protocol_handlers_10\n\n# ======================================================================================================================\n# Bolt Protocol Handlers\n# ======================================================================================================================", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.protocol_version\n\t\tif protocol_version == 0:\n\t\t\treturn cls.protocol_0_handlers\n\t\telif protocol_version == 1:\n\t\t\treturn cls.protocol_1_handlers\n\t\telif protocol_version == 2:\n\t\t\treturn cls.protocol_2_handlers\n\t\telse:\n\t\t\traise ValueError(f\"Invalid protocol version: {protocol_version}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls.protocol_v1(), cls.protocol_v2()\n\t\telif protocol_version == 1:\n\t\t\treturn cls.protocol_v1()\n\t\telif protocol_version == 2:\n\t\t\treturn cls.protocol_v2()\n\t\telse:\n\t\t\traise ValueError(\"Unknown protocol version.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls._protocol_handlers\n\t\t# Protocol version is specified.\n\t\tif protocol_version in cls._protocol_handlers:\n\t\t\treturn cls._protocol_handlers[protocol_version]\n\t\t# Protocol version is not supported.\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls.protocol_handlers()\n\t\telif protocol_version is 0:\n\t\t\treturn cls.protocol_handlers_0()\n\t\telif protocol_version is 1:\n\t\t\treturn cls.protocol_handlers_1()\n\t\telif protocol_version is 2:\n\t\t\treturn cls.protocol_handlers_2()\n\t\telse:\n\t\t\traise ValueError(\"Unknown protocol version: {}\".format(protocol_version))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.protocol_version\n\t\tif protocol_version == 1:\n\t\t\treturn {\n\t\t\t\t'bolt': cls.bolt_1,\n\t\t\t\t'bolt_1': cls.bolt_1,\n\t\t\t\t'bolt_0_1': cls.bolt_0_1,\n\t\t\t}\n\t\telif protocol_version == 0:\n\t\t\treturn {\n\t\t\t\t'bolt': cls.bolt_0,\n\t\t\t\t'bolt_1': cls.bolt_0,\n\t\t\t\t'bolt_0_1': cls.bolt_0_1,\n\t\t\t}\n\t\telif protocol_version == 0.1:\n\t\t\treturn {\n\t\t\t\t'bolt': cls.bolt_0_1,\n\t\t\t\t'bolt_1': cls.bolt_0_1,\n\t\t\t\t'bolt_0_1': cls.bolt_0_1,\n\t\t\t}\n\t\telse:\n\t\t\traise ValueError(\"Protocol version %s is not supported\" % protocol_version)\n\n\t@classmethod\n\tdef bolt_0_1(cls, request, protocol_version, **kwargs):\n\t\t\"\"\"\n\t\tThis is the first release of bolt.\n\t\t\"\"\"\n\t\t# Parse the request and make sure it is valid\n\t\tif not isinstance(request, dict):\n\t\t\traise ValueError(\"Request must be a dict\")\n\t\tif '", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls._protocol_handlers\n\t\telse:\n\t\t\treturn cls._protocol_handlers.get(protocol_version, None)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\n        if protocol_version is None:\n            return BoltProtocolHandler\n        if protocol_version == '1':\n            return BoltProtocolV1Handler\n        if protocol_version == '3':\n            return BoltProtocolV3Handler\n        raise Exception('Protocol version {} is not supported'.format(protocol_version))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\tif protocol_version == 1:\n\t\t\treturn cls._protocol_1_handlers\n\t\telif protocol_version == 2:\n\t\t\treturn cls._protocol_2_handlers\n\t\telse:\n\t\t\treturn cls._protocol_3_handlers\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\treturn lambda fn: UnitOfWork(fn, metadata, timeout)\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n@unit_of_work(metadata={'foo': 'bar'}, timeout=5)", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tThe decorator function to be returned.\n\t\t\"\"\"\n\t\t# Set the metadata and timeout attributes.\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t/docs/source/api/unit_of_work.rst\n.. _api_unit_of_work:\n\n=========\nUnitOfWork\n=========\n\n.. automodule:: pyramid_unit_of_work.unit_of_work\n.. autoclass:: UnitOfWork\n\t:members:\n\t:undoc-members:\n\t:show-inheritance:\n/pyramid_unit_of_work/session.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n\nfrom zope.interface import implementer\n\nfrom pyramid.settings import asbool\nfrom pyramid.interfaces import IRegistry\n\nfrom.interfaces import IUnitOfWorkSession\n\n@implementer(IUnitOfWorkSession)", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\n\treturn decorator\n\n\n@unit_of_work()", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\tdef _wrap(f):\n\t\t\tf.metadata = metadata or {}\n\t\t\tf.timeout = timeout or 120\n\t\t\treturn f\n\t\treturn _wrap\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tWraps the decorated function.\n\t\t\"\"\"\n\t\t\n\t\t@wraps(func)\n\t\tdef decorated_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe decorated function.\n\t\t\t\"\"\"\n\t\t\t\n\t\t\treturn func(*args, **kwargs)\n\t\t\n\t\tdecorated_func.metadata = metadata\n\t\tdecorated_func.timeout = timeout\n\t\t\n\t\treturn decorated_func\n\t\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\treturn lambda fn: UnitOfWork(fn, metadata, timeout)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n@unit_of_work(metadata={'foo': 'bar'}, timeout=5)", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tThe decorator function to be returned.\n\t\t\"\"\"\n\t\t# Set the metadata and timeout attributes.\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t/docs/source/api/unit_of_work.rst\n.. _api_unit_of_work:\n\n=========\nUnitOfWork\n=========\n\n.. automodule:: pyramid_unit_of_work.unit_of_work\n.. autoclass:: UnitOfWork\n\t:members:\n\t:undoc-members:\n\t:show-inheritance:\n/pyramid_unit_of_work/session.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n\nfrom zope.interface import implementer\n\nfrom pyramid.settings import asbool\nfrom pyramid.interfaces import IRegistry\n\nfrom.interfaces import IUnitOfWorkSession\n\n@implementer(IUnitOfWorkSession)", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\n\treturn decorator\n\n\n@unit_of_work()", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\tdef _wrap(f):\n\t\t\tf.metadata = metadata or {}\n\t\t\tf.timeout = timeout or 120\n\t\t\treturn f\n\t\treturn _wrap\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tWraps the decorated function.\n\t\t\"\"\"\n\t\t\n\t\t@wraps(func)\n\t\tdef decorated_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe decorated function.\n\t\t\t\"\"\"\n\t\t\t\n\t\t\treturn func(*args, **kwargs)\n\t\t\n\t\tdecorated_func.metadata = metadata\n\t\tdecorated_func.timeout = timeout\n\t\t\n\t\treturn decorated_func\n\t\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn self.data[key]\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn hash(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn int(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn int(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn self.key.index(key)\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn self.hashFunction(key)\n/README.md\n# CS-253-Project\nCS253 Project 1-2\n/project2/HashTable.py", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\traise ValueError(\"key must be an integer\")\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn ord(key) - ord('a')\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn self.data[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn hash(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn int(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn int(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn self.key.index(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn self.hashFunction(key)\n/README.md\n# CS-253-Project\nCS253 Project 1-2\n/project2/HashTable.py", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\traise ValueError(\"key must be an integer\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\t\treturn ord(key) - ord('a')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn self[self.index][keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get(k) for k in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index[k] for k in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index[k] for k in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self._values[x] for x in keys]\n\n\tdef _set_values(self, values):\n\t\"\"\"\n\tSets the values of the key filtered by self.index.\n\t\"\"\"\n\t\tfor i, v in zip(self.index, values):\n\t\t\tself._values[i] = v\n\n\tvalues = property(values, _set_values)\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an interator over the values.\n\t\"\"\"\n\t\treturn iter(self._values)\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturns a string representation of the object.\n\t\"\"\"\n\t\treturn str(self._values)\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturns a string representation of the object.\n\t\"\"\"\n\t\treturn str(self._values)\n\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tReturns the key filtered by self.index.\n\t\"\"\"\n\t\treturn self._values[key]\n\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSets the key filtered by self.index to the given value.\n\t\"\"\"\n\t\tself._values[key] = value\n\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDeletes the key filtered by self.index.\n\t\"\"\"\n\t\tdel self._values[key]", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index[k] for k in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get_value(key) for key in keys]\n\t\t", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get(key, None) for key in keys]\n\n\tdef __init__(self, filename = None):\n\t\tself.index = dict()\n\t\tself.header = []\n\t\tif filename:\n\t\t\tself.load(filename)\n\n\tdef load(self, filename):\n\t\t\"\"\"\n\t\tReads a PDB file into this object's index.\n\t\t\"\"\"\n\t\tself.index = load(filename)\n\t\tself.header = self.index.keys()\n\n\tdef save(self, filename):\n\t\t\"\"\"\n\t\tWrites this object's index to a PDB file.\n\t\t\"\"\"\n\t\twith open(filename, 'w') as f:\n\t\t\tfor key, value in self.index.items():\n\t\t\t\tf.write('%-6s%5d%-4s%1s%4s%1s%4d%12.3f%8.3f%8.3f%6.2f%6.2f%12s\\n' % value)\n\n\tdef get_lines(self, *keys):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [self.index.get(key, None) for key in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self[k] for k in keys]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get(k, None) for k in keys]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn self[self.index][keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get(k) for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index[k] for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index[k] for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self._values[x] for x in keys]\n\n\tdef _set_values(self, values):\n\t\"\"\"\n\tSets the values of the key filtered by self.index.\n\t\"\"\"\n\t\tfor i, v in zip(self.index, values):\n\t\t\tself._values[i] = v\n\n\tvalues = property(values, _set_values)\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an interator over the values.\n\t\"\"\"\n\t\treturn iter(self._values)\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturns a string representation of the object.\n\t\"\"\"\n\t\treturn str(self._values)\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturns a string representation of the object.\n\t\"\"\"\n\t\treturn str(self._values)\n\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tReturns the key filtered by self.index.\n\t\"\"\"\n\t\treturn self._values[key]\n\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSets the key filtered by self.index to the given value.\n\t\"\"\"\n\t\tself._values[key] = value\n\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDeletes the key filtered by self.index.\n\t\"\"\"\n\t\tdel self._values[key]", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index[k] for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get_value(key) for key in keys]\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get(key, None) for key in keys]\n\n\tdef __init__(self, filename = None):\n\t\tself.index = dict()\n\t\tself.header = []\n\t\tif filename:\n\t\t\tself.load(filename)\n\n\tdef load(self, filename):\n\t\t\"\"\"\n\t\tReads a PDB file into this object's index.\n\t\t\"\"\"\n\t\tself.index = load(filename)\n\t\tself.header = self.index.keys()\n\n\tdef save(self, filename):\n\t\t\"\"\"\n\t\tWrites this object's index to a PDB file.\n\t\t\"\"\"\n\t\twith open(filename, 'w') as f:\n\t\t\tfor key, value in self.index.items():\n\t\t\t\tf.write('%-6s%5d%-4s%1s%4s%1s%4d%12.3f%8.3f%8.3f%6.2f%6.2f%12s\\n' % value)\n\n\tdef get_lines(self, *keys):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [self.index.get(key, None) for key in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self[k] for k in keys]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\treturn [self.index.get(k, None) for k in keys]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t \n\treturn self.__data(*keys)\n\t", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self.exporter.data(*keys)\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self._data(keys)\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self._data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self.data.keys()\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t \n\treturn self.export_data(*keys)\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn keys\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\tif len(keys) == 1:\n\t\t\treturn self.data_keys[keys[0]]\n\t\telse:\n\t\t\treturn [self.data_keys[k] for k in keys]\n\n\n\tdef __str__(self):\n\t\treturn self.data\n\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\n\tdef __iter__(self):\n\t\tfor key in self.data_keys:\n\t\t\tyield self.data_keys[key]\n\n\n\tdef __getitem__(self, key):\n\t\treturn self.data_keys[key]\n\n\n\tdef __setitem__(self, key, value):\n\t\tself.data_keys[key] = value\n\n\n\tdef __delitem__(self, key):\n\t\tdel self.data_keys[key]\n\n\n\tdef __contains__(self, key):\n\t\treturn key in self.data_keys\n\n\n\tdef __eq__(self, other):\n\t\treturn self.data == other.data\n\n\n\tdef __ne__(self, other):\n\t\treturn self.data!= other.data\n\n\n\tdef __hash__(self):\n\t\treturn id(self)\n\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\n        return self.data_dict.keys()\n\n    def __call__(self, *args, **kwargs):\n\t\"\"\"\n\tCalls the method transform of the RecordExporter class with the keys and values from the current iterator.\n\t\n\t\"\"\"\n        return self.export_object.transform(*args, **kwargs)\n\n    def __iter__(self):\n        \"\"\"\n        Returns an iterator over the keys processed by the transform method of the RecordExporter class.\n        \"\"\"\n        return self.data()\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t \n\treturn self.__data(*keys)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self.exporter.data(*keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self._data(keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self._data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn self.data.keys()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t \n\treturn self.export_data(*keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\treturn keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\tif len(keys) == 1:\n\t\t\treturn self.data_keys[keys[0]]\n\t\telse:\n\t\t\treturn [self.data_keys[k] for k in keys]\n\n\n\tdef __str__(self):\n\t\treturn self.data\n\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\n\tdef __iter__(self):\n\t\tfor key in self.data_keys:\n\t\t\tyield self.data_keys[key]\n\n\n\tdef __getitem__(self, key):\n\t\treturn self.data_keys[key]\n\n\n\tdef __setitem__(self, key, value):\n\t\tself.data_keys[key] = value\n\n\n\tdef __delitem__(self, key):\n\t\tdel self.data_keys[key]\n\n\n\tdef __contains__(self, key):\n\t\treturn key in self.data_keys\n\n\n\tdef __eq__(self, other):\n\t\treturn self.data == other.data\n\n\n\tdef __ne__(self, other):\n\t\treturn self.data!= other.data\n\n\n\tdef __hash__(self):\n\t\treturn id(self)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\n        return self.data_dict.keys()\n\n    def __call__(self, *args, **kwargs):\n\t\"\"\"\n\tCalls the method transform of the RecordExporter class with the keys and values from the current iterator.\n\t\n\t\"\"\"\n        return self.export_object.transform(*args, **kwargs)\n\n    def __iter__(self):\n        \"\"\"\n        Returns an iterator over the keys processed by the transform method of the RecordExporter class.\n        \"\"\"\n        return self.data()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if len(self.data) < 2:\n            raise IndexError('No more elements')\n        else:\n            return self.data.pop() + self.data.pop() * 256\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) >= 2:\n\t\t\treturn self.data.pop() + self.data.pop() << 8\n\t\telse:\n\t\t\treturn None\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 2:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-2]\n\tself.data = self.data[:-2]\n\treturn res\n\n    def pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 4:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-4]\n\tself.data = self.data[:-4]\n\treturn res\n\n    def pop_i8(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 1:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-1]\n\tself.data = self.data[:-1]\n\treturn res\n\n    def pop_i16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 2:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-2]\n\tself.data = self.data[:-2]\n\treturn res\n\n    def pop_i32(self):\n\t", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\traise IndexError('pop from empty list')\n\t\tval = self.data[-2:][::-1]\n\t\tdel self.data[-2:]\n\t\treturn val\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) == 0:\n\t\t\traise ValueError(\"Can not pop from empty list\")\n\t\tv = self.data.pop()\n\t\tv <<= 8\n\t\tv += self.data.pop()\n\t\treturn v\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if len(self.data)!= 0:\n            value = self.data.pop() + self.data.pop() * 256\n            return value\n        else:\n            return None\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_f(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_f32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_f64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_char(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_string(self):\n\t\"\"\"\n\tRemove the last string in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_byte(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size() < 2:\n\t\t\traise ValueError(\"Cannot pop from an empty stack\")\n\t\telse:\n\t\t\tself.data = self.data[0:len(self.data)-2]\n\t\t\treturn self.data[len(self.data)-2:len(self.data)]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size() < 2:\n\t\t\traise ValueError(\"Cannot pop from an empty stack\")\n\t\telse:\n\t\t\tself.data = self.data[0:len(self.data)-2]\n\t\t\treturn self.data[len(self.data)-2:len(self.data)]\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size() < 2:\n\t\t\traise ValueError(\"Cannot pop from an empty stack\")\n\t\telse:\n\t\t\tself.data = self.data[0:len(self.data)-2]\n\t\t\treturn self.data[len(self.data)-2:len(self.data)]\n\n\tdef pop_u128(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\traise Empty(\"stack is empty\")\n\t\tresult = self.data.pop()\n\t\tresult = self.data.pop()\n\t\treturn result\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\traise IndexError(\"Data stack is empty\")\n\t\telse:\n\t\t\tself.data.pop()\n\t\t\tself.data.pop()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if len(self.data) < 2:\n            raise IndexError('No more elements')\n        else:\n            return self.data.pop() + self.data.pop() * 256\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) >= 2:\n\t\t\treturn self.data.pop() + self.data.pop() << 8\n\t\telse:\n\t\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 2:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-2]\n\tself.data = self.data[:-2]\n\treturn res\n\n    def pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 4:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-4]\n\tself.data = self.data[:-4]\n\treturn res\n\n    def pop_i8(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 1:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-1]\n\tself.data = self.data[:-1]\n\treturn res\n\n    def pop_i16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if self.size() < 2:\n            raise IndexError('pop from empty buffer')\n        \n\tres = self.data[self.size()-2]\n\tself.data = self.data[:-2]\n\treturn res\n\n    def pop_i32(self):\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\traise IndexError('pop from empty list')\n\t\tval = self.data[-2:][::-1]\n\t\tdel self.data[-2:]\n\t\treturn val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) == 0:\n\t\t\traise ValueError(\"Can not pop from empty list\")\n\t\tv = self.data.pop()\n\t\tv <<= 8\n\t\tv += self.data.pop()\n\t\treturn v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n        if len(self.data)!= 0:\n            value = self.data.pop() + self.data.pop() * 256\n            return value\n        else:\n            return None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_f(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_f32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_f64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_char(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_string(self):\n\t\"\"\"\n\tRemove the last string in self.data and return\n\t\"\"\"\n\t\treturn self.data.pop()\n\tdef pop_byte(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size() < 2:\n\t\t\traise ValueError(\"Cannot pop from an empty stack\")\n\t\telse:\n\t\t\tself.data = self.data[0:len(self.data)-2]\n\t\t\treturn self.data[len(self.data)-2:len(self.data)]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size() < 2:\n\t\t\traise ValueError(\"Cannot pop from an empty stack\")\n\t\telse:\n\t\t\tself.data = self.data[0:len(self.data)-2]\n\t\t\treturn self.data[len(self.data)-2:len(self.data)]\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size() < 2:\n\t\t\traise ValueError(\"Cannot pop from an empty stack\")\n\t\telse:\n\t\t\tself.data = self.data[0:len(self.data)-2]\n\t\t\treturn self.data[len(self.data)-2:len(self.data)]\n\n\tdef pop_u128(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif self.size", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\traise Empty(\"stack is empty\")\n\t\tresult = self.data.pop()\n\t\tresult = self.data.pop()\n\t\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\traise IndexError(\"Data stack is empty\")\n\t\telse:\n\t\t\tself.data.pop()\n\t\t\tself.data.pop()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        self.out.append(packstream.pack(\n            packstream.DISCARD,\n            n=n, qid=qid,\n            dehydration_hooks=dehydration_hooks,\n            hydration_hooks=hydration_hooks,\n            **handlers))\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if dehydration_hooks is None:\n            dehydration_hooks = {}\n        if hydration_hooks is None:\n            hydration_hooks = {}\n        data = {'n': n, 'qid': qid}\n        return self.send('DISCARD', data, dehydration_hooks, hydration_hooks,\n                         **handlers)\n\n\n    def dehydrate_type(self, t, dehydration_hooks):\n        if type(t) in dehydration_hooks:\n            return dehydration_hooks[type(t)](t)\n        return t\n\n\n    def hydrate_type(self, t, hydration_hooks):\n        if type(t) in hydration_hooks:\n            return hydration_hooks[type(t)](t)\n        return t\n\n\n    def send(self, name, data, dehydration_hooks, hydration_hooks,\n             **handlers):\n\n        self._check_connected()\n\n        self.write_data(name, data, dehydration_hooks)\n        self.flush()\n\n        return Response(self, dehydration_hooks, hydration_hooks, **handlers)\n\n\n    def write_data(self, name, data, dehydration_hooks):\n        d = {}\n        for (k, v) in data.items():\n            d[k] = self.dehydrate_type(v, dehyd", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tif n < 0:\n\t\t\tn = self.output.last_discarded\n\t\telif n == 0:\n\t\t\treturn\n\t\tself.output.last_discarded = n\n\t\tself.output.queue.append(packstream.packb(packstream.Message(\n\t\t\tpackstream.DISCARD, n, qid\n\t\t)))\n\t\treturn self.response(dehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n\n\tdef dehydrate(self, value, dehydration_hooks=None):\n\t\t\"\"\"\n\tConverts a value to a type understood by packstream.\n\n:param value: the value to convert\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:returns: the converted value\n\t\"\"\"\n\t\tif dehydration_hooks is None:\n\t\t\tdehydration_hooks = {}\n\t\tdehydration_hooks.update(self.dehydration_hooks)\n\t\tfor cls in type(value).__mro__:\n\t\t\tif cls in dehydration_hooks:\n\t\t\t\treturn dehydration_hooks[cls](value)\n\t\treturn value\n\n\tdef hydrate", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        self.outqueue.append(self.packstream.pack(\n            n, qid, handlers, self.packstream.DISCARDED,\n            dehydration_hooks=dehydration_hooks,\n            hydration_hooks=hydration_hooks))\n\n    def dehydrate(self, data, dehydration_hooks=None):\n        \"\"\"\n        Dehydrate data into a packstream object.\n\n        :param data: data to dehydrate\n        :param dehydration_hooks:\n            Hooks to dehydrate types (dict from type (class) to\n            dehydration function). Dehydration functions receive the\n            value and returns an object of type understood by\n            packstream.\n        :returns: packstream object\n        \"\"\"\n        return self.packstream.pack(data, dehydration_hooks=dehydration_hooks)\n\n    def hydrate(self, data, hydration_hooks=None):\n        \"\"\"\n        Hydrate data from a packstream object.\n\n        :param data: data to hydrate\n        :param hydration_hooks:\n            Hooks to hydrate types (mapping from type (class) to\n            dehydration function). Dehydration functions receive the\n            value of type understood by packstream and are free to\n            return anything.\n        :returns: dehydrated object\n        \"\"\"\n        return self.packstream.unpack(data, hydration_hooks", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tassert (n >= -1)\n\t\tassert (qid >= -1)\n\n\t\tif self._output is None:\n\t\t\traise self.Error('No output')\n\n\t\tself._output.append(Discard(n, qid, dehydration_hooks,\n\t\t\t\thydration_hooks, **handlers))\n\t\treturn self.Response()\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        # TODO: check that n is a valid integer (non-negative)\n        # TODO: check that qid is a valid integer (non-negative)\n        # TODO: check that dehydration_hooks is a dict\n        # TODO: check that hydration_hooks is a dict\n\n\t# check for valid handlers\n\tif len(handlers) > 0 and not _is_valid_handlers(handlers):\n\t\traise Exception(\"Invalid handler(s) passed to discard\")\n\n\t# check for valid dehydration/hydration hooks\n\tif dehydration_hooks is not None:\n\t\tif not isinstance(dehydration_hooks, dict):\n\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\t\tfor type in dehydration_hooks:\n\t\t\tif not isinstance(type, type):\n\t\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\t\t\tif not (isclass(type) and issubclass(type, object)):\n\t\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\t\t\tif not callable(dehydration_hooks[type]):\n\t\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\n\tif hydration_hooks is not None:\n\t\tif not isinstance(hydration_hooks, dict):\n\t\t\traise Exception(\"Invalid hydration_hooks passed to discard\")\n\t\tfor type in", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\treturn self._send(b'\\x03', n, qid, dehydration_hooks,\n                hydration_hooks, **handlers)\n\n    def dehydrate_type(self, value, dehydration_hooks=None):\n        \"\"\"\n        Dehydrate a type to something understood by packstream.\n\n:param value: value to dehydrate\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:return: dehydrated value\n:raises: TypeError\n        \"\"\"\n        if dehydration_hooks:\n            for type_ in dehydration_hooks:\n                if isinstance(value, type_):\n                    return dehydration_hooks[type_](value)\n        return value\n\n    def hydrate_type(self, value, hydration_hooks=None):\n        \"\"\"\n        Hydrate a type to something understood by packstream.\n\n:param value: value to hydrate\n:param hydration_hooks:\n    Hooks to hydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value of type\n    understood by packstream and are free to return anything.\n:return: hydrate value\n:raises: TypeError\n        \"\"\"", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        self.output.append(\n            packstream.pack(\n                packstream.DISCARD,\n                n, qid, dehydration_hooks, hydration_hooks, **handlers))\n```\n\n---\n\n## Response\n\nThis class represents a response to a message.\n\n```python\n    class Response(object):\n\t\"\"\"\n\tA Response object represents the response to a message.\n\t\"\"\"\n\n\t\tdef __init__(self, raw_response, dehydration_hooks=None,\n                hydration_hooks=None):\n            \"\"\"\n            Initializes a new Response object.\n\n            :param raw_response:\n                The raw response to parse. This should be a tuple\n                (header, data) where header is an integer and data is\n                an array of bytes.\n            :param dehydration_hooks:\n                Hooks to dehydrate types (dict from type (class) to\n                dehydration function). Dehydration functions receive the\n                value and returns an object of type understood by\n                packstream.\n            :param hydration_hooks:\n                Hooks to hydrate types (mapping from type (class) to\n                dehydration function). Dehydration functions receive the\n                value of type understood by packstream and are free\n                to return anything.\n            \"\"\"\n\n            self.raw_response = raw_response\n            self.dehydration_hooks = dehydration_hooks\n            self.hydration_hooks", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed')\n\n        self._write(self.DISCARD, n, qid, dehydration_hooks, hydration_hooks, **handlers)\n\n    def ping(self, data=None, **handlers):\n\t\"\"\"\n\tAppends a PING message to the output queue.\n\n:param data: data to send in the ping, default = None\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed')\n\n        self._write(self.PING, data, **handlers)\n\n    def pong(self, data=None, **handlers):\n\t\"\"\"\n\tAppends a PONG message to the output queue.\n\n:param data: data to send in the pong, default = None\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed')\n\n        self._write(self.PONG, data, **handlers)\n\n    def auth(self, user, password, **handlers):\n\t\"\"\"\n\tAppends an AUTH message to the output queue.\n\n:param user: username string\n:param password:  string\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        self.out.append(packstream.pack(\n            packstream.DISCARD,\n            n=n, qid=qid,\n            dehydration_hooks=dehydration_hooks,\n            hydration_hooks=hydration_hooks,\n            **handlers))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if dehydration_hooks is None:\n            dehydration_hooks = {}\n        if hydration_hooks is None:\n            hydration_hooks = {}\n        data = {'n': n, 'qid': qid}\n        return self.send('DISCARD', data, dehydration_hooks, hydration_hooks,\n                         **handlers)\n\n\n    def dehydrate_type(self, t, dehydration_hooks):\n        if type(t) in dehydration_hooks:\n            return dehydration_hooks[type(t)](t)\n        return t\n\n\n    def hydrate_type(self, t, hydration_hooks):\n        if type(t) in hydration_hooks:\n            return hydration_hooks[type(t)](t)\n        return t\n\n\n    def send(self, name, data, dehydration_hooks, hydration_hooks,\n             **handlers):\n\n        self._check_connected()\n\n        self.write_data(name, data, dehydration_hooks)\n        self.flush()\n\n        return Response(self, dehydration_hooks, hydration_hooks, **handlers)\n\n\n    def write_data(self, name, data, dehydration_hooks):\n        d = {}\n        for (k, v) in data.items():\n            d[k] = self.dehydrate_type(v, dehyd", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tif n < 0:\n\t\t\tn = self.output.last_discarded\n\t\telif n == 0:\n\t\t\treturn\n\t\tself.output.last_discarded = n\n\t\tself.output.queue.append(packstream.packb(packstream.Message(\n\t\t\tpackstream.DISCARD, n, qid\n\t\t)))\n\t\treturn self.response(dehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n\n\tdef dehydrate(self, value, dehydration_hooks=None):\n\t\t\"\"\"\n\tConverts a value to a type understood by packstream.\n\n:param value: the value to convert\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:returns: the converted value\n\t\"\"\"\n\t\tif dehydration_hooks is None:\n\t\t\tdehydration_hooks = {}\n\t\tdehydration_hooks.update(self.dehydration_hooks)\n\t\tfor cls in type(value).__mro__:\n\t\t\tif cls in dehydration_hooks:\n\t\t\t\treturn dehydration_hooks[cls](value)\n\t\treturn value\n\n\tdef hydrate", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        self.outqueue.append(self.packstream.pack(\n            n, qid, handlers, self.packstream.DISCARDED,\n            dehydration_hooks=dehydration_hooks,\n            hydration_hooks=hydration_hooks))\n\n    def dehydrate(self, data, dehydration_hooks=None):\n        \"\"\"\n        Dehydrate data into a packstream object.\n\n        :param data: data to dehydrate\n        :param dehydration_hooks:\n            Hooks to dehydrate types (dict from type (class) to\n            dehydration function). Dehydration functions receive the\n            value and returns an object of type understood by\n            packstream.\n        :returns: packstream object\n        \"\"\"\n        return self.packstream.pack(data, dehydration_hooks=dehydration_hooks)\n\n    def hydrate(self, data, hydration_hooks=None):\n        \"\"\"\n        Hydrate data from a packstream object.\n\n        :param data: data to hydrate\n        :param hydration_hooks:\n            Hooks to hydrate types (mapping from type (class) to\n            dehydration function). Dehydration functions receive the\n            value of type understood by packstream and are free to\n            return anything.\n        :returns: dehydrated object\n        \"\"\"\n        return self.packstream.unpack(data, hydration_hooks", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tassert (n >= -1)\n\t\tassert (qid >= -1)\n\n\t\tif self._output is None:\n\t\t\traise self.Error('No output')\n\n\t\tself._output.append(Discard(n, qid, dehydration_hooks,\n\t\t\t\thydration_hooks, **handlers))\n\t\treturn self.Response()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        # TODO: check that n is a valid integer (non-negative)\n        # TODO: check that qid is a valid integer (non-negative)\n        # TODO: check that dehydration_hooks is a dict\n        # TODO: check that hydration_hooks is a dict\n\n\t# check for valid handlers\n\tif len(handlers) > 0 and not _is_valid_handlers(handlers):\n\t\traise Exception(\"Invalid handler(s) passed to discard\")\n\n\t# check for valid dehydration/hydration hooks\n\tif dehydration_hooks is not None:\n\t\tif not isinstance(dehydration_hooks, dict):\n\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\t\tfor type in dehydration_hooks:\n\t\t\tif not isinstance(type, type):\n\t\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\t\t\tif not (isclass(type) and issubclass(type, object)):\n\t\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\t\t\tif not callable(dehydration_hooks[type]):\n\t\t\t\traise Exception(\"Invalid dehydration_hooks passed to discard\")\n\n\tif hydration_hooks is not None:\n\t\tif not isinstance(hydration_hooks, dict):\n\t\t\traise Exception(\"Invalid hydration_hooks passed to discard\")\n\t\tfor type in", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\treturn self._send(b'\\x03', n, qid, dehydration_hooks,\n                hydration_hooks, **handlers)\n\n    def dehydrate_type(self, value, dehydration_hooks=None):\n        \"\"\"\n        Dehydrate a type to something understood by packstream.\n\n:param value: value to dehydrate\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:return: dehydrated value\n:raises: TypeError\n        \"\"\"\n        if dehydration_hooks:\n            for type_ in dehydration_hooks:\n                if isinstance(value, type_):\n                    return dehydration_hooks[type_](value)\n        return value\n\n    def hydrate_type(self, value, hydration_hooks=None):\n        \"\"\"\n        Hydrate a type to something understood by packstream.\n\n:param value: value to hydrate\n:param hydration_hooks:\n    Hooks to hydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value of type\n    understood by packstream and are free to return anything.\n:return: hydrate value\n:raises: TypeError\n        \"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        self.output.append(\n            packstream.pack(\n                packstream.DISCARD,\n                n, qid, dehydration_hooks, hydration_hooks, **handlers))\n```\n\n---\n\n## Response\n\nThis class represents a response to a message.\n\n```python\n    class Response(object):\n\t\"\"\"\n\tA Response object represents the response to a message.\n\t\"\"\"\n\n\t\tdef __init__(self, raw_response, dehydration_hooks=None,\n                hydration_hooks=None):\n            \"\"\"\n            Initializes a new Response object.\n\n            :param raw_response:\n                The raw response to parse. This should be a tuple\n                (header, data) where header is an integer and data is\n                an array of bytes.\n            :param dehydration_hooks:\n                Hooks to dehydrate types (dict from type (class) to\n                dehydration function). Dehydration functions receive the\n                value and returns an object of type understood by\n                packstream.\n            :param hydration_hooks:\n                Hooks to hydrate types (mapping from type (class) to\n                dehydration function). Dehydration functions receive the\n                value of type understood by packstream and are free\n                to return anything.\n            \"\"\"\n\n            self.raw_response = raw_response\n            self.dehydration_hooks = dehydration_hooks\n            self.hydration_hooks", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed')\n\n        self._write(self.DISCARD, n, qid, dehydration_hooks, hydration_hooks, **handlers)\n\n    def ping(self, data=None, **handlers):\n\t\"\"\"\n\tAppends a PING message to the output queue.\n\n:param data: data to send in the ping, default = None\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed')\n\n        self._write(self.PING, data, **handlers)\n\n    def pong(self, data=None, **handlers):\n\t\"\"\"\n\tAppends a PONG message to the output queue.\n\n:param data: data to send in the pong, default = None\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed')\n\n        self._write(self.PONG, data, **handlers)\n\n    def auth(self, user, password, **handlers):\n\t\"\"\"\n\tAppends an AUTH message to the output queue.\n\n:param user: username string\n:param password:  string\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        if self._closed:\n            raise ValueError('Output queue closed", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(\"BEGIN\", mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tif mode is not None and mode not in [\"READ\", \"WRITE\"]:\n\t\t\traise ValueError(\"Invalid mode '%s'\" % mode)\n\n\t\trequest = BoltRequest(self.protocol, BoltRequest.BEGIN, {\n\t\t\t\"metadata\": metadata,\n\t\t\t\"bookmarks\": bookmarks,\n\t\t\t\"timeout\": timeout,\n\t\t\t\"database\": db,\n\t\t\t\"imp_user\": imp_user,\n\t\t\t\"dehydration_hooks\": dehydration_hooks,\n\t\t\t\"hydration_hooks\": hydration_hooks,\n\t\t})\n\t\treturn self.send(request, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tself._pack()\n\t\tself._write([self.BEGIN, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks])\n\t\treturn self._read(handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tif not self.in_transaction:\n\t\t\tself._begin(mode, bookmarks, metadata, timeout, db, imp_user)\n\t\treturn self.response(handlers)\n\n\tdef begin_transaction(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\t\"\"\"\n\t\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehyd", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tif not self._db_name:\n\t\t\traise RuntimeError(\"Database name must be set before beginning a transaction\")\n\t\tif self._cursor:\n\t\t\traise RuntimeError(\"Cannot begin a new transaction while an existing one is open\")\n\t\tif mode and mode not in [\"READ\", \"WRITE\"]:\n\t\t\traise ValueError(\"mode must be 'READ' or 'WRITE'\")\n\t\tif not db and mode == \"WRITE\":\n\t\t\traise ValueError(\"WRITE transactions require a database name\")\n\t\tif db and mode == \"WRITE\":\n\t\t\tself._db_name = db\n\t\tif bookmarks and not isinstance(bookmarks, (list, tuple)):\n\t\t\traise ValueError(\"bookmarks must be an iterable\")\n\t\tif metadata and not isinstance(metadata, dict):\n\t\t\traise ValueError(\"metadata must be a dict\")\n\t\tif timeout and not isinstance(timeout, (int, float)):\n\t\t\traise ValueError(\"timeout must be a number\")\n\t\tif dehydration_hooks and not isinstance(dehydration_hooks, dict):\n\t\t\traise ValueError(\"dehydration_hooks must be a dict\")\n\t\tif hydration_hooks and not isinstance(hydration_hooks, dict):\n\t\t\traise ValueError(\"hydration_hooks must be a dict\")\n\t\tif not self._db_name:\n\t\t\traise RuntimeError(\"A database name must be set before beginning a transaction\")\n\t\tmsg = BEGIN_MSG.copy()\n\t", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(BEGIN, mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(\n\t\t\tBegin(mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t\t\tdehydration_hooks, hydration_hooks), **handlers)\n\n# #######################\n# #\n# # Cypher\n# #\n# #######################\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tmsg = self._pack_message(BEGIN, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\t\tself.send_message(msg)\n\t\treturn self._receive_response(**handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self._send(begin, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(self.bolt._create_begin_message(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks), **handlers)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(\"BEGIN\", mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tif mode is not None and mode not in [\"READ\", \"WRITE\"]:\n\t\t\traise ValueError(\"Invalid mode '%s'\" % mode)\n\n\t\trequest = BoltRequest(self.protocol, BoltRequest.BEGIN, {\n\t\t\t\"metadata\": metadata,\n\t\t\t\"bookmarks\": bookmarks,\n\t\t\t\"timeout\": timeout,\n\t\t\t\"database\": db,\n\t\t\t\"imp_user\": imp_user,\n\t\t\t\"dehydration_hooks\": dehydration_hooks,\n\t\t\t\"hydration_hooks\": hydration_hooks,\n\t\t})\n\t\treturn self.send(request, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tself._pack()\n\t\tself._write([self.BEGIN, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks])\n\t\treturn self._read(handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tif not self.in_transaction:\n\t\t\tself._begin(mode, bookmarks, metadata, timeout, db, imp_user)\n\t\treturn self.response(handlers)\n\n\tdef begin_transaction(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\t\"\"\"\n\t\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehyd", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tif not self._db_name:\n\t\t\traise RuntimeError(\"Database name must be set before beginning a transaction\")\n\t\tif self._cursor:\n\t\t\traise RuntimeError(\"Cannot begin a new transaction while an existing one is open\")\n\t\tif mode and mode not in [\"READ\", \"WRITE\"]:\n\t\t\traise ValueError(\"mode must be 'READ' or 'WRITE'\")\n\t\tif not db and mode == \"WRITE\":\n\t\t\traise ValueError(\"WRITE transactions require a database name\")\n\t\tif db and mode == \"WRITE\":\n\t\t\tself._db_name = db\n\t\tif bookmarks and not isinstance(bookmarks, (list, tuple)):\n\t\t\traise ValueError(\"bookmarks must be an iterable\")\n\t\tif metadata and not isinstance(metadata, dict):\n\t\t\traise ValueError(\"metadata must be a dict\")\n\t\tif timeout and not isinstance(timeout, (int, float)):\n\t\t\traise ValueError(\"timeout must be a number\")\n\t\tif dehydration_hooks and not isinstance(dehydration_hooks, dict):\n\t\t\traise ValueError(\"dehydration_hooks must be a dict\")\n\t\tif hydration_hooks and not isinstance(hydration_hooks, dict):\n\t\t\traise ValueError(\"hydration_hooks must be a dict\")\n\t\tif not self._db_name:\n\t\t\traise RuntimeError(\"A database name must be set before beginning a transaction\")\n\t\tmsg = BEGIN_MSG.copy()\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(BEGIN, mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(\n\t\t\tBegin(mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t\t\tdehydration_hooks, hydration_hooks), **handlers)\n\n# #######################\n# #\n# # Cypher\n# #\n# #######################\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\tmsg = self._pack_message(BEGIN, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\t\tself.send_message(msg)\n\t\treturn self._receive_response(**handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self._send(begin, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\treturn self.run(self.bolt._create_begin_message(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks), **handlers)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t/src/main.py\nfrom flask import Flask, render_template, request\n\napp = Flask(__name__)\n\n@app.route('/')", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn round(n + 0.5)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\n\n\treturn int(n + 0.5)/src/main.py\nfrom pyfiglet import Figlet\nimport os\n\nimport colorama\nfrom colorama import Fore, Back, Style\ncolorama.init()\n\nfrom src.menu import Menu\nfrom src.classes import Player\nfrom src.classes import Monster\nfrom src.classes import Item\nfrom src.classes import Room\nfrom src.classes import Game\n\nfrom src.functions import clear_screen\nfrom src.functions import print_pause\nfrom src.functions import random_num\nfrom src.functions import round_half_to_even\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn int(n + 0.5)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\tn = int(n)\n\t\treturn n if abs(n - round(n)) < 0.5 else round(n + 0.5)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\tif n % 1 < 0.5:\n\t\t\treturn math.floor(n)\n\t\telif n % 1 > 0.5:\n\t\t\treturn math.ceil(n)\n\t\telse:\n\t\t\treturn n\n\t", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn math.floor(n + 0.5)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t0.5 will be rounded to the nearest even number\n\t\"\"\"\n\treturn np.round(n - 0.5) + 0.5\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\tfrac = int(n*1000000000000000000)%1000000000000000000\n\t\tif frac > 500000000000000000:\n\t\t\treturn int(n) + 1\n\t\telse:\n\t\t\treturn int(n)\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn int(math.floor(n + 0.5))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t/src/main.py\nfrom flask import Flask, render_template, request\n\napp = Flask(__name__)\n\n@app.route('/')", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn round(n + 0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\n\n\treturn int(n + 0.5)/src/main.py\nfrom pyfiglet import Figlet\nimport os\n\nimport colorama\nfrom colorama import Fore, Back, Style\ncolorama.init()\n\nfrom src.menu import Menu\nfrom src.classes import Player\nfrom src.classes import Monster\nfrom src.classes import Item\nfrom src.classes import Room\nfrom src.classes import Game\n\nfrom src.functions import clear_screen\nfrom src.functions import print_pause\nfrom src.functions import random_num\nfrom src.functions import round_half_to_even\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn int(n + 0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\tn = int(n)\n\t\treturn n if abs(n - round(n)) < 0.5 else round(n + 0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\tif n % 1 < 0.5:\n\t\t\treturn math.floor(n)\n\t\telif n % 1 > 0.5:\n\t\t\treturn math.ceil(n)\n\t\telse:\n\t\t\treturn n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn math.floor(n + 0.5)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t0.5 will be rounded to the nearest even number\n\t\"\"\"\n\treturn np.round(n - 0.5) + 0.5\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\tfrac = int(n*1000000000000000000)%1000000000000000000\n\t\tif frac > 500000000000000000:\n\t\t\treturn int(n) + 1\n\t\telse:\n\t\t\treturn int(n)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\t\treturn int(math.floor(n + 0.5))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\tfields = [\n\t\t\t(name + '_x', models.FloatField()),\n\t\t\t(name + '_y', models.FloatField()),\n\t\t\t(name + '_z', models.FloatField(null=True, blank=True)),\n\t\t\t('geom', models.PointField(srid=srid_map[name])),\n\t\t]\n\t\treturn type(name, (models.Model,), dict(\n\t\t\tMeta=type('Meta', (object,), dict(\n\t\t\t\tapp_label='django.contrib.gis',\n\t\t\t\tproxy=True,\n\t\t\t)),\n\t\t\t__module__='django.contrib.gis.db.models',\n\t\t\t__str__=lambda self: 'Point'+ str(self.geom),\n\t\t\t__unicode__=lambda self: 'Point'+ str(self.geom),\n\t\t\t__new__=lambda cls, *args, **kwargs: models.Model.__new__(cls, *args, **kwargs),\n\t\t\tpk=models.AutoField(primary_key=True, serialize=False),\n\t\t\t_meta=models.Model._meta,\n\t\t\t__slots__=tuple(f[0] for f in fields),\n\t\t\t__dict__=dict(fields),\n\t\t\t_default_manager=models.GeoManager(),\n\t\t))\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tclass_text = \"class %s(Point):\\n\" % name\n\tclass_text += \"    __tablename__ = \\\"%s\\\"\\n\" % name\n\tclass_text += \"    __table_args__ = {'autoload':True}\\n\"\n\tclass_text += \"    __spatial_index__ = True\\n\"\n\n\tfor field in fields:\n\t\tclass_text += \"    %s = Column(Integer)\\n\" % field\n\n\tclass_text += \"\\n\"\n\n\tfor field in fields:\n\t\tclass_text += \"    def %s(self):\\n\" % field\n\t\tclass_text += \"        return self.%s\\n\" % field\n\t\tclass_text += \"\\n\"\n\n\tclass_text += \"    def __repr__(self): \\n\"\n\tclass_text += \"        return '<%s: (%s, %s)>' % (self.__class__.__name__, self.x, self.y)\\n\"\n\tclass_text += \"\\n\"\n\n\tclass_text += \"    def __init__(self, x, y, **kwargs):\\n\"\n\tclass_text += \"        self.x = x\\n\"\n\tclass_text += \"        self.y = y\\n\"\n\tclass_text += \"\\n\"\n\n\tfor field in fields:\n\t\tclass_", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\t# Class definition\n\tcode = \"class %s(Point):\\n\" % name\n\t# Fields\n\tfor field in fields:\n\t\tcode += \"\\t%s = models.FloatField()\\n\" % field\n\t# SRID Map\n\tfor srid in srid_map:\n\t\tcode += \"\\tsrid_%s = models.IntegerField()\\n\" % srid\n\t# Meta Class\n\tcode += \"\\tclass Meta:\\n\"\n\tcode += \"\\t\\tapp_label = 'point'\\n\"\n\tcode += \"\\t\\tverbose_name = '%s'\\n\" % name\n\tcode += \"\\t\\tverbose_name_plural = '%ss'\\n\" % name\n\t# Return\n\treturn code\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t \n\tif name in srid_map:\n\t\tsrid = srid_map[name]\n\telse:\n\t\tsrid = 4326\n\n\treturn type(name, (Point,), {\n\t\t'__tablename__': name,\n\t\t'__table_args__': (\n\t\t\tsa.PrimaryKeyConstraint('id'),\n\t\t\tsa.UniqueConstraint(name, 'id'),\n\t\t\tsa.Index(name + '_gix', name, sa.func.ST_Gix(srid)),\n\t\t),\n\t\t'__mapper_args__': dict(\n\t\t\tinclude_properties=fields,\n\t\t\texclude_properties=['__table__'],\n\t\t\texclude_properties_on_mapper=True,\n\t\t),\n\t})\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tpoint_type = type(str(name), (Point,), {})\n\n\t# add fields\n\tfor field_name in fields:\n\t\tpoint_type.add_field(field_name, models.FloatField())\n\n\t# add srid\n\tpoint_type.add_field(\"srid\", models.IntegerField(default=srid_map[name]))\n\n\t# add geometry\n\tpoint_type.add_field(\"geometry\", models.PointField(srid=srid_map[name]))\n\n\t# add manager\n\tpoint_type.add_manager(PointManager())\n\n\treturn point_type\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tclass Point(object):\n\t\t\"\"\"\n\t\tSpatial Point Class\n\t\t\"\"\"\n\t\tdef __init__(self, x, y):\n\t\t\tself.x = x\n\t\t\tself.y = y\n\n\t\tdef __getattr__(self, attr):\n\t\t\t\"\"\"\n\t\t\tReturns the attribute of the Point object.\n\t\t\t\"\"\"\n\t\t\treturn getattr(self, attr)\n\n\t\tdef __getitem__(self, item):\n\t\t\t\"\"\"\n\t\t\tReturns the attribute of the Point object by index.\n\t\t\t\"\"\"\n\t\t\tif item == 0:\n\t\t\t\treturn self.x\n\t\t\telif item == 1:\n\t\t\t\treturn self.y\n\t\t\telse:\n\t\t\t\traise IndexError\n\n\t\tdef __repr__(self):\n\t\t\t\"\"\"\n\t\t\tReturns a string representation of the object.\n\t\t\t\"\"\"\n\t\t\treturn 'POINT({0} {1})'.format(self.x, self.y)\n\n\t\tdef __str__(self):\n\t\t\t\"\"\"\n\t\t\tReturns a string representation of the object.\n\t\t\t\"\"\"\n\t\t\treturn 'POINT({0} {1})'.format(self.x, self.y)\n\n\t\tdef __eq__(self, other):\n\t\t\t\"\"\"\n\t\t\tReturns a boolean indicating if the two objects are equal.\n\t\t\t\"\"\"\n\t\t\tif isinstance(other, Point):\n\t\t\t\treturn self.x == other.x and self.y == other.y\n\t\t\telse:\n\t\t\t\treturn False\n\n\t", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tsrid = srid_map[name]\n\tfields = ','.join(fields)\n\tpoint_class_name = name.title()\n\tpoint_class_string = \"\"\"", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tclass Point(object):\n\t\tdef __init__(self, *args):\n\t\t\tif len(args) == 1 and isinstance(args[0], dict):\n\t\t\t\tself.set_dict(args[0])\n\t\t\telse:\n\t\t\t\tself.set_args(args)\n\n\t\tdef set_args(self, args):\n\t\t\tfor index, value in enumerate(args):\n\t\t\t\tsetattr(self, fields[index], value)\n\n\t\tdef set_dict(self, dict):\n\t\t\tfor key, value in dict.items():\n\t\t\t\tsetattr(self, key, value)\n\n\t\tdef to_dict(self):\n\t\t\tdict = {}\n\t\t\tfor field in fields:\n\t\t\t\tdict[field] = getattr(self, field)\n\t\t\treturn dict\n\n\t\tdef __repr__(self):\n\t\t\treturn \"Point: \" + str(self.to_dict())\n\n\treturn Point\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\t# fields = [field.name for field in model._meta.get_fields()]\n\t# fields.remove('id')\n\t# fields.remove('geom')\n\t# fields.remove('srid')\n\t# fields.remove('geo_type')\n\t# fields.remove('geo_level')\n\n\t# if 'geom' in fields:\n\t# \tfields.remove('geom')\n\n\t# if'srid' in fields:\n\t# \tfields.remove('srid')\n\n\t# if 'geo_type' in fields:\n\t# \tfields.remove('geo_type')\n\n\t# if 'geo_level' in fields:\n\t# \tfields.remove('geo_level')\n\n\t# if 'id' in fields:\n\t# \tfields.remove('id')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\t# define Point class\n\tpoint_class = type(name, (Point,), dict(\n\t\t__slots__=tuple(fields),\n\t\t__new__=lambda cls, *args: super(Point, cls).__new__(cls, *(args + (tuple(),))),\n\t\t__init__=lambda self, *args: None,\n\t\t__repr__=lambda self: \"Point(%s)\" % \", \".join(str(getattr(self, f)) for f in fields)\n\t))\n\n\t# add fields and methods\n\tfor field in fields:\n\t\tpoint_class.__slots__ = point_class.__slots__ + (field,)\n\t\tpoint_class.__new__.__code__ = point_class.__new__.__code__.replace(\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(fields).encode('ascii')),\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(point_class.__slots__).encode('ascii'))\n\t\t)\n\t\tpoint_class.__init__.__code__ = point_class.__init__.__code__.replace(\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(fields).encode('ascii')),\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(point_class"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\tfields = [\n\t\t\t(name + '_x', models.FloatField()),\n\t\t\t(name + '_y', models.FloatField()),\n\t\t\t(name + '_z', models.FloatField(null=True, blank=True)),\n\t\t\t('geom', models.PointField(srid=srid_map[name])),\n\t\t]\n\t\treturn type(name, (models.Model,), dict(\n\t\t\tMeta=type('Meta', (object,), dict(\n\t\t\t\tapp_label='django.contrib.gis',\n\t\t\t\tproxy=True,\n\t\t\t)),\n\t\t\t__module__='django.contrib.gis.db.models',\n\t\t\t__str__=lambda self: 'Point'+ str(self.geom),\n\t\t\t__unicode__=lambda self: 'Point'+ str(self.geom),\n\t\t\t__new__=lambda cls, *args, **kwargs: models.Model.__new__(cls, *args, **kwargs),\n\t\t\tpk=models.AutoField(primary_key=True, serialize=False),\n\t\t\t_meta=models.Model._meta,\n\t\t\t__slots__=tuple(f[0] for f in fields),\n\t\t\t__dict__=dict(fields),\n\t\t\t_default_manager=models.GeoManager(),\n\t\t))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tclass_text = \"class %s(Point):\\n\" % name\n\tclass_text += \"    __tablename__ = \\\"%s\\\"\\n\" % name\n\tclass_text += \"    __table_args__ = {'autoload':True}\\n\"\n\tclass_text += \"    __spatial_index__ = True\\n\"\n\n\tfor field in fields:\n\t\tclass_text += \"    %s = Column(Integer)\\n\" % field\n\n\tclass_text += \"\\n\"\n\n\tfor field in fields:\n\t\tclass_text += \"    def %s(self):\\n\" % field\n\t\tclass_text += \"        return self.%s\\n\" % field\n\t\tclass_text += \"\\n\"\n\n\tclass_text += \"    def __repr__(self): \\n\"\n\tclass_text += \"        return '<%s: (%s, %s)>' % (self.__class__.__name__, self.x, self.y)\\n\"\n\tclass_text += \"\\n\"\n\n\tclass_text += \"    def __init__(self, x, y, **kwargs):\\n\"\n\tclass_text += \"        self.x = x\\n\"\n\tclass_text += \"        self.y = y\\n\"\n\tclass_text += \"\\n\"\n\n\tfor field in fields:\n\t\tclass_", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\t# Class definition\n\tcode = \"class %s(Point):\\n\" % name\n\t# Fields\n\tfor field in fields:\n\t\tcode += \"\\t%s = models.FloatField()\\n\" % field\n\t# SRID Map\n\tfor srid in srid_map:\n\t\tcode += \"\\tsrid_%s = models.IntegerField()\\n\" % srid\n\t# Meta Class\n\tcode += \"\\tclass Meta:\\n\"\n\tcode += \"\\t\\tapp_label = 'point'\\n\"\n\tcode += \"\\t\\tverbose_name = '%s'\\n\" % name\n\tcode += \"\\t\\tverbose_name_plural = '%ss'\\n\" % name\n\t# Return\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t \n\tif name in srid_map:\n\t\tsrid = srid_map[name]\n\telse:\n\t\tsrid = 4326\n\n\treturn type(name, (Point,), {\n\t\t'__tablename__': name,\n\t\t'__table_args__': (\n\t\t\tsa.PrimaryKeyConstraint('id'),\n\t\t\tsa.UniqueConstraint(name, 'id'),\n\t\t\tsa.Index(name + '_gix', name, sa.func.ST_Gix(srid)),\n\t\t),\n\t\t'__mapper_args__': dict(\n\t\t\tinclude_properties=fields,\n\t\t\texclude_properties=['__table__'],\n\t\t\texclude_properties_on_mapper=True,\n\t\t),\n\t})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tpoint_type = type(str(name), (Point,), {})\n\n\t# add fields\n\tfor field_name in fields:\n\t\tpoint_type.add_field(field_name, models.FloatField())\n\n\t# add srid\n\tpoint_type.add_field(\"srid\", models.IntegerField(default=srid_map[name]))\n\n\t# add geometry\n\tpoint_type.add_field(\"geometry\", models.PointField(srid=srid_map[name]))\n\n\t# add manager\n\tpoint_type.add_manager(PointManager())\n\n\treturn point_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tclass Point(object):\n\t\t\"\"\"\n\t\tSpatial Point Class\n\t\t\"\"\"\n\t\tdef __init__(self, x, y):\n\t\t\tself.x = x\n\t\t\tself.y = y\n\n\t\tdef __getattr__(self, attr):\n\t\t\t\"\"\"\n\t\t\tReturns the attribute of the Point object.\n\t\t\t\"\"\"\n\t\t\treturn getattr(self, attr)\n\n\t\tdef __getitem__(self, item):\n\t\t\t\"\"\"\n\t\t\tReturns the attribute of the Point object by index.\n\t\t\t\"\"\"\n\t\t\tif item == 0:\n\t\t\t\treturn self.x\n\t\t\telif item == 1:\n\t\t\t\treturn self.y\n\t\t\telse:\n\t\t\t\traise IndexError\n\n\t\tdef __repr__(self):\n\t\t\t\"\"\"\n\t\t\tReturns a string representation of the object.\n\t\t\t\"\"\"\n\t\t\treturn 'POINT({0} {1})'.format(self.x, self.y)\n\n\t\tdef __str__(self):\n\t\t\t\"\"\"\n\t\t\tReturns a string representation of the object.\n\t\t\t\"\"\"\n\t\t\treturn 'POINT({0} {1})'.format(self.x, self.y)\n\n\t\tdef __eq__(self, other):\n\t\t\t\"\"\"\n\t\t\tReturns a boolean indicating if the two objects are equal.\n\t\t\t\"\"\"\n\t\t\tif isinstance(other, Point):\n\t\t\t\treturn self.x == other.x and self.y == other.y\n\t\t\telse:\n\t\t\t\treturn False\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tsrid = srid_map[name]\n\tfields = ','.join(fields)\n\tpoint_class_name = name.title()\n\tpoint_class_string = \"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\tclass Point(object):\n\t\tdef __init__(self, *args):\n\t\t\tif len(args) == 1 and isinstance(args[0], dict):\n\t\t\t\tself.set_dict(args[0])\n\t\t\telse:\n\t\t\t\tself.set_args(args)\n\n\t\tdef set_args(self, args):\n\t\t\tfor index, value in enumerate(args):\n\t\t\t\tsetattr(self, fields[index], value)\n\n\t\tdef set_dict(self, dict):\n\t\t\tfor key, value in dict.items():\n\t\t\t\tsetattr(self, key, value)\n\n\t\tdef to_dict(self):\n\t\t\tdict = {}\n\t\t\tfor field in fields:\n\t\t\t\tdict[field] = getattr(self, field)\n\t\t\treturn dict\n\n\t\tdef __repr__(self):\n\t\t\treturn \"Point: \" + str(self.to_dict())\n\n\treturn Point\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\t# fields = [field.name for field in model._meta.get_fields()]\n\t# fields.remove('id')\n\t# fields.remove('geom')\n\t# fields.remove('srid')\n\t# fields.remove('geo_type')\n\t# fields.remove('geo_level')\n\n\t# if 'geom' in fields:\n\t# \tfields.remove('geom')\n\n\t# if'srid' in fields:\n\t# \tfields.remove('srid')\n\n\t# if 'geo_type' in fields:\n\t# \tfields.remove('geo_type')\n\n\t# if 'geo_level' in fields:\n\t# \tfields.remove('geo_level')\n\n\t# if 'id' in fields:\n\t# \tfields.remove('id')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:\n\t# \tfields.remove('name')\n\n\t# if 'name' in fields:", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\n\t# define Point class\n\tpoint_class = type(name, (Point,), dict(\n\t\t__slots__=tuple(fields),\n\t\t__new__=lambda cls, *args: super(Point, cls).__new__(cls, *(args + (tuple(),))),\n\t\t__init__=lambda self, *args: None,\n\t\t__repr__=lambda self: \"Point(%s)\" % \", \".join(str(getattr(self, f)) for f in fields)\n\t))\n\n\t# add fields and methods\n\tfor field in fields:\n\t\tpoint_class.__slots__ = point_class.__slots__ + (field,)\n\t\tpoint_class.__new__.__code__ = point_class.__new__.__code__.replace(\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(fields).encode('ascii')),\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(point_class.__slots__).encode('ascii'))\n\t\t)\n\t\tpoint_class.__init__.__code__ = point_class.__init__.__code__.replace(\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(fields).encode('ascii')),\n\t\t\tcodecs.encode(b'co_varnames=(%s)' % \", \".join(point_class", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\t# This function is copied from the python docs:\n\t\t# https://docs.python.org/3/library/exceptions.html#DeprecationWarning\n\t\t#\n\t\t# This can be used as a decorator:\n\t\t# @deprecated(message)\n\t\t# def my_function(args):\n\t\t#  ...\n\t\t#\n\t\t# or as a function:\n\t\t# deprecated(message)(my_function)\n\t\tdef decorator(func):\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\t\tprint(message)\n\t\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\t\treturn wrapper\n\t\treturn decorator\n\n# Create a class that will be instantiated as the main script", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\n\t\t@functools.wraps(func)\n\t\tdef replacement_function(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treplacement_function.__name__ = func.__name__\n\t\treplacement_function.__doc__ = func.__doc__\n\n\t\treturn replacement_function\n\n\treturn decorator\n\n\n# @deprecated('This function will be deprecated in the future. Use the new_function instead.')\n# def old_function():\n# \t\"\"\"\n# \tThis is the old function that should be deprecated.\n# \t\"\"\"\n# \tpass\n\n# new_function()\n\n# ---------------------------------------------\n\n# @deprecated('This function will be deprecated in the future. Use the new_function instead.')\n# def old_function():\n# \t\"\"\"\n# \tThis is the old function that should be deprecated.\n# \t\"\"\"\n# \tpass\n\n# new_function()\n\n# ---------------------------------------------\n\n# @deprecated('This function will be deprecated in the future. Use the new_function instead.')\n# def old_function():\n# \t\"\"\"\n# \tThis is the old function that should be deprecated.\n# \t\"\"\"\n# \tpass\n\n# new_function()\n\n# ---------------------------------------------\n\n# @deprecated('This", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper for deprecated function\n\t\t\t\"\"\"\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.__name__ = func.__name__\n\t\treturn wrapper\n\treturn decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\twarnings.warn(message, category=DeprecationWarning)\n\t\t\t\treturn func(*args, **kwargs)\n\n\t\t\treturn wrapper\n\n\t\treturn decorator\n\n\tdef _wrap_error_message(func, error_message):\n\t\t\"\"\"\n\t\tWraps an error message around a function.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\traise type(e)(error_message)\n\n\t\treturn wrapper\n\n\tdef _wrap_error_message_with_args(func, error_message, *args, **kwargs):\n\t\t\"\"\"\n\t\tWraps an error message around a function.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\traise type(e)(error_message % args)\n\n\t\treturn wrapper\n\n\tdef _wrap_error_message_with_args_and_kwargs(func, error_message, args, kwargs):\n\t\t\"\"\"\n\t\tWraps an error message around a function.\n\t\t\"\"\"\n\t", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\tdef new_func(*args, **kwargs):\n\t\t\t\t\t\tprint(message)\n\t\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\t\tnew_func.__name__ = func.__name__\n\t\t\t\tnew_func.__doc__ = func.__doc__\n\t\t\t\tnew_func.__dict__.update(func.__dict__)\n\t\t\t\treturn new_func\n\t\treturn decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\n\treturn decorator\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\timport warnings\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n@deprecated('Use `nox.sessions` instead')", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\treturn func\n\treturn deprecated\nfrom.core import *\nfrom.utils import *\n# D3-Plots\n\nThis repo is a collection of tools to create a variety of D3 plots\n\n## Installation\n\n```console\n$ git clone https://github.com/d3-python/d3plots.git\n$ cd d3-plots\n$ pip install -r requirements.txt\n```\n\n## Usage\n\n```python\nimport d3plots\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\nd3plots.scatter_plot(df, x='x', y='y',\n    output='scatter.html')\n```\n\n## License\n\nMIT\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\twarnings.simplefilter(\"always\", DeprecationWarning)\n\t\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\t\twarnings.simplefilter(\"default\", DeprecationWarning)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn decorator\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\t# This function is copied from the python docs:\n\t\t# https://docs.python.org/3/library/exceptions.html#DeprecationWarning\n\t\t#\n\t\t# This can be used as a decorator:\n\t\t# @deprecated(message)\n\t\t# def my_function(args):\n\t\t#  ...\n\t\t#\n\t\t# or as a function:\n\t\t# deprecated(message)(my_function)\n\t\tdef decorator(func):\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\t\tprint(message)\n\t\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\t\treturn wrapper\n\t\treturn decorator\n\n# Create a class that will be instantiated as the main script", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\n\t\t@functools.wraps(func)\n\t\tdef replacement_function(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treplacement_function.__name__ = func.__name__\n\t\treplacement_function.__doc__ = func.__doc__\n\n\t\treturn replacement_function\n\n\treturn decorator\n\n\n# @deprecated('This function will be deprecated in the future. Use the new_function instead.')\n# def old_function():\n# \t\"\"\"\n# \tThis is the old function that should be deprecated.\n# \t\"\"\"\n# \tpass\n\n# new_function()\n\n# ---------------------------------------------\n\n# @deprecated('This function will be deprecated in the future. Use the new_function instead.')\n# def old_function():\n# \t\"\"\"\n# \tThis is the old function that should be deprecated.\n# \t\"\"\"\n# \tpass\n\n# new_function()\n\n# ---------------------------------------------\n\n# @deprecated('This function will be deprecated in the future. Use the new_function instead.')\n# def old_function():\n# \t\"\"\"\n# \tThis is the old function that should be deprecated.\n# \t\"\"\"\n# \tpass\n\n# new_function()\n\n# ---------------------------------------------\n\n# @deprecated('This", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper for deprecated function\n\t\t\t\"\"\"\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.__name__ = func.__name__\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\twarnings.warn(message, category=DeprecationWarning)\n\t\t\t\treturn func(*args, **kwargs)\n\n\t\t\treturn wrapper\n\n\t\treturn decorator\n\n\tdef _wrap_error_message(func, error_message):\n\t\t\"\"\"\n\t\tWraps an error message around a function.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\traise type(e)(error_message)\n\n\t\treturn wrapper\n\n\tdef _wrap_error_message_with_args(func, error_message, *args, **kwargs):\n\t\t\"\"\"\n\t\tWraps an error message around a function.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\texcept Exception as e:\n\t\t\t\traise type(e)(error_message % args)\n\n\t\treturn wrapper\n\n\tdef _wrap_error_message_with_args_and_kwargs(func, error_message, args, kwargs):\n\t\t\"\"\"\n\t\tWraps an error message around a function.\n\t\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\tdef new_func(*args, **kwargs):\n\t\t\t\t\t\tprint(message)\n\t\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\t\tnew_func.__name__ = func.__name__\n\t\t\t\tnew_func.__doc__ = func.__doc__\n\t\t\t\tnew_func.__dict__.update(func.__dict__)\n\t\t\t\treturn new_func\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\timport warnings\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n@deprecated('Use `nox.sessions` instead')", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\treturn func\n\treturn deprecated\nfrom.core import *\nfrom.utils import *\n# D3-Plots\n\nThis repo is a collection of tools to create a variety of D3 plots\n\n## Installation\n\n```console\n$ git clone https://github.com/d3-python/d3plots.git\n$ cd d3-plots\n$ pip install -r requirements.txt\n```\n\n## Usage\n\n```python\nimport d3plots\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\nd3plots.scatter_plot(df, x='x', y='y',\n    output='scatter.html')\n```\n\n## License\n\nMIT\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\twarnings.simplefilter(\"always\", DeprecationWarning)\n\t\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\t\twarnings.simplefilter(\"default\", DeprecationWarning)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn \"\"\"\\\nlibrary(methods)\noptions(stringsAsFactors=FALSE)\noptions(useFancyQuotes=FALSE)\n\"\"\" + code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\t# TODO: add a way to change this via env var\n\treturn (\n\t\tf\"options(repos = c(CRAN = \\\"{cran_repo_url}\\\")); \"\n\t\t+ code\n\t)\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tcode = 'Sys.setenv(RSTUDIO_PANDOC = \"C:/Program Files/RStudio/bin/pandoc\")'\n\t\tcode += 'options(htmltools.dir.version = FALSE, pandoc.path = \"C:/Program Files/RStudio/bin/pandoc\")'\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn \"\"\"\nlibrary(knitr)\nopts_chunk$set(fig.width=10, fig.height=10)\n\"\"\"\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t # set R to not use X11\n\treturn \"options(bitmapType='cairo', device='pdf'); \" + code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\n\treturn code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t \n\tcode = code.replace(\"\\n\", \"\")\n\tcode = code.replace(\"\\r\", \"\")\n\tcode = code.replace(\"\\t\", \"\")\n\treturn code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t # R_MAX_VSIZE is the maximum size of the R process\n\tr_max_vsize = 128 * 1024 * 1024\n\treturn f'options(shiny.maxRequestSize={r_max_vsize}); {code}'\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn f\"\"\"\noptions(stringsAsFactors=FALSE)\noptions(repos=c(CRAN=\"https://cloud.r-project.org\"))\noptions(r_setup_code={{\"{\"}}code{{\"}\"}})\n\"\"\"\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn \"\"\"\\\nlibrary(methods)\noptions(stringsAsFactors=FALSE)\noptions(useFancyQuotes=FALSE)\n\"\"\" + code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\t# TODO: add a way to change this via env var\n\treturn (\n\t\tf\"options(repos = c(CRAN = \\\"{cran_repo_url}\\\")); \"\n\t\t+ code\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tcode = 'Sys.setenv(RSTUDIO_PANDOC = \"C:/Program Files/RStudio/bin/pandoc\")'\n\t\tcode += 'options(htmltools.dir.version = FALSE, pandoc.path = \"C:/Program Files/RStudio/bin/pandoc\")'\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn \"\"\"\nlibrary(knitr)\nopts_chunk$set(fig.width=10, fig.height=10)\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t # set R to not use X11\n\treturn \"options(bitmapType='cairo', device='pdf'); \" + code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\n\treturn code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t \n\tcode = code.replace(\"\\n\", \"\")\n\tcode = code.replace(\"\\r\", \"\")\n\tcode = code.replace(\"\\t\", \"\")\n\treturn code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t # R_MAX_VSIZE is the maximum size of the R process\n\tr_max_vsize = 128 * 1024 * 1024\n\treturn f'options(shiny.maxRequestSize={r_max_vsize}); {code}'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn f\"\"\"\noptions(stringsAsFactors=FALSE)\noptions(repos=c(CRAN=\"https://cloud.r-project.org\"))\noptions(r_setup_code={{\"{\"}}code{{\"}\"}})\n\"\"\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\t) -> Iterator[str]:\n    \"\"\"\n    Simplified Implementation of Xargs in Linux\n    \"\"\"\n    args = [cmd]\n    for arg in varargs:\n        if len(args[-1]) + len(arg) + 1 > _max_length:\n            yield from exec_cmd(args, color, target_concurrency, **kwargs)\n            args = [cmd]\n        args[-1] += f\" {arg}\"\n    yield from exec_cmd(args, color, target_concurrency, **kwargs)\n\n", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\tprint(\n\t\t\tcmd,\n\t\t\tfile=sys.stderr,\n\t\t\tflush=True,\n\t\t)\n\t\tprint(\n\t\t\tvarargs,\n\t\t\tfile=sys.stderr,\n\t\t\tflush=True,\n\t\t)\n\t\tif color:\n\t\t\tcmd = [\n\t\t\t\tc for c in cmd if c!= \"\"\n\t\t\t]\n\t\t\tcmd = [\n\t\t\t\tcolored(c, \"yellow\")\n\t\t\t\tfor c in cmd\n\t\t\t]\n\t\t\tcmd = [\n\t\t\t\tcolored(c, \"red\")\n\t\t\t\tfor c in varargs\n\t\t\t]\n\t\tcmd = \" \".join(\n\t\t\tcmd\n\t\t)\n\t\tcmd = cmd.split()\n\t\tvarargs = [\n\t\t\tv.split()\n\t\t\tfor v in varargs\n\t\t]\n\t\tvarargs = [\n\t\t\tv\n\t\t\tfor v in varargs\n\t\t]\n\t\tvarargs = \" \".join(\n\t\t\tvarargs\n\t\t)\n\t\tcmd = cmd + varargs.split()\n\t\tcmd = [\n\t\t\tc\n\t\t\tfor c in cmd\n\t\t\tif c!= \"\"\n\t\t]\n\t\tcmd = [\n\t\t\tc\n\t\t\tfor c in cmd\n\t\t\tif c not in [\n\t\t\t\t\"--\",\n\t\t\t]\n\t\t]\n\t\tif target_concurrency > 1:\n\t\t\tcmd = [\n\t\t\t\t\"xargs\",\n\t\t\t\t\"-n\",\n\t\t\t\tstr(target_concurrency),", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if target_concurrency > 1:\n            return xtask_multi(cmd, varargs, color, target_concurrency, _max_length, **kwargs)\n        else:\n            return xtask_single(cmd, varargs, color, _max_length, **kwargs)\n/xtask/xtask_multi.py\nimport os\nimport subprocess\nimport sys\nimport time\nfrom typing import Sequence\nfrom xtask.xtask_single import xtask_single\nfrom xtask.xtask_single import print_green\nfrom xtask.xtask_single import print_red\n\n", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\t) -> None:\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n        if not cmd:\n            raise ValueError(\"Argument 'cmd' cannot be empty\")\n        elif not isinstance(cmd, tuple):\n            raise TypeError(\"Argument 'cmd' must be tuple\")\n        elif not varargs:\n            raise ValueError(\"Argument 'varargs' cannot be empty\")\n        elif not isinstance(varargs, Sequence):\n            raise TypeError(\"Argument 'varargs' must be Sequence\")\n        elif target_concurrency < 1:\n            raise ValueError(\n                \"Argument 'target_concurrency' must be greater than 0\"\n            )\n        elif not isinstance(color, bool):\n            raise TypeError(\"Argument 'color' must be bool\")\n        elif not isinstance(_max_length, int):\n            raise TypeError(\"Argument '_max_length' must be int\")\n        elif not isinstance(kwargs, dict):\n            raise TypeError(\"Argument 'kwargs' must be dict\")\n        self.cmd = cmd\n        self.varargs = varargs\n        self.target_concurrency = target_concurrency\n        self.color = color\n        self.kwargs = kwargs\n        self.args: list[str] = []\n        self._max_length = _max_length\n        self._process: Optional[\"_XargsProcess\"] = None\n        self._pid: Optional[int] = None\n        self._queue: \"Queue\" = Queue()\n        self", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\tprocess: subprocess.Popen[bytes] = subprocess.Popen(\n\t\t\tcmd, bufsize=0, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs\n\t\t)\n\t\tstdout, stderr = process.communicate()\n\t\tif process.returncode:\n\t\t\traise subprocess.CalledProcessError(process.returncode, cmd)\n\t\tstdout, stderr = stdout.decode(), stderr.decode()\n\t\tstdout, stderr = stdout.splitlines(), stderr.splitlines()\n\t\tstdout, stderr = (\n\t\t\tlist(stdout)[::-1],\n\t\t\tlist(stderr)[::-1],\n\t\t)\n\t\ttry:\n\t\t\tstdout = [\n\t\t\t\twrap_line(line, color=color, _max_length=_max_length)\n\t\t\t\tfor line in stdout\n\t\t\t]\n\t\t\tstderr = [\n\t\t\t\twrap_line(line, color=color, _max_length=_max_length)\n\t\t\t\tfor line in stderr\n\t\t\t]\n\t\texcept UnicodeEncodeError:\n\t\t\t__tracebackhide__ = True\n\t\t\tprint(\"ERROR: Encoding Error while wrapping line. This is a bug.\")\n\t\t\tprint(\n\t\t\t\tf\"ERROR: Please report this issue at https://github.com/jart/prettycli/issues\"\n\t\t\t)\n\t\t\tprint(\"ERROR: with the following information:\")\n\t\t\tprint(f\"ERROR: Command: {' '.join(cmd)}\")\n\t\t", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if target_concurrency > 1:\n            cmd = [\"xargs\", \"-P\", str(target_concurrency), \"-n\", \"1\", \"-I{}\"] + list(cmd)\n\n        cmd = [str(c) for c in cmd]\n\n        if varargs:\n            cmd += list(varargs)\n\n        process = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            **kwargs,\n        )\n\n        def _run_in_thread(\n            input_iter: Iterable[str],\n            output_queue: queue.Queue[str],\n            error_queue: queue.Queue[str],\n        ):\n            for item in input_iter:\n                output, error = process.communicate(input=item.encode(\"utf-8\"))\n                if color:\n                    output = _colorize(output.decode(\"utf-8\"))\n                output_queue.put(output)\n                if error:\n                    error_queue.put(error.decode(\"utf-8\"))\n\n        output_queue: queue.Queue[str] = queue.Queue()\n        error_queue: queue.Queue[str] = queue.Queue()\n\n        threads: list[threading.Thread] = []\n        for i in range(target_concurrency):\n            thread = threading.Thread(\n                target=_run_in_thread,\n                args=(iter(varargs), output_queue", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if _max_length is not None:\n            varargs = list(varargs)\n            while len(varargs) > _max_length:\n                varargs.pop()\n            varargs = tuple(varargs)\n\n        cmd = (shlex.quote(c) for c in cmd)\n        command = (\n            \"xargs\",\n            \"--max-procs\",\n            str(target_concurrency),\n            \"--\",\n            *cmd,\n        )\n        command = shlex.split(\" \".join(command))\n        command.extend(shlex.quote(v) for v in varargs)\n        env = dict(os.environ)\n        env.update(kwargs)\n        try:\n            return run(command, env=env, color=color)\n        except CalledProcessError:\n            if target_concurrency > 1:\n                # Try again with only one subprocess\n                return xargs(cmd, varargs, target_concurrency=1, **kwargs)\n            else:\n                raise\n\n", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\t_ = kwargs\n\t\tassert target_concurrency > 0\n\t\tif len(varargs) == 0:\n\t\t\treturn\n\t\tassert varargs[0] == cmd[0]\n\t\tassert varargs[0] in _get_platform_xargs_builtins()\n\t\tif varargs[0] == 'xargs':\n\t\t\tcmd = varargs[:1] + cmd[1:]\n\t\t\tvarargs = varargs[1:]\n\t\tif len(cmd) == 1:\n\t\t\tcmd = shlex.split(cmd[0])\n\t\tif len(varargs) == 1:\n\t\t\tvarargs = shlex.split(varargs[0])\n\t\tif len(cmd) == 0:\n\t\t\tcmd = [varargs[0]]\n\t\twhile len(varargs) > 1 and varargs[0] == '':\n\t\t\tvarargs = varargs[1:]\n\t\tif len(varargs) == 0:\n\t\t\treturn\n\t\tif varargs[0] == '' and len(varargs) > 1:\n\t\t\tvarargs = varargs[1:]\n\t\tmax_length = len(varargs[0])\n\t\tif max_length > _max_length:\n\t\t\tmax_length = _max_length\n\t\tif max_length <= 0:\n\t\t\treturn\n\t\tif type(cmd) == str", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if not varargs:\n            raise ValueError(\"varargs is an empty list\")\n        if len(varargs) == 1:\n            return cmd[0], varargs[0]\n        if len(cmd) == 1:\n            return cmd[0], varargs\n        # 1. Wrap with shell\n        if len(cmd) > 1:\n            cmd = cmd[0], f\"{{{', '.join(cmd[1:])}}}\"\n        cmd = _wrap_with_shell(cmd, target_concurrency, color=color)\n        # 2. Escape varargs\n        varargs = [_escape_arg(arg) for arg in varargs]\n        # 3. Join with \" \"\n        varargs = \" \".join(varargs)\n        # 4. Split with max length\n        varargs = _split_with_max_length(varargs, _max_length)\n        return cmd, varargs\n\n    def _get_platform_max_length() -> int:\n        return os.sysconf(\"PC_PATH_MAX\")\n\n    def _wrap_with_shell(\n        cmd: tuple[str,...],\n        target_concurrency: int,\n        *,\n        color: bool = False,\n    ) -> str:\n        if color:\n            cmd = _color_on_cmd(cmd)\n        if target_concurrency == 1:\n            return cmd[0]\n        return f\"x", "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if not varargs:\n            raise ValueError(\"Xargs requires at least one argument.\")\n        if len(cmd) < 1:\n            raise ValueError(\"Xargs requires at least one argument.\")\n\n        if isinstance(cmd, str):\n            cmd = shlex.split(cmd)\n\n        if not cmd:\n            raise ValueError(\"Xargs requires at least one argument.\")\n\n        if not isinstance(varargs, Sequence):\n            varargs = [varargs]\n\n        if not isinstance(cmd, Sequence):\n            cmd = [cmd]\n\n        if len(varargs) > 1:\n            raise ValueError(\"Xargs requires at most one argument.\")\n\n        if len(cmd) > 1:\n            raise ValueError(\"Xargs requires at most one argument.\")\n\n        def _args_to_string(args: Sequence[str]) -> str:\n            if len(args) > 1:\n                return \" \".join(map(shlex.quote, args))\n            return args[0]\n\n        def _args_to_args(args: Sequence[str]) -> list[str]:\n            return [_args_to_string(args)]\n\n        def _run_subprocess(\n            args: Sequence[str],\n            target_concurrency: int,\n            max_length: int,\n            color: bool,\n            **kwargs,\n        ) -> None:\n            # Run subprocess\n            try:\n                subprocess.run(\n                    args,\n                    shell=True,\n                    stdout=subprocess."], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\t) -> Iterator[str]:\n    \"\"\"\n    Simplified Implementation of Xargs in Linux\n    \"\"\"\n    args = [cmd]\n    for arg in varargs:\n        if len(args[-1]) + len(arg) + 1 > _max_length:\n            yield from exec_cmd(args, color, target_concurrency, **kwargs)\n            args = [cmd]\n        args[-1] += f\" {arg}\"\n    yield from exec_cmd(args, color, target_concurrency, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\tprint(\n\t\t\tcmd,\n\t\t\tfile=sys.stderr,\n\t\t\tflush=True,\n\t\t)\n\t\tprint(\n\t\t\tvarargs,\n\t\t\tfile=sys.stderr,\n\t\t\tflush=True,\n\t\t)\n\t\tif color:\n\t\t\tcmd = [\n\t\t\t\tc for c in cmd if c!= \"\"\n\t\t\t]\n\t\t\tcmd = [\n\t\t\t\tcolored(c, \"yellow\")\n\t\t\t\tfor c in cmd\n\t\t\t]\n\t\t\tcmd = [\n\t\t\t\tcolored(c, \"red\")\n\t\t\t\tfor c in varargs\n\t\t\t]\n\t\tcmd = \" \".join(\n\t\t\tcmd\n\t\t)\n\t\tcmd = cmd.split()\n\t\tvarargs = [\n\t\t\tv.split()\n\t\t\tfor v in varargs\n\t\t]\n\t\tvarargs = [\n\t\t\tv\n\t\t\tfor v in varargs\n\t\t]\n\t\tvarargs = \" \".join(\n\t\t\tvarargs\n\t\t)\n\t\tcmd = cmd + varargs.split()\n\t\tcmd = [\n\t\t\tc\n\t\t\tfor c in cmd\n\t\t\tif c!= \"\"\n\t\t]\n\t\tcmd = [\n\t\t\tc\n\t\t\tfor c in cmd\n\t\t\tif c not in [\n\t\t\t\t\"--\",\n\t\t\t]\n\t\t]\n\t\tif target_concurrency > 1:\n\t\t\tcmd = [\n\t\t\t\t\"xargs\",\n\t\t\t\t\"-n\",\n\t\t\t\tstr(target_concurrency),", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if target_concurrency > 1:\n            return xtask_multi(cmd, varargs, color, target_concurrency, _max_length, **kwargs)\n        else:\n            return xtask_single(cmd, varargs, color, _max_length, **kwargs)\n/xtask/xtask_multi.py\nimport os\nimport subprocess\nimport sys\nimport time\nfrom typing import Sequence\nfrom xtask.xtask_single import xtask_single\nfrom xtask.xtask_single import print_green\nfrom xtask.xtask_single import print_red\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\t) -> None:\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n        if not cmd:\n            raise ValueError(\"Argument 'cmd' cannot be empty\")\n        elif not isinstance(cmd, tuple):\n            raise TypeError(\"Argument 'cmd' must be tuple\")\n        elif not varargs:\n            raise ValueError(\"Argument 'varargs' cannot be empty\")\n        elif not isinstance(varargs, Sequence):\n            raise TypeError(\"Argument 'varargs' must be Sequence\")\n        elif target_concurrency < 1:\n            raise ValueError(\n                \"Argument 'target_concurrency' must be greater than 0\"\n            )\n        elif not isinstance(color, bool):\n            raise TypeError(\"Argument 'color' must be bool\")\n        elif not isinstance(_max_length, int):\n            raise TypeError(\"Argument '_max_length' must be int\")\n        elif not isinstance(kwargs, dict):\n            raise TypeError(\"Argument 'kwargs' must be dict\")\n        self.cmd = cmd\n        self.varargs = varargs\n        self.target_concurrency = target_concurrency\n        self.color = color\n        self.kwargs = kwargs\n        self.args: list[str] = []\n        self._max_length = _max_length\n        self._process: Optional[\"_XargsProcess\"] = None\n        self._pid: Optional[int] = None\n        self._queue: \"Queue\" = Queue()\n        self", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\tprocess: subprocess.Popen[bytes] = subprocess.Popen(\n\t\t\tcmd, bufsize=0, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs\n\t\t)\n\t\tstdout, stderr = process.communicate()\n\t\tif process.returncode:\n\t\t\traise subprocess.CalledProcessError(process.returncode, cmd)\n\t\tstdout, stderr = stdout.decode(), stderr.decode()\n\t\tstdout, stderr = stdout.splitlines(), stderr.splitlines()\n\t\tstdout, stderr = (\n\t\t\tlist(stdout)[::-1],\n\t\t\tlist(stderr)[::-1],\n\t\t)\n\t\ttry:\n\t\t\tstdout = [\n\t\t\t\twrap_line(line, color=color, _max_length=_max_length)\n\t\t\t\tfor line in stdout\n\t\t\t]\n\t\t\tstderr = [\n\t\t\t\twrap_line(line, color=color, _max_length=_max_length)\n\t\t\t\tfor line in stderr\n\t\t\t]\n\t\texcept UnicodeEncodeError:\n\t\t\t__tracebackhide__ = True\n\t\t\tprint(\"ERROR: Encoding Error while wrapping line. This is a bug.\")\n\t\t\tprint(\n\t\t\t\tf\"ERROR: Please report this issue at https://github.com/jart/prettycli/issues\"\n\t\t\t)\n\t\t\tprint(\"ERROR: with the following information:\")\n\t\t\tprint(f\"ERROR: Command: {' '.join(cmd)}\")\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if target_concurrency > 1:\n            cmd = [\"xargs\", \"-P\", str(target_concurrency), \"-n\", \"1\", \"-I{}\"] + list(cmd)\n\n        cmd = [str(c) for c in cmd]\n\n        if varargs:\n            cmd += list(varargs)\n\n        process = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            **kwargs,\n        )\n\n        def _run_in_thread(\n            input_iter: Iterable[str],\n            output_queue: queue.Queue[str],\n            error_queue: queue.Queue[str],\n        ):\n            for item in input_iter:\n                output, error = process.communicate(input=item.encode(\"utf-8\"))\n                if color:\n                    output = _colorize(output.decode(\"utf-8\"))\n                output_queue.put(output)\n                if error:\n                    error_queue.put(error.decode(\"utf-8\"))\n\n        output_queue: queue.Queue[str] = queue.Queue()\n        error_queue: queue.Queue[str] = queue.Queue()\n\n        threads: list[threading.Thread] = []\n        for i in range(target_concurrency):\n            thread = threading.Thread(\n                target=_run_in_thread,\n                args=(iter(varargs), output_queue", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if _max_length is not None:\n            varargs = list(varargs)\n            while len(varargs) > _max_length:\n                varargs.pop()\n            varargs = tuple(varargs)\n\n        cmd = (shlex.quote(c) for c in cmd)\n        command = (\n            \"xargs\",\n            \"--max-procs\",\n            str(target_concurrency),\n            \"--\",\n            *cmd,\n        )\n        command = shlex.split(\" \".join(command))\n        command.extend(shlex.quote(v) for v in varargs)\n        env = dict(os.environ)\n        env.update(kwargs)\n        try:\n            return run(command, env=env, color=color)\n        except CalledProcessError:\n            if target_concurrency > 1:\n                # Try again with only one subprocess\n                return xargs(cmd, varargs, target_concurrency=1, **kwargs)\n            else:\n                raise\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\t_ = kwargs\n\t\tassert target_concurrency > 0\n\t\tif len(varargs) == 0:\n\t\t\treturn\n\t\tassert varargs[0] == cmd[0]\n\t\tassert varargs[0] in _get_platform_xargs_builtins()\n\t\tif varargs[0] == 'xargs':\n\t\t\tcmd = varargs[:1] + cmd[1:]\n\t\t\tvarargs = varargs[1:]\n\t\tif len(cmd) == 1:\n\t\t\tcmd = shlex.split(cmd[0])\n\t\tif len(varargs) == 1:\n\t\t\tvarargs = shlex.split(varargs[0])\n\t\tif len(cmd) == 0:\n\t\t\tcmd = [varargs[0]]\n\t\twhile len(varargs) > 1 and varargs[0] == '':\n\t\t\tvarargs = varargs[1:]\n\t\tif len(varargs) == 0:\n\t\t\treturn\n\t\tif varargs[0] == '' and len(varargs) > 1:\n\t\t\tvarargs = varargs[1:]\n\t\tmax_length = len(varargs[0])\n\t\tif max_length > _max_length:\n\t\t\tmax_length = _max_length\n\t\tif max_length <= 0:\n\t\t\treturn\n\t\tif type(cmd) == str", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if not varargs:\n            raise ValueError(\"varargs is an empty list\")\n        if len(varargs) == 1:\n            return cmd[0], varargs[0]\n        if len(cmd) == 1:\n            return cmd[0], varargs\n        # 1. Wrap with shell\n        if len(cmd) > 1:\n            cmd = cmd[0], f\"{{{', '.join(cmd[1:])}}}\"\n        cmd = _wrap_with_shell(cmd, target_concurrency, color=color)\n        # 2. Escape varargs\n        varargs = [_escape_arg(arg) for arg in varargs]\n        # 3. Join with \" \"\n        varargs = \" \".join(varargs)\n        # 4. Split with max length\n        varargs = _split_with_max_length(varargs, _max_length)\n        return cmd, varargs\n\n    def _get_platform_max_length() -> int:\n        return os.sysconf(\"PC_PATH_MAX\")\n\n    def _wrap_with_shell(\n        cmd: tuple[str,...],\n        target_concurrency: int,\n        *,\n        color: bool = False,\n    ) -> str:\n        if color:\n            cmd = _color_on_cmd(cmd)\n        if target_concurrency == 1:\n            return cmd[0]\n        return f\"x", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str,...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\t\n        if not varargs:\n            raise ValueError(\"Xargs requires at least one argument.\")\n        if len(cmd) < 1:\n            raise ValueError(\"Xargs requires at least one argument.\")\n\n        if isinstance(cmd, str):\n            cmd = shlex.split(cmd)\n\n        if not cmd:\n            raise ValueError(\"Xargs requires at least one argument.\")\n\n        if not isinstance(varargs, Sequence):\n            varargs = [varargs]\n\n        if not isinstance(cmd, Sequence):\n            cmd = [cmd]\n\n        if len(varargs) > 1:\n            raise ValueError(\"Xargs requires at most one argument.\")\n\n        if len(cmd) > 1:\n            raise ValueError(\"Xargs requires at most one argument.\")\n\n        def _args_to_string(args: Sequence[str]) -> str:\n            if len(args) > 1:\n                return \" \".join(map(shlex.quote, args))\n            return args[0]\n\n        def _args_to_args(args: Sequence[str]) -> list[str]:\n            return [_args_to_string(args)]\n\n        def _run_subprocess(\n            args: Sequence[str],\n            target_concurrency: int,\n            max_length: int,\n            color: bool,\n            **kwargs,\n        ) -> None:\n            # Run subprocess\n            try:\n                subprocess.run(\n                    args,\n                    shell=True,\n                    stdout=subprocess.", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t # we'll use a fixed seed (for reproducibility)\n\trandom.seed(FIX_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n# create the dataset", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t \n\tif FIXED_RANDOM_SEED:\n\t\tseq = seq.copy()\n\t\trandom.seed(FIXED_RANDOM_SEED)\n\t\trandom.shuffle(seq)\n\treturn seq\n \n# ---------------------------------------------------------------------\n#   __main__\n# ---------------------------------------------------------------------", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t \n\trng = np.random.default_rng(FIXED_RANDOM_SEED)\n\trng.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\tshuffled = copy.copy(seq)\n\trandom.shuffle(shuffled)\n\treturn shuffled\n\n@pytest.fixture", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).sample(list(seq), len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\trandom.seed(FIXED_RANDOM_SEED)\n\t\treturn random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).shuffle(seq)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t # we'll use a fixed seed (for reproducibility)\n\trandom.seed(FIX_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n# create the dataset", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t \n\tif FIXED_RANDOM_SEED:\n\t\tseq = seq.copy()\n\t\trandom.seed(FIXED_RANDOM_SEED)\n\t\trandom.shuffle(seq)\n\treturn seq\n \n# ---------------------------------------------------------------------\n#   __main__\n# ---------------------------------------------------------------------", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t \n\trng = np.random.default_rng(FIXED_RANDOM_SEED)\n\trng.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\tshuffled = copy.copy(seq)\n\trandom.shuffle(shuffled)\n\treturn shuffled\n\n@pytest.fixture", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).sample(list(seq), len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\n\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\trandom.seed(FIXED_RANDOM_SEED)\n\t\treturn random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\t\treturn random.Random(FIXED_RANDOM_SEED).shuffle(seq)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t \n\t# Split string by dot\n\tparts = s.split('.')\n\n\t# Convert each part to an integer\n\t# and add it to the tuple\n\treturn tuple(map(int, parts))\n\n# Convert version string to a tuple\nversion = parse_version(\"1.2.3\")\n\n# Display version as a formatted string\nprint(f\"Version: {version[0]}.{version[1]}.{version[2]}\")\n\n# Display version as a one-line string\nprint(\"Version:\", \".\".join(map(str, version)))\n\n# Display version as a list of integers\nprint(\"Version:\", list(version))\n\n# Display version as a tuple of strings\nprint(f\"Version: {'.'.join(map(str, version))}\")\n\n# Display version as a list of strings\nprint(\"Version:\", [str(n) for n in version])\n\n# Display version as a tuple of strings\nprint(f\"Version: {' '.join(map(str, version))}\")\n\n# Display version as a list of strings\nprint(\"Version:\", list(map(str, version)))\n\n# Display version as a tuple of strings\nprint(f\"Version: {' '.join(map(str, version))}\")\n\n# Display version as a list of strings\nprint(\"Version:\", [str(n) for n in version", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n# the following function is a stub function.\n# you need to implement it.", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(int(x) for x in s.split(\".\"))\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(map(int, s.split('.')))\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(int(x) for x in s.split('.'))\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n\n\treturn tuple(int(x) for x in s.split(\".\"))\n\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t0, *nums = map(int, s.split('.'))\n\treturn nums\n\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t \n\treturn tuple(map(int, s.split(\".\")))\n\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(map(int, s.split(\".\")))\n", "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t # Split string by dot\n\tnumbers = s.split('.')\n\t# Check if the length of numbers is 3\n\tif len(numbers)!= 3:\n\t\traise Exception('Invalid version string')\n\t# Convert each number to integer\n\treturn tuple(map(int, numbers))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t \n\t# Split string by dot\n\tparts = s.split('.')\n\n\t# Convert each part to an integer\n\t# and add it to the tuple\n\treturn tuple(map(int, parts))\n\n# Convert version string to a tuple\nversion = parse_version(\"1.2.3\")\n\n# Display version as a formatted string\nprint(f\"Version: {version[0]}.{version[1]}.{version[2]}\")\n\n# Display version as a one-line string\nprint(\"Version:\", \".\".join(map(str, version)))\n\n# Display version as a list of integers\nprint(\"Version:\", list(version))\n\n# Display version as a tuple of strings\nprint(f\"Version: {'.'.join(map(str, version))}\")\n\n# Display version as a list of strings\nprint(\"Version:\", [str(n) for n in version])\n\n# Display version as a tuple of strings\nprint(f\"Version: {' '.join(map(str, version))}\")\n\n# Display version as a list of strings\nprint(\"Version:\", list(map(str, version)))\n\n# Display version as a tuple of strings\nprint(f\"Version: {' '.join(map(str, version))}\")\n\n# Display version as a list of strings\nprint(\"Version:\", [str(n) for n in version", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n# the following function is a stub function.\n# you need to implement it.", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(int(x) for x in s.split(\".\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(map(int, s.split('.')))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(int(x) for x in s.split('.'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n\n\treturn tuple(int(x) for x in s.split(\".\"))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t0, *nums = map(int, s.split('.'))\n\treturn nums\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t \n\treturn tuple(map(int, s.split(\".\")))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\treturn tuple(map(int, s.split(\".\")))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t # Split string by dot\n\tnumbers = s.split('.')\n\t# Check if the length of numbers is 3\n\tif len(numbers)!= 3:\n\t\traise Exception('Invalid version string')\n\t# Convert each number to integer\n\treturn tuple(map(int, numbers))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\tif cmd[0] == 'node.exe':\n\t\t\tcmd = tuple(cmd[1:])\n\t\treturn cmd\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\treturn tuple(os.path.normpath(os.path.abspath(path)) for path in cmd)\n/tests/test_run_command.py\nimport unittest\nimport unittest.mock as mock\n\nimport run_command\n\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\t# get the exe path\n\texe = cmd[0]\n\n\t\t# if the exe path is an absolute path or a relative path that starts with./\n\tif os.path.isabs(exe) or exe.startswith('./'):\n\n\t\t# return the original path\n\t\treturn cmd\n\n\t\t# if the exe path is a relative path that doesn't start with./\n\telif not exe.startswith('.'):\n\n\t\t# search for the exe path in the PATH environment variable\n\t\tpaths = [path for path in os.environ['PATH'].split(os.pathsep)]\n\n\t\t# for each path in the PATH environment variable\n\t\tfor path in paths:\n\n\t\t\t# construct the path to the exe\n\t\t\tpath_to_exe = os.path.join(path, exe)\n\n\t\t\t# if the path to the exe exists\n\t\t\tif os.path.exists(path_to_exe):\n\n\t\t\t\t# return the path to the exe\n\t\t\t\treturn (path_to_exe, *cmd[1:])\n\n\t\t# return the original path\n\t\treturn cmd\n\n\t# return the original path\n\treturn cmd\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\treturn tuple([x if not x.endswith(\".exe\") else x[:-4] for x in cmd])\n\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\t# remove full path exe\n\tcmd = tuple([item.split(os.sep)[-1] for item in cmd])\n\treturn cmd\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\treturn tuple(\n\t\t\tos.path.normpath(os.path.expanduser(os.path.expandvars(p)))\n\t\t\tfor p in cmd\n\t\t)\n\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\t# TODO: Find a way to remove this when we have a more generic tool to\n\t\t# normalize paths.\n\tif sys.platform == 'win32':\n\t\treturn (cmd[0],) + tuple(os.path.normcase(p) for p in cmd[1:])\n\telse:\n\t\treturn cmd\n\t\t", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\n\n\texe = cmd[0]\n\tif os.path.isfile(exe):\n\t\treturn cmd\n\tif os.path.isdir(exe):\n\t\texe = os.path.join(exe, 'bin', 'cmd.exe')\n\tif os.path.isfile(exe):\n\t\treturn (exe,) + cmd[1:]\n\traise Exception(f'Could not find {exe}')\n\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\n\n\tcmd_list = list(cmd)\n\texe_name = cmd_list[0]\n\texe_path = Path(exe_name)\n\tif exe_path.is_absolute():\n\t\treturn cmd\n\telse:\n\t\treturn (Path(get_executable()).parent / exe_path) + tuple(cmd_list[1:])\n\n", "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\tif cmd[0] == 'node.exe':\n\t\t\tcmd = tuple(cmd[1:])\n\t\treturn cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\treturn tuple(os.path.normpath(os.path.abspath(path)) for path in cmd)\n/tests/test_run_command.py\nimport unittest\nimport unittest.mock as mock\n\nimport run_command\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\t# get the exe path\n\texe = cmd[0]\n\n\t\t# if the exe path is an absolute path or a relative path that starts with./\n\tif os.path.isabs(exe) or exe.startswith('./'):\n\n\t\t# return the original path\n\t\treturn cmd\n\n\t\t# if the exe path is a relative path that doesn't start with./\n\telif not exe.startswith('.'):\n\n\t\t# search for the exe path in the PATH environment variable\n\t\tpaths = [path for path in os.environ['PATH'].split(os.pathsep)]\n\n\t\t# for each path in the PATH environment variable\n\t\tfor path in paths:\n\n\t\t\t# construct the path to the exe\n\t\t\tpath_to_exe = os.path.join(path, exe)\n\n\t\t\t# if the path to the exe exists\n\t\t\tif os.path.exists(path_to_exe):\n\n\t\t\t\t# return the path to the exe\n\t\t\t\treturn (path_to_exe, *cmd[1:])\n\n\t\t# return the original path\n\t\treturn cmd\n\n\t# return the original path\n\treturn cmd\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\treturn tuple([x if not x.endswith(\".exe\") else x[:-4] for x in cmd])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\t# remove full path exe\n\tcmd = tuple([item.split(os.sep)[-1] for item in cmd])\n\treturn cmd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\treturn tuple(\n\t\t\tos.path.normpath(os.path.expanduser(os.path.expandvars(p)))\n\t\t\tfor p in cmd\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\t# TODO: Find a way to remove this when we have a more generic tool to\n\t\t# normalize paths.\n\tif sys.platform == 'win32':\n\t\treturn (cmd[0],) + tuple(os.path.normcase(p) for p in cmd[1:])\n\telse:\n\t\treturn cmd\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\n\n\texe = cmd[0]\n\tif os.path.isfile(exe):\n\t\treturn cmd\n\tif os.path.isdir(exe):\n\t\texe = os.path.join(exe, 'bin', 'cmd.exe')\n\tif os.path.isfile(exe):\n\t\treturn (exe,) + cmd[1:]\n\traise Exception(f'Could not find {exe}')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t\n\n\tcmd_list = list(cmd)\n\texe_name = cmd_list[0]\n\texe_path = Path(exe_name)\n\tif exe_path.is_absolute():\n\t\treturn cmd\n\telse:\n\t\treturn (Path(get_executable()).parent / exe_path) + tuple(cmd_list[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tres = cache.get(key)\n\t\t\t\tif res is None:\n\t\t\t\t\tres = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, res)\n\t\t\texcept Exception, e:\n\t\t\t\traise e\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn res\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(function):\n\t\tdef wrapper(*args, **kw):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tkey_ = key(*args, **kw)\n\t\t\t\tif key_ in cache:\n\t\t\t\t\treturn cache[key_]\n\t\t\t\tcache[key_] = function(*args, **kw)\n\t\t\t\treturn cache[key_]\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tCaches the results of the function\n\t\t\"\"\"\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function\n\t\t\t\"\"\"\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\tif key is None:\n\t\t\t\tkey = hashkey(*args, **kwargs)\n\t\t\tif key in cache:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\tdef inner(f):\n\t\t\n\t\tdef _f(*args, **kwargs):\n\t\t\targs = tuple(args)\n\t\t\tkwargs = tuple(kwargs)\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tif key in cache:\n\t\t\t\t\tval = cache[key]\n\t\t\t\t\tif not isinstance(val, Deferred):\n\t\t\t\t\t\treturn val\n\t\t\t\t\telse:\n\t\t\t\t\t\tdef _g():\n\t\t\t\t\t\t\tval = f(*args, **kwargs)\n\t\t\t\t\t\t\tcache[key] = val\n\t\t\t\t\t\t\treturn val\n\t\t\t\t\t\treturn _g()\n\t\t\t\telse:\n\t\t\t\t\tval = f(*args, **kwargs)\n\t\t\t\t\tcache[key] = val\n\t\t\t\treturn val\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\n\t\treturn _f\n\n\treturn inner\n/py/src/main/java/com/github/shyiko/commons/io/Files.java\npackage com.github.shyiko.commons.io;\n\nimport java.io.File;\nimport java.net.URI;\nimport java.net.URL;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.List;\n\n/**\n * This class provides a set of useful methods for working with {@link File}s.\n *", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\t#def cached(func):\n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tReturns the cached result if available\n\t\t\"\"\"\n\t\t#if lock:\n\t\t#\tlock.acquire()\n\t\t#\ttry:\n\t\t#\t\tif cache.has_key(key(args, kwargs)):\n\t\t#\t\t\treturn cache[key(args, kwargs)]\n\t\t#\texcept KeyError:\n\t\t#\t\tpass\n\n\t\t#\tresult = func(*args, **kwargs)\n\t\t#\tcache[key(args, kwargs)] = result\n\t\t#\treturn result\n\t\t#else:\n\t\tif cache.has_key(key(args, kwargs)):\n\t\t\t\treturn cache[key(args, kwargs)]\n\t\tresult = func(*args, **kwargs)\n\t\tcache[key(args, kwargs)] = result\n\t\treturn result\n\treturn wrapper\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\tdef decorator(f):\n\t\t\"\"\"\n\t\tThe decorator function\n\t\t\"\"\"\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe wrapper function\n\t\t\t\"\"\"\n\t\t\tif not lock:\n\t\t\t\tlock = get_lock(key)\n\t\t\twith lock:\n\t\t\t\tif key in cache:\n\t\t\t\t\treturn cache[key]\n\t\t\t\telse:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\treturn wrapper\n\n\treturn decorator/main.py\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom flask_restful import Api, Resource\nfrom flask_caching import Cache\nfrom flask_mongoengine import MongoEngine\nfrom flask_jwt_extended import JWTManager\n\nfrom datetime import timedelta\nfrom flask_bcrypt import Bcrypt\n\nimport os\n\nfrom.errors import *\nfrom.utils import *\nfrom.settings import *\n\n\napp = Flask(__name__)\napp.config['MONGODB_SETTINGS'] = MONGODB\napp.config['JWT_ACCESS_TOKEN_EXPIRES'] = timedelta(minutes=30)\napp.config['JWT_SECRET_KEY'] = os.urandom(24)\napp.config['MONGODB_SETTINGS']['connect'] = False\nCORS(app)\n\n# Mongoengine", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(func, *args, **kwargs):\n\t\t\"\"\"\n\t\tThe wrapper function\n\t\t\"\"\"\n\t\t# The key is the function name and its args and kwargs\n\t\tif lock is not None:\n\t\t\tlock.acquire()\n\t\tkey = key(func, *args, **kwargs)\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\texcept KeyError:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\tfinally:\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\t\treturn result\n\treturn decorator\n\n# The following is an example of how to use the cache decorator\n\n# First we need to create a cache\ncache = {}\n\n# We can then create a decorator that stores the results in the cache\n@cached(cache)", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t#get the key to use\n\t\t\tif callable(key):\n\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t#look up the result in the cache\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\tresult = cache.get(key)\n\t\t\telse:\n\t\t\t\tresult = cache.get(key)\n\t\t\t#if we found it in the cache, return the result\n\t\t\tif result:\n\t\t\t\treturn result\n\t\t\t#otherwise, run the function and store the result in the cache\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tcache.set(key, result)\n\t\t\t\telse:\n\t\t\t\t\tcache.set(key, result)\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tres = cache.get(key)\n\t\t\t\tif res is None:\n\t\t\t\t\tres = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key, res)\n\t\t\texcept Exception, e:\n\t\t\t\traise e\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn res\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(function):\n\t\tdef wrapper(*args, **kw):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tkey_ = key(*args, **kw)\n\t\t\t\tif key_ in cache:\n\t\t\t\t\treturn cache[key_]\n\t\t\t\tcache[key_] = function(*args, **kw)\n\t\t\t\treturn cache[key_]\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tCaches the results of the function\n\t\t\"\"\"\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper function\n\t\t\t\"\"\"\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\tif key is None:\n\t\t\t\tkey = hashkey(*args, **kwargs)\n\t\t\tif key in cache:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\tdef inner(f):\n\t\t\n\t\tdef _f(*args, **kwargs):\n\t\t\targs = tuple(args)\n\t\t\tkwargs = tuple(kwargs)\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tif key in cache:\n\t\t\t\t\tval = cache[key]\n\t\t\t\t\tif not isinstance(val, Deferred):\n\t\t\t\t\t\treturn val\n\t\t\t\t\telse:\n\t\t\t\t\t\tdef _g():\n\t\t\t\t\t\t\tval = f(*args, **kwargs)\n\t\t\t\t\t\t\tcache[key] = val\n\t\t\t\t\t\t\treturn val\n\t\t\t\t\t\treturn _g()\n\t\t\t\telse:\n\t\t\t\t\tval = f(*args, **kwargs)\n\t\t\t\t\tcache[key] = val\n\t\t\t\treturn val\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\n\t\treturn _f\n\n\treturn inner\n/py/src/main/java/com/github/shyiko/commons/io/Files.java\npackage com.github.shyiko.commons.io;\n\nimport java.io.File;\nimport java.net.URI;\nimport java.net.URL;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.List;\n\n/**\n * This class provides a set of useful methods for working with {@link File}s.\n *", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\t#def cached(func):\n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tReturns the cached result if available\n\t\t\"\"\"\n\t\t#if lock:\n\t\t#\tlock.acquire()\n\t\t#\ttry:\n\t\t#\t\tif cache.has_key(key(args, kwargs)):\n\t\t#\t\t\treturn cache[key(args, kwargs)]\n\t\t#\texcept KeyError:\n\t\t#\t\tpass\n\n\t\t#\tresult = func(*args, **kwargs)\n\t\t#\tcache[key(args, kwargs)] = result\n\t\t#\treturn result\n\t\t#else:\n\t\tif cache.has_key(key(args, kwargs)):\n\t\t\t\treturn cache[key(args, kwargs)]\n\t\tresult = func(*args, **kwargs)\n\t\tcache[key(args, kwargs)] = result\n\t\treturn result\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t\n\n\tdef decorator(f):\n\t\t\"\"\"\n\t\tThe decorator function\n\t\t\"\"\"\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe wrapper function\n\t\t\t\"\"\"\n\t\t\tif not lock:\n\t\t\t\tlock = get_lock(key)\n\t\t\twith lock:\n\t\t\t\tif key in cache:\n\t\t\t\t\treturn cache[key]\n\t\t\t\telse:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\treturn wrapper\n\n\treturn decorator/main.py\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom flask_restful import Api, Resource\nfrom flask_caching import Cache\nfrom flask_mongoengine import MongoEngine\nfrom flask_jwt_extended import JWTManager\n\nfrom datetime import timedelta\nfrom flask_bcrypt import Bcrypt\n\nimport os\n\nfrom.errors import *\nfrom.utils import *\nfrom.settings import *\n\n\napp = Flask(__name__)\napp.config['MONGODB_SETTINGS'] = MONGODB\napp.config['JWT_ACCESS_TOKEN_EXPIRES'] = timedelta(minutes=30)\napp.config['JWT_SECRET_KEY'] = os.urandom(24)\napp.config['MONGODB_SETTINGS']['connect'] = False\nCORS(app)\n\n# Mongoengine", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(func, *args, **kwargs):\n\t\t\"\"\"\n\t\tThe wrapper function\n\t\t\"\"\"\n\t\t# The key is the function name and its args and kwargs\n\t\tif lock is not None:\n\t\t\tlock.acquire()\n\t\tkey = key(func, *args, **kwargs)\n\t\ttry:\n\t\t\tresult = cache[key]\n\t\texcept KeyError:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\tfinally:\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\t\treturn result\n\treturn decorator\n\n# The following is an example of how to use the cache decorator\n\n# First we need to create a cache\ncache = {}\n\n# We can then create a decorator that stores the results in the cache\n@cached(cache)", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t#get the key to use\n\t\t\tif callable(key):\n\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t#look up the result in the cache\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\tresult = cache.get(key)\n\t\t\telse:\n\t\t\t\tresult = cache.get(key)\n\t\t\t#if we found it in the cache, return the result\n\t\t\tif result:\n\t\t\t\treturn result\n\t\t\t#otherwise, run the function and store the result in the cache\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tcache.set(key, result)\n\t\t\t\telse:\n\t\t\t\t\tcache.set(key, result)\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tcache = OrderedDict()\n\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tThe internal decorator function that is returned.\n\t\t\"\"\"\n\t\tdef helper(*args, **kw):\n\t\t\t\"\"\"\n\t\t\tThe helper function that is returned.\n\t\t\t\"\"\"\n\t\t\tkey = key = (f, args, kw)\n\t\t\ttry:\n\t\t\t\tif typed:\n\t\t\t\t\tkey += tuple(type(v) for v in args) + tuple(type(v) for v in kw.values())\n\t\t\t\telse:\n\t\t\t\t\tkey += tuple(args) + tuple(kw.items())\n\t\t\texcept TypeError:\n\t\t\t\tkey = None\n\t\t\tif key is not None:\n\t\t\t\ttry:\n\t\t\t\t\tvalue = cache[key]\n\t\t\t\t\tif timer() < value[1]:\n\t\t\t\t\t\treturn value[0]\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tkey = None\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\tif key is None:\n\t\t\t\t# uncacheable key\n\t\t\t\tvalue = f(*args, **kw)\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\t# clean up\n\t\t\t\t\twhile True:\n\t\t\t\t\t\tkey, (value, timestamp) = cache.popitem(last=False)\n\t\t\t\t\t\tif timestamp + ttl > timer():\n\t\t\t\t\t\t\tcache[key] = (value, timer())\n\t\t\t\t\t\t\tbreak\n\t\t\t\tcache[key] = (value, timer())\n\t\t\t\treturn value\n\t\t\treturn", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tcache = {}\n\t\tttlcache = {}\n\n\t\tdef decorator(f):\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif not typed:\n\t\t\t\t\tkey = args + tuple(sorted(kwargs.items()))\n\t\t\t\telse:\n\t\t\t\t\tkey = (f, args, tuple(sorted(kwargs.items())))\n\t\t\t\ttry:\n\t\t\t\t\tresult, ts = ttlcache[key]\n\t\t\t\t\tif timer() - ts < ttl:\n\t\t\t\t\t\treturn result\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tttlcache[key] = (result, timer())\n\t\t\t\tif len(ttlcache) > maxsize:\n\t\t\t\t\tclean_ttlcache()\n\t\t\t\treturn result\n\t\t\twrapper.cache_info = lambda: (len(cache), maxsize, len(ttlcache))\n\t\t\twrapper.cache_clear = lambda: (cache.clear(), ttlcache.clear())\n\t\t\treturn wrapper\n\n\t\tdef clean_ttlcache():\n\t\t\tcount = len(ttlcache)\n\t\t\tif not count:\n\t\t\t\treturn\n\t\t\tnow = timer()\n\t\t\twhile count > maxsize:\n\t\t\t\tfor k, (result, ts) in list(ttlcache.items()):\n\t\t\t\t\tif now - ts >= ttl:\n\t\t\t\t\t\tdel ttlcache[k]\n\t\t\t\t\t\tcount -= 1\n\t\t\t\tif not count:\n\t\t\t\t\tbreak\n\n\t\treturn decorator\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tdef decorate(func):\n\t\t\tcache = OrderedDict()\n\t\t\tif not hasattr(func, '_ttl_cache'):\n\t\t\t\tfunc._ttl_cache = {}\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tkey = str(args) + str(kwargs)\n\t\t\t\tif key in cache:\n\t\t\t\t\tif time.monotonic() < cache[key][1]:\n\t\t\t\t\t\treturn cache[key][0]\n\t\t\t\telse:\n\t\t\t\t\tcache[key] = (func(*args, **kwargs), timer() + ttl)\n\t\t\t\t\twhile len(cache) > maxsize:\n\t\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\treturn cache[key][0]\n\t\t\twrapper._ttl_cache[func] = (maxsize, ttl, timer, typed)\n\t\t\treturn wrapper\n\t\treturn decorate\n\n\tdef __repr__(self):\n\t\treturn '<%s object at %s>' % (self.__class__.__name__, hex(id(self)))\n\n\tdef __eq__(self, other):\n\t\treturn type(self) == type(other) and self.key == other.key\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __hash__(self):\n\t\treturn hash(self.key)\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef memoize(f):\n\t\tcache = {}\n\t\tif maxsize == 0:\n\t\t\treturn f\n\n\t\tdef helper(x):\n\t\t\tif x not in cache:\n\t\t\t\tcache[x] = (f(x), timer())\n\t\t\telif timer() - cache[x][1] > ttl:\n\t\t\t\t#print(f'Cache hit for {x} with TTL')\n\t\t\t\tcache[x] = (f(x), timer())\n\t\t\treturn cache[x][0]\n\n\t\treturn helper\n\n\treturn memoize\n\n@ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False)", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t \n\tdef on_full_cache(cache, key, value):\n\t\t# Remove the oldest item when the cache is full\n\t\tcache.popitem(last=False)\n\t \n\tdef on_miss_cache(cache, key):\n\t\t# Remove the item from the cache when it gets a miss\n\t\tcache.pop(key, None)\n\t \n\tdef on_set_cache(cache, key, value):\n\t\t# Set the TTL for the item in the cache\n\t\tcache.ttl[key] = ttl + timer()\n\t \n\tdef on_eviction_cache(cache, key, value):\n\t\t# Remove the item from the cache when it expires\n\t\tcache.pop(key, None)\n\t \n\tdef wrap(func):\n\t\tcache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer,\n\t\t\t\t\t\ttyped=typed,\n\t\t\t\t\t\ton_full=on_full_cache,\n\t\t\t\t\t\ton_miss=on_miss_cache,\n\t\t\t\t\t\ton_set=on_set_cache,\n\t\t\t\t\t\ton_eviction=on_eviction_cache)\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func, args, frozenset(kwargs.items()))\n\t\t\tvalue = cache.get(key)\n\t\t\tif value is None:\n\t\t\t\tvalue =", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\tdef decorate(f):\n\t\t\tdef awrap(*args, **kws):\n\t\t\t\treturn cache.get(args + tuple(kws.items()), lambda: f(*args, **kws))\n\t\t\treturn update_wrapper(awrap, f)\n\t\treturn decorate\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nall results in a dictionary. The dictionary is not bounded in size,\nso it grows without limit.\n\t\"\"\"\n\tdef decorate(f):\n\t\tcache = {}\n\t\tdef awrap(*args):\n\t\t\tkey = args\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tval = f(*args)\n\t\t\t\tcache[key] = val\n\t\t\t\treturn val\n\t\treturn update_wrapper(awrap, f)\n\treturn decorate\n\n@lru_cache(maxsize=100000)", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\n\tcache = dict()\n\n\tdef decorate(function):\n\t\t\"\"\"\n\t\tReturn a cached version of the given function.\n\t\t\"\"\"\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tReturn the cached value if it's available. Otherwise,\n\t\t\tcall the underlying function and cache the result for\n\t\t\tfuture calls.\n\t\t\t\"\"\"\n\t\t\tkey = args\n\t\t\tif kwargs:\n\t\t\t\tkey += tuple(sorted(kwargs.items()))\n\n\t\t\tif key in cache:\n\t\t\t\titem = cache[key]\n\t\t\t\tif timer() - item[1] < ttl:\n\t\t\t\t\treturn item[0]\n\t\t\t\telse:\n\t\t\t\t\tdel cache[key]\n\n\t\t\tresult = function(*args, **kwargs)\n\t\t\tcache[key] = (result, timer())\n\t\t\tif len(cache) > maxsize:\n\t\t\t\t# purge the least recently used cache item.\n\t\t\t\tcache.popitem(last=False)\n\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorate\n\n\n@ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False)", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t \n\tclass LRU(dict):\n\t\t'''\n\t\tLeast Recently Used (LRU) cache\n\t\t'''\n\t\tdef __init__(self, maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.ttl = ttl\n\t\t\tself.timer = timer\n\t\t\tself.typed = typed\n\t\t\tsuper().__init__()\n\t\n\t\tdef __getitem__(self, key):\n\t\t\ttry:\n\t\t\t\tvalue, timestamp = super().__getitem__(key)\n\t\t\texcept KeyError:\n\t\t\t\traise KeyError(key) from None\n\t\t\tself.move_to_end(key)\n\t\t\tif self.timer() > timestamp + self.ttl:\n\t\t\t\traise KeyError(key)\n\t\t\treturn value\n\t\n\t\tdef __setitem__(self, key, value):\n\t\t\tif key in self:\n\t\t\t\tself.move_to_end(key)\n\t\t\tsuper().__setitem__(key, (value, self.timer()))\n\t\t\tif len(self) > self.maxsize:\n\t\t\t\tself.popitem(last=False)\n\t\t\t\t\n\t\tdef __delitem__(self, key):\n\t\t\tsuper().__delitem__(key)\n\t\t\tif self.typed:\n\t\t\t\tdel self[key]\n\t\t\t\t\n\t\tdef clear(self):\n\t\t\tsuper().clear()\n\t\t\tif self.typed:\n\t\t\t\t", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tcache = OrderedDict()\n\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tThe internal decorator function that is returned.\n\t\t\"\"\"\n\t\tdef helper(*args, **kw):\n\t\t\t\"\"\"\n\t\t\tThe helper function that is returned.\n\t\t\t\"\"\"\n\t\t\tkey = key = (f, args, kw)\n\t\t\ttry:\n\t\t\t\tif typed:\n\t\t\t\t\tkey += tuple(type(v) for v in args) + tuple(type(v) for v in kw.values())\n\t\t\t\telse:\n\t\t\t\t\tkey += tuple(args) + tuple(kw.items())\n\t\t\texcept TypeError:\n\t\t\t\tkey = None\n\t\t\tif key is not None:\n\t\t\t\ttry:\n\t\t\t\t\tvalue = cache[key]\n\t\t\t\t\tif timer() < value[1]:\n\t\t\t\t\t\treturn value[0]\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tkey = None\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\tif key is None:\n\t\t\t\t# uncacheable key\n\t\t\t\tvalue = f(*args, **kw)\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\t# clean up\n\t\t\t\t\twhile True:\n\t\t\t\t\t\tkey, (value, timestamp) = cache.popitem(last=False)\n\t\t\t\t\t\tif timestamp + ttl > timer():\n\t\t\t\t\t\t\tcache[key] = (value, timer())\n\t\t\t\t\t\t\tbreak\n\t\t\t\tcache[key] = (value, timer())\n\t\t\t\treturn value\n\t\t\treturn", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tcache = {}\n\t\tttlcache = {}\n\n\t\tdef decorator(f):\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif not typed:\n\t\t\t\t\tkey = args + tuple(sorted(kwargs.items()))\n\t\t\t\telse:\n\t\t\t\t\tkey = (f, args, tuple(sorted(kwargs.items())))\n\t\t\t\ttry:\n\t\t\t\t\tresult, ts = ttlcache[key]\n\t\t\t\t\tif timer() - ts < ttl:\n\t\t\t\t\t\treturn result\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tttlcache[key] = (result, timer())\n\t\t\t\tif len(ttlcache) > maxsize:\n\t\t\t\t\tclean_ttlcache()\n\t\t\t\treturn result\n\t\t\twrapper.cache_info = lambda: (len(cache), maxsize, len(ttlcache))\n\t\t\twrapper.cache_clear = lambda: (cache.clear(), ttlcache.clear())\n\t\t\treturn wrapper\n\n\t\tdef clean_ttlcache():\n\t\t\tcount = len(ttlcache)\n\t\t\tif not count:\n\t\t\t\treturn\n\t\t\tnow = timer()\n\t\t\twhile count > maxsize:\n\t\t\t\tfor k, (result, ts) in list(ttlcache.items()):\n\t\t\t\t\tif now - ts >= ttl:\n\t\t\t\t\t\tdel ttlcache[k]\n\t\t\t\t\t\tcount -= 1\n\t\t\t\tif not count:\n\t\t\t\t\tbreak\n\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tdef decorate(func):\n\t\t\tcache = OrderedDict()\n\t\t\tif not hasattr(func, '_ttl_cache'):\n\t\t\t\tfunc._ttl_cache = {}\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tkey = str(args) + str(kwargs)\n\t\t\t\tif key in cache:\n\t\t\t\t\tif time.monotonic() < cache[key][1]:\n\t\t\t\t\t\treturn cache[key][0]\n\t\t\t\telse:\n\t\t\t\t\tcache[key] = (func(*args, **kwargs), timer() + ttl)\n\t\t\t\t\twhile len(cache) > maxsize:\n\t\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\treturn cache[key][0]\n\t\t\twrapper._ttl_cache[func] = (maxsize, ttl, timer, typed)\n\t\t\treturn wrapper\n\t\treturn decorate\n\n\tdef __repr__(self):\n\t\treturn '<%s object at %s>' % (self.__class__.__name__, hex(id(self)))\n\n\tdef __eq__(self, other):\n\t\treturn type(self) == type(other) and self.key == other.key\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __hash__(self):\n\t\treturn hash(self.key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef memoize(f):\n\t\tcache = {}\n\t\tif maxsize == 0:\n\t\t\treturn f\n\n\t\tdef helper(x):\n\t\t\tif x not in cache:\n\t\t\t\tcache[x] = (f(x), timer())\n\t\t\telif timer() - cache[x][1] > ttl:\n\t\t\t\t#print(f'Cache hit for {x} with TTL')\n\t\t\t\tcache[x] = (f(x), timer())\n\t\t\treturn cache[x][0]\n\n\t\treturn helper\n\n\treturn memoize\n\n@ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False)", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t \n\tdef on_full_cache(cache, key, value):\n\t\t# Remove the oldest item when the cache is full\n\t\tcache.popitem(last=False)\n\t \n\tdef on_miss_cache(cache, key):\n\t\t# Remove the item from the cache when it gets a miss\n\t\tcache.pop(key, None)\n\t \n\tdef on_set_cache(cache, key, value):\n\t\t# Set the TTL for the item in the cache\n\t\tcache.ttl[key] = ttl + timer()\n\t \n\tdef on_eviction_cache(cache, key, value):\n\t\t# Remove the item from the cache when it expires\n\t\tcache.pop(key, None)\n\t \n\tdef wrap(func):\n\t\tcache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer,\n\t\t\t\t\t\ttyped=typed,\n\t\t\t\t\t\ton_full=on_full_cache,\n\t\t\t\t\t\ton_miss=on_miss_cache,\n\t\t\t\t\t\ton_set=on_set_cache,\n\t\t\t\t\t\ton_eviction=on_eviction_cache)\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func, args, frozenset(kwargs.items()))\n\t\t\tvalue = cache.get(key)\n\t\t\tif value is None:\n\t\t\t\tvalue =", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\tdef decorate(f):\n\t\t\tdef awrap(*args, **kws):\n\t\t\t\treturn cache.get(args + tuple(kws.items()), lambda: f(*args, **kws))\n\t\t\treturn update_wrapper(awrap, f)\n\t\treturn decorate\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nall results in a dictionary. The dictionary is not bounded in size,\nso it grows without limit.\n\t\"\"\"\n\tdef decorate(f):\n\t\tcache = {}\n\t\tdef awrap(*args):\n\t\t\tkey = args\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tval = f(*args)\n\t\t\t\tcache[key] = val\n\t\t\t\treturn val\n\t\treturn update_wrapper(awrap, f)\n\treturn decorate\n\n@lru_cache(maxsize=100000)", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\n\tcache = dict()\n\n\tdef decorate(function):\n\t\t\"\"\"\n\t\tReturn a cached version of the given function.\n\t\t\"\"\"\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tReturn the cached value if it's available. Otherwise,\n\t\t\tcall the underlying function and cache the result for\n\t\t\tfuture calls.\n\t\t\t\"\"\"\n\t\t\tkey = args\n\t\t\tif kwargs:\n\t\t\t\tkey += tuple(sorted(kwargs.items()))\n\n\t\t\tif key in cache:\n\t\t\t\titem = cache[key]\n\t\t\t\tif timer() - item[1] < ttl:\n\t\t\t\t\treturn item[0]\n\t\t\t\telse:\n\t\t\t\t\tdel cache[key]\n\n\t\t\tresult = function(*args, **kwargs)\n\t\t\tcache[key] = (result, timer())\n\t\t\tif len(cache) > maxsize:\n\t\t\t\t# purge the least recently used cache item.\n\t\t\t\tcache.popitem(last=False)\n\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorate\n\n\n@ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False)", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t \n\tclass LRU(dict):\n\t\t'''\n\t\tLeast Recently Used (LRU) cache\n\t\t'''\n\t\tdef __init__(self, maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.ttl = ttl\n\t\t\tself.timer = timer\n\t\t\tself.typed = typed\n\t\t\tsuper().__init__()\n\t\n\t\tdef __getitem__(self, key):\n\t\t\ttry:\n\t\t\t\tvalue, timestamp = super().__getitem__(key)\n\t\t\texcept KeyError:\n\t\t\t\traise KeyError(key) from None\n\t\t\tself.move_to_end(key)\n\t\t\tif self.timer() > timestamp + self.ttl:\n\t\t\t\traise KeyError(key)\n\t\t\treturn value\n\t\n\t\tdef __setitem__(self, key, value):\n\t\t\tif key in self:\n\t\t\t\tself.move_to_end(key)\n\t\t\tsuper().__setitem__(key, (value, self.timer()))\n\t\t\tif len(self) > self.maxsize:\n\t\t\t\tself.popitem(last=False)\n\t\t\t\t\n\t\tdef __delitem__(self, key):\n\t\t\tsuper().__delitem__(key)\n\t\t\tif self.typed:\n\t\t\t\tdel self[key]\n\t\t\t\t\n\t\tdef clear(self):\n\t\t\tsuper().clear()\n\t\t\tif self.typed:\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tdef _decorator(user_func):\n\t \tclass MRUCache:\n\t \t\tdef __init__(self, maxsize, typed):\n\t \t\t\tself.maxsize = maxsize\n\t \t\t\tself.typed = typed\n\t \t\t\tself.cache = LRUCache(maxsize, typed)\n\t \t\tdef __call__(self, *args):\n\t \t\t\tkey = self._key(args)\n\t \t\t\tif key in self.cache:\n\t \t\t\t\treturn self.cache[key]\n\t \t\t\telse:\n\t \t\t\t\tvalue = user_func(*args)\n\t \t\t\t\tself.cache[key] = value\n\t \t\t\t\treturn value\n\t \t\n\t \tdef _key(self, args):\n\t \t\tif self.typed:\n\t \t\t\tkey = str(args)\n\t \t\t\treturn key\n\t \t\telse:\n\t \t\t\treturn args\n\t \t\n\t \treturn MRUCache(maxsize, typed)\n\n\treturn _decorator\n\n@mru_cache(maxsize=64, typed=False)", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\tdef decorating_function(user_function):\n\t\tcache = LRU(maxsize, typed)\n\n\t\tdef wrapper_function(*args, **kwargs):\n\t\treturn cache.get(args + tuple(kwargs.items()), lambda: user_function(*args, **kwargs))\n\n\t\treturn update_wrapper(wrapper_function, user_function)\n\t\treturn decorating_function\n\treturn mru_cache\n\n@mru_cache", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\tdef wrapper(user_function):\n\t\t\t\tcache = LRUCache(maxsize, typed)\n\t\t\t\tdef memoized_function(*args, **kwargs):\n\t\t\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\t\t\t\treturn cache.get(key, lambda: user_function(*args, **kwargs))\n\t\t\t\tmemoized_function.cache_info = cache.cache_info\n\t\t\t\tmemoized_function.cache_clear = cache.cache_clear\n\t\t\t\treturn memoized_function\n\t\treturn wrapper\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tcache = LRUCache(maxsize, typed)\n\t \n\tdef decorating_function(user_function):\n\t\t\"\"\"\n\t\tDecorator function that wraps a user function with a function that saves and fetches\n\t\tfrom the cache.\n\t\t\"\"\"\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# This is the function signature f(*args, **kwds)\n\t\t\t# It is important to have it as *args, **kwds to work with\n\t\t\t# user_function that accept a variable number of parameters.\n\t\t\t# The following line is the key to make the magic work.\n\t\t\tkey = args\n\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t# We use a tuple here because we can use it as a key to a\n\t\t\t# dictionary.\n\t\t\t# If we used a list, the order of the elements in the list\n\t\t\t# would be important, and we couldn't use lists as keys\n\t\t\t# in a dictionary.\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\t# We have not seen this key before. Compute the result,\n\t\t\t\t# save it in the cache and return it.\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function/server/", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tclass MRUCache(object):\n\t\t\"Memoize with MRU eviction\"\n\t\tdef __init__(self, func, maxsize=128):\n\t\t\tself.func = func\n\t\t\tself.maxsize = maxsize\n\t\t\tself.cache = collections.OrderedDict()\n\t\tdef __call__(self, *args):\n\t\t\t# Look up key, return value if found.\n\t\t\tkey = self._key(args)\n\t\t\tif key in self.cache:\n\t\t\t\tvalue = self.cache.pop(key)\n\t\t\t\tself.cache[key] = value  # reinsert last used value\n\t\t\t\treturn value\n\t\t\t# Key wasn't found, compute value and insert it.\n\t\t\tvalue = self.func(*args)\n\t\t\tif len(self.cache) >= self.maxsize:\n\t\t\t\tself.cache.popitem(last=False)\n\t\t\tself.cache[key] = value\n\t\t\treturn value\n\t\tdef _key(self, args):\n\t\t\t\"Construct a key from an arbitrary set of arguments.\"\n\t\t\tkey = args\n\t\t\tif self.typed:\n\t\t\t\tkey += tuple(type(arg) for arg in args)\n\t\t\treturn key\n\t\tdef __repr__(self):\n\t\t\treturn '<%s(%s)>' % (self.__class__.__name__, self.func.__name__)\n\treturn MRUCache(maxsize", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef mru_cache_decorator(user_func):\n\t\tcache = LRUCache(maxsize, typed)\n\n\t\t@functools.wraps(user_func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tCaches the result of the call to `user_func` with the\n\t\t\targuments `args` and `kwargs` as the key.\n\n\t\t\tIf `user_func` raises an exception, then it is not\n\t\t\tcached.\n\t\t\t\"\"\"\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\ttry:\n\t\t\t\tresult = cache.__getitem__(key)\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_func(*args, **kwargs)\n\t\t\t\tcache.__setitem__(key, result)\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn mru_cache_decorator\n\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tdef _lru_cache(func):\n\t\t# cache for this function\n\t\tcache = {}\n\t\t# cache for args to function\n\t\tcache_args = {}\n\t\t# cache for args to function\n\t\tcache_values = {}\n\t\t# cache for args to function\n\t\tcache_accesses = {}\n\t\t# cache for args to function\n\t\tcache_hits = {}\n\t\t# cache for args to function\n\t\tcache_misses = {}\n\t\t# number of maxsize\n\t\tcount = 0\n\t\t# number of hits\n\t\thits = 0\n\t\t# number of misses\n\t\tmisses = 0\n\t\t# number of accesses\n\t\taccesses = 0\n\t\t \n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# number of accesses\n\t\t\taccesses += 1\n\t\t\t# check in cache\n\t\t\tif args in cache_args and kwargs in cache_args[args]:\n\t\t\t\t# number of hits\n\t\t\t\thits += 1\n\t\t\t\t# cache hit\n\t\t\t\tvalue = cache_values[args][kwargs]\n\t\t\t\t# cache hit\n\t\t\t\tcache_accesses[args][kwargs] += 1\n\t\t\t\treturn value\n\t\t\telse:\n\t\t\t\t# number of misses\n\t\t\t\tmisses += 1\n\t\t\t\t# call function\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t# cache miss\n\t\t\t\tcache_accesses[args][kwargs]"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tdef _decorator(user_func):\n\t \tclass MRUCache:\n\t \t\tdef __init__(self, maxsize, typed):\n\t \t\t\tself.maxsize = maxsize\n\t \t\t\tself.typed = typed\n\t \t\t\tself.cache = LRUCache(maxsize, typed)\n\t \t\tdef __call__(self, *args):\n\t \t\t\tkey = self._key(args)\n\t \t\t\tif key in self.cache:\n\t \t\t\t\treturn self.cache[key]\n\t \t\t\telse:\n\t \t\t\t\tvalue = user_func(*args)\n\t \t\t\t\tself.cache[key] = value\n\t \t\t\t\treturn value\n\t \t\n\t \tdef _key(self, args):\n\t \t\tif self.typed:\n\t \t\t\tkey = str(args)\n\t \t\t\treturn key\n\t \t\telse:\n\t \t\t\treturn args\n\t \t\n\t \treturn MRUCache(maxsize, typed)\n\n\treturn _decorator\n\n@mru_cache(maxsize=64, typed=False)", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\tdef decorating_function(user_function):\n\t\tcache = LRU(maxsize, typed)\n\n\t\tdef wrapper_function(*args, **kwargs):\n\t\treturn cache.get(args + tuple(kwargs.items()), lambda: user_function(*args, **kwargs))\n\n\t\treturn update_wrapper(wrapper_function, user_function)\n\t\treturn decorating_function\n\treturn mru_cache\n\n@mru_cache", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\tdef wrapper(user_function):\n\t\t\t\tcache = LRUCache(maxsize, typed)\n\t\t\t\tdef memoized_function(*args, **kwargs):\n\t\t\t\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\t\t\t\treturn cache.get(key, lambda: user_function(*args, **kwargs))\n\t\t\t\tmemoized_function.cache_info = cache.cache_info\n\t\t\t\tmemoized_function.cache_clear = cache.cache_clear\n\t\t\t\treturn memoized_function\n\t\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tcache = LRUCache(maxsize, typed)\n\t \n\tdef decorating_function(user_function):\n\t\t\"\"\"\n\t\tDecorator function that wraps a user function with a function that saves and fetches\n\t\tfrom the cache.\n\t\t\"\"\"\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# This is the function signature f(*args, **kwds)\n\t\t\t# It is important to have it as *args, **kwds to work with\n\t\t\t# user_function that accept a variable number of parameters.\n\t\t\t# The following line is the key to make the magic work.\n\t\t\tkey = args\n\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t# We use a tuple here because we can use it as a key to a\n\t\t\t# dictionary.\n\t\t\t# If we used a list, the order of the elements in the list\n\t\t\t# would be important, and we couldn't use lists as keys\n\t\t\t# in a dictionary.\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\t# We have not seen this key before. Compute the result,\n\t\t\t\t# save it in the cache and return it.\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function/server/", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tclass MRUCache(object):\n\t\t\"Memoize with MRU eviction\"\n\t\tdef __init__(self, func, maxsize=128):\n\t\t\tself.func = func\n\t\t\tself.maxsize = maxsize\n\t\t\tself.cache = collections.OrderedDict()\n\t\tdef __call__(self, *args):\n\t\t\t# Look up key, return value if found.\n\t\t\tkey = self._key(args)\n\t\t\tif key in self.cache:\n\t\t\t\tvalue = self.cache.pop(key)\n\t\t\t\tself.cache[key] = value  # reinsert last used value\n\t\t\t\treturn value\n\t\t\t# Key wasn't found, compute value and insert it.\n\t\t\tvalue = self.func(*args)\n\t\t\tif len(self.cache) >= self.maxsize:\n\t\t\t\tself.cache.popitem(last=False)\n\t\t\tself.cache[key] = value\n\t\t\treturn value\n\t\tdef _key(self, args):\n\t\t\t\"Construct a key from an arbitrary set of arguments.\"\n\t\t\tkey = args\n\t\t\tif self.typed:\n\t\t\t\tkey += tuple(type(arg) for arg in args)\n\t\t\treturn key\n\t\tdef __repr__(self):\n\t\t\treturn '<%s(%s)>' % (self.__class__.__name__, self.func.__name__)\n\treturn MRUCache(maxsize", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef mru_cache_decorator(user_func):\n\t\tcache = LRUCache(maxsize, typed)\n\n\t\t@functools.wraps(user_func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tCaches the result of the call to `user_func` with the\n\t\t\targuments `args` and `kwargs` as the key.\n\n\t\t\tIf `user_func` raises an exception, then it is not\n\t\t\tcached.\n\t\t\t\"\"\"\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\ttry:\n\t\t\t\tresult = cache.__getitem__(key)\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_func(*args, **kwargs)\n\t\t\t\tcache.__setitem__(key, result)\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn mru_cache_decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t \n\tdef _lru_cache(func):\n\t\t# cache for this function\n\t\tcache = {}\n\t\t# cache for args to function\n\t\tcache_args = {}\n\t\t# cache for args to function\n\t\tcache_values = {}\n\t\t# cache for args to function\n\t\tcache_accesses = {}\n\t\t# cache for args to function\n\t\tcache_hits = {}\n\t\t# cache for args to function\n\t\tcache_misses = {}\n\t\t# number of maxsize\n\t\tcount = 0\n\t\t# number of hits\n\t\thits = 0\n\t\t# number of misses\n\t\tmisses = 0\n\t\t# number of accesses\n\t\taccesses = 0\n\t\t \n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# number of accesses\n\t\t\taccesses += 1\n\t\t\t# check in cache\n\t\t\tif args in cache_args and kwargs in cache_args[args]:\n\t\t\t\t# number of hits\n\t\t\t\thits += 1\n\t\t\t\t# cache hit\n\t\t\t\tvalue = cache_values[args][kwargs]\n\t\t\t\t# cache hit\n\t\t\t\tcache_accesses[args][kwargs] += 1\n\t\t\t\treturn value\n\t\t\telse:\n\t\t\t\t# number of misses\n\t\t\t\tmisses += 1\n\t\t\t\t# call function\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t# cache miss\n\t\t\t\tcache_accesses[args][kwargs]", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n```\n\n#### 3.1.3 \u7c7b\u88c5\u9970\u5668\n\n\u7c7b\u88c5\u9970\u5668\u901a\u5e38\u5728\u6a21\u5757\u7ea7\u522b\u5b9a\u4e49\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u7528\u5230`@classmethod`\u548c`@staticmethod`\u88c5\u9970\u5668\u6216`__call__`\u6765\u63a5\u6536\u7c7b\u672c\u8eab\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u6b64\u5904\u6211\u4eec\u7528\u524d\u8005\u6765\u5b9e\u73b0\uff1a\n\n```python", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t \n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tmiss = object()\n\t\t_make_key = make_key = _keymaker(user_function, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = _make_key(args, kwds, None)\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = cache[key] = miss\n\t\t\tif result is not miss:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tif key in cache:\n\t\t\t\tcache.move_to_end(key)\n\t\t\telse:\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef _lru_cache_wrapper(arg):\n\t\t\twrapper.cache_clear()\n\t\t\treturn wrapper(arg)\n\t\treturn _lru_cache_wrapper\n\treturn decorating_function\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef gensim_test():\n\tfrom gensim.test.utils import common_texts\n\tfrom gensim.models import Word2Vec\n\tmodel = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)\n\tmodel.save(\"word2vec.model\")\n\tprint(model.wv.most_similar(\"computer\"))\n\treturn\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef memoize(obj, *args, **kwargs):\n\t\t\tcache = obj._cache\n\t\t\tif cache is None:\n\t\t\t\tcache = obj._cache = LRUCache(maxsize, typed)\n\t\t\treturn cache.get(args + tuple(sorted(kwargs.items()))\n\t\treturn memoize\n\treturn memoize\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n```\n\n- 2.1.3. functools.lru_cache\n\n```python\n# functools.lru_cache(maxsize=128, typed=False)\n# \u88c5\u9970\u5668\uff0c\u7528\u6765\u5305\u88c5\u4e00\u4e2a\u53ef\u8c03\u7528\u5bf9\u8c61\uff0c\u8fd9\u4e2a\u53ef\u8c03\u7528\u5bf9\u8c61\u4fdd\u5b58\u4e86\u5bf9\u51fd\u6570\u7684\u8c03\u7528\u7ed3\u679c\uff0c\u53ea\u8981\u53c2\u6570\u76f8\u540c\uff0c\u8fd4\u56de\u7684\u7ed3\u679c\u5c31\u76f8\u540c\uff0c\u5982\u679c\u53c2\u6570\u4e0d\u540c\uff0c\u5c31\u8981\u91cd\u65b0\u8ba1\u7b97\u7ed3\u679c\uff0c\u4ece\u800c\u8282\u7701\u8ba1\u7b97\u65f6\u95f4\u3002\n# \u53c2\u6570\n# maxsize \u2013 \u4fdd\u5b58\u7ed3\u679c\u7684\u6570\u91cf\u3002\u9ed8\u8ba4\u503c\u4e3a128\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3aNone\uff0c\u8868\u793a\u4fdd\u5b58\u7ed3\u679c\u7684\u6570\u91cf\u65e0\u9650\u5236\u3002\n# typed \u2013 \u5982\u679c\u4e3aTrue\uff0c\u90a3\u4e48\u4e0d\u540c\u7c7b\u578b\u7684\u53c2\u6570\u4f1a\u88ab\u89c6\u4f5c\u4e0d\u540c\u8c03\u7528\u3002\u9ed8\u8ba4\u4e3aFalse\u3002\n\n# \u4f7f\u7528\u65b9\u6cd5\n# 1\u3001\u6700\u7b80\u5355\u7684\u4f7f\u7528\u65b9\u6cd5\nimport functools\n\n@functools.lru_cache(maxsize=None)", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\tdef decorating_function(user_function):\n\t\tcache = LRU(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\tresult = cache.get(args, kwds)\n\t\tif result is not None:\n\t\treturn result\n\t\tresult = user_function(*args, **kwds)\n\t\tcache[args, kwds] = result\n\t\treturn result\n\t\treturn wrapper\n\t\treturn decorating_function", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\tdef decorate(func):\n\t\tcache = OrderedDict()\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif typed:\n\t\t\t\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\t\telse:\n\t\t\t\t\t\tkey = args\n\t\t\t\tif key in cache:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\telse:\n\t\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\t\tcache[key] = value\n\t\t\t\t\t\treturn value\n\t\twrapper.cache_clear = cache.clear\n\t\twrapper.cache_info = cache.items\n\t\twrapper.cache_size = len\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorate\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n```\n\n#### 3.1.3 \u7c7b\u88c5\u9970\u5668\n\n\u7c7b\u88c5\u9970\u5668\u901a\u5e38\u5728\u6a21\u5757\u7ea7\u522b\u5b9a\u4e49\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u7528\u5230`@classmethod`\u548c`@staticmethod`\u88c5\u9970\u5668\u6216`__call__`\u6765\u63a5\u6536\u7c7b\u672c\u8eab\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u6b64\u5904\u6211\u4eec\u7528\u524d\u8005\u6765\u5b9e\u73b0\uff1a\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t \n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tmiss = object()\n\t\t_make_key = make_key = _keymaker(user_function, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = _make_key(args, kwds, None)\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = cache[key] = miss\n\t\t\tif result is not miss:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tif key in cache:\n\t\t\t\tcache.move_to_end(key)\n\t\t\telse:\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef _lru_cache_wrapper(arg):\n\t\t\twrapper.cache_clear()\n\t\t\treturn wrapper(arg)\n\t\treturn _lru_cache_wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tdef gensim_test():\n\tfrom gensim.test.utils import common_texts\n\tfrom gensim.models import Word2Vec\n\tmodel = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)\n\tmodel.save(\"word2vec.model\")\n\tprint(model.wv.most_similar(\"computer\"))\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef memoize(obj, *args, **kwargs):\n\t\t\tcache = obj._cache\n\t\t\tif cache is None:\n\t\t\t\tcache = obj._cache = LRUCache(maxsize, typed)\n\t\t\treturn cache.get(args + tuple(sorted(kwargs.items()))\n\t\treturn memoize\n\treturn memoize\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n```\n\n- 2.1.3. functools.lru_cache\n\n```python\n# functools.lru_cache(maxsize=128, typed=False)\n# \u88c5\u9970\u5668\uff0c\u7528\u6765\u5305\u88c5\u4e00\u4e2a\u53ef\u8c03\u7528\u5bf9\u8c61\uff0c\u8fd9\u4e2a\u53ef\u8c03\u7528\u5bf9\u8c61\u4fdd\u5b58\u4e86\u5bf9\u51fd\u6570\u7684\u8c03\u7528\u7ed3\u679c\uff0c\u53ea\u8981\u53c2\u6570\u76f8\u540c\uff0c\u8fd4\u56de\u7684\u7ed3\u679c\u5c31\u76f8\u540c\uff0c\u5982\u679c\u53c2\u6570\u4e0d\u540c\uff0c\u5c31\u8981\u91cd\u65b0\u8ba1\u7b97\u7ed3\u679c\uff0c\u4ece\u800c\u8282\u7701\u8ba1\u7b97\u65f6\u95f4\u3002\n# \u53c2\u6570\n# maxsize \u2013 \u4fdd\u5b58\u7ed3\u679c\u7684\u6570\u91cf\u3002\u9ed8\u8ba4\u503c\u4e3a128\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3aNone\uff0c\u8868\u793a\u4fdd\u5b58\u7ed3\u679c\u7684\u6570\u91cf\u65e0\u9650\u5236\u3002\n# typed \u2013 \u5982\u679c\u4e3aTrue\uff0c\u90a3\u4e48\u4e0d\u540c\u7c7b\u578b\u7684\u53c2\u6570\u4f1a\u88ab\u89c6\u4f5c\u4e0d\u540c\u8c03\u7528\u3002\u9ed8\u8ba4\u4e3aFalse\u3002\n\n# \u4f7f\u7528\u65b9\u6cd5\n# 1\u3001\u6700\u7b80\u5355\u7684\u4f7f\u7528\u65b9\u6cd5\nimport functools\n\n@functools.lru_cache(maxsize=None)", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\tdef decorating_function(user_function):\n\t\tcache = LRU(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\tresult = cache.get(args, kwds)\n\t\tif result is not None:\n\t\treturn result\n\t\tresult = user_function(*args, **kwds)\n\t\tcache[args, kwds] = result\n\t\treturn result\n\t\treturn wrapper\n\t\treturn decorating_function", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\tdef decorate(func):\n\t\tcache = OrderedDict()\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif typed:\n\t\t\t\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\t\telse:\n\t\t\t\t\t\tkey = args\n\t\t\t\tif key in cache:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\telse:\n\t\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\t\tcache[key] = value\n\t\t\t\t\t\treturn value\n\t\twrapper.cache_clear = cache.clear\n\t\twrapper.cache_info = cache.items\n\t\twrapper.cache_size = len\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorate\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n# \tdef decorator(func):\n# \t\tcache = func.__name__ = FuncCache(maxsize, typed)\n# \t\treturn cache.wrapper\n# \t\n# \treturn decorator\n\t\n# \tdef wrapper(self, *args, **kwargs):\n# \t\ttry:\n# \t\t\treturn self.cache[args, frozenset(sorted(kwargs.items()))]\n# \t\texcept KeyError:\n# \t\t\tvalue = self.cache[args, frozenset(sorted(kwargs.items()))] = self.func(*args, **kwargs)\n# \t\t\treturn value\n# \t\n# \tdef __get__(self, instance, owner):\n# \t\tif instance is None:\n# \t\t\treturn self\n# \t\tself.instance = instance\n# \t\treturn self\n# \t\n# \tdef __getattr__(self, name):\n# \t\tif name == \"func\":\n# \t\t\treturn getattr(self.func, name)\n# \t\tif name == \"cache\":\n# \t\t\treturn getattr(self.instance, '_FuncCache__cache')\n# \t\treturn getattr(self.func, name)\n# \t\n# \tdef __repr__(self):\n# \t\treturn '<FuncCache of %s>' % self.func.__name__\n# \t\n# \tdef __call", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tcache = OrderedDict()\n\t\tdef memoize(f, *args):\n\t\t\tif func.__name__=='get_all_users':\n\t\t\t\treturn f(*args)\n\t\t\tif not cache:\n\t\t\t\treturn f(*args)\n\t\t\tkey = (f.__name__, args)\n\t\t\tif key in cache:\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tvalue = f(*args)\n\t\t\t\tcache[key] = value\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\treturn value\n\t\treturn memoize\n\treturn decorator\n/utils/models.py\nfrom django.db import models\nfrom django.contrib.auth.models import User\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t/README.md\n# Python-Learning\n\n## 1. \u5217\u8868\n\n### 1.1 Python\u5217\u8868\n\n- \u5217\u8868\u662fPython\u4e2d\u4f7f\u7528\u6700\u9891\u7e41\u7684\u6570\u636e\u7ed3\u6784\n- \u5217\u8868\u662f\u4e00\u79cd\u6709\u5e8f\u7684\u96c6\u5408\uff0c\u53ef\u4ee5\u968f\u65f6\u6dfb\u52a0\u548c\u5220\u9664\u5176\u4e2d\u7684\u5143\u7d20\n- \u5217\u8868\u7528`[]`\u8868\u793a\uff0c\u5143\u7d20\u4e4b\u95f4\u7528`,`\u9694\u5f00\n- \u5143\u7d20\u53ef\u4ee5\u662f\u4efb\u610f\u6570\u636e\u7c7b\u578b\uff0c\u751a\u81f3\u53ef\u4ee5\u662f\u53e6\u4e00\u4e2a\u5217\u8868\n- \u5217\u8868\u7684\u957f\u5ea6\u603b\u662f\u56fa\u5b9a\u7684\uff0c\u4e0d\u80fd\u968f\u610f\u5220\u9664\u548c\u6dfb\u52a0\u5143\u7d20\n\n```python\nclassmates = ['Michael', 'Bob', 'Tracy']\nclassmates[0]\nclassmates[-1]\nclassmates.append('Adam')\nclassmates.insert(1, 'Jack')\nclassmates.pop()\nclassmates.pop(1)\nclassmates[1] = 'Sarah'\nclassmates\n```\n\n### 1.2 \u5143\u7ec4\n\n- \u53e6\u4e00\u79cd\u6709\u5e8f\u5217\u8868\u53eb\u5143\u7ec4\uff1atuple\u3002tuple\u548clist\u975e\u5e38\u7c7b\u4f3c\uff0c\u4f46\u662ftuple\u4e00\u65e6\u521d\u59cb\u5316\u5c31\u4e0d\u80fd\u4fee\u6539\n- \u5143\u7ec4\u4f7f\u7528\u5706\u62ec\u53f7()\u8868\u793a\uff0c\u5143\u7d20\u4e4b\u95f4\u7528`,`\u9694\u5f00\n- \u5143\u7ec4\u4e2d\u5143\u7d20\u4e0d\u53ef\u53d8\uff0c\u4f46\u662f\u5143\u7ec4\u5305\u542b\u7684\u5143\u7d20\u53ef\u4ee5\u662f\u5217\u8868\n\n```python\nclassmates = ('Michael', 'Bob', 'Tracy')\nt =", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef wrapper(func):\n\t\t\n\t\tcache = {}\n\t\tmaxsize = int(maxsize)\n\t\t# OrderedDict is used as a cache, with the least\n\t\t# recently used entry being the first to be popped\n\t\t# out when maxsize is exceeded.\n\t\tcache_order = collections.OrderedDict()\n\n\t\tdef memoize(*args, **kwargs):\n\t\t\tif kwargs:\n\t\t\t\tkey = str(args) + str(sorted(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = str(args)\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) == maxsize:\n\t\t\t\t\t# Cache is full, so pop the least recently used.\n\t\t\t\t\tcache.popitem(0)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tcache_order[key] = 1\n\t\t\telse:\n\t\t\t\tresult = cache[key]\n\t\t\t\tcache_order[key] += 1\n\t\t\treturn result\n\n\t\tdef clear_cache():\n\t\t\tcache.clear()\n\t\t\tcache_order.clear()\n\n\t\tdef get_cache():\n\t\t\treturn cache\n\n\t\tdef get_cache_order():\n\t\t\treturn cache_order\n\n\t\tmemoize.cache = cache\n\t\tmemoize.cache_order = cache_order\n\t\tmemoize.clear = clear_cache\n\t\tmemoize.get_cache =", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n# \tdef _decorator(f):\n# \t\tcache = {}\n# \t\tdef _lookup(key):\n# \t\t\treturn cache[key]\n# \t\tdef _memoize(key, value):\n# \t\t\tif len(cache) >= maxsize:\n# \t\t\t\t# LFU algorithm\n# \t\t\t\tpass\n# \t\t\tcache[key] = value\n# \t\tdef _f(*args, **kwargs):\n# \t\t\tkey = (f, args, frozenset(kwargs.items()))\n# \t\t\ttry:\n# \t\t\t\treturn _lookup(key)\n# \t\t\texcept KeyError:\n# \t\t\t\tvalue = f(*args, **kwargs)\n# \t\t\t\t_memoize(key, value)\n# \t\t\t\treturn value\n# \t\treturn _f\n# \treturn _decorator\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\tcache = {}\n\t\tdef decorator(f):\n\t\t\tdef g(*args):\n\t\t\t\tkey = args\n\t\t\t\tif len(key) == 1 and not typed:\n\t\t\t\t\tkey = key[0]\n\t\t\t\t\t\tif key not in cache:\n\t\t\t\t\t\t\tcache[key] = f(*args)\n\t\t\t\t\t\t\t\treturn cache[key]\n\treturn g\n\n@memoize", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\tcache = LFUCache(maxsize, typed=typed)\n\t\tdef decorate(f):\n\t\t\t\t@wraps(f)\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\t\tresult = cache.get(args + tuple(kwargs.items()))\n\t\t\t\t\t\tif result is not None:\n\t\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\t\tcache[args + tuple(kwargs.items())] = result\n\t\t\t\t\t\treturn result\n\t\t\t\treturn wrapper\n\t\treturn decorate\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\tcache = lfu_cache.cache = {}\n\t\tdef decorating_function(user_function):\n\t\t\t\"\"\"\n\t\t\tDecorator.\n\t\t\t\t`user_function`\n\t\t\t\t\tThe function to wrap.\n\t\t\t\t`maxsize`\n\t\t\t\t\tThe maximum number of results to save.\n\t\t\t\t`typed`\n\t\t\t\t\tCache `user_function` with distinct argument\n\t\t\t\t\tsignatures.\n\t\t\t\"\"\"\n\t\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tThe wrapper function.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\t\tkey = args\n\t\t\t\t\t\tif kwds:\n\t\t\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\t\t\tif typed:\n\t\t\t\t\t\t\tkey += user_function.func_code.co_filename,\n\t\t\t\t\t\t\tuser_function.func_code.co_firstlineno\n\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\treturn result\n\t\t\t\t\treturn update_wrapper(wrapper, user_function)\n\t\t\t\treturn update_wrapper(decorating_function, user_function)\n\t\t\treturn decorator_maker(4, update_wrapper)\n\t\treturn lfu_cache\n\treturn lfu_cache\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\t# The function to be decorated.\n\tdef decorate(func):\n\t\t\t# Keep a cache of up to maxsize results:\n\t\t\tcache = {}\n\t\t\t\t# Remember the number of times each output is generated:\n\t\t\t\tcount = collections.defaultdict(int)\n\t\t\t\t\t# The list of keys sorted by their use frequency:\n\t\t\t\t\torder = []\n\t\t\t\t\t\t# When wrapped is called, return the cached value if\n\t\t\t\t\t\t# possible:\n\t\t\t\t\t\tdef wrapper(*args):\n\t\t\t\t\t\t\t\tif not typed:\n\t\t\t\t\t\t\t\t\t\tkey = args\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\tkey = args, tuple(type(arg) for arg in args)\n\t\t\t\t\t\t\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tcount[key] += 1\n\t\t\t\t\t\t\t\t\t\t\t\t\t\torder.append(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalue = func(*args)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcache[key] = value\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcount[key] += 1\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\torder.append(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# purge least frequently used:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmincount = min(count.values())\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor key in order:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif count[key] == mincount:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdel count[key]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdel cache[key]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\torder.remove(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# print 'CacheInfo(count=%s,order=%s)' % (count,order)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef _decorator(func):\n\t\tcache = {}\n\t\tdef _memoize(*args):\n\t\t\tif args in cache:\n\t\t\t\treturn cache[args]\n\t\t\telse:\n\t\t\t\tresult = func(*args)\n\t\t\t\tcache[args] = result\n\t\t\t\treturn result\n\t\t_memoize.cache = cache\n\t\treturn _memoize\n\n\treturn _decorator\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n# \tdef decorator(func):\n# \t\tcache = func.__name__ = FuncCache(maxsize, typed)\n# \t\treturn cache.wrapper\n# \t\n# \treturn decorator\n\t\n# \tdef wrapper(self, *args, **kwargs):\n# \t\ttry:\n# \t\t\treturn self.cache[args, frozenset(sorted(kwargs.items()))]\n# \t\texcept KeyError:\n# \t\t\tvalue = self.cache[args, frozenset(sorted(kwargs.items()))] = self.func(*args, **kwargs)\n# \t\t\treturn value\n# \t\n# \tdef __get__(self, instance, owner):\n# \t\tif instance is None:\n# \t\t\treturn self\n# \t\tself.instance = instance\n# \t\treturn self\n# \t\n# \tdef __getattr__(self, name):\n# \t\tif name == \"func\":\n# \t\t\treturn getattr(self.func, name)\n# \t\tif name == \"cache\":\n# \t\t\treturn getattr(self.instance, '_FuncCache__cache')\n# \t\treturn getattr(self.func, name)\n# \t\n# \tdef __repr__(self):\n# \t\treturn '<FuncCache of %s>' % self.func.__name__\n# \t\n# \tdef __call", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tcache = OrderedDict()\n\t\tdef memoize(f, *args):\n\t\t\tif func.__name__=='get_all_users':\n\t\t\t\treturn f(*args)\n\t\t\tif not cache:\n\t\t\t\treturn f(*args)\n\t\t\tkey = (f.__name__, args)\n\t\t\tif key in cache:\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tvalue = f(*args)\n\t\t\t\tcache[key] = value\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\treturn value\n\t\treturn memoize\n\treturn decorator\n/utils/models.py\nfrom django.db import models\nfrom django.contrib.auth.models import User\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t/README.md\n# Python-Learning\n\n## 1. \u5217\u8868\n\n### 1.1 Python\u5217\u8868\n\n- \u5217\u8868\u662fPython\u4e2d\u4f7f\u7528\u6700\u9891\u7e41\u7684\u6570\u636e\u7ed3\u6784\n- \u5217\u8868\u662f\u4e00\u79cd\u6709\u5e8f\u7684\u96c6\u5408\uff0c\u53ef\u4ee5\u968f\u65f6\u6dfb\u52a0\u548c\u5220\u9664\u5176\u4e2d\u7684\u5143\u7d20\n- \u5217\u8868\u7528`[]`\u8868\u793a\uff0c\u5143\u7d20\u4e4b\u95f4\u7528`,`\u9694\u5f00\n- \u5143\u7d20\u53ef\u4ee5\u662f\u4efb\u610f\u6570\u636e\u7c7b\u578b\uff0c\u751a\u81f3\u53ef\u4ee5\u662f\u53e6\u4e00\u4e2a\u5217\u8868\n- \u5217\u8868\u7684\u957f\u5ea6\u603b\u662f\u56fa\u5b9a\u7684\uff0c\u4e0d\u80fd\u968f\u610f\u5220\u9664\u548c\u6dfb\u52a0\u5143\u7d20\n\n```python\nclassmates = ['Michael', 'Bob', 'Tracy']\nclassmates[0]\nclassmates[-1]\nclassmates.append('Adam')\nclassmates.insert(1, 'Jack')\nclassmates.pop()\nclassmates.pop(1)\nclassmates[1] = 'Sarah'\nclassmates\n```\n\n### 1.2 \u5143\u7ec4\n\n- \u53e6\u4e00\u79cd\u6709\u5e8f\u5217\u8868\u53eb\u5143\u7ec4\uff1atuple\u3002tuple\u548clist\u975e\u5e38\u7c7b\u4f3c\uff0c\u4f46\u662ftuple\u4e00\u65e6\u521d\u59cb\u5316\u5c31\u4e0d\u80fd\u4fee\u6539\n- \u5143\u7ec4\u4f7f\u7528\u5706\u62ec\u53f7()\u8868\u793a\uff0c\u5143\u7d20\u4e4b\u95f4\u7528`,`\u9694\u5f00\n- \u5143\u7ec4\u4e2d\u5143\u7d20\u4e0d\u53ef\u53d8\uff0c\u4f46\u662f\u5143\u7ec4\u5305\u542b\u7684\u5143\u7d20\u53ef\u4ee5\u662f\u5217\u8868\n\n```python\nclassmates = ('Michael', 'Bob', 'Tracy')\nt =", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef wrapper(func):\n\t\t\n\t\tcache = {}\n\t\tmaxsize = int(maxsize)\n\t\t# OrderedDict is used as a cache, with the least\n\t\t# recently used entry being the first to be popped\n\t\t# out when maxsize is exceeded.\n\t\tcache_order = collections.OrderedDict()\n\n\t\tdef memoize(*args, **kwargs):\n\t\t\tif kwargs:\n\t\t\t\tkey = str(args) + str(sorted(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = str(args)\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) == maxsize:\n\t\t\t\t\t# Cache is full, so pop the least recently used.\n\t\t\t\t\tcache.popitem(0)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tcache_order[key] = 1\n\t\t\telse:\n\t\t\t\tresult = cache[key]\n\t\t\t\tcache_order[key] += 1\n\t\t\treturn result\n\n\t\tdef clear_cache():\n\t\t\tcache.clear()\n\t\t\tcache_order.clear()\n\n\t\tdef get_cache():\n\t\t\treturn cache\n\n\t\tdef get_cache_order():\n\t\t\treturn cache_order\n\n\t\tmemoize.cache = cache\n\t\tmemoize.cache_order = cache_order\n\t\tmemoize.clear = clear_cache\n\t\tmemoize.get_cache =", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n# \tdef _decorator(f):\n# \t\tcache = {}\n# \t\tdef _lookup(key):\n# \t\t\treturn cache[key]\n# \t\tdef _memoize(key, value):\n# \t\t\tif len(cache) >= maxsize:\n# \t\t\t\t# LFU algorithm\n# \t\t\t\tpass\n# \t\t\tcache[key] = value\n# \t\tdef _f(*args, **kwargs):\n# \t\t\tkey = (f, args, frozenset(kwargs.items()))\n# \t\t\ttry:\n# \t\t\t\treturn _lookup(key)\n# \t\t\texcept KeyError:\n# \t\t\t\tvalue = f(*args, **kwargs)\n# \t\t\t\t_memoize(key, value)\n# \t\t\t\treturn value\n# \t\treturn _f\n# \treturn _decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\tcache = {}\n\t\tdef decorator(f):\n\t\t\tdef g(*args):\n\t\t\t\tkey = args\n\t\t\t\tif len(key) == 1 and not typed:\n\t\t\t\t\tkey = key[0]\n\t\t\t\t\t\tif key not in cache:\n\t\t\t\t\t\t\tcache[key] = f(*args)\n\t\t\t\t\t\t\t\treturn cache[key]\n\treturn g\n\n@memoize", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\tcache = LFUCache(maxsize, typed=typed)\n\t\tdef decorate(f):\n\t\t\t\t@wraps(f)\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\t\tresult = cache.get(args + tuple(kwargs.items()))\n\t\t\t\t\t\tif result is not None:\n\t\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\t\tcache[args + tuple(kwargs.items())] = result\n\t\t\t\t\t\treturn result\n\t\t\t\treturn wrapper\n\t\treturn decorate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\tcache = lfu_cache.cache = {}\n\t\tdef decorating_function(user_function):\n\t\t\t\"\"\"\n\t\t\tDecorator.\n\t\t\t\t`user_function`\n\t\t\t\t\tThe function to wrap.\n\t\t\t\t`maxsize`\n\t\t\t\t\tThe maximum number of results to save.\n\t\t\t\t`typed`\n\t\t\t\t\tCache `user_function` with distinct argument\n\t\t\t\t\tsignatures.\n\t\t\t\"\"\"\n\t\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tThe wrapper function.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\t\tkey = args\n\t\t\t\t\t\tif kwds:\n\t\t\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\t\t\tif typed:\n\t\t\t\t\t\t\tkey += user_function.func_code.co_filename,\n\t\t\t\t\t\t\tuser_function.func_code.co_firstlineno\n\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\treturn result\n\t\t\t\t\treturn update_wrapper(wrapper, user_function)\n\t\t\t\treturn update_wrapper(decorating_function, user_function)\n\t\t\treturn decorator_maker(4, update_wrapper)\n\t\treturn lfu_cache\n\treturn lfu_cache\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\t# The function to be decorated.\n\tdef decorate(func):\n\t\t\t# Keep a cache of up to maxsize results:\n\t\t\tcache = {}\n\t\t\t\t# Remember the number of times each output is generated:\n\t\t\t\tcount = collections.defaultdict(int)\n\t\t\t\t\t# The list of keys sorted by their use frequency:\n\t\t\t\t\torder = []\n\t\t\t\t\t\t# When wrapped is called, return the cached value if\n\t\t\t\t\t\t# possible:\n\t\t\t\t\t\tdef wrapper(*args):\n\t\t\t\t\t\t\t\tif not typed:\n\t\t\t\t\t\t\t\t\t\tkey = args\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\tkey = args, tuple(type(arg) for arg in args)\n\t\t\t\t\t\t\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tcount[key] += 1\n\t\t\t\t\t\t\t\t\t\t\t\t\t\torder.append(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalue = func(*args)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcache[key] = value\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcount[key] += 1\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\torder.append(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# purge least frequently used:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmincount = min(count.values())\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor key in order:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif count[key] == mincount:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdel count[key]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdel cache[key]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\torder.remove(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# print 'CacheInfo(count=%s,order=%s)' % (count,order)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\t\n\n\tdef _decorator(func):\n\t\tcache = {}\n\t\tdef _memoize(*args):\n\t\t\tif args in cache:\n\t\t\t\treturn cache[args]\n\t\t\telse:\n\t\t\t\tresult = func(*args)\n\t\t\t\tcache[args] = result\n\t\t\t\treturn result\n\t\t_memoize.cache = cache\n\t\treturn _memoize\n\n\treturn _decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tpass\n\t# code here\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tif self._size == 0:\n\t\t\traise KeyError(\"Dictionary is empty\")\n\t\tfirst_node = self._head\n\t\tself._head = self._head.next\n\t\treturn (first_node.key, first_node.value)\n\n\tdef keys(self):\n\t\"\"\"\n\tReturns a list of all the keys in the dictionary.\n\t\"\"\"\n\t\tif self._size == 0:\n\t\t\traise KeyError(\"Dictionary is empty\")\n\t\tcurr_node = self._head\n\t\tkeys = []\n\t\twhile curr_node is not None:\n\t\t\tkeys.append(curr_node.key)\n\t\t\tcurr_node = curr_node.next\n\t\treturn keys\n\n\tdef values(self):\n\t\"\"\"\n\tReturns a list of all the values in the dictionary.\n\t\"\"\"\n\t\tif self._size == 0:\n\t\t\traise KeyError(\"Dictionary is empty\")\n\t\tcurr_node = self._head\n\t\tvalues = []\n\t\twhile curr_node is not None:\n\t\t\tvalues.append(curr_node.value)\n\t\t\tcurr_node = curr_node.next\n\t\treturn values\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of elements in the dictionary.\n\t\"\"\"\n\t\treturn self._size\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn if a key is in the dictionary", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\treturn self.popitem(self.head)\n\t\n\tdef pop(self, key):\n\t\t\"\"\"\n\t\tRemove the value corresponding to the key and returns the value.\n\t\t\"\"\"\n\t\tif not self.head:\n\t\t\traise KeyError('The key does not exist.')\n\t\tif self.head.key == key:\n\t\t\tself.head = self.head.next\n\t\telse:\n\t\t\tcurrent = self.head\n\t\t\twhile current:\n\t\t\t\tif current.next.key == key:\n\t\t\t\t\tbreak\n\t\t\t\tcurrent = current.next\n\t\t\tif current:\n\t\t\t\tcurrent.next = current.next.next\n\t\t\telse:\n\t\t\t\traise KeyError('The key does not exist.')\n\t\treturn key\n\t\n\tdef __str__(self):\n\t\ts = ''\n\t\tcurrent = self.head\n\t\twhile current:\n\t\t\ts += f'[{current.key}: {current.value}]'\n\t\t\tcurrent = current.next\n\t\treturn s\n\nif __name__ == '__main__':\n\t\n\tcache = LRUCache(3)\n\tcache.put(1, 1)\n\tcache.put(2, 2)\n\tcache.put(3, 3)\n\n\tprint(cache)\n\tprint(cache.get(1))\n\tprint()\n\n\tcache.put(4, 4)\n\tprint(cache)\n\tprint(", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tkey = self.head.key\n\t\tvalue = self.head.value\n\t\tself.remove_node(self.head)\n\t\treturn key,value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tif self.head is None:\n\t\t\traise IndexError(\"The queue is empty\")\n\t\telse:\n\t\t\tval = self.head.value\n\t\t\tself.head = self.head.next\n\t\t\treturn (val, self.head.value)\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\t# TODO: Implement this function\n\tpass\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tif self.head is None:\n\t\t\traise KeyError(\"The queue is empty!\")\n\t\telse:\n\t\t\titem = self.head\n\t\t\tself.head = self.head.next\n\t\t\titem.next = None\n\t\t\treturn item.data, item.value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tpass\n\t# code here\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tif self._size == 0:\n\t\t\traise KeyError(\"Dictionary is empty\")\n\t\tfirst_node = self._head\n\t\tself._head = self._head.next\n\t\treturn (first_node.key, first_node.value)\n\n\tdef keys(self):\n\t\"\"\"\n\tReturns a list of all the keys in the dictionary.\n\t\"\"\"\n\t\tif self._size == 0:\n\t\t\traise KeyError(\"Dictionary is empty\")\n\t\tcurr_node = self._head\n\t\tkeys = []\n\t\twhile curr_node is not None:\n\t\t\tkeys.append(curr_node.key)\n\t\t\tcurr_node = curr_node.next\n\t\treturn keys\n\n\tdef values(self):\n\t\"\"\"\n\tReturns a list of all the values in the dictionary.\n\t\"\"\"\n\t\tif self._size == 0:\n\t\t\traise KeyError(\"Dictionary is empty\")\n\t\tcurr_node = self._head\n\t\tvalues = []\n\t\twhile curr_node is not None:\n\t\t\tvalues.append(curr_node.value)\n\t\t\tcurr_node = curr_node.next\n\t\treturn values\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of elements in the dictionary.\n\t\"\"\"\n\t\treturn self._size\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn if a key is in the dictionary", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\treturn self.popitem(self.head)\n\t\n\tdef pop(self, key):\n\t\t\"\"\"\n\t\tRemove the value corresponding to the key and returns the value.\n\t\t\"\"\"\n\t\tif not self.head:\n\t\t\traise KeyError('The key does not exist.')\n\t\tif self.head.key == key:\n\t\t\tself.head = self.head.next\n\t\telse:\n\t\t\tcurrent = self.head\n\t\t\twhile current:\n\t\t\t\tif current.next.key == key:\n\t\t\t\t\tbreak\n\t\t\t\tcurrent = current.next\n\t\t\tif current:\n\t\t\t\tcurrent.next = current.next.next\n\t\t\telse:\n\t\t\t\traise KeyError('The key does not exist.')\n\t\treturn key\n\t\n\tdef __str__(self):\n\t\ts = ''\n\t\tcurrent = self.head\n\t\twhile current:\n\t\t\ts += f'[{current.key}: {current.value}]'\n\t\t\tcurrent = current.next\n\t\treturn s\n\nif __name__ == '__main__':\n\t\n\tcache = LRUCache(3)\n\tcache.put(1, 1)\n\tcache.put(2, 2)\n\tcache.put(3, 3)\n\n\tprint(cache)\n\tprint(cache.get(1))\n\tprint()\n\n\tcache.put(4, 4)\n\tprint(cache)\n\tprint(", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tkey = self.head.key\n\t\tvalue = self.head.value\n\t\tself.remove_node(self.head)\n\t\treturn key,value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tif self.head is None:\n\t\t\traise IndexError(\"The queue is empty\")\n\t\telse:\n\t\t\tval = self.head.value\n\t\t\tself.head = self.head.next\n\t\t\treturn (val, self.head.value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\t# TODO: Implement this function\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t\tif self.head is None:\n\t\t\traise KeyError(\"The queue is empty!\")\n\t\telse:\n\t\t\titem = self.head\n\t\t\tself.head = self.head.next\n\t\t\titem.next = None\n\t\t\treturn item.data, item.value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif (key in self.__dict__):\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\tself.__dict__[key] = default\n\t\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\treturn self.data.setdefault(key, default)\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\n#\tprint(\"defaultdict.setdefault(self, key, default=None) called\")\n\tif key in self._data:\n\t\treturn self._data[key]\n\telse:\n\t\tself._data[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif not key in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.keys():\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\tself.__dict__[key] = default\n\t\treturn self.__dict__[key]\n\nprint(Test(3).setdefault('key', 10))\nprint(Test(3).setdefault('name', 10))\n\n# 4.2.3.20. Test if a key exists in a class\n# Test if a key exists in a class", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif self.has_key(key):\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n\tdef __repr__(self):\n\t\tmy_dict = dict()\n\t\tfor key, value in self.items():\n\t\t\tmy_dict[key] = value\n\t\treturn repr(my_dict)\n\tdef __str__(self):\n\t\tmy_dict = dict()\n\t\tfor key, value in self.items():\n\t\t\tmy_dict[key] = value\n\t\treturn str(my_dict)\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\tself.__dict__[key] = default\n\t\t\treturn default\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif (key in self.__dict__):\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\tself.__dict__[key] = default\n\t\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\treturn self.data.setdefault(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\n#\tprint(\"defaultdict.setdefault(self, key, default=None) called\")\n\tif key in self._data:\n\t\treturn self._data[key]\n\telse:\n\t\tself._data[key] = default\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif not key in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.keys():\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\tself.__dict__[key] = default\n\t\treturn self.__dict__[key]\n\nprint(Test(3).setdefault('key', 10))\nprint(Test(3).setdefault('name', 10))\n\n# 4.2.3.20. Test if a key exists in a class\n# Test if a key exists in a class", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif self.has_key(key):\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n\tdef __repr__(self):\n\t\tmy_dict = dict()\n\t\tfor key, value in self.items():\n\t\t\tmy_dict[key] = value\n\t\treturn repr(my_dict)\n\tdef __str__(self):\n\t\tmy_dict = dict()\n\t\tfor key, value in self.items():\n\t\t\tmy_dict[key] = value\n\t\treturn str(my_dict)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\tself.__dict__[key] = default\n\t\t\treturn default\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.dict:\n\t\t\treturn self.dict[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef set(self, key, value):\n\t\"\"\"\n\tSets a key to the value.\n\t\"\"\"\n\t\tself.dict[key] = value\n\n# Class that stores the information about the game.", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self.data.get(key, default)\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.key_value_dict:\n\t\t\t\treturn self.key_value_dict[key]\n\t\treturn default\n\nkey_value_dict = {'key1': 'value1', 'key2': 'value2'}\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self._dict.get(key, default)\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tIf a key exists in the class, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\treturn key in self._dict\n\t# end of Dict class\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.data:\n\t\t\treturn self.data[key]\n\t\telse:\n\t\t\treturn default\n\n# This is just an example of how you would call the class.\n\n# A dictionary of the objects we are going to create\n# The key is the name of the file, the value is the object\nobjectDict = {}\n\n# Loop over each line in the file\nfor line in open(inputFileName, 'r'):\n\t# Remove the newline character at the end of the line\n\tline = line.rstrip()\n\t# Split the line on the tab character\n\tlineList = line.split('\\t')\n\t# Create an object of the class using the first field in the line as the key\n\tobjectDict[lineList[0]] = MyClass(line)\n\n# Loop over each key in the objectDict\nfor key in objectDict:\n\t# Print the key and the value corresponding to the key\n\tprint key, \"\\t\", objectDict[key].get('sample_name')\n(p[1] - p[0])\n\nfor i in range(len(p) - 1):\n\tprint p[i][0], p[i][1], p[i + 1][0], p[i + 1][1], dist(p[i], p[i + 1]), dist", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\n\n\tif key in self.values:\n\t\treturn self.values[key]\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self.__dict__.get(key, default)\n```\n\nThe code above simply checks whether the key exists in the class. If it does, it returns the value corresponding to the key. Otherwise, it returns the default value.\n\n**Note:** The `self` parameter is used to refer to the class itself. It is necessary to provide this parameter when calling methods, as it is used to retrieve the attributes of the class.\n\nIn addition, you can use the `get` method to check whether a key exists in the class. For example, if you want to check whether the key `age` exists in the class `User`, you can use the code below:\n\n```\nif User.get(\"age\"):\n\tprint(\"The key 'age' exists in the class 'User'\")\n```\n\nYou can also use the `get` method to retrieve the value corresponding to the key. For example, if you want to retrieve the value of the key `age`, you can use the code below:\n\n```\nage = User.get(\"age\")\nif age:\n\tprint(\"The value corresponding to the key 'age' is\", age)\n```\n\n**Note:** The `get` method in the code above returns `None` if the key does not exist in the class. Therefore, it is necessary to check whether the value returned by `get` is `None` before using it", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self.get_value(key, default)\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self._dict.get(key, default)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.dict:\n\t\t\treturn self.dict[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef set(self, key, value):\n\t\"\"\"\n\tSets a key to the value.\n\t\"\"\"\n\t\tself.dict[key] = value\n\n# Class that stores the information about the game.", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self.data.get(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.key_value_dict:\n\t\t\t\treturn self.key_value_dict[key]\n\t\treturn default\n\nkey_value_dict = {'key1': 'value1', 'key2': 'value2'}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self._dict.get(key, default)\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tIf a key exists in the class, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\treturn key in self._dict\n\t# end of Dict class\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.data:\n\t\t\treturn self.data[key]\n\t\telse:\n\t\t\treturn default\n\n# This is just an example of how you would call the class.\n\n# A dictionary of the objects we are going to create\n# The key is the name of the file, the value is the object\nobjectDict = {}\n\n# Loop over each line in the file\nfor line in open(inputFileName, 'r'):\n\t# Remove the newline character at the end of the line\n\tline = line.rstrip()\n\t# Split the line on the tab character\n\tlineList = line.split('\\t')\n\t# Create an object of the class using the first field in the line as the key\n\tobjectDict[lineList[0]] = MyClass(line)\n\n# Loop over each key in the objectDict\nfor key in objectDict:\n\t# Print the key and the value corresponding to the key\n\tprint key, \"\\t\", objectDict[key].get('sample_name')\n(p[1] - p[0])\n\nfor i in range(len(p) - 1):\n\tprint p[i][0], p[i][1], p[i + 1][0], p[i + 1][1], dist(p[i], p[i + 1]), dist", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\n\n\tif key in self.values:\n\t\treturn self.values[key]\n\telse:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self.__dict__.get(key, default)\n```\n\nThe code above simply checks whether the key exists in the class. If it does, it returns the value corresponding to the key. Otherwise, it returns the default value.\n\n**Note:** The `self` parameter is used to refer to the class itself. It is necessary to provide this parameter when calling methods, as it is used to retrieve the attributes of the class.\n\nIn addition, you can use the `get` method to check whether a key exists in the class. For example, if you want to check whether the key `age` exists in the class `User`, you can use the code below:\n\n```\nif User.get(\"age\"):\n\tprint(\"The key 'age' exists in the class 'User'\")\n```\n\nYou can also use the `get` method to retrieve the value corresponding to the key. For example, if you want to retrieve the value of the key `age`, you can use the code below:\n\n```\nage = User.get(\"age\")\nif age:\n\tprint(\"The value corresponding to the key 'age' is\", age)\n```\n\n**Note:** The `get` method in the code above returns `None` if the key does not exist in the class. Therefore, it is necessary to check whether the value returned by `get` is `None` before using it", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self.get_value(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\t\treturn self._dict.get(key, default)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\n@cachedmethod(cache, key=hashkey)", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\n#\tdef decorator(func):\n#\t\t\"\"\"\n#\t\tCache the result of the function.\n#\t\t\"\"\"\n#\t\tdef wrapper(*args, **kwargs):\n#\t\t\t# Get the function name\n#\t\t\tfuncname = func.__name__\n#\t\t\t# Create a dictionary of the function arguments\n#\t\t\targdict = dict(zip(func.func_code.co_varnames, args))\n#\t\t\targdict.update(kwargs)\n#\t\t\t# Create a key from the function name and arguments\n#\t\t\tkey = key(funcname, argdict)\n#\t\t\t# Get the result from cache\n#\t\t\tresult = cache.get(key)\n#\t\t\tif result is None:\n#\t\t\t\t# Lock the cache\n#\t\t\t\tif lock:\n#\t\t\t\t\tlock.acquire()\n#\t\t\t\t# Try to get the result from the cache\n#\t\t\t\tresult = cache.get(key)\n#\t\t\t\tif result is None:\n#\t\t\t\t\t# Run the function and cache the result\n#\t\t\t\t\tresult = func(*args, **kwargs)\n#\t\t\t\t\tcache.set(key, result)\n#\t\t\t\tif lock:\n#\t\t\t\t\tlock.release()\n#\t\t\treturn result\n#\t\treturn wrapper\n#\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\tdef call(*args):\n\t\t\t\t\t\t# Check for a cached result\n\t\t\t\t\t\tif cache.has_key(key(*args)):\n\t\t\t\t\t\t\t\treturn cache[key(*args)]\n\t\t\t\t\t\t\t\t# We don't have a cached result, so we need to compute it\n\t\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\t\tresult = func(*args)\n\t\t\t\t\t\t# Cache the result\n\t\t\t\t\t\tcache[key(*args)] = result\n\t\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\t\t\tlock.release()\n\t\t\t\t\t\t\t\treturn result\n\t\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\n        def decorator(func):\n                def wrapper(*args, **kwargs):\n                        if lock is None:\n                                l = cache.lock\n                        else:\n                                l = lock\n                        with l.acquire(blocking=False):\n                                r = cache.get(key(*args, **kwargs))\n                        if r is None:\n                                with l.acquire(blocking=False):\n                                        r = cache.get(key(*args, **kwargs))\n                                if r is None:\n                                        r = func(*args, **kwargs)\n                                        cache.set(key(*args, **kwargs), r)\n                        return r\n                return wrapper\n        return decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(f):\n\t\t\t@wraps(f)\n\t\t\tdef _f(*args, **kwargs):\n\t\t\t\tif not cache:\n\t\t\t\t\treturn f(*args, **kwargs)\n\t\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\tif lock:\n\t\t\t\t\t\t\tlock.release()\n\t\t\t\t\treturn result\n\t\t\treturn _f\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tdef call(self, *args, **kwargs):\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\tkey = key(self, *args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\t\tcache[key] = result\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\t\treturn call\n\t\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tdef wrapper(self, *args, **kwargs):\n\t\t\t\thashkey = key(self, *args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\tif cache.get(hashkey) is None:\n\t\t\t\t\tcache[hashkey] = func(self, *args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn cache[hashkey]\n\t\t\treturn wrapper\n\t\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\t\"\"\"\n\t\tReturns a decorator function that can call a result in the cache.\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kw):\n\t\t\t\"\"\"\n\t\t\tReturns a decorator function that can call a result in the cache.\n\t\t\t\"\"\"\n\t\t\tkey = key(*args, **kw)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\t# first try to get the result from the cache\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tif isinstance(result, Deferred):\n\t\t\t\t\t\tdef callback(result):\n\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tdef errback(failure):\n\t\t\t\t\t\t\traise failure.value\n\t\t\t\t\t\tresult.addCallbacks(callback, errback)\n\t\t\t\t\t\treturn result\n\t\t\t\texcept KeyError:\n\t\t\t\t\t# if the result is not in the cache,\n\t\t\t\t\t# call the function and store the result in the cache\n\t\t\t\t\tresult = func(*args, **kw)\n\t\t\t\t\tif isinstance(result, Deferred):\n\t\t\t\t\t\tdef callback(result):\n\t\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tdef errback(failure):\n\t\t\t\t\t\t\traise failure.value\n\t\t\t\t\t\tresult.addCallbacks(callback, errback)\n\t\t\t\t\telse:\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\t@functools.wraps(func)\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\t\tif lock:\n\t\t\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\t\t\tcache[key(*args, **kwargs)] = result = func(*args, **kwargs)\n\t\t\t\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tfinally:\n\t\t\t\t\t\t\t\tif lock:\n\t\t\t\t\t\t\t\t\t\tlock.release()\n\t\t\t\treturn wrapper\n\t\treturn decorator\n\n@cachedmethod(cache)"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\n@cachedmethod(cache, key=hashkey)", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\n#\tdef decorator(func):\n#\t\t\"\"\"\n#\t\tCache the result of the function.\n#\t\t\"\"\"\n#\t\tdef wrapper(*args, **kwargs):\n#\t\t\t# Get the function name\n#\t\t\tfuncname = func.__name__\n#\t\t\t# Create a dictionary of the function arguments\n#\t\t\targdict = dict(zip(func.func_code.co_varnames, args))\n#\t\t\targdict.update(kwargs)\n#\t\t\t# Create a key from the function name and arguments\n#\t\t\tkey = key(funcname, argdict)\n#\t\t\t# Get the result from cache\n#\t\t\tresult = cache.get(key)\n#\t\t\tif result is None:\n#\t\t\t\t# Lock the cache\n#\t\t\t\tif lock:\n#\t\t\t\t\tlock.acquire()\n#\t\t\t\t# Try to get the result from the cache\n#\t\t\t\tresult = cache.get(key)\n#\t\t\t\tif result is None:\n#\t\t\t\t\t# Run the function and cache the result\n#\t\t\t\t\tresult = func(*args, **kwargs)\n#\t\t\t\t\tcache.set(key, result)\n#\t\t\t\tif lock:\n#\t\t\t\t\tlock.release()\n#\t\t\treturn result\n#\t\treturn wrapper\n#\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\tdef call(*args):\n\t\t\t\t\t\t# Check for a cached result\n\t\t\t\t\t\tif cache.has_key(key(*args)):\n\t\t\t\t\t\t\t\treturn cache[key(*args)]\n\t\t\t\t\t\t\t\t# We don't have a cached result, so we need to compute it\n\t\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\t\tresult = func(*args)\n\t\t\t\t\t\t# Cache the result\n\t\t\t\t\t\tcache[key(*args)] = result\n\t\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\t\t\tlock.release()\n\t\t\t\t\t\t\t\treturn result\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\n        def decorator(func):\n                def wrapper(*args, **kwargs):\n                        if lock is None:\n                                l = cache.lock\n                        else:\n                                l = lock\n                        with l.acquire(blocking=False):\n                                r = cache.get(key(*args, **kwargs))\n                        if r is None:\n                                with l.acquire(blocking=False):\n                                        r = cache.get(key(*args, **kwargs))\n                                if r is None:\n                                        r = func(*args, **kwargs)\n                                        cache.set(key(*args, **kwargs), r)\n                        return r\n                return wrapper\n        return decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(f):\n\t\t\t@wraps(f)\n\t\t\tdef _f(*args, **kwargs):\n\t\t\t\tif not cache:\n\t\t\t\t\treturn f(*args, **kwargs)\n\t\t\t\t\tkey = key(*args, **kwargs)\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\tif lock:\n\t\t\t\t\t\t\tlock.release()\n\t\t\t\t\treturn result\n\t\t\treturn _f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tdef call(self, *args, **kwargs):\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\tkey = key(self, *args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\t\tcache[key] = result\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn result\n\t\t\treturn call\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tdef wrapper(self, *args, **kwargs):\n\t\t\t\thashkey = key(self, *args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\tif cache.get(hashkey) is None:\n\t\t\t\t\tcache[hashkey] = func(self, *args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn cache[hashkey]\n\t\t\treturn wrapper\n\t\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\t\"\"\"\n\t\tReturns a decorator function that can call a result in the cache.\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kw):\n\t\t\t\"\"\"\n\t\t\tReturns a decorator function that can call a result in the cache.\n\t\t\t\"\"\"\n\t\t\tkey = key(*args, **kw)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\t# first try to get the result from the cache\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tif isinstance(result, Deferred):\n\t\t\t\t\t\tdef callback(result):\n\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tdef errback(failure):\n\t\t\t\t\t\t\traise failure.value\n\t\t\t\t\t\tresult.addCallbacks(callback, errback)\n\t\t\t\t\t\treturn result\n\t\t\t\texcept KeyError:\n\t\t\t\t\t# if the result is not in the cache,\n\t\t\t\t\t# call the function and store the result in the cache\n\t\t\t\t\tresult = func(*args, **kw)\n\t\t\t\t\tif isinstance(result, Deferred):\n\t\t\t\t\t\tdef callback(result):\n\t\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tdef errback(failure):\n\t\t\t\t\t\t\traise failure.value\n\t\t\t\t\t\tresult.addCallbacks(callback, errback)\n\t\t\t\t\telse:\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\t@functools.wraps(func)\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\t\tif lock:\n\t\t\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\t\t\tcache[key(*args, **kwargs)] = result = func(*args, **kwargs)\n\t\t\t\t\t\t\t\t\t\treturn result\n\t\t\t\t\t\tfinally:\n\t\t\t\t\t\t\t\tif lock:\n\t\t\t\t\t\t\t\t\t\tlock.release()\n\t\t\t\treturn wrapper\n\t\treturn decorator\n\n@cachedmethod(cache)", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\treturn cls.do_extostr(e, 0, max_level, max_path_level)\n\n#\n#\n#\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\n\t# The exception message\n\tmsg = str(e)\n\t# The exception module name\n\tmod = str(e.__class__.__module__)\n\t# The exception class name\n\tcls_name = str(e.__class__.__name__)\n\t# The exception full name\n\tfull_name = mod + \".\" + cls_name\n\t# The exception full name\n\tfull_name = mod + \".\" + cls_name\n\n\t# The exception path\n\tpath = [mod]\n\ttry:\n\t\tlast_mod = mod\n\t\twhile mod!= last_mod:\n\t\t\tlast_mod = mod\n\t\t\tmod = mod.split('.')[0]\n\t\t\tpath.insert(0, mod)\n\texcept:\n\t\tpass\n\t# The exception path\n\tpath_str = \".\".join(path[:max_path_level])\n\n\t# The exception message\n\terr_msg = msg\n\tif len(msg) > 50:\n\t\terr_msg = msg[:50] + \"...\"\n\n\t# The exception traceback\n\ttb = traceback.format_exc()\n\t# Format the traceback\n\ttb = tb.split(\"\\n\")\n\t# Remove the first line (which is empty)\n\ttb.pop(0)\n\n\t# Get the exception level\n\tlevel = 0\n\t# Get the exception level\n\tfor", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\n\t# Get the exception class name\n\tif isinstance(e, Exception):\n\t\texception_class_name = e.__class__.__name__\n\telse:\n\t\texception_class_name = None\n\n\t# Get the exception message\n\tif isinstance(e, Exception):\n\t\texception_message = e.message\n\telse:\n\t\texception_message = None\n\n\t# Get the exception stack\n\tif isinstance(e, Exception):\n\t\texception_stack = traceback.format_exception(e.__class__, e, e.__traceback__)\n\telse:\n\t\texception_stack = None\n\n\t# Get the exception stack as a string\n\tif isinstance(exception_stack, list):\n\t\texception_stack = \"\".join(exception_stack)\n\telse:\n\t\texception_stack = None\n\n\t# Get the exception stack as a string\n\tif not isinstance(exception_stack, str):\n\t\texception_stack = None\n\n\t# Return the string\n\treturn cls.exception_to_string(exception_class_name, exception_message, exception_stack, max_level, max_path_level)\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        from sys import exc_info # XXX: this is a hack and should be avoided.\n\tif e is None: e = exc_info()[1]\n\tif e is None: return \"\"\n        if max_level < 0: max_level = 30\n        if max_path_level < 0: max_path_level = 5\n\ts = str(e)\n\tif len(s) > 500: s = s[:500] + \"...\"\n\ts = s.replace(\"\\n\", \"\\n\\t\")\n\ts = s.replace(\"\\t\", \" \")\n\ts = s.replace(\"\\r\", \" \")\n\ts = s.replace(\"\\0\", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\treturn ''.join([cls._get_traceback_frame(frame, max_level, max_path_level)\n\t\t\tfor frame in cls._get_traceback(e, max_level=max_level)])\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        # Get the exception type and the exception object\n        etype = type(e)\n        evalue = e\n        # Format the exception type and the exception value\n        etype_str = etype.__name__\n        evalue_str = str(e)\n        # Get the traceback object\n        tb = sys.exc_info()[2]\n        # Format the first line of the traceback\n        first_line = traceback.format_tb(tb)[0]\n        # Format the exception message\n        e_message = '%s: %s\\n%s' % (etype_str, evalue_str, first_line)\n        # Format the rest of the traceback\n        tb_list = traceback.format_tb(tb)[1:]\n        # Remove the last frames if the traceback is too long\n        if len(tb_list) > max_level:\n            del tb_list[-max_level:]\n        # Format the traceback\n        tb_str = ''.join(tb_list)\n        # Remove the path to the python source files\n        tb_str = re.sub(\n            r'((?:%s/[^/]+/){%d})' % (sys.prefix, max_path_level),\n            r'\\1...', tb_str)\n        # Remove the last frame\n        tb_str = tb_str[:-1]\n        # Add the exception message to the traceback\n        tb_str = e_message + '\\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        # get the name of the exception\n        name = e.__class__.__name__\n\n        # find the exception's path\n        path = [ name ]\n        for t in e.__traceback__.tb_frame.f_back.f_locals:\n            if t == name:\n                break\n            path.append(t)\n\n        # how many frames to show in the trace?\n        limit = min(max_level, len(path))\n\n        # if we have a path, make the path a string\n        if len(path) > 1:\n            path ='in '.join(path)\n            if len(path) > max_path_level:\n                path ='in '.join(path.split(' ')[0:max_path_level]) + '...'\n            path ='(' + path + ')'\n        else:\n            path = ''\n\n        # format the exception\n        return name + ':'+ str(e) + path\n\n    @classmethod\n    def ex2str(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        # get the name of the exception\n        name = e.__class__.__name__\n\n        # find the exception's path\n        path = [ name ]\n        for t in e.__traceback__.tb_frame.f_back.f_locals:\n            if t == name:\n                break\n            path.append", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\t# get the message\n\tmsg = e.msg if e.msg else \"An exception occurred\"\n\t# get the exception type\n\ttype_str = str(e.__class__).split('.')[-1]\n\t# get the traceback\n\ttb = traceback.format_exc()\n\t# get the path\n\tpath = e.path if e.path else \"\"\n\t# get the stack\n\tstack = e.stack if e.stack else \"\"\n\t# get the level\n\tlevel = e.level if e.level else \"\"\n\t# get the file\n\tfile = e.file if e.file else \"\"\n\t# get the line\n\tline = e.line if e.line else \"\"\n\t# get the line\n\tline = e.line if e.line else \"\"\n\t# get the function\n\tfunc = e.func if e.func else \"\"\n\t# get the args\n\targs = e.args if e.args else \"\"\n\t# get the kwargs\n\tkwargs = e.kwargs if e.kwargs else \"\"\n\t# get the error\n\terror = e.error if e.error else \"\"\n\t# return the formatted string\n\treturn \"\"\"\n{0}\n\nException Type: {1}\nMessage: {2}\nPath: {3}\nLevel: {4}\nFile: {5}\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t \n\t# Exception message\n\tmsg = \"{0}: {1}\".format(type(e).__name__, e)\n\n\t# Exception traceback\n\ttb = traceback.format_exc().splitlines()\n\n\t# Maximum traceback levels\n\tmax_tb = min(len(tb), max_level)\n\n\t# Maximum traceback path levels\n\tmax_path = min(len(tb[2].split(\"/\")) - 3, max_path_level)\n\n\t# Exception formatted message\n\tmsg += \"\\n\" + \"\\n\".join(tb[1:max_tb])\n\tmsg += \"\\n\" + tb[2].split(\"/\")[2][:max_path]\n\n\treturn msg\n\n\n# -----------------------------\n# Exception classes\n# -----------------------------\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\n\tif max_level < 0 or max_path_level < 0:\n\t\treturn str(e)\n\n\ts = str(e)\n\tif hasattr(e, 'args'):\n\t\ts += '\\n  Args:'+ str(e.args)\n\tif hasattr(e,'message'):\n\t\ts += '\\n  Message:'+ str(e.message)\n\n\tif max_level > 0:\n\t\tif hasattr(e, 'exc_info'):\n\t\t\teinfo = e.exc_info\n\t\t\tif einfo is not None:\n\t\t\t\ts += '\\n  Exception args:\\n'\n\t\t\t\tfor l in formatException(einfo[2]):\n\t\t\t\t\ts +=' '+ l\n\t\tif hasattr(e, 'exc_type'):\n\t\t\ts += '\\n  Exception type:'+ str(e.exc_type)\n\t\tif hasattr(e, 'exc_value'):\n\t\t\ts += '\\n  Exception value:'+ str(e.exc_value)\n\n\tif hasattr(e, 'exc_traceback'):\n\t\ts += '\\n  Traceback:\\n'\n\t\tfor l in formatTraceback(e.exc_traceback, max_path_level):\n\t\t\ts +=' '+ l\n\treturn s\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\treturn cls.do_extostr(e, 0, max_level, max_path_level)\n\n#\n#\n#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\n\t# The exception message\n\tmsg = str(e)\n\t# The exception module name\n\tmod = str(e.__class__.__module__)\n\t# The exception class name\n\tcls_name = str(e.__class__.__name__)\n\t# The exception full name\n\tfull_name = mod + \".\" + cls_name\n\t# The exception full name\n\tfull_name = mod + \".\" + cls_name\n\n\t# The exception path\n\tpath = [mod]\n\ttry:\n\t\tlast_mod = mod\n\t\twhile mod!= last_mod:\n\t\t\tlast_mod = mod\n\t\t\tmod = mod.split('.')[0]\n\t\t\tpath.insert(0, mod)\n\texcept:\n\t\tpass\n\t# The exception path\n\tpath_str = \".\".join(path[:max_path_level])\n\n\t# The exception message\n\terr_msg = msg\n\tif len(msg) > 50:\n\t\terr_msg = msg[:50] + \"...\"\n\n\t# The exception traceback\n\ttb = traceback.format_exc()\n\t# Format the traceback\n\ttb = tb.split(\"\\n\")\n\t# Remove the first line (which is empty)\n\ttb.pop(0)\n\n\t# Get the exception level\n\tlevel = 0\n\t# Get the exception level\n\tfor", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\n\t# Get the exception class name\n\tif isinstance(e, Exception):\n\t\texception_class_name = e.__class__.__name__\n\telse:\n\t\texception_class_name = None\n\n\t# Get the exception message\n\tif isinstance(e, Exception):\n\t\texception_message = e.message\n\telse:\n\t\texception_message = None\n\n\t# Get the exception stack\n\tif isinstance(e, Exception):\n\t\texception_stack = traceback.format_exception(e.__class__, e, e.__traceback__)\n\telse:\n\t\texception_stack = None\n\n\t# Get the exception stack as a string\n\tif isinstance(exception_stack, list):\n\t\texception_stack = \"\".join(exception_stack)\n\telse:\n\t\texception_stack = None\n\n\t# Get the exception stack as a string\n\tif not isinstance(exception_stack, str):\n\t\texception_stack = None\n\n\t# Return the string\n\treturn cls.exception_to_string(exception_class_name, exception_message, exception_stack, max_level, max_path_level)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        from sys import exc_info # XXX: this is a hack and should be avoided.\n\tif e is None: e = exc_info()[1]\n\tif e is None: return \"\"\n        if max_level < 0: max_level = 30\n        if max_path_level < 0: max_path_level = 5\n\ts = str(e)\n\tif len(s) > 500: s = s[:500] + \"...\"\n\ts = s.replace(\"\\n\", \"\\n\\t\")\n\ts = s.replace(\"\\t\", \" \")\n\ts = s.replace(\"\\r\", \" \")\n\ts = s.replace(\"\\0\", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s.replace(\"  \", \" \")\n\ts = s", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\treturn ''.join([cls._get_traceback_frame(frame, max_level, max_path_level)\n\t\t\tfor frame in cls._get_traceback(e, max_level=max_level)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        # Get the exception type and the exception object\n        etype = type(e)\n        evalue = e\n        # Format the exception type and the exception value\n        etype_str = etype.__name__\n        evalue_str = str(e)\n        # Get the traceback object\n        tb = sys.exc_info()[2]\n        # Format the first line of the traceback\n        first_line = traceback.format_tb(tb)[0]\n        # Format the exception message\n        e_message = '%s: %s\\n%s' % (etype_str, evalue_str, first_line)\n        # Format the rest of the traceback\n        tb_list = traceback.format_tb(tb)[1:]\n        # Remove the last frames if the traceback is too long\n        if len(tb_list) > max_level:\n            del tb_list[-max_level:]\n        # Format the traceback\n        tb_str = ''.join(tb_list)\n        # Remove the path to the python source files\n        tb_str = re.sub(\n            r'((?:%s/[^/]+/){%d})' % (sys.prefix, max_path_level),\n            r'\\1...', tb_str)\n        # Remove the last frame\n        tb_str = tb_str[:-1]\n        # Add the exception message to the traceback\n        tb_str = e_message + '\\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        # get the name of the exception\n        name = e.__class__.__name__\n\n        # find the exception's path\n        path = [ name ]\n        for t in e.__traceback__.tb_frame.f_back.f_locals:\n            if t == name:\n                break\n            path.append(t)\n\n        # how many frames to show in the trace?\n        limit = min(max_level, len(path))\n\n        # if we have a path, make the path a string\n        if len(path) > 1:\n            path ='in '.join(path)\n            if len(path) > max_path_level:\n                path ='in '.join(path.split(' ')[0:max_path_level]) + '...'\n            path ='(' + path + ')'\n        else:\n            path = ''\n\n        # format the exception\n        return name + ':'+ str(e) + path\n\n    @classmethod\n    def ex2str(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n        # get the name of the exception\n        name = e.__class__.__name__\n\n        # find the exception's path\n        path = [ name ]\n        for t in e.__traceback__.tb_frame.f_back.f_locals:\n            if t == name:\n                break\n            path.append", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\t# get the message\n\tmsg = e.msg if e.msg else \"An exception occurred\"\n\t# get the exception type\n\ttype_str = str(e.__class__).split('.')[-1]\n\t# get the traceback\n\ttb = traceback.format_exc()\n\t# get the path\n\tpath = e.path if e.path else \"\"\n\t# get the stack\n\tstack = e.stack if e.stack else \"\"\n\t# get the level\n\tlevel = e.level if e.level else \"\"\n\t# get the file\n\tfile = e.file if e.file else \"\"\n\t# get the line\n\tline = e.line if e.line else \"\"\n\t# get the line\n\tline = e.line if e.line else \"\"\n\t# get the function\n\tfunc = e.func if e.func else \"\"\n\t# get the args\n\targs = e.args if e.args else \"\"\n\t# get the kwargs\n\tkwargs = e.kwargs if e.kwargs else \"\"\n\t# get the error\n\terror = e.error if e.error else \"\"\n\t# return the formatted string\n\treturn \"\"\"\n{0}\n\nException Type: {1}\nMessage: {2}\nPath: {3}\nLevel: {4}\nFile: {5}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t \n\t# Exception message\n\tmsg = \"{0}: {1}\".format(type(e).__name__, e)\n\n\t# Exception traceback\n\ttb = traceback.format_exc().splitlines()\n\n\t# Maximum traceback levels\n\tmax_tb = min(len(tb), max_level)\n\n\t# Maximum traceback path levels\n\tmax_path = min(len(tb[2].split(\"/\")) - 3, max_path_level)\n\n\t# Exception formatted message\n\tmsg += \"\\n\" + \"\\n\".join(tb[1:max_tb])\n\tmsg += \"\\n\" + tb[2].split(\"/\")[2][:max_path]\n\n\treturn msg\n\n\n# -----------------------------\n# Exception classes\n# -----------------------------\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\n\tif max_level < 0 or max_path_level < 0:\n\t\treturn str(e)\n\n\ts = str(e)\n\tif hasattr(e, 'args'):\n\t\ts += '\\n  Args:'+ str(e.args)\n\tif hasattr(e,'message'):\n\t\ts += '\\n  Message:'+ str(e.message)\n\n\tif max_level > 0:\n\t\tif hasattr(e, 'exc_info'):\n\t\t\teinfo = e.exc_info\n\t\t\tif einfo is not None:\n\t\t\t\ts += '\\n  Exception args:\\n'\n\t\t\t\tfor l in formatException(einfo[2]):\n\t\t\t\t\ts +=' '+ l\n\t\tif hasattr(e, 'exc_type'):\n\t\t\ts += '\\n  Exception type:'+ str(e.exc_type)\n\t\tif hasattr(e, 'exc_value'):\n\t\t\ts += '\\n  Exception value:'+ str(e.exc_value)\n\n\tif hasattr(e, 'exc_traceback'):\n\t\ts += '\\n  Traceback:\\n'\n\t\tfor l in formatTraceback(e.exc_traceback, max_path_level):\n\t\t\ts +=' '+ l\n\treturn s\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t \n\tif (os.path.exists(file_name)):\n\t\tif(overwrite == True):\n\t\t\tos.remove(file_name)\n\t\telse:\n\t\t\treturn False\n\t\n\ttry:\n\t\tf = open(file_name, 'w')\n\texcept:\n\t\tprint(\"Error: Could not open file '%s' for writing.\" % file_name)\n\t\treturn False\n\t\n\ttry:\n\t\tf.write(text_buffer)\n\texcept:\n\t\tprint(\"Error: Could not write to file '%s'.\" % file_name)\n\t\treturn False\n\t\n\ttry:\n\t\tf.close()\n\texcept:\n\t\tprint(\"Error: Could not close file '%s'.\" % file_name)\n\t\treturn False\n\t\n\treturn True\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite:\n\t\twith open(file_name, \"w\", encoding=encoding) as f:\n\t\t\tf.write(text_buffer)\n\telse:\n\t\twith open(file_name, \"a\", encoding=encoding) as f:\n\t\t\tf.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n \t# Open the file for writing\n\tif overwrite:\n\t\tfile_handle = open(file_name, 'w')\n\telse:\n\t\tfile_handle = open(file_name, 'a')\n\t\n \t# Write the text to the file\n\tfile_handle.write(text_buffer)\n\tfile_handle.close()\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\t\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif os.path.exists(file_name) and overwrite == False:\n\t\tprint 'File %s already exists.' % file_name\n\t\treturn\n\n\tif overwrite == False:\n\t\tfile_name = file_name + '.tmp'\n\n\tprint 'Writing to %s' % file_name\n\n\tf = open(file_name, 'wb')\n\tf.write(text_buffer)\n\tf.close()\n\n\tif overwrite == False:\n\t\tos.rename(file_name, file_name[:-4])\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\t# If the file does not exist, create it\n\tif not os.path.isfile(file_name):\n\t\tprint 'Creating', file_name\n\t\tos.open(file_name, os.O_RDWR | os.O_CREAT)\n\t\tos.chmod(file_name, 0666)\n\n\t# Append the text to the file\n\tif overwrite:\n\t\tos.remove(file_name)\n\t\tos.open(file_name, os.O_RDWR | os.O_CREAT)\n\t\tos.chmod(file_name, 0666)\n\t\tf = open(file_name, 'w', encoding)\n\t\tf.write(text_buffer)\n\t\tf.close()\n\telse:\n\t\tf = open(file_name, 'a', encoding)\n\t\tf.write(text_buffer)\n\t\tf.close()\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite:\n\t\topen(file_name, 'w').close()\n\n\twith codecs.open(file_name, 'a', encoding) as file:\n\t\tfile.write(text_buffer)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite or not os.path.exists(file_name):\n\t\twith codecs.open(file_name, \"w\", encoding) as f:\n\t\t\tf.write(text_buffer)\n\telse:\n\t\twith codecs.open(file_name, \"a\", encoding) as f:\n\t\t\tf.write(text_buffer)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t \n\tif (os.path.exists(file_name)):\n\t\tif(overwrite == True):\n\t\t\tos.remove(file_name)\n\t\telse:\n\t\t\treturn False\n\t\n\ttry:\n\t\tf = open(file_name, 'w')\n\texcept:\n\t\tprint(\"Error: Could not open file '%s' for writing.\" % file_name)\n\t\treturn False\n\t\n\ttry:\n\t\tf.write(text_buffer)\n\texcept:\n\t\tprint(\"Error: Could not write to file '%s'.\" % file_name)\n\t\treturn False\n\t\n\ttry:\n\t\tf.close()\n\texcept:\n\t\tprint(\"Error: Could not close file '%s'.\" % file_name)\n\t\treturn False\n\t\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite:\n\t\twith open(file_name, \"w\", encoding=encoding) as f:\n\t\t\tf.write(text_buffer)\n\telse:\n\t\twith open(file_name, \"a\", encoding=encoding) as f:\n\t\t\tf.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n \t# Open the file for writing\n\tif overwrite:\n\t\tfile_handle = open(file_name, 'w')\n\telse:\n\t\tfile_handle = open(file_name, 'a')\n\t\n \t# Write the text to the file\n\tfile_handle.write(text_buffer)\n\tfile_handle.close()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite:\n\t\tmode = \"w\"\n\telse:\n\t\tmode = \"a\"\n\t\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif os.path.exists(file_name) and overwrite == False:\n\t\tprint 'File %s already exists.' % file_name\n\t\treturn\n\n\tif overwrite == False:\n\t\tfile_name = file_name + '.tmp'\n\n\tprint 'Writing to %s' % file_name\n\n\tf = open(file_name, 'wb')\n\tf.write(text_buffer)\n\tf.close()\n\n\tif overwrite == False:\n\t\tos.rename(file_name, file_name[:-4])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\t# If the file does not exist, create it\n\tif not os.path.isfile(file_name):\n\t\tprint 'Creating', file_name\n\t\tos.open(file_name, os.O_RDWR | os.O_CREAT)\n\t\tos.chmod(file_name, 0666)\n\n\t# Append the text to the file\n\tif overwrite:\n\t\tos.remove(file_name)\n\t\tos.open(file_name, os.O_RDWR | os.O_CREAT)\n\t\tos.chmod(file_name, 0666)\n\t\tf = open(file_name, 'w', encoding)\n\t\tf.write(text_buffer)\n\t\tf.close()\n\telse:\n\t\tf = open(file_name, 'a', encoding)\n\t\tf.write(text_buffer)\n\t\tf.close()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite:\n\t\topen(file_name, 'w').close()\n\n\twith codecs.open(file_name, 'a', encoding) as file:\n\t\tfile.write(text_buffer)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\n\tif overwrite or not os.path.exists(file_name):\n\t\twith codecs.open(file_name, \"w\", encoding) as f:\n\t\t\tf.write(text_buffer)\n\telse:\n\t\twith codecs.open(file_name, \"a\", encoding) as f:\n\t\t\tf.write(text_buffer)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\twith codecs.open(file_name, \"r\", encoding) as file:\n\twith codecs.open(file_name, \"r\", encoding) as file:\n\t\ttextbuffer = file.read()\n\n\treturn textbuffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\tfile_obj = open(file_name, 'rb')\n#\tfile_obj.seek(0, 2)\n#\tfile_size = file_obj.tell()\n#\tfile_obj.seek(0)\n#\tfile_bytes = file_obj.read(file_size)\n#\tfile_obj.close()\n#\t\n#\t# create text buffer\n#\ttext_buffer = gtk.TextBuffer()\n#\ttext_buffer.set_text(file_bytes)\n\ttext_buffer = gtk.TextBuffer()\n\ttext_buffer.set_text(open(file_name, 'rb').read())\n\treturn text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\tprint \"file_to_textbuffer\",file_name\n\tbuffer = gtk.TextBuffer()\n\tbuffer.set_text(open(file_name, \"r\").read())\n\treturn buffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\n\twith open(file_name, encoding=encoding) as f:\n\t\treturn f.read()\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\n\ttb = TextBuffer()\n\ttry:\n\t\tf = open(file_name, 'r', encoding=encoding)\n\texcept:\n\t\tf = open(file_name, 'r', encoding='utf-8')\n\tf.seek(0)\n\tlines = f.readlines()\n\tf.close()\n\tfor line in lines:\n\t\ttb.append(line)\n\treturn tb\n\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t \n\ttext_buffer = Gtk.TextBuffer()\n\ttext_buffer.set_text(get_file_content(file_name, encoding))\n\treturn text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\tprint \"file_to_textbuffer\"\n\t\n\t# Create a new text buffer\n\ttext_buffer = gtk.TextBuffer()\n\t\n\t# Open the file\n\tfile_handler = codecs.open(file_name, mode=\"r\", encoding=encoding)\n\t\n\t# Get the contents of the file\n\tfile_contents = file_handler.read()\n\tfile_handler.close()\n\t\n\t# Insert the contents of the file into the text buffer\n\ttext_buffer.set_text(file_contents)\n\t\n\t# Return the text buffer\n\treturn text_buffer\n\t\n\t", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\twith open(file_name, 'r', encoding = encoding) as f:\n\t\t\ttextbuffer = f.read()\n\t\treturn textbuffer\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\twith codecs.open(file_name, \"r\", encoding) as file:\n\twith codecs.open(file_name, \"r\", encoding) as file:\n\t\ttextbuffer = file.read()\n\n\treturn textbuffer\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\tfile_obj = open(file_name, 'rb')\n#\tfile_obj.seek(0, 2)\n#\tfile_size = file_obj.tell()\n#\tfile_obj.seek(0)\n#\tfile_bytes = file_obj.read(file_size)\n#\tfile_obj.close()\n#\t\n#\t# create text buffer\n#\ttext_buffer = gtk.TextBuffer()\n#\ttext_buffer.set_text(file_bytes)\n\ttext_buffer = gtk.TextBuffer()\n\ttext_buffer.set_text(open(file_name, 'rb').read())\n\treturn text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\tprint \"file_to_textbuffer\",file_name\n\tbuffer = gtk.TextBuffer()\n\tbuffer.set_text(open(file_name, \"r\").read())\n\treturn buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\n\twith open(file_name, encoding=encoding) as f:\n\t\treturn f.read()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\n\ttb = TextBuffer()\n\ttry:\n\t\tf = open(file_name, 'r', encoding=encoding)\n\texcept:\n\t\tf = open(file_name, 'r', encoding='utf-8')\n\tf.seek(0)\n\tlines = f.readlines()\n\tf.close()\n\tfor line in lines:\n\t\ttb.append(line)\n\treturn tb\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t \n\ttext_buffer = Gtk.TextBuffer()\n\ttext_buffer.set_text(get_file_content(file_name, encoding))\n\treturn text_buffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n#\tprint \"file_to_textbuffer\"\n\t\n\t# Create a new text buffer\n\ttext_buffer = gtk.TextBuffer()\n\t\n\t# Open the file\n\tfile_handler = codecs.open(file_name, mode=\"r\", encoding=encoding)\n\t\n\t# Get the contents of the file\n\tfile_contents = file_handler.read()\n\tfile_handler.close()\n\t\n\t# Insert the contents of the file into the text buffer\n\ttext_buffer.set_text(file_contents)\n\t\n\t# Return the text buffer\n\treturn text_buffer\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\twith open(file_name, 'r', encoding = encoding) as f:\n\t\t\ttextbuffer = f.read()\n\t\treturn textbuffer\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tis_exist = os.path.exists(file_name)\n\n\tif not is_exist:\n\t\tprint \"File name: %s does not exist!\" % file_name\n\t\texit(1)\n\n\treturn is_exist\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\treturn os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tfor root, dirs, files in os.walk('.'):\n\t\tif file_name in files:\n\t\t\treturn True\n\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\treturn os.path.exists(file_name)\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t # If file is present print it\n\tif os.path.isfile(file_name):\n\t\tprint(file_name+\" is present\")\n\telse:\n\t\tprint(file_name+\" is not present\")\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tfor root, dirs, files in os.walk(\".\"):\n\t\tif file_name in files:\n\t\t\treturn True\n\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\treturn os.path.exists(file_name)\n\t", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t\treturn os.path.exists(file_name)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tis_exist = os.path.exists(file_name)\n\n\tif not is_exist:\n\t\tprint \"File name: %s does not exist!\" % file_name\n\t\texit(1)\n\n\treturn is_exist\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\treturn os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tfor root, dirs, files in os.walk('.'):\n\t\tif file_name in files:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\treturn os.path.exists(file_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t # If file is present print it\n\tif os.path.isfile(file_name):\n\t\tprint(file_name+\" is present\")\n\telse:\n\t\tprint(file_name+\" is not present\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\tfor root, dirs, files in os.walk(\".\"):\n\t\tif file_name in files:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t \n\treturn os.path.exists(file_name)\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\t\treturn os.path.exists(file_name)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t \n\tlogging.root.handlers = []\n\tlogging.basicConfig(level=logging.DEBUG,\n\t\t\t\t\t\tformat='%(asctime)s %(levelname)s %(message)s',\n\t\t\t\t\t\tdatefmt='%Y-%m-%d %I:%M:%S %p',\n\t\t\t\t\t\tfilename='log.log',\n\t\t\t\t\t\tfilemode='w')\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\tcls.logger = logging.getLogger(cls.__name__)\n\t\tcls.logger.setLevel(logging.DEBUG)\n\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\n\t\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\t\tch.setFormatter(formatter)\n\t\tcls.logger.addHandler(ch)\n\n\t\tcls.logger.info(\"Logging initialized\")\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n#\tlogging.getLogger().handlers = []\n\tlogging.getLogger().setLevel(logging.NOTSET)\n\tlogging.getLogger().addHandler(logging.StreamHandler())\n\tlogging.getLogger().setLevel(logging.INFO)\n\tlogging.getLogger().handlers[0].setLevel(logging.INFO)\n\t\n\treturn\n\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n#\tcls.log_path = ''\n#\tcls.log_level = logging.INFO\n#\tcls.log_format = '[%(asctime)s] %(levelname)s: %(message)s'\n#\tcls.log_date_format = '%Y-%m-%d %H:%M:%S'\n#\tcls.log_file = ''\n#\tcls.log_file_level = logging.INFO\n#\tcls.log_file_format = '[%(asctime)s] %(levelname)s: %(message)s'\n#\tcls.log_file_date_format = '%Y-%m-%d %H:%M:%S'\n#\tcls.log_file_size = 1024 * 1024 * 10 # 10MB\n#\tcls.log_file_count = 5\n\n#\tcls.log_console = logging.INFO\n#\tcls.log_console_format = '[%(asctime)s] %(levelname)s: %(message)s'\n#\tcls.log_console_date_format = '%Y-%m-%d %H:%M:%S'\n\n#\tlogging.basicConfig(\n#\t\tfilename = cls.log_file,\n#\t\tlevel = cls.log_file_level,\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n### Logging system\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n\n\t# Reset the logging system\n\tlogging.shutdown()\n\tlogging.root.handlers = []\n\tlogging.root.setLevel(logging.DEBUG)\n\tlogging.root.propagate = True\n\n\t# Create a console handler\n\tch = logging.StreamHandler(sys.stdout)\n\tch.setLevel(logging.DEBUG)\n\t\n\t# Create a file handler\n\tfh = logging.FileHandler(cls.log_file, mode=\"w\")\n\tfh.setLevel(logging.DEBUG)\n\n\t# Set the format\n\tformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\")\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# Add the handlers to the logging system\n\tlogging.root.addHandler(ch)\n\tlogging.root.addHandler(fh)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t \n\tlogging.root.handlers = []\n\tlogging.basicConfig(level=logging.DEBUG,\n\t\t\t\t\t\tformat='%(asctime)s %(levelname)s %(message)s',\n\t\t\t\t\t\tdatefmt='%Y-%m-%d %I:%M:%S %p',\n\t\t\t\t\t\tfilename='log.log',\n\t\t\t\t\t\tfilemode='w')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\tcls.logger = logging.getLogger(cls.__name__)\n\t\tcls.logger.setLevel(logging.DEBUG)\n\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\n\t\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\t\tch.setFormatter(formatter)\n\t\tcls.logger.addHandler(ch)\n\n\t\tcls.logger.info(\"Logging initialized\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n#\tlogging.getLogger().handlers = []\n\tlogging.getLogger().setLevel(logging.NOTSET)\n\tlogging.getLogger().addHandler(logging.StreamHandler())\n\tlogging.getLogger().setLevel(logging.INFO)\n\tlogging.getLogger().handlers[0].setLevel(logging.INFO)\n\t\n\treturn\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n#\tcls.log_path = ''\n#\tcls.log_level = logging.INFO\n#\tcls.log_format = '[%(asctime)s] %(levelname)s: %(message)s'\n#\tcls.log_date_format = '%Y-%m-%d %H:%M:%S'\n#\tcls.log_file = ''\n#\tcls.log_file_level = logging.INFO\n#\tcls.log_file_format = '[%(asctime)s] %(levelname)s: %(message)s'\n#\tcls.log_file_date_format = '%Y-%m-%d %H:%M:%S'\n#\tcls.log_file_size = 1024 * 1024 * 10 # 10MB\n#\tcls.log_file_count = 5\n\n#\tcls.log_console = logging.INFO\n#\tcls.log_console_format = '[%(asctime)s] %(levelname)s: %(message)s'\n#\tcls.log_console_date_format = '%Y-%m-%d %H:%M:%S'\n\n#\tlogging.basicConfig(\n#\t\tfilename = cls.log_file,\n#\t\tlevel = cls.log_file_level,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n### Logging system\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\t\n\n\t# Reset the logging system\n\tlogging.shutdown()\n\tlogging.root.handlers = []\n\tlogging.root.setLevel(logging.DEBUG)\n\tlogging.root.propagate = True\n\n\t# Create a console handler\n\tch = logging.StreamHandler(sys.stdout)\n\tch.setLevel(logging.DEBUG)\n\t\n\t# Create a file handler\n\tfh = logging.FileHandler(cls.log_file, mode=\"w\")\n\tfh.setLevel(logging.DEBUG)\n\n\t# Set the format\n\tformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\")\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# Add the handlers to the logging system\n\tlogging.root.addHandler(ch)\n\tlogging.root.addHandler(fh)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn None\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn \"Py\"\n\tdef _getTargetInterface(self):\n\t\treturn IPlugInFactory\n\tdef _getTargetList(self):\n\t\treturn (self._getTargetClass(),)\n\tdef _getFactory(self):\n\t\treturn self._factory\n\tdef _getRegistry(self):\n\t\treturn self._registry\n\tdef _getUtility(self):\n\t\treturn self._utility\n\tdef _getMultiAdapter(self):\n\t\treturn self._multiadapter\n\tdef _removePlugin(self, plugin):\n\t\tself._factory.removePlugin(plugin)\n\tdef _getPlugin(self, name):\n\t\treturn self._factory.getPlugin(name)\n\tdef _getPlugins(self, name):\n\t\treturn self._factory.getPlugins(name)\n\tdef _getPluginNames(self, name):\n\t\treturn self._factory.getPluginNames(name)\n\tdef _loadPlugins(self, name):\n\t\treturn self._factory.loadPlugins(name)\n\tdef _registerUtility(self, utility, name=None):\n\t\treturn self._utility.registerUtility(utility, name)\n\tdef _unregisterUtility(self, utility):\n\t\treturn self._utility.unregisterUtility(utility)\n\tdef _getUtility(self, utility, name=None):\n\t\treturn self._utility.getUtility(utility, name)\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn self.implementation\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn \"Py\"\n\tdef _getTargetInterface(self):\n\t\treturn IPlugInFactory\n\tdef _getTargetList(self):\n\t\treturn (self._getTargetClass(),)\n\tdef _getFactory(self):\n\t\treturn self._factory\n\tdef _getRegistry(self):\n\t\treturn self._registry\n\tdef _getUtility(self):\n\t\treturn self._utility\n\tdef _getMultiAdapter(self):\n\t\treturn self._multiadapter\n\tdef _removePlugin(self, plugin):\n\t\tself._factory.removePlugin(plugin)\n\tdef _getPlugin(self, name):\n\t\treturn self._factory.getPlugin(name)\n\tdef _getPlugins(self, name):\n\t\treturn self._factory.getPlugins(name)\n\tdef _getPluginNames(self, name):\n\t\treturn self._factory.getPluginNames(name)\n\tdef _loadPlugins(self, name):\n\t\treturn self._factory.loadPlugins(name)\n\tdef _registerUtility(self, utility, name=None):\n\t\treturn self._utility.registerUtility(utility, name)\n\tdef _unregisterUtility(self, utility):\n\t\treturn self._utility.unregisterUtility(utility)\n\tdef _getUtility(self, utility, name=None):\n\t\treturn self._utility.getUtility(utility, name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn self.implementation\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\ttry:\n\t\t\torderings = orderings.split()\n\t\texcept:\n\t\t\treturn orderings\n\t\treturn list(OrderedDict.fromkeys(orderings))\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\tmerged = []\n\t\tfor ordering in orderings:\n\t\t\tfor element in ordering:\n\t\t\t\tif element not in merged:\n\t\t\t\t\tmerged.append(element)\n\t\treturn merged\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in merged:\n\t\t\t\tmerged.append(element)\n\treturn merged\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\n\t# convert to a set\n\torderings = set(orderings)\n\n\t# convert to a list\n\torderings = list(orderings)\n\n\t# sort it\n\torderings.sort()\n\n\treturn orderings\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tif orderings is None:\n\t\treturn None\n\t\n\tcombined = []\n\tfor ordering in orderings:\n\t\tcombined += ordering\n\t\tcombined = list(set(combined))\n\t\n\treturn combined\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\t#print(\"orderings\", orderings)\n\tmerged_orderings = []\n\tfor ordering in orderings:\n\t\tfor i in range(len(ordering)):\n\t\t\tif ordering[i] not in merged_orderings:\n\t\t\t\tmerged_orderings.append(ordering[i])\n\t#print(\"merged_orderings\", merged_orderings)\n\treturn merged_orderings\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in merged:\n\t\t\t\tmerged.append(element)\n\treturn merged\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in merged:\n\t\t\t\tmerged.append(element)\n\t\n\treturn merged\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n#\tfor ordering in orderings:\n#\t\tif not isinstance(ordering, list):\n#\t\t\traise ValueError(\"An ordering list must be passed to the mergeOrderings() method. Instead, a list of lists was passed.\")\n#\t\telif not isinstance(ordering[0], tuple):\n#\t\t\traise ValueError(\"Each ordering list must be a list of tuples. Instead, a list of lists was passed.\")\n#\t\telif len(ordering[0])!= 3:\n#\t\t\traise ValueError(\"Each ordering list must be a list of tuples. Instead, a list of lists was passed.\")\n#\t\telif ordering[0][0] in {None, ''}:\n#\t\t\traise ValueError(\"The ordering list must have a non-empty string for the first element of each tuple. Instead, an empty string was found.\")\n#\t\telif ordering[0][1] in {None, ''}:\n#\t\t\traise ValueError(\"The ordering list must have a non-empty string for the second element of each tuple. Instead, an empty string was found.\")\n#\t\telif ordering[0][2] in {None, ''}:\n#\t\t\traise ValueError(\"The ordering list must have a non-empty string for the third element of each tuple. Instead, an empty string was found.\")\n#\t\telse:\n#\t\t\tpass\n#\t\tif len("], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\ttry:\n\t\t\torderings = orderings.split()\n\t\texcept:\n\t\t\treturn orderings\n\t\treturn list(OrderedDict.fromkeys(orderings))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\tmerged = []\n\t\tfor ordering in orderings:\n\t\t\tfor element in ordering:\n\t\t\t\tif element not in merged:\n\t\t\t\t\tmerged.append(element)\n\t\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in merged:\n\t\t\t\tmerged.append(element)\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\n\t# convert to a set\n\torderings = set(orderings)\n\n\t# convert to a list\n\torderings = list(orderings)\n\n\t# sort it\n\torderings.sort()\n\n\treturn orderings\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tif orderings is None:\n\t\treturn None\n\t\n\tcombined = []\n\tfor ordering in orderings:\n\t\tcombined += ordering\n\t\tcombined = list(set(combined))\n\t\n\treturn combined\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\t#print(\"orderings\", orderings)\n\tmerged_orderings = []\n\tfor ordering in orderings:\n\t\tfor i in range(len(ordering)):\n\t\t\tif ordering[i] not in merged_orderings:\n\t\t\t\tmerged_orderings.append(ordering[i])\n\t#print(\"merged_orderings\", merged_orderings)\n\treturn merged_orderings\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in merged:\n\t\t\t\tmerged.append(element)\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t \n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in merged:\n\t\t\t\tmerged.append(element)\n\t\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n#\tfor ordering in orderings:\n#\t\tif not isinstance(ordering, list):\n#\t\t\traise ValueError(\"An ordering list must be passed to the mergeOrderings() method. Instead, a list of lists was passed.\")\n#\t\telif not isinstance(ordering[0], tuple):\n#\t\t\traise ValueError(\"Each ordering list must be a list of tuples. Instead, a list of lists was passed.\")\n#\t\telif len(ordering[0])!= 3:\n#\t\t\traise ValueError(\"Each ordering list must be a list of tuples. Instead, a list of lists was passed.\")\n#\t\telif ordering[0][0] in {None, ''}:\n#\t\t\traise ValueError(\"The ordering list must have a non-empty string for the first element of each tuple. Instead, an empty string was found.\")\n#\t\telif ordering[0][1] in {None, ''}:\n#\t\t\traise ValueError(\"The ordering list must have a non-empty string for the second element of each tuple. Instead, an empty string was found.\")\n#\t\telif ordering[0][2] in {None, ''}:\n#\t\t\traise ValueError(\"The ordering list must have a non-empty string for the third element of each tuple. Instead, an empty string was found.\")\n#\t\telse:\n#\t\t\tpass\n#\t\tif len(", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\treturn IDeclaration(object, (), ())\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\n\ndirectlyProvidedBy(object)\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\treturn IDeclaration(object, (), ())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\n\ndirectlyProvidedBy(object)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t/lib/bacon.py\n\"\"\"\nbacon.py\n\nContains the bacon class which is used to store the information\nof each bacon.\n\"\"\"\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\"\"\"\n\tclasses = []\n\tfor class_ in classes:\n\t\tif class_ not in classes:\n\t\t\tclasses.append(class_)\n\treturn classes\n\"\"\"\n\n\tminimal_classes = []\n\tfor class_ in classes:\n\t\tif class_ not in minimal_classes:\n\t\t\tminimal_classes.append(class_)\n\treturn minimal_classes\n\n\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\n\tbaseClasses = []\n\tfor c in classes:\n\t\tfor b in c.baseClasses:\n\t\t\tif b not in baseClasses:\n\t\t\t\tbaseClasses.append(b)\n\t\n\treturn baseClasses\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\n\tminimal = []\n\tfor c in classes:\n\t\tif not c.getSubClasses():\n\t\t\tminimal.append(c)\n\treturn minimal\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t/README.md\n# py-class-hierarchy\nPython Classes Hierarchy\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\n\tbase_classes = []\n\tfor cls in classes:\n\t\tif not issubclass(cls, (list,tuple)):\n\t\t\tbase_classes.append(cls)\n\n\treturn base_classes\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t/README.md\n# py-classes\nA collection of classes for the Python programming language.\n\n### Installation\n`pip install python-classes`\n\n### Classes\n* `BaseClass`\n* `BaseObject`\n* `BaseFile`\n* `BaseFolder`\n* `BaseProject`\n* `BaseDatabase`\n* `BaseTable`\n* `BaseColumn`\n* `BaseRow`\n* `BaseQuery`\n* `BaseCommand`\n* `BaseMessage`\n* `BaseNetwork`\n* `BaseNetworkAdapter`\n\n### Usage\n```python\nfrom classes import BaseObject\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t \n\tbase = {}\n\tfor b in classes:\n\t\tbase[b] = True\n\t\tfor a in classes[b]:\n\t\t\tbase[a] = False\n\treturn [ b for (b,v) in base.items() if v ]\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t/lib/bacon.py\n\"\"\"\nbacon.py\n\nContains the bacon class which is used to store the information\nof each bacon.\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\"\"\"\n\tclasses = []\n\tfor class_ in classes:\n\t\tif class_ not in classes:\n\t\t\tclasses.append(class_)\n\treturn classes\n\"\"\"\n\n\tminimal_classes = []\n\tfor class_ in classes:\n\t\tif class_ not in minimal_classes:\n\t\t\tminimal_classes.append(class_)\n\treturn minimal_classes\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\n\tbaseClasses = []\n\tfor c in classes:\n\t\tfor b in c.baseClasses:\n\t\t\tif b not in baseClasses:\n\t\t\t\tbaseClasses.append(b)\n\t\n\treturn baseClasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\n\tminimal = []\n\tfor c in classes:\n\t\tif not c.getSubClasses():\n\t\t\tminimal.append(c)\n\treturn minimal\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t/README.md\n# py-class-hierarchy\nPython Classes Hierarchy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t\n\n\tbase_classes = []\n\tfor cls in classes:\n\t\tif not issubclass(cls, (list,tuple)):\n\t\t\tbase_classes.append(cls)\n\n\treturn base_classes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t/README.md\n# py-classes\nA collection of classes for the Python programming language.\n\n### Installation\n`pip install python-classes`\n\n### Classes\n* `BaseClass`\n* `BaseObject`\n* `BaseFile`\n* `BaseFolder`\n* `BaseProject`\n* `BaseDatabase`\n* `BaseTable`\n* `BaseColumn`\n* `BaseRow`\n* `BaseQuery`\n* `BaseCommand`\n* `BaseMessage`\n* `BaseNetwork`\n* `BaseNetworkAdapter`\n\n### Usage\n```python\nfrom classes import BaseObject\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t \n\tbase = {}\n\tfor b in classes:\n\t\tbase[b] = True\n\t\tfor a in classes[b]:\n\t\t\tbase[a] = False\n\treturn [ b for (b,v) in base.items() if v ]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\n        if all:\n            for key, value in self.__dict__.items():\n                yield key, value\n        else:\n            for key in self.names():\n                yield key, self.__dict__[key]\n                \n    def names(self):\n\t\"\"\"\n\tReturns the list of names of the current class attributes.\n\t\"\"\"\n\t\n        return self.__dict__.keys()\n/src/tests/test_math/test_matrix/test_matrix_test.py\nimport unittest\n\nfrom tmath.matrix import Matrix\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn [(k, v.get('name', k), v.get('description', '')) for k, v in self.classAttributes.iteritems()]\n\t\telse:\n\t\t\treturn [(k, v.get('name', k), v.get('description', '')) for k, v in self.attributes.iteritems()]\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tnames = []\n\t\tdescriptions = []\n\t\tfor attr in self.__dict__:\n\t\t\tif not attr.startswith(\"__\"):\n\t\t\t\tnames.append(attr)\n\t\t\t\tdescriptions.append(getattr(self, attr))\n\t\tif all:\n\t\t\treturn zip(names, descriptions)\n\t\telse:\n\t\t\treturn zip(names, descriptions)\n\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tTest the class.\n\t\"\"\"\n\tclass Test(Base):\n\t\t\"\"\"\n\t\tA test class.\n\t\t\"\"\"\n\t\tdef __init__(self, name, age, height, weight):\n\t\t\t\"\"\"\n\t\t\tInitializes the class.\n\t\t\t\"\"\"\n\t\t\tsuper(Test, self).__init__(name, age, height, weight)\n\t\t\tself.name = name\n\t\t\tself.age = age\n\t\t\tself.height = height\n\t\t\tself.weight = weight\n\n\tt = Test(\"Jim\", 10, 111, 12)\n\tprint t.namesAndDescriptions(all=True)\n/python/modules/pexpect/pexpect_test.py\n#!/usr/bin/env python\n\"\"\"\nA pexpect example.\n\"\"\"\n\nimport pexpect\nimport sys\nimport time\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif not self.exists():\n\t\t\treturn []\n\t\tif not all:\n\t\t\treturn [(self.name(), self.description())]\n\t\telse:\n\t\t\treturn [(attr.name(), attr.description()) for attr in self.attributes()]\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\n#\tif all:\n#\t\treturn [self.getAttrNames(), self.getAttrDescriptions()]\n#\telse:\n#\t\treturn [self.getAttrName(self.index), self.getAttrDescription(self.index)]\n\t\n\treturn [self.getAttrName(self.index), self.getAttrDescription(self.index)]\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tdesc = []\n\t\tfor attr in dir(self):\n\t\t\tif not attr.startswith('_'):\n\t\t\t\tdesc.append((attr, self.__dict__[attr]))\n\t\treturn desc\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tnames = self.attributeNames(all=all)\n\t\tdescriptions = []\n\t\tfor name in names:\n\t\t\tdescription = self.description(name)\n\t\t\tif description:\n\t\t\t\tdescriptions.append(description)\n\n\t\treturn zip(names, descriptions)\n\n\tdef description(self, name):\n\t\t\"\"\"\n\t\tReturns the description of the current class attribute.\n\t\t\"\"\"\n\t\treturn self.getAttributeDescription(name)\n\n\tdef attributeNames(self, all=False):\n\t\t\"\"\"\n\t\tReturns the names of the current class attributes.\n\t\t\"\"\"\n\t\tif all:\n\t\t\treturn self.allAttributeNames()\n\n\t\telse:\n\t\t\treturn self.currentAttributeNames()\n\n\tdef allAttributeNames(self):\n\t\t\"\"\"\n\t\tReturns all class attribute names, including those of superclasses.\n\t\t\"\"\"\n\t\treturn self.allClassAttributeNames()\n\n\tdef currentAttributeNames(self):\n\t\t\"\"\"\n\t\tReturns the names of the current class attributes.\n\t\t\"\"\"\n\t\treturn self.classAttributeNames()\n\n\tdef allClassAttributeNames(self, all=False):\n\t\t\"\"\"\n\t\tReturns all class attribute names, including those of superclasses.\n\t\t\"\"\"\n\t\tnames = []\n\t\tfor cls in self.allSuperClasses():\n\t\t\tnames.extend(cls.classAttributeNames(all=all))\n\n\t\treturn names\n\n\tdef currentClassAttributeNames(", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn [(attr, getattr(self, attr)) for attr in dir(self)\n\t\t\t\tif not callable(getattr(self, attr))\n\t\t\t\tand not attr.startswith(\"__\")]\n\t\telse:\n\t\t\treturn [(attr, getattr(self, attr)) for attr in dir(self)\n\t\t\t\tif not callable(getattr(self, attr))\n\t\t\t\tand not attr.startswith(\"__\")\n\t\t\t\tand hasattr(getattr(self, attr), 'description')]\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tnames = []\n\t\tdescriptions = []\n\t\tfor name, value in self.__dict__.items():\n\t\t\tif value is not None:\n\t\t\t\tnames.append(name)\n\t\t\t\tdescriptions.append(value)\n\n\t\tif all:\n\t\t\treturn names, descriptions\n\t\telse:\n\t\t\treturn names[0], descriptions[0]\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn self.all_namesAndDescriptions()\n\t\telse:\n\t\t\treturn self.namesAndDescriptions()\n\n\tdef all_namesAndDescriptions(self):\n\t\t\"\"\"\n\t\tReturns the name and description of the current class attribute.\n\t\t\"\"\"\n\t\treturn self.classAttribute.namesAndDescriptions()\n\n\tdef set(self, value):\n\t\t\"\"\"\n\t\tSets the value of the class attribute.\n\t\t\"\"\"\n\t\tself.classAttribute.set(value)\n\n\tdef get(self):\n\t\t\"\"\"\n\t\tReturns the value of the class attribute.\n\t\t\"\"\"\n\t\treturn self.classAttribute.get()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\n        if all:\n            for key, value in self.__dict__.items():\n                yield key, value\n        else:\n            for key in self.names():\n                yield key, self.__dict__[key]\n                \n    def names(self):\n\t\"\"\"\n\tReturns the list of names of the current class attributes.\n\t\"\"\"\n\t\n        return self.__dict__.keys()\n/src/tests/test_math/test_matrix/test_matrix_test.py\nimport unittest\n\nfrom tmath.matrix import Matrix\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn [(k, v.get('name', k), v.get('description', '')) for k, v in self.classAttributes.iteritems()]\n\t\telse:\n\t\t\treturn [(k, v.get('name', k), v.get('description', '')) for k, v in self.attributes.iteritems()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tnames = []\n\t\tdescriptions = []\n\t\tfor attr in self.__dict__:\n\t\t\tif not attr.startswith(\"__\"):\n\t\t\t\tnames.append(attr)\n\t\t\t\tdescriptions.append(getattr(self, attr))\n\t\tif all:\n\t\t\treturn zip(names, descriptions)\n\t\telse:\n\t\t\treturn zip(names, descriptions)\n\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tTest the class.\n\t\"\"\"\n\tclass Test(Base):\n\t\t\"\"\"\n\t\tA test class.\n\t\t\"\"\"\n\t\tdef __init__(self, name, age, height, weight):\n\t\t\t\"\"\"\n\t\t\tInitializes the class.\n\t\t\t\"\"\"\n\t\t\tsuper(Test, self).__init__(name, age, height, weight)\n\t\t\tself.name = name\n\t\t\tself.age = age\n\t\t\tself.height = height\n\t\t\tself.weight = weight\n\n\tt = Test(\"Jim\", 10, 111, 12)\n\tprint t.namesAndDescriptions(all=True)\n/python/modules/pexpect/pexpect_test.py\n#!/usr/bin/env python\n\"\"\"\nA pexpect example.\n\"\"\"\n\nimport pexpect\nimport sys\nimport time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif not self.exists():\n\t\t\treturn []\n\t\tif not all:\n\t\t\treturn [(self.name(), self.description())]\n\t\telse:\n\t\t\treturn [(attr.name(), attr.description()) for attr in self.attributes()]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\n#\tif all:\n#\t\treturn [self.getAttrNames(), self.getAttrDescriptions()]\n#\telse:\n#\t\treturn [self.getAttrName(self.index), self.getAttrDescription(self.index)]\n\t\n\treturn [self.getAttrName(self.index), self.getAttrDescription(self.index)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tdesc = []\n\t\tfor attr in dir(self):\n\t\t\tif not attr.startswith('_'):\n\t\t\t\tdesc.append((attr, self.__dict__[attr]))\n\t\treturn desc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tnames = self.attributeNames(all=all)\n\t\tdescriptions = []\n\t\tfor name in names:\n\t\t\tdescription = self.description(name)\n\t\t\tif description:\n\t\t\t\tdescriptions.append(description)\n\n\t\treturn zip(names, descriptions)\n\n\tdef description(self, name):\n\t\t\"\"\"\n\t\tReturns the description of the current class attribute.\n\t\t\"\"\"\n\t\treturn self.getAttributeDescription(name)\n\n\tdef attributeNames(self, all=False):\n\t\t\"\"\"\n\t\tReturns the names of the current class attributes.\n\t\t\"\"\"\n\t\tif all:\n\t\t\treturn self.allAttributeNames()\n\n\t\telse:\n\t\t\treturn self.currentAttributeNames()\n\n\tdef allAttributeNames(self):\n\t\t\"\"\"\n\t\tReturns all class attribute names, including those of superclasses.\n\t\t\"\"\"\n\t\treturn self.allClassAttributeNames()\n\n\tdef currentAttributeNames(self):\n\t\t\"\"\"\n\t\tReturns the names of the current class attributes.\n\t\t\"\"\"\n\t\treturn self.classAttributeNames()\n\n\tdef allClassAttributeNames(self, all=False):\n\t\t\"\"\"\n\t\tReturns all class attribute names, including those of superclasses.\n\t\t\"\"\"\n\t\tnames = []\n\t\tfor cls in self.allSuperClasses():\n\t\t\tnames.extend(cls.classAttributeNames(all=all))\n\n\t\treturn names\n\n\tdef currentClassAttributeNames(", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn [(attr, getattr(self, attr)) for attr in dir(self)\n\t\t\t\tif not callable(getattr(self, attr))\n\t\t\t\tand not attr.startswith(\"__\")]\n\t\telse:\n\t\t\treturn [(attr, getattr(self, attr)) for attr in dir(self)\n\t\t\t\tif not callable(getattr(self, attr))\n\t\t\t\tand not attr.startswith(\"__\")\n\t\t\t\tand hasattr(getattr(self, attr), 'description')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tnames = []\n\t\tdescriptions = []\n\t\tfor name, value in self.__dict__.items():\n\t\t\tif value is not None:\n\t\t\t\tnames.append(name)\n\t\t\t\tdescriptions.append(value)\n\n\t\tif all:\n\t\t\treturn names, descriptions\n\t\telse:\n\t\t\treturn names[0], descriptions[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn self.all_namesAndDescriptions()\n\t\telse:\n\t\t\treturn self.namesAndDescriptions()\n\n\tdef all_namesAndDescriptions(self):\n\t\t\"\"\"\n\t\tReturns the name and description of the current class attribute.\n\t\t\"\"\"\n\t\treturn self.classAttribute.namesAndDescriptions()\n\n\tdef set(self, value):\n\t\t\"\"\"\n\t\tSets the value of the class attribute.\n\t\t\"\"\"\n\t\tself.classAttribute.set(value)\n\n\tdef get(self):\n\t\t\"\"\"\n\t\tReturns the value of the class attribute.\n\t\t\"\"\"\n\t\treturn self.classAttribute.get()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\treturn self.attributes(all).keys()\n\n\tdef attributes(self, all=False):\n\t\"\"\"\n\tReturn attributes of current class as dict.\n\t\"\"\"\n\t\treturn self.__dict__.keys()\n\n\tdef __getattr__(self, name):\n\t\"\"\"\n\tReturn attribute of current class.\n\t\"\"\"\n\t\treturn self.__dict__[name]\n\n\tdef __setattr__(self, name, value):\n\t\"\"\"\n\tSet attribute of current class.\n\t\"\"\"\n\t\tself.__dict__[name] = value\n\n\tdef __delattr__(self, name):\n\t\"\"\"\n\tDelete attribute of current class.\n\t\"\"\"\n\t\tdel self.__dict__[name]\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturn string representation of current class.\n\t\"\"\"\n\t\treturn self.attributes()\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn string representation of current class.\n\t\"\"\"\n\t\treturn self.__str__()\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn length of current class.\n\t\"\"\"\n\t\treturn len(self.attributes())\n\n\tdef __hash__(self):\n\t\"\"\"\n\tReturn hash value of current class.\n\t\"\"\"\n\t\treturn hash(self.__str__())\n\n\tdef __eq__(self, other):\n\t\"\"\"\n\tReturn True of current class is equal to other", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn dir(self)\n\t\telse:\n\t\t\treturn self.__dict__.keys()\n\t\n\tdef __getattribute__(self, attr):\n\t\t\"\"\"\n\t\tOverride the default attribute access.\n\n\t\tIt is necessary to override getattr to implement a custom attribute\n\t\tstorage model.  Also note that this is a proxy class, so any attribute\n\t\tthat is delegated back to the original class is only readable (not\n\t\twritable)\n\t\t\"\"\"\n\t\tprint(\"getattribute\")\n\t\tif attr == 'data':\n\t\t\treturn self.__dict__['data']\n\t\telse:\n\t\t\treturn self.__dict__['data'].__getattribute__(attr)\n\t\n\tdef __setattr__(self, attr, value):\n\t\t\"\"\"\n\t\tOverride the default attribute access.\n\n\t\tIt is necessary to override getattr to implement a custom attribute\n\t\tstorage model.  Also note that this is a proxy class, so any attribute\n\t\tthat is delegated back to the original class is only readable (not\n\t\twritable)\n\t\t\"\"\"\n\t\tprint(\"setattr\")\n\t\tif attr == 'data':\n\t\t\tself.__dict__[attr] = value\n\t\telse:\n\t\t\tself.__dict__['data'].__setattr__(attr, value)\n\t\n\tdef __delattr__(self, attr):\n\t\t\"\"\"\n\t\tOverride the default attribute access.\n\n\t\tIt is necessary to override getattr to implement a", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn self.__dict__.keys()\n\t\telse:\n\t\t\treturn filter(lambda x: not x.startswith('_'), self.__dict__.keys())\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n        res = []\n        for attr in dir(self):\n\t    if not attr.startswith('_'):\n\t\tres.append(attr)\n\tif all:\n\t    for attr in self.__dict__:\n\t\tif not attr.startswith('_'):\n\t\t    res.append(attr)\n\treturn res\n\n    def get_attr(self, attr):\n\t\"\"\"\n\tReturn the attribute value of current class.\n\t\"\"\"\n\t\n        return getattr(self, attr)\n\n    def set_attr(self, attr, value):\n\t\"\"\"\n\tSet the attribute value of current class.\n\t\"\"\"\n\t\n        setattr(self, attr, value)\n\n    def rename_attr(self, attr, new_name):\n\t\"\"\"\n\tRename the attribute of current class.\n\t\"\"\"\n\t\n        setattr(self, new_name, getattr(self, attr))\n        delattr(self, attr)\n\t\n    def get_attrs(self, attrs):\n\t\"\"\"\n\tReturn the attribute values of current class.\n\t\"\"\"\n\t\n        res = {}\n        for attr in attrs:\n\t    res[attr] = self.get_attr(attr)\n\treturn res\n\n    def set_attrs(self, attrs):\n\t\"\"\"\n\tSet the attribute values of current class.\n\t\"\"\"\n\t\n        for attr in attrs:\n\t    self.set_attr(attr, attrs[attr])\n\n    def rename_attrs(self, attrs, new_", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\treturn self.get_attributes(all=all, hidden=False)\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tnames = set(self.__dict__)\n\t\tfor base in self.__class__.__bases__:\n\t\t\tnames.update(base.__dict__)\n\t\tnames = sorted(names)\n\t\tif all:\n\t\t\treturn tuple(names)\n\t\telse:\n\t\t\treturn tuple(names[3:])\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n@classmethod", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\treturn [attr for attr in self.__dict__.keys() if not callable(attr) and not attr.startswith(\"__\") and not all]\n\n\tdef __getattr__(self, name):\n\t\t\"\"\"\n\t\tReturn a new instance of type named by name.\n\t\t\"\"\"\n\t\treturn type(name, (self,), {})()\n\n\tdef __getitem__(self, name):\n\t\t\"\"\"\n\t\tReturn the attribute named by name.\n\t\t\"\"\"\n\t\treturn getattr(self, name)\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tres = []\n\n\t\tif(all):\n\t\t\tfor val in self.__dict__:\n\t\t\t\tres.append(val)\n\t\telse:\n\t\t\tfor val in self.__dict__:\n\t\t\t\tif(not val.startswith(\"__\")):\n\t\t\t\t\tres.append(val)\n\n\t\treturn res\n\n\tdef __repr__(self):\n\t\treturn str(self.names(True))\n\n\nif(__name__ == \"__main__\"):\n\tprint(\"Testing Class\")\n\n\tclass TestClass(object):\n\t\tdef __init__(self, name=\"\"):\n\t\t\tself.name = name\n\t\t\tself.age = 28\n\n\t\tdef print_name(self):\n\t\t\treturn self.name\n\n\ttest = TestClass()\n\n\tprint(test)\n\tprint(test.print_name())\n/python/data_structures/bst.py\n#!/usr/bin/python3\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn [self.name for self.name in dir(self)]\n\t\telse:\n\t\t\treturn [self.name for self.name in dir(self) if not self.name.startswith('_')]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\treturn self.attributes(all).keys()\n\n\tdef attributes(self, all=False):\n\t\"\"\"\n\tReturn attributes of current class as dict.\n\t\"\"\"\n\t\treturn self.__dict__.keys()\n\n\tdef __getattr__(self, name):\n\t\"\"\"\n\tReturn attribute of current class.\n\t\"\"\"\n\t\treturn self.__dict__[name]\n\n\tdef __setattr__(self, name, value):\n\t\"\"\"\n\tSet attribute of current class.\n\t\"\"\"\n\t\tself.__dict__[name] = value\n\n\tdef __delattr__(self, name):\n\t\"\"\"\n\tDelete attribute of current class.\n\t\"\"\"\n\t\tdel self.__dict__[name]\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturn string representation of current class.\n\t\"\"\"\n\t\treturn self.attributes()\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn string representation of current class.\n\t\"\"\"\n\t\treturn self.__str__()\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn length of current class.\n\t\"\"\"\n\t\treturn len(self.attributes())\n\n\tdef __hash__(self):\n\t\"\"\"\n\tReturn hash value of current class.\n\t\"\"\"\n\t\treturn hash(self.__str__())\n\n\tdef __eq__(self, other):\n\t\"\"\"\n\tReturn True of current class is equal to other", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn dir(self)\n\t\telse:\n\t\t\treturn self.__dict__.keys()\n\t\n\tdef __getattribute__(self, attr):\n\t\t\"\"\"\n\t\tOverride the default attribute access.\n\n\t\tIt is necessary to override getattr to implement a custom attribute\n\t\tstorage model.  Also note that this is a proxy class, so any attribute\n\t\tthat is delegated back to the original class is only readable (not\n\t\twritable)\n\t\t\"\"\"\n\t\tprint(\"getattribute\")\n\t\tif attr == 'data':\n\t\t\treturn self.__dict__['data']\n\t\telse:\n\t\t\treturn self.__dict__['data'].__getattribute__(attr)\n\t\n\tdef __setattr__(self, attr, value):\n\t\t\"\"\"\n\t\tOverride the default attribute access.\n\n\t\tIt is necessary to override getattr to implement a custom attribute\n\t\tstorage model.  Also note that this is a proxy class, so any attribute\n\t\tthat is delegated back to the original class is only readable (not\n\t\twritable)\n\t\t\"\"\"\n\t\tprint(\"setattr\")\n\t\tif attr == 'data':\n\t\t\tself.__dict__[attr] = value\n\t\telse:\n\t\t\tself.__dict__['data'].__setattr__(attr, value)\n\t\n\tdef __delattr__(self, attr):\n\t\t\"\"\"\n\t\tOverride the default attribute access.\n\n\t\tIt is necessary to override getattr to implement a", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn self.__dict__.keys()\n\t\telse:\n\t\t\treturn filter(lambda x: not x.startswith('_'), self.__dict__.keys())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n        res = []\n        for attr in dir(self):\n\t    if not attr.startswith('_'):\n\t\tres.append(attr)\n\tif all:\n\t    for attr in self.__dict__:\n\t\tif not attr.startswith('_'):\n\t\t    res.append(attr)\n\treturn res\n\n    def get_attr(self, attr):\n\t\"\"\"\n\tReturn the attribute value of current class.\n\t\"\"\"\n\t\n        return getattr(self, attr)\n\n    def set_attr(self, attr, value):\n\t\"\"\"\n\tSet the attribute value of current class.\n\t\"\"\"\n\t\n        setattr(self, attr, value)\n\n    def rename_attr(self, attr, new_name):\n\t\"\"\"\n\tRename the attribute of current class.\n\t\"\"\"\n\t\n        setattr(self, new_name, getattr(self, attr))\n        delattr(self, attr)\n\t\n    def get_attrs(self, attrs):\n\t\"\"\"\n\tReturn the attribute values of current class.\n\t\"\"\"\n\t\n        res = {}\n        for attr in attrs:\n\t    res[attr] = self.get_attr(attr)\n\treturn res\n\n    def set_attrs(self, attrs):\n\t\"\"\"\n\tSet the attribute values of current class.\n\t\"\"\"\n\t\n        for attr in attrs:\n\t    self.set_attr(attr, attrs[attr])\n\n    def rename_attrs(self, attrs, new_", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\treturn self.get_attributes(all=all, hidden=False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tnames = set(self.__dict__)\n\t\tfor base in self.__class__.__bases__:\n\t\t\tnames.update(base.__dict__)\n\t\tnames = sorted(names)\n\t\tif all:\n\t\t\treturn tuple(names)\n\t\telse:\n\t\t\treturn tuple(names[3:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\treturn [attr for attr in self.__dict__.keys() if not callable(attr) and not attr.startswith(\"__\") and not all]\n\n\tdef __getattr__(self, name):\n\t\t\"\"\"\n\t\tReturn a new instance of type named by name.\n\t\t\"\"\"\n\t\treturn type(name, (self,), {})()\n\n\tdef __getitem__(self, name):\n\t\t\"\"\"\n\t\tReturn the attribute named by name.\n\t\t\"\"\"\n\t\treturn getattr(self, name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tres = []\n\n\t\tif(all):\n\t\t\tfor val in self.__dict__:\n\t\t\t\tres.append(val)\n\t\telse:\n\t\t\tfor val in self.__dict__:\n\t\t\t\tif(not val.startswith(\"__\")):\n\t\t\t\t\tres.append(val)\n\n\t\treturn res\n\n\tdef __repr__(self):\n\t\treturn str(self.names(True))\n\n\nif(__name__ == \"__main__\"):\n\tprint(\"Testing Class\")\n\n\tclass TestClass(object):\n\t\tdef __init__(self, name=\"\"):\n\t\t\tself.name = name\n\t\t\tself.age = 28\n\n\t\tdef print_name(self):\n\t\t\treturn self.name\n\n\ttest = TestClass()\n\n\tprint(test)\n\tprint(test.print_name())\n/python/data_structures/bst.py\n#!/usr/bin/python3\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\tif all:\n\t\t\treturn [self.name for self.name in dir(self)]\n\t\telse:\n\t\t\treturn [self.name for self.name in dir(self) if not self.name.startswith('_')]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif isinstance(sequence, (tuple, list)):\n\t\treturn [_normalizeargs(x, output=output) for x in sequence]\n\telif isinstance(sequence, Declaration):\n\t\treturn sequence\n\telif isinstance(sequence, Interface):\n\t\treturn sequence\n\telif isinstance(sequence, (InterfaceSpec, ImplementsSpec)):\n\t\treturn (sequence, output)\n\t\n\n\traise TypeError('Invalid type %s for argument' % type(sequence))\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tdef _normalize(spec):\n\t\t\"\"\"\n\t\tNormalize a spec\n\n\t\tThe spec might be a Declaration, a tuple, or a single interface.\n\t\t\"\"\"\n\t\t\n\t\tif isinstance(spec, tuple):\n\t\t\treturn tuple(_normalize(item) for item in spec)\n\t\tif isinstance(spec, Declaration):\n\t\t\treturn spec\n\t\tif isinstance(spec, Interface):\n\t\t\treturn spec\n\t\traise ValueError('Invalid spec %r' % spec)\n\t\n\treturn tuple(_normalize(spec) for spec in sequence)\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n_normalize_args = _normalizeargs\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n#\tif isinstance(sequence, Declaration):\n#\t\treturn [sequence]\n#\tif isinstance(sequence, tuple):\n#\t\treturn list(sequence)\n#\treturn sequence\n\n#\tfor item in sequence:\n#\t\tif isinstance(item, Declaration):\n#\t\t\tif output is None:\n#\t\t\t\toutput = []\n#\t\t\toutput.append(item)\n#\t\telif isinstance(item, tuple):\n#\t\t\tfor _item in item:\n#\t\t\t\tif isinstance(_item, Declaration):\n#\t\t\t\t\tif output is None:\n#\t\t\t\t\t\toutput = []\n#\t\t\t\t\toutput.append(_item)\n#\t\telse:\n#\t\t\t# not a declaration or tuple\n#\t\t\tif output is None:\n#\t\t\t\toutput = []\n#\t\t\toutput.append(item)\n#\treturn output\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif isinstance(sequence, (tuple, list)):\n\t\t# Expand tuple\n\t\tsequence = [ _normalizeargs(item) for item in sequence ]\n\telif isinstance(sequence, Interface):\n\t\t# Expand interface\n\t\tsequence = sequence.implements\n\telif isinstance(sequence, Declaration):\n\t\t# Expand declaration\n\t\tsequence = sequence.arguments\n\telse:\n\t\t# Expand single argument\n\t\tsequence = [ sequence ]\n\n\tif output is None:\n\t\t# Return a copy\n\t\treturn list(sequence)\n\telse:\n\t\t# Return into an existing list\n\t\toutput.extend(sequence)\n\t\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t/pypi_install_script/py3status-weather-1.0.0.tar/setup.py\nfrom setuptools import setup\n\nsetup(\n    name='py3status-weather',\n    version='1.0.0',\n    url='https://github.com/lukedotpro/py3status-weather',\n    license='MIT',\n    author='',\n    author_email='',\n    description='py3status module for displaying weather',\n    py_modules=['py3status_weather'],\n    install_requires=['py3status'],\n    packages=['py3status_weather'],\n    # data_files=[('share/py3status/py3status_weather', [\n    #     'py3status_weather/py3status_weather.py'\n    # ])],\n    # entry_points={\n    #     'py3.status': [\n    #         'weather = py3status_weather:Py3status',\n    #     ]\n    # },\n    classifiers=[\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n    ],\n)\n/pycfiles/", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif isinstance(sequence, (tuple, list)):\n\t\treturn [_normalizeargs(x, output=output) for x in sequence]\n\telif isinstance(sequence, Declaration):\n\t\treturn sequence\n\telif isinstance(sequence, Interface):\n\t\treturn sequence\n\telif isinstance(sequence, (InterfaceSpec, ImplementsSpec)):\n\t\treturn (sequence, output)\n\t\n\n\traise TypeError('Invalid type %s for argument' % type(sequence))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tdef _normalize(spec):\n\t\t\"\"\"\n\t\tNormalize a spec\n\n\t\tThe spec might be a Declaration, a tuple, or a single interface.\n\t\t\"\"\"\n\t\t\n\t\tif isinstance(spec, tuple):\n\t\t\treturn tuple(_normalize(item) for item in spec)\n\t\tif isinstance(spec, Declaration):\n\t\t\treturn spec\n\t\tif isinstance(spec, Interface):\n\t\t\treturn spec\n\t\traise ValueError('Invalid spec %r' % spec)\n\t\n\treturn tuple(_normalize(spec) for spec in sequence)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n_normalize_args = _normalizeargs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n#\tif isinstance(sequence, Declaration):\n#\t\treturn [sequence]\n#\tif isinstance(sequence, tuple):\n#\t\treturn list(sequence)\n#\treturn sequence\n\n#\tfor item in sequence:\n#\t\tif isinstance(item, Declaration):\n#\t\t\tif output is None:\n#\t\t\t\toutput = []\n#\t\t\toutput.append(item)\n#\t\telif isinstance(item, tuple):\n#\t\t\tfor _item in item:\n#\t\t\t\tif isinstance(_item, Declaration):\n#\t\t\t\t\tif output is None:\n#\t\t\t\t\t\toutput = []\n#\t\t\t\t\toutput.append(_item)\n#\t\telse:\n#\t\t\t# not a declaration or tuple\n#\t\t\tif output is None:\n#\t\t\t\toutput = []\n#\t\t\toutput.append(item)\n#\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif isinstance(sequence, (tuple, list)):\n\t\t# Expand tuple\n\t\tsequence = [ _normalizeargs(item) for item in sequence ]\n\telif isinstance(sequence, Interface):\n\t\t# Expand interface\n\t\tsequence = sequence.implements\n\telif isinstance(sequence, Declaration):\n\t\t# Expand declaration\n\t\tsequence = sequence.arguments\n\telse:\n\t\t# Expand single argument\n\t\tsequence = [ sequence ]\n\n\tif output is None:\n\t\t# Return a copy\n\t\treturn list(sequence)\n\telse:\n\t\t# Return into an existing list\n\t\toutput.extend(sequence)\n\t\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t/pypi_install_script/py3status-weather-1.0.0.tar/setup.py\nfrom setuptools import setup\n\nsetup(\n    name='py3status-weather',\n    version='1.0.0',\n    url='https://github.com/lukedotpro/py3status-weather',\n    license='MIT',\n    author='',\n    author_email='',\n    description='py3status module for displaying weather',\n    py_modules=['py3status_weather'],\n    install_requires=['py3status'],\n    packages=['py3status_weather'],\n    # data_files=[('share/py3status/py3status_weather', [\n    #     'py3status_weather/py3status_weather.py'\n    # ])],\n    # entry_points={\n    #     'py3.status': [\n    #         'weather = py3status_weather:Py3status',\n    #     ]\n    # },\n    classifiers=[\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n    ],\n)\n/pycfiles/", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\ttry:\n#\t\timport scipy.optimize as opt\n\timport scipy.optimize as opt\n#\texcept:\n#\t\ttry:\n#\t\t\timport c.optimize as opt\n#\t\texcept:\n#\t\t\topt = False\n\t\n\treturn opt\n\n_optimizations_available = _c_optimizations_available()\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tlogging.info(\"C optimizations available: \" + str(_c_optimizations_available))\n\t\n\t# NOTE: This is the old way of doing things, which is now deprecated.\n\t# The new way is to just return the module itself.\n\t#if _c_optimizations_available:\n\t#\treturn _c_optimization_module\n\t#else:\n\t#\treturn False\n\n\treturn _c_optimization_module\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tif _c_optimizations_available():\n#\t\treturn _c_optimizations\n#\t\n#\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n\tname = 'openxdata',\n\tversion = '0.1',\n\tpackages = find_packages(),\n\tpackage_data = {'openxdata': ['*.py']},\n\tlicense = 'MIT',\n\tdescription = 'OpenXData is a Python package for the analysis of financial time series data.',\n\tlong_description = open('README.rst').read(),\n\tauthor = '',\n\tauthor_email = '',\n\turl = 'https://github.com/JGilbert100/openxdata',\n\tdownload_url = 'https://github.com/JGilbert100/openxdata/archive/0.1.tar.gz',\n\tkeywords = ['open', 'openxdata', 'financial', 'timeseries', 'analysis', 'finance', 'quant'],\n\tclassifiers = [\n\t\t'Development Status :: 3 - Alpha',\n\t\t'License :: OSI Approved :: MIT License',\n\t\t'Operating System :: OS Independent',\n\t\t'Programming Language :: Python',\n\t\t'Programming Language :: Python :: 2',\n\t\t'Programming Language :: Python :: 2.7',\n\t\t'Programming Language :: Python :: 3',\n\t\t'Programming Language :: Python :: 3.3',\n\t\t'", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\ttry:\n#\t\timport _c_optimizations as _c_optimizations\n#\texcept ImportError:\n#\t\treturn False\n#\t\n#\treturn _c_optimizations\n\n\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tif sys.platform.startswith('win'):\n#\t\treturn False\n#\telse:\n\treturn True\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t \n\ttry:\n\t\tfrom. import c_optimizations\n\t\treturn c_optimizations\n\texcept:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tif _c_optimizations_available_flag:\n#\t\treturn _c_optimizations\n#\telse:\n#\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t \n\ttry:\n\t\timport c_optimizations\n\texcept ImportError as e:\n\t\treturn False\n\telse:\n\t\treturn c_optimizations\n\n# If available, import the C optimization module.\n_c_optimizations = _c_optimizations_available()\ndel _c_optimizations_available\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\ttry:\n#\t\timport scipy.optimize as opt\n\timport scipy.optimize as opt\n#\texcept:\n#\t\ttry:\n#\t\t\timport c.optimize as opt\n#\t\texcept:\n#\t\t\topt = False\n\t\n\treturn opt\n\n_optimizations_available = _c_optimizations_available()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tlogging.info(\"C optimizations available: \" + str(_c_optimizations_available))\n\t\n\t# NOTE: This is the old way of doing things, which is now deprecated.\n\t# The new way is to just return the module itself.\n\t#if _c_optimizations_available:\n\t#\treturn _c_optimization_module\n\t#else:\n\t#\treturn False\n\n\treturn _c_optimization_module\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tif _c_optimizations_available():\n#\t\treturn _c_optimizations\n#\t\n#\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n\tname = 'openxdata',\n\tversion = '0.1',\n\tpackages = find_packages(),\n\tpackage_data = {'openxdata': ['*.py']},\n\tlicense = 'MIT',\n\tdescription = 'OpenXData is a Python package for the analysis of financial time series data.',\n\tlong_description = open('README.rst').read(),\n\tauthor = '',\n\tauthor_email = '',\n\turl = 'https://github.com/JGilbert100/openxdata',\n\tdownload_url = 'https://github.com/JGilbert100/openxdata/archive/0.1.tar.gz',\n\tkeywords = ['open', 'openxdata', 'financial', 'timeseries', 'analysis', 'finance', 'quant'],\n\tclassifiers = [\n\t\t'Development Status :: 3 - Alpha',\n\t\t'License :: OSI Approved :: MIT License',\n\t\t'Operating System :: OS Independent',\n\t\t'Programming Language :: Python',\n\t\t'Programming Language :: Python :: 2',\n\t\t'Programming Language :: Python :: 2.7',\n\t\t'Programming Language :: Python :: 3',\n\t\t'Programming Language :: Python :: 3.3',\n\t\t'", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\ttry:\n#\t\timport _c_optimizations as _c_optimizations\n#\texcept ImportError:\n#\t\treturn False\n#\t\n#\treturn _c_optimizations\n\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tif sys.platform.startswith('win'):\n#\t\treturn False\n#\telse:\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t \n\ttry:\n\t\tfrom. import c_optimizations\n\t\treturn c_optimizations\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t\n#\tif _c_optimizations_available_flag:\n#\t\treturn _c_optimizations\n#\telse:\n#\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\t \n\ttry:\n\t\timport c_optimizations\n\texcept ImportError as e:\n\t\treturn False\n\telse:\n\t\treturn c_optimizations\n\n# If available, import the C optimization module.\n_c_optimizations = _c_optimizations_available()\ndel _c_optimizations_available\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\treturn False  # TODO: set to True when C optimizations are ready.\n\n\treturn True\n\t\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\treturn _is_cython_enabled() and _should_use_c_optimizations()\n\treturn _should_use_c_optimizations()\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\tif _should_attempt_c_optimizations():\n#\t\treturn True\n#\telse:\n#\t\treturn False\n#\t\treturn True\n\treturn False\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\tif not _should_attempt_python_optimizations():\n#\t\treturn True\n#\t\n#\tif os.environ.has_key('PYTHONOPTIMIZE'):\n#\t\treturn True\n#\t\n#\treturn False\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n\n\t# Check for the presence of the Cython module.\n\tif not Cython:\n\t\treturn False\n\n\t# Check for the presence of the C file.\n\tif not os.path.exists(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"py_c_optimizations.c\")):\n\t\treturn False\n\n\t# Check for the presence of the C file.\n\tif not os.path.exists(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"py_c_optimizations.cpp\")):\n\t\treturn False\n\n\t# Check if we are in a debug environment.\n\tif sys.flags.debug:\n\t\treturn False\n\n\t# Check if we are using Python 2.\n\tif sys.version_info[0] < 3:\n\t\treturn True\n\n\treturn False\n\n\n# Initialize the C optimizations if we need to.\nif _should_attempt_c_optimizations():\n\timport py_c_optimizations\n\n\n# Initialize the C++ optimizations if we need to.\nif _should_attempt_cpp_optimizations():\n\timport py_cpp_optimizations\n\n\n# Define a function to determine if a value is a string.", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t \n\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\treturn False  # TODO: set to True when C optimizations are ready.\n\n\treturn True\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\treturn _is_cython_enabled() and _should_use_c_optimizations()\n\treturn _should_use_c_optimizations()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\tif _should_attempt_c_optimizations():\n#\t\treturn True\n#\telse:\n#\t\treturn False\n#\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n#\tif not _should_attempt_python_optimizations():\n#\t\treturn True\n#\t\n#\tif os.environ.has_key('PYTHONOPTIMIZE'):\n#\t\treturn True\n#\t\n#\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n\n\t# Check for the presence of the Cython module.\n\tif not Cython:\n\t\treturn False\n\n\t# Check for the presence of the C file.\n\tif not os.path.exists(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"py_c_optimizations.c\")):\n\t\treturn False\n\n\t# Check for the presence of the C file.\n\tif not os.path.exists(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"py_c_optimizations.cpp\")):\n\t\treturn False\n\n\t# Check if we are in a debug environment.\n\tif sys.flags.debug:\n\t\treturn False\n\n\t# Check if we are using Python 2.\n\tif sys.version_info[0] < 3:\n\t\treturn True\n\n\treturn False\n\n\n# Initialize the C optimizations if we need to.\nif _should_attempt_c_optimizations():\n\timport py_c_optimizations\n\n\n# Initialize the C++ optimizations if we need to.\nif _should_attempt_cpp_optimizations():\n\timport py_cpp_optimizations\n\n\n# Define a function to determine if a value is a string.", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t \n\treturn True\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t\n#\treturn os.environ.get(\"PURE_PYTHON\",\"0\")!= \"0\"\n\treturn False\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t0\n\treturn os.getenv(\"PURE_PYTHON\", \"0\")!= \"0\"\n\nif __name__ == \"__main__\":\n\texit(main())\nimport os\nimport sys\nimport platform\nimport traceback\nimport subprocess\nimport unittest\n\nimport c_optimizations\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t \n\treturn os.environ.get('PURE_PYTHON') and not os.environ.get('PURE_PYTHON').lower() == '0'\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t \n\tif os.environ.get('PURE_PYTHON', None):\n\t\treturn True\n\treturn False\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t\n\n\tif get_env(\"PURE_PYTHON\") and get_env(\"PURE_PYTHON\")!= \"0\":\n\t\treturn True\n\telse:\n\t\treturn False\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t\nis_pure_python = _c_optimizations_ignored()\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t\n#\treturn os.environ.get(\"PURE_PYTHON\",\"0\")!= \"0\"\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t0\n\treturn os.getenv(\"PURE_PYTHON\", \"0\")!= \"0\"\n\nif __name__ == \"__main__\":\n\texit(main())\nimport os\nimport sys\nimport platform\nimport traceback\nimport subprocess\nimport unittest\n\nimport c_optimizations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t \n\treturn os.environ.get('PURE_PYTHON') and not os.environ.get('PURE_PYTHON').lower() == '0'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t \n\tif os.environ.get('PURE_PYTHON', None):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t\n\n\tif get_env(\"PURE_PYTHON\") and get_env(\"PURE_PYTHON\")!= \"0\":\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\t\nis_pure_python = _c_optimizations_ignored()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n#\tThis version is not required.\n#\treturn 1;\n\treturn 0;\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n#\tif sys.platform == 'win32':\n#\t\treturn False\n#\telse:\n#\t\treturn True\n\t\n\treturn True\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n#\treturn True\n\treturn False\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n\n\n\tif _c_optimizations_required_val == None:\n\t\t_c_optimizations_required_val = _c_optimizations_required_impl()\n\n\treturn _c_optimizations_required_val\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n#\tThis version is not required.\n#\treturn 1;\n\treturn 0;\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n#\tif sys.platform == 'win32':\n#\t\treturn False\n#\telse:\n#\t\treturn True\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n#\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t\n\n\n\tif _c_optimizations_required_val == None:\n\t\t_c_optimizations_required_val = _c_optimizations_required_impl()\n\n\treturn _c_optimizations_required_val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.__dict__[\"context\"] = {}\n\t\tself.__dict__[\"bins\"] = {}\n\t\tself.__dict__[\"bins\"] = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.bins = self.make_bins()\n\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\tself.bins = self.make_bins()\n\tself._cache = {}\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.bins = self.make_bins(self.initial_value)\n\n\tdef make_bins(self, value):\n\t\t\"\"\"\n\t\tInitializes the bins dict with a value.\n\t\t\"\"\"\n\t\treturn {k: value for k in self.bin_keys}\n\n\tdef get_bin_keys(self):\n\t\t\"\"\"\n\t\tGenerates a list of the keys of the bins dict.\n\t\t\"\"\"\n\t\treturn list(self.bins.keys())\n\n\tdef get_bin_values(self):\n\t\t\"\"\"\n\t\tGenerates a list of values of the bins dict.\n\t\t\"\"\"\n\t\treturn list(self.bins.values())\n\n\tdef get_bins(self):\n\t\t\"\"\"\n\t\tReturns the bins dict.\n\t\t\"\"\"\n\t\treturn self.bins\n\n\tdef get_bin(self, key):\n\t\t\"\"\"\n\t\tReturns the value of the bin with the specified key.\n\t\t\"\"\"\n\t\treturn self.bins[key]\n\n\tdef set_initial_value(self, value):\n\t\t\"\"\"\n\t\tSets the initial value to the specified value.\n\t\t\"\"\"\n\t\tself.initial_value = value\n\n\tdef get_initial_value(self):\n\t\t\"\"\"\n\t\tReturns the initial value.\n\t\t\"\"\"\n\t\treturn self.initial_value\n\n\tdef set_bin_keys(self,", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\tself.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.bins = self.make_bins()\n\n\tdef __repr__(self):\n\t\treturn 'Context: {}'.format(self.context)/README.md\n# Contextual Bins\n\n## Contextual Bins for Python\n\nThis package contains a class `ContextualBins` for contextualizing the values of a given list. The `ContextualBins` class is able to bin any kind of values, but the most common use case is binning numerical values. \n\n## Installation\n\n`pip install contextualbins`\n\n## Usage\n\n```python\nfrom contextualbins import ContextualBins\n\nvalues = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\n\nbins = ContextualBins(values, bins=5, initial_value=1)\n\n# bins.context returns the context of the values\nprint(bins.context)\n\n# bins.bins returns the bins of the class\nprint(bins.bins)\n\n# bins.reset() resets the class to its initial state\nbins.reset()\n\n# bins.bin() bins the given value according to the bins\nbins.bin(5)\n\n# bins.count() returns the number of values in the bin\nprint(bins.count", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.current = dict()\n\t\tself.bins = [self.make_bins(i) for i in range(self.nbins)]\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself._context = {}\n\t\tfor bin in self._bins:\n\t\t\tbin.reset()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.__dict__[\"context\"] = {}\n\t\tself.__dict__[\"bins\"] = {}\n\t\tself.__dict__[\"bins\"] = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.bins = self.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\tself.bins = self.make_bins()\n\tself._cache = {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.bins = self.make_bins(self.initial_value)\n\n\tdef make_bins(self, value):\n\t\t\"\"\"\n\t\tInitializes the bins dict with a value.\n\t\t\"\"\"\n\t\treturn {k: value for k in self.bin_keys}\n\n\tdef get_bin_keys(self):\n\t\t\"\"\"\n\t\tGenerates a list of the keys of the bins dict.\n\t\t\"\"\"\n\t\treturn list(self.bins.keys())\n\n\tdef get_bin_values(self):\n\t\t\"\"\"\n\t\tGenerates a list of values of the bins dict.\n\t\t\"\"\"\n\t\treturn list(self.bins.values())\n\n\tdef get_bins(self):\n\t\t\"\"\"\n\t\tReturns the bins dict.\n\t\t\"\"\"\n\t\treturn self.bins\n\n\tdef get_bin(self, key):\n\t\t\"\"\"\n\t\tReturns the value of the bin with the specified key.\n\t\t\"\"\"\n\t\treturn self.bins[key]\n\n\tdef set_initial_value(self, value):\n\t\t\"\"\"\n\t\tSets the initial value to the specified value.\n\t\t\"\"\"\n\t\tself.initial_value = value\n\n\tdef get_initial_value(self):\n\t\t\"\"\"\n\t\tReturns the initial value.\n\t\t\"\"\"\n\t\treturn self.initial_value\n\n\tdef set_bin_keys(self,", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\tself.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.context = {}\n\t\tself.bins = self.make_bins()\n\n\tdef __repr__(self):\n\t\treturn 'Context: {}'.format(self.context)/README.md\n# Contextual Bins\n\n## Contextual Bins for Python\n\nThis package contains a class `ContextualBins` for contextualizing the values of a given list. The `ContextualBins` class is able to bin any kind of values, but the most common use case is binning numerical values. \n\n## Installation\n\n`pip install contextualbins`\n\n## Usage\n\n```python\nfrom contextualbins import ContextualBins\n\nvalues = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\n\nbins = ContextualBins(values, bins=5, initial_value=1)\n\n# bins.context returns the context of the values\nprint(bins.context)\n\n# bins.bins returns the bins of the class\nprint(bins.bins)\n\n# bins.reset() resets the class to its initial state\nbins.reset()\n\n# bins.bin() bins the given value according to the bins\nbins.bin(5)\n\n# bins.count() returns the number of values in the bin\nprint(bins.count", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself.current = dict()\n\t\tself.bins = [self.make_bins(i) for i in range(self.nbins)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\t\tself._context = {}\n\t\tfor bin in self._bins:\n\t\t\tbin.reset()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\t# check\n\t\tif header is not None and not isinstance(header, basestring):\n\t\t\traise TypeError(\"header must be a string or None\")\n\t\t#\n\t\tdef to_string(i):\n\t\t\tif isinstance(i, basestring):\n\t\t\t\treturn i\n\t\t\telse:\n\t\t\t\treturn str(i)\n\t\t#\n\t\tdef to_value(i):\n\t\t\tif isinstance(i, basestring):\n\t\t\t\treturn i\n\t\t\telse:\n\t\t\t\treturn str(i)\n\t\t#\n\t\ts = \"\"\n\t\tif header is not None:\n\t\t\ts += str(header) + \"\\n\"\n\t\t#\n\t\tfor p in self:\n\t\t\ts += separator.join(map(to_string, p.coord)) + \"\\t\" + separator.join(map(to_value, p.value)) + \"\\n\"\n\t\t#\n\t\treturn s\n\t#\n\tdef __repr__(self):\n\t\t\"\"\"\n.. deprecated:: 0.5 in Lena 0.5 __repr__ is not used.\n      Iterables are converted to tables.\n\nConvert :class:`Graph` to string representation.\n\nThe format of the result is:\n\n* ``G(dim,n)``, where *dim* is the dimension of the graph,\nand *n* is the number of its points;\n* points are in ``[(x1", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecationWarning(\n\t\t\"0.5 in Lena 0.5 to_csv is not used.\")\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecatedWarning(\"to_csv is deprecated, use graph.to_table()\")\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\nfrom deprecated import deprecated\n\n@deprecated(\"0.5\", \"to_csv is not used. Use ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecationWarning(\"lena.graph.Graph.to_csv is not used. Use lena.output.ToCSV instead.\")\n\n\tdef to_text(self, header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_text is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to text.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to strings\n(separated by spaces), then each part of its value.\n\nTo convert :class:`Graph` to text inside a Lena sequence,\nuse :class:`lena.output.ToText`.\n\t\"\"\"\n\t\traise DeprecationWarning(\"lena.graph.Graph.to_text is not used. Use lena.output.ToText instead.\")\n\n\tdef to_json(self):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_json is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to JSON.\n\nTo convert :class:`Graph` to JSON inside a Lena sequence,\nuse :class:`lena.output.ToJSON`.\n\t\"\"\"\n\t\traise DeprecationWarning", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\treturn self.to_table(separator, header)\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\treturn self.to_table(separator, header)\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise Deprecated(\"to_csv is not used\")\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\tif header is not None:\n\t\t\theader = \"\\n\" + header\n\t\telse:\n\t\t\theader = \"\"\n\t\treturn separator.join(map(str, p)) + \"\\n\" + header\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\t# check\n\t\tif header is not None and not isinstance(header, basestring):\n\t\t\traise TypeError(\"header must be a string or None\")\n\t\t#\n\t\tdef to_string(i):\n\t\t\tif isinstance(i, basestring):\n\t\t\t\treturn i\n\t\t\telse:\n\t\t\t\treturn str(i)\n\t\t#\n\t\tdef to_value(i):\n\t\t\tif isinstance(i, basestring):\n\t\t\t\treturn i\n\t\t\telse:\n\t\t\t\treturn str(i)\n\t\t#\n\t\ts = \"\"\n\t\tif header is not None:\n\t\t\ts += str(header) + \"\\n\"\n\t\t#\n\t\tfor p in self:\n\t\t\ts += separator.join(map(to_string, p.coord)) + \"\\t\" + separator.join(map(to_value, p.value)) + \"\\n\"\n\t\t#\n\t\treturn s\n\t#\n\tdef __repr__(self):\n\t\t\"\"\"\n.. deprecated:: 0.5 in Lena 0.5 __repr__ is not used.\n      Iterables are converted to tables.\n\nConvert :class:`Graph` to string representation.\n\nThe format of the result is:\n\n* ``G(dim,n)``, where *dim* is the dimension of the graph,\nand *n* is the number of its points;\n* points are in ``[(x1", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecationWarning(\n\t\t\"0.5 in Lena 0.5 to_csv is not used.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecatedWarning(\"to_csv is deprecated, use graph.to_table()\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\nfrom deprecated import deprecated\n\n@deprecated(\"0.5\", \"to_csv is not used. Use ToCSV instead.\")", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecationWarning(\"lena.graph.Graph.to_csv is not used. Use lena.output.ToCSV instead.\")\n\n\tdef to_text(self, header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_text is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to text.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to strings\n(separated by spaces), then each part of its value.\n\nTo convert :class:`Graph` to text inside a Lena sequence,\nuse :class:`lena.output.ToText`.\n\t\"\"\"\n\t\traise DeprecationWarning(\"lena.graph.Graph.to_text is not used. Use lena.output.ToText instead.\")\n\n\tdef to_json(self):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_json is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to JSON.\n\nTo convert :class:`Graph` to JSON inside a Lena sequence,\nuse :class:`lena.output.ToJSON`.\n\t\"\"\"\n\t\traise DeprecationWarning", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\treturn self.to_table(separator, header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\treturn self.to_table(separator, header)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise Deprecated(\"to_csv is not used\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\tif header is not None:\n\t\t\theader = \"\\n\" + header\n\t\telse:\n\t\t\theader = \"\"\n\t\treturn separator.join(map(str, p)) + \"\\n\" + header\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\n\tif coord_name == 'T':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'T']\n\telif coord_name == 'P':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'P']\n\telif coord_name == 'X':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'X']\n\telif coord_name == 'Z':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'Z']\n\telif coord_name == 'E':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'E']\n\telse:\n\t\traise ValueError('coord_name must be one of: T, P, X, Z, E')\n\n\treturn idx\n\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\terr_coord_names = self.get_err_coord_names()\n\t\treturn np.where(np.array(err_coord_names) == coord_name)[0]\n\n\tdef get_err_coord_names(self):\n\t\t\"\"\"\n\t\tReturn a list of the coordinate names corresponding to errors.\n\t\t\"\"\"\n\t\tif self.err_coord_names is not None:\n\t\t\treturn self.err_coord_names\n\t\terr_coord_names = []\n\t\tfor coord_name in self.coord_names:\n\t\t\tif coord_name in self.err_indices:\n\t\t\t\terr_coord_names.append(coord_name)\n\t\tself.err_coord_names = err_coord_names\n\t\treturn self.err_coord_names\n\n\tdef get_err_indices(self, coord_name):\n\t\t\"\"\"\n\t\tReturn a list of the coordinate indices corresponding to errors.\n\t\t\"\"\"\n\t\tif self.err_indices is not None:\n\t\t\treturn self.err_indices\n\t\terr_indices = []\n\t\tfor coord_index in self.coord_indices:\n\t\t\tif coord_index in self.err_indices:\n\t\t\t\terr_indices.append(coord_index)\n\t\tself.err_indices = err_indices\n\t\treturn self.err_indices\n\n\tdef _get_err_indices(self", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\tif self._err_inds is None:\n\t\t\treturn None\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn self._err_inds[coord_name]\n\t\t\texcept KeyError:\n\t\t\t\treturn None\n\n\tdef _set_err_inds(self, coord_name, err_inds):\n\t\t\"\"\"\n\t\tSet the error indexes corresponding to coord_name.\n\t\t\"\"\"\n\t\tif self._err_inds is None:\n\t\t\tself._err_inds = {}\n\t\tself._err_inds[coord_name] = err_inds\n\n\tdef get_err_inds(self, coord_name):\n\t\t\"\"\"\n\t\tGet the error indexes corresponding to coord_name.\n\t\t\"\"\"\n\t\treturn self._get_err_indices(coord_name)\n\n\tdef set_err_inds(self, coord_name, err_inds):\n\t\t\"\"\"\n\t\tSet the error indexes corresponding to coord_name.\n\t\t\"\"\"\n\t\tself._set_err_inds(coord_name, err_inds)\n\n\tdef get_err_inds_dict(self):\n\t\t\"\"\"\n\t\tGet the error indexes corresponding to all coordinate names.\n\t\t\"\"\"\n\t\treturn self._err_inds\n\n\tdef set_err_inds_dict(self, err_inds_dict):\n\t\t\"\"\"\n\t\tSet the error indexes corresponding to all coordinate names.\n\t\t\"\"\"\n\t\tself", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\treturn np.where(self.get_coords()[coord_name] < 0)[0]\n\n\tdef _get_err_coords(self, coord_name):\n\t\t\"\"\"\n\t\tFind all error coordinates corresponding to coord_name.\n\t\t\"\"\"\n\t\treturn self.get_coords()[coord_name][self._get_err_indices(coord_name)]\n\n\tdef _get_err_coords_dict(self, coord_name):\n\t\t\"\"\"\n\t\tFind all error coordinates corresponding to coord_name and return in dict.\n\t\t\"\"\"\n\t\terr_coords = self._get_err_coords(coord_name)\n\t\terr_coords_dict = {}\n\t\tfor i in range(len(err_coords)):\n\t\t\terr_coords_dict[i] = err_coords[i]\n\t\treturn err_coords_dict\n\n\tdef _get_err_coords_dict_by_index(self, coord_name, index):\n\t\t\"\"\"\n\t\tFind error coordinates corresponding to coord_name and index and return in dict.\n\t\t\"\"\"\n\t\terr_coords = self._get_err_coords(coord_name)\n\t\terr_coords_dict = {}\n\t\terr_coords_dict[index] = err_coords[index]\n\t\treturn err_coords_dict\n\n\tdef _get_err_coords_dict_by_index", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name not in self.coords:\n            raise ValueError(\"Invalid coordinate name: %s\" % coord_name)\n\n        indices = []\n        for i in range(len(self.coords[coord_name])):\n            if self.coords[coord_name][i] is not None:\n                indices.append(i)\n\n        return indices\n\n        \n    def _get_err_coords(self, coord_name):\n\t\"\"\"\n\tFind all error coordinates corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name not in self.coords:\n            raise ValueError(\"Invalid coordinate name: %s\" % coord_name)\n\n        coords = []\n        for i in range(len(self.coords[coord_name])):\n            if self.coords[coord_name][i] is not None:\n                coords.append(self.coords[coord_name][i])\n\n        return coords\n\n    \n    def _set_err_coords(self, coord_name, coords):\n\t\"\"\"\n\tSet error coordinates corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name not in self.coords:\n            raise ValueError(\"Invalid coordinate name: %s\" % coord_name)\n\n        if len(coords)!= len(self.coords[coord_name]):\n            raise ValueError(\"Mismatched number of coordinates\")\n\n        for i in range(len(self.coords[coord_name])):\n            if self", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name == 'time':\n            indices = []\n            for i,coord in enumerate(self.coords):\n                if coord.get('err_index', None) is not None:\n                    indices.append(i)\n            return indices\n        else:\n            indices = [i for i,coord in enumerate(self.coords) if coord['name'] == coord_name]\n            return indices\n\n    def _get_err_coords(self, coord_name):\n\t\"\"\"\n\tFind all error coordinates corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name == 'time':\n            coords = []\n            for i,coord in enumerate(self.coords):\n                if coord.get('err_index', None) is not None:\n                    coords.append(coord)\n            return coords\n        else:\n            coords = [coord for i,coord in enumerate(self.coords) if coord['name'] == coord_name]\n            return coords\n\n    def _get_err_index(self, coord_name):\n\t\"\"\"\n\tFind error index corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name == 'time':\n            for i,coord in enumerate(self.coords):\n                if coord.get('err_index', None) is not None:\n                    return i\n        else:\n            for i,coord in enumerate(self.coords):\n                if coord['name'] == coord_name:\n                   ", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n#\ttry:\n#\t\tif coord_name in self._coord_err_indices:\n#\t\t\treturn self._coord_err_indices[coord_name]\n#\texcept KeyError:\n#\t\tpass\n\terr_indices = []\n\tfor i in range(len(self._coord_names)):\n\t\tif coord_name in self._coord_names[i]:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n\n    def _get_coord_err_dict(self, coord_name):\n\t\"\"\"\n\tGet the error values for coord_name.\n\t\"\"\"\n\t\n\ttry:\n\t\tif coord_name in self._coord_err_dict:\n\t\t\treturn self._coord_err_dict[coord_name]\n\texcept KeyError:\n\t\tpass\n\terr_indices = self._get_err_indices(coord_name)\n\tif len(err_indices) > 0:\n\t\treturn self._err_values[err_indices]\n\telse:\n\t\treturn None\n\n    def _get_err_values(self, coord_name):\n\t\"\"\"\n\tGet the error values for coord_name.\n\t\"\"\"\n\t\n\ttry:\n\t\tif coord_name in self._coord_err_values:\n\t\t\treturn self._coord_err_values[coord_name]\n\texcept", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\treturn self.coords[coord_name].err_indices\n\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\tindices = []\n\t\tfor i, c in enumerate(self.coords):\n\t\t\tif c.name == coord_name:\n\t\t\t\tindices.append(i)\n\t\treturn indices\n\t", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\t# Find the error coordinates\n\t\terr_coord = self.get_coords(self.get_err_coords_name())\n\n\t\t# Find the index of the first and last error coordinate\n\t\terr_coord_first_index = err_coord.get_coord_index(coord_name)\n\t\terr_coord_last_index = err_coord_first_index + err_coord.get_coord_size(coord_name) - 1\n\n\t\t# Find the index of the first and last actual coordinate\n\t\tcoord = self.get_coords(coord_name)\n\t\tcoord_first_index = coord.get_coord_index(coord_name)\n\t\tcoord_last_index = coord_first_index + coord.get_coord_size(coord_name) - 1\n\n\t\t# Find the index of the first and last error coordinate\n\t\terr_coord_indices = range(err_coord_first_index, err_coord_last_index + 1)\n\t\tcoord_indices = range(coord_first_index, coord_last_index + 1)\n\n\t\t# Find the indices of the error coordinates and the actual coordinates\n\t\terr_coord_indices = [i for i in err_coord_indices if i in coord_indices]\n\t\tcoord_indices = [i for i in coord_indices if i in err_coord"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\n\tif coord_name == 'T':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'T']\n\telif coord_name == 'P':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'P']\n\telif coord_name == 'X':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'X']\n\telif coord_name == 'Z':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'Z']\n\telif coord_name == 'E':\n\t\tidx = [i for i, x in enumerate(self.coord_names) if x == 'E']\n\telse:\n\t\traise ValueError('coord_name must be one of: T, P, X, Z, E')\n\n\treturn idx\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\terr_coord_names = self.get_err_coord_names()\n\t\treturn np.where(np.array(err_coord_names) == coord_name)[0]\n\n\tdef get_err_coord_names(self):\n\t\t\"\"\"\n\t\tReturn a list of the coordinate names corresponding to errors.\n\t\t\"\"\"\n\t\tif self.err_coord_names is not None:\n\t\t\treturn self.err_coord_names\n\t\terr_coord_names = []\n\t\tfor coord_name in self.coord_names:\n\t\t\tif coord_name in self.err_indices:\n\t\t\t\terr_coord_names.append(coord_name)\n\t\tself.err_coord_names = err_coord_names\n\t\treturn self.err_coord_names\n\n\tdef get_err_indices(self, coord_name):\n\t\t\"\"\"\n\t\tReturn a list of the coordinate indices corresponding to errors.\n\t\t\"\"\"\n\t\tif self.err_indices is not None:\n\t\t\treturn self.err_indices\n\t\terr_indices = []\n\t\tfor coord_index in self.coord_indices:\n\t\t\tif coord_index in self.err_indices:\n\t\t\t\terr_indices.append(coord_index)\n\t\tself.err_indices = err_indices\n\t\treturn self.err_indices\n\n\tdef _get_err_indices(self", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\tif self._err_inds is None:\n\t\t\treturn None\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn self._err_inds[coord_name]\n\t\t\texcept KeyError:\n\t\t\t\treturn None\n\n\tdef _set_err_inds(self, coord_name, err_inds):\n\t\t\"\"\"\n\t\tSet the error indexes corresponding to coord_name.\n\t\t\"\"\"\n\t\tif self._err_inds is None:\n\t\t\tself._err_inds = {}\n\t\tself._err_inds[coord_name] = err_inds\n\n\tdef get_err_inds(self, coord_name):\n\t\t\"\"\"\n\t\tGet the error indexes corresponding to coord_name.\n\t\t\"\"\"\n\t\treturn self._get_err_indices(coord_name)\n\n\tdef set_err_inds(self, coord_name, err_inds):\n\t\t\"\"\"\n\t\tSet the error indexes corresponding to coord_name.\n\t\t\"\"\"\n\t\tself._set_err_inds(coord_name, err_inds)\n\n\tdef get_err_inds_dict(self):\n\t\t\"\"\"\n\t\tGet the error indexes corresponding to all coordinate names.\n\t\t\"\"\"\n\t\treturn self._err_inds\n\n\tdef set_err_inds_dict(self, err_inds_dict):\n\t\t\"\"\"\n\t\tSet the error indexes corresponding to all coordinate names.\n\t\t\"\"\"\n\t\tself", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\treturn np.where(self.get_coords()[coord_name] < 0)[0]\n\n\tdef _get_err_coords(self, coord_name):\n\t\t\"\"\"\n\t\tFind all error coordinates corresponding to coord_name.\n\t\t\"\"\"\n\t\treturn self.get_coords()[coord_name][self._get_err_indices(coord_name)]\n\n\tdef _get_err_coords_dict(self, coord_name):\n\t\t\"\"\"\n\t\tFind all error coordinates corresponding to coord_name and return in dict.\n\t\t\"\"\"\n\t\terr_coords = self._get_err_coords(coord_name)\n\t\terr_coords_dict = {}\n\t\tfor i in range(len(err_coords)):\n\t\t\terr_coords_dict[i] = err_coords[i]\n\t\treturn err_coords_dict\n\n\tdef _get_err_coords_dict_by_index(self, coord_name, index):\n\t\t\"\"\"\n\t\tFind error coordinates corresponding to coord_name and index and return in dict.\n\t\t\"\"\"\n\t\terr_coords = self._get_err_coords(coord_name)\n\t\terr_coords_dict = {}\n\t\terr_coords_dict[index] = err_coords[index]\n\t\treturn err_coords_dict\n\n\tdef _get_err_coords_dict_by_index", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name not in self.coords:\n            raise ValueError(\"Invalid coordinate name: %s\" % coord_name)\n\n        indices = []\n        for i in range(len(self.coords[coord_name])):\n            if self.coords[coord_name][i] is not None:\n                indices.append(i)\n\n        return indices\n\n        \n    def _get_err_coords(self, coord_name):\n\t\"\"\"\n\tFind all error coordinates corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name not in self.coords:\n            raise ValueError(\"Invalid coordinate name: %s\" % coord_name)\n\n        coords = []\n        for i in range(len(self.coords[coord_name])):\n            if self.coords[coord_name][i] is not None:\n                coords.append(self.coords[coord_name][i])\n\n        return coords\n\n    \n    def _set_err_coords(self, coord_name, coords):\n\t\"\"\"\n\tSet error coordinates corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name not in self.coords:\n            raise ValueError(\"Invalid coordinate name: %s\" % coord_name)\n\n        if len(coords)!= len(self.coords[coord_name]):\n            raise ValueError(\"Mismatched number of coordinates\")\n\n        for i in range(len(self.coords[coord_name])):\n            if self", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name == 'time':\n            indices = []\n            for i,coord in enumerate(self.coords):\n                if coord.get('err_index', None) is not None:\n                    indices.append(i)\n            return indices\n        else:\n            indices = [i for i,coord in enumerate(self.coords) if coord['name'] == coord_name]\n            return indices\n\n    def _get_err_coords(self, coord_name):\n\t\"\"\"\n\tFind all error coordinates corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name == 'time':\n            coords = []\n            for i,coord in enumerate(self.coords):\n                if coord.get('err_index', None) is not None:\n                    coords.append(coord)\n            return coords\n        else:\n            coords = [coord for i,coord in enumerate(self.coords) if coord['name'] == coord_name]\n            return coords\n\n    def _get_err_index(self, coord_name):\n\t\"\"\"\n\tFind error index corresponding to coord_name.\n\t\"\"\"\n\t\n        if coord_name == 'time':\n            for i,coord in enumerate(self.coords):\n                if coord.get('err_index', None) is not None:\n                    return i\n        else:\n            for i,coord in enumerate(self.coords):\n                if coord['name'] == coord_name:\n                   ", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n#\ttry:\n#\t\tif coord_name in self._coord_err_indices:\n#\t\t\treturn self._coord_err_indices[coord_name]\n#\texcept KeyError:\n#\t\tpass\n\terr_indices = []\n\tfor i in range(len(self._coord_names)):\n\t\tif coord_name in self._coord_names[i]:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n\n    def _get_coord_err_dict(self, coord_name):\n\t\"\"\"\n\tGet the error values for coord_name.\n\t\"\"\"\n\t\n\ttry:\n\t\tif coord_name in self._coord_err_dict:\n\t\t\treturn self._coord_err_dict[coord_name]\n\texcept KeyError:\n\t\tpass\n\terr_indices = self._get_err_indices(coord_name)\n\tif len(err_indices) > 0:\n\t\treturn self._err_values[err_indices]\n\telse:\n\t\treturn None\n\n    def _get_err_values(self, coord_name):\n\t\"\"\"\n\tGet the error values for coord_name.\n\t\"\"\"\n\t\n\ttry:\n\t\tif coord_name in self._coord_err_values:\n\t\t\treturn self._coord_err_values[coord_name]\n\texcept", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\treturn self.coords[coord_name].err_indices\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\tindices = []\n\t\tfor i, c in enumerate(self.coords):\n\t\t\tif c.name == coord_name:\n\t\t\t\tindices.append(i)\n\t\treturn indices\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\t# Find the error coordinates\n\t\terr_coord = self.get_coords(self.get_err_coords_name())\n\n\t\t# Find the index of the first and last error coordinate\n\t\terr_coord_first_index = err_coord.get_coord_index(coord_name)\n\t\terr_coord_last_index = err_coord_first_index + err_coord.get_coord_size(coord_name) - 1\n\n\t\t# Find the index of the first and last actual coordinate\n\t\tcoord = self.get_coords(coord_name)\n\t\tcoord_first_index = coord.get_coord_index(coord_name)\n\t\tcoord_last_index = coord_first_index + coord.get_coord_size(coord_name) - 1\n\n\t\t# Find the index of the first and last error coordinate\n\t\terr_coord_indices = range(err_coord_first_index, err_coord_last_index + 1)\n\t\tcoord_indices = range(coord_first_index, coord_last_index + 1)\n\n\t\t# Find the indices of the error coordinates and the actual coordinates\n\t\terr_coord_indices = [i for i in err_coord_indices if i in coord_indices]\n\t\tcoord_indices = [i for i in coord_indices if i in err_coord", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tif type(context)!= dict:\n\t\t\tself.__context_update(context)\n\t\t\treturn\n\t\tvalue = context.get('value', {})\n\t\tfor key, val in self.__dict__.items():\n\t\t\tif key.startswith('_') or key in ('error', 'value'):\n\t\t\t\tcontinue\n\t\t\tif val is None:\n\t\t\t\tcontinue\n\t\t\tif type(val) == str:\n\t\t\t\tif key in value:\n\t\t\t\t\tcontinue\n\t\t\t\tvalue[key] = val\n\t\t\t\tcontinue\n\t\t\tif type(val) == dict:\n\t\t\t\tif key in value:\n\t\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\t\tcontinue\n\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\tcontinue\n\t\t\tif type(val) == list:\n\t\t\t\tif key in value:\n\t\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\t\tcontinue\n\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\tcontinue\n\t\t\tvalue[key] = val\n\n\t\tif 'error' in context:\n\t\t\terror = context['error']\n\t\t\tif not type(error) == dict:\n\t\t\t\tself.__context_update(error, context['error'])\n\t\t\telse:\n\t\t\t\tfor key, val in context['error'].items():\n\t\t\t\t\tself.__context_update(error.setdefault(key, {}), val)\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tcontext.value.update(self._to_dict())\n\t\tif self._error is not None:\n\t\t\tfor error_name, error_dict in self._error.items():\n\t\t\t\tcontext.error.setdefault(error_name, {})\n\t\t\t\tcontext.error[error_name].update(error_dict)\n\n#\tdef _from_dict(self, d):\n#\t\t\"\"\"\n#\t\tSet values and errors from *d*,\n#\t\twhich is a dictionary as returned by :meth:`_to_dict`.\n#\t\t\"\"\"\n#\t\t# TODO: do we need this?\n#\t\tself._error = d.get(\"error\")\n#\t\tself._x = d.get(\"x\")\n#\t\tself._y = d.get(\"y\")\n#\t\tself._z = d.get(\"z\")\n#\n#\tdef _to_dict(self):\n#\t\t\"\"\"\n#\t\tReturn a dictionary with values and errors.\n#\t\t\"\"\"\n#\t\td = {}\n#\t\tif self._error is not None:\n#\t\t\td[\"error\"] = self._error\n#\t\tif self._x is not None:\n#\t\t\td[\"x\"] = self._x\n#\t\tif self._y is not None:\n#\t\t\td[\"", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tpass\n\t\tcontext.value[self.name] = {\n\t\t\t\t\"x\": self.x,\n\t\t\t\t\"y\": self.y,\n\t\t\t\t\"z\": self.z,\n\t\t\t\t}\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t#\tcontext.value.update(self.properties)\n\t\t#\tcontext.error.update(self.properties)\n\t#\tif self._fields:\n\t#\t\tcontext.error[\"x_low\"] = self._fields\n\t#\t\tcontext.error[\"x_high\"] = self._fields\n\t#\t\tcontext.error[\"y_low\"] = self._fields\n\t#\t\tcontext.error[\"y_high\"] = self._fields\n\t#\t\tcontext.error[\"z_low\"] = self._fields\n\t#\t\tcontext.error[\"z_high\"] = self._fields\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t# Remove duplicates (and keep order)\n\tself.fields = list(set(self.fields))\n\t# Update context\n\tcontext.add_value(\"graph_type\", self.name)\n\tcontext.add_value(\"graph_fields\", self.fields)\n\tcontext.add_value(\"graph_fields_count\", len(self.fields))\n\tcontext.add_value(\"graph_fields_joined\", \", \".join(self.fields))\n\tcontext.add_value(\"graph_labels\", self.labels)\n\tcontext.add_value(\"graph_labels_count\", len(self.labels))\n\tcontext.add_value(\"graph_labels_joined\", \", \".join(self.labels))\n\tcontext.add_value(\"graph_lines_count\", len(self.lines))\n\tif self.fields:\n\t\tcontext.add_value(\"graph_fields_first\", self.fields[0])\n\t\tcontext.add_value(\"graph_fields_last\", self.fields[-1])\n\tif self.labels:\n\t\tcontext.add_value(\"graph_labels_first\", self.labels[0])\n\t\tcontext.add_value(\"graph_labels_last\", self.labels[-1])\n\tif self.lines:\n\t\tcontext.add_value(\"graph_lines_first\", self.lines[0", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tif self.error is not None:\n\t\t\tself.error.update_context(context)\n\t\tif self.error_E_low is not None:\n\t\t\tself.error_E_low.update_context(context)\n\t\tif self.error_E_high is not None:\n\t\t\tself.error_E_high.update_context(context)\n\t\tif self.error_t_low is not None:\n\t\t\tself.error_t_low.update_context(context)\n\t\tif self.error_t_high is not None:\n\t\t\tself.error_t_high.update_context(context)\n\t\tif self.error_z_low is not None:\n\t\t\tself.error_z_low.update_context(context)\n\t\tif self.error_z_high is not None:\n\t\t\tself.error_z_high.update_context(context)\n\t\tif self.error_y_low is not None:\n\t\t\tself.error_y_low.update_context(context)\n\t\tif self.error_y_high is not None:\n\t\t\tself.error_y_high.update_context(context)\n\t\tif self.error_x_low is not None:\n\t\t\tself.error_x_low.update_context(context)\n\t\tif self.error_x_", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tcontext.update(self._context)\n\t\t# context.values.update(self._values)\n\t\t# context.error.update(self._error)\n\n\tdef _get_error_low(self):\n\t\t\"\"\"Return the lowest error value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn min(self._error.values())\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_error_high(self):\n\t\t\"\"\"Return the highest error value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn max(self._error.values())\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_x_low(self):\n\t\t\"\"\"Return the lowest x value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn min(self._values[self._x_axis_name])\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_x_high(self):\n\t\t\"\"\"Return the highest x value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn max(self._values[self._x_axis_name])\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_x_unit(self):\n\t\t\"\"\"Return the unit for the x axis.\"\"\"\n\t\treturn self._x_unit\n\n\tdef _get_y_low(self):\n\t\t\"\"\"Return the lowest y value in this graph.\"\"\"\n\t\ttry:\n\t\t\t", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t#TODO: this should not be necessary here,\n\t\t# but it is, because in _get_context()\n\t\t# we add values to context.value,\n\t\t# and _get_context() is called before\n\t\t# _update_context().\n\t\t# We should reimplement it,\n\t\t# so that we add to context.value\n\t\t# only at the end of the update cycle.\n\t\tassert isinstance(context, dict)\n\t\tif \"value\" not in context:\n\t\t\tcontext[\"value\"] = {}\n\t\tif \"error\" not in context:\n\t\t\tcontext[\"error\"] = {}\n\t\tfor key in self.fields:\n\t\t\tif key not in context[\"value\"]:\n\t\t\t\tcontext[\"value\"][key] = []\n\t\t\tif key not in context[\"error\"]:\n\t\t\t\tcontext[\"error\"][key] = []\n\t\t\tif len(self.fields[key]) > 0:\n\t\t\t\t#TODO: self.fields[key] might be a list of\n\t\t\t\t# lists, but context[\"value\"][key] is a list.\n\t\t\t\t# The code should be more general.\n\t\t\t\tcontext[\"value\"][key].append(self.fields[key][-1])\n\t\t\tif len(self.errors[key]) > 0:\n\t\t\t\tcontext[\"error\"][key].append(self.errors[key][-1])\n\t\tif len(self.errors", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\traise NotImplementedError\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tcontext.value.update(self.props)\n\t\tfor x in [\"error\", \"value\", \"x\", \"y\", \"z\"]:\n\t\t\tif x in self.props:\n\t\t\t\tcontext.error.append(x)\n\t\t\t\tcontext.value[x] = self.props[x]\n\t\t\tif x in context.error:\n\t\t\t\tcontext.error.remove(x)\n\t\tif self.x is not None:\n\t\t\tcontext.error.append(\"x\")\n\t\t\tcontext.value[\"x\"] = self.x\n\t\tif self.y is not None:\n\t\t\tcontext.error.append(\"y\")\n\t\t\tcontext.value[\"y\"] = self.y\n\t\tif self.z is not None:\n\t\t\tcontext.error.append(\"z\")\n\t\t\tcontext.value[\"z\"] = self.z\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tif type(context)!= dict:\n\t\t\tself.__context_update(context)\n\t\t\treturn\n\t\tvalue = context.get('value', {})\n\t\tfor key, val in self.__dict__.items():\n\t\t\tif key.startswith('_') or key in ('error', 'value'):\n\t\t\t\tcontinue\n\t\t\tif val is None:\n\t\t\t\tcontinue\n\t\t\tif type(val) == str:\n\t\t\t\tif key in value:\n\t\t\t\t\tcontinue\n\t\t\t\tvalue[key] = val\n\t\t\t\tcontinue\n\t\t\tif type(val) == dict:\n\t\t\t\tif key in value:\n\t\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\t\tcontinue\n\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\tcontinue\n\t\t\tif type(val) == list:\n\t\t\t\tif key in value:\n\t\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\t\tcontinue\n\t\t\t\tself.__context_update(value[key], val)\n\t\t\t\tcontinue\n\t\t\tvalue[key] = val\n\n\t\tif 'error' in context:\n\t\t\terror = context['error']\n\t\t\tif not type(error) == dict:\n\t\t\t\tself.__context_update(error, context['error'])\n\t\t\telse:\n\t\t\t\tfor key, val in context['error'].items():\n\t\t\t\t\tself.__context_update(error.setdefault(key, {}), val)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tcontext.value.update(self._to_dict())\n\t\tif self._error is not None:\n\t\t\tfor error_name, error_dict in self._error.items():\n\t\t\t\tcontext.error.setdefault(error_name, {})\n\t\t\t\tcontext.error[error_name].update(error_dict)\n\n#\tdef _from_dict(self, d):\n#\t\t\"\"\"\n#\t\tSet values and errors from *d*,\n#\t\twhich is a dictionary as returned by :meth:`_to_dict`.\n#\t\t\"\"\"\n#\t\t# TODO: do we need this?\n#\t\tself._error = d.get(\"error\")\n#\t\tself._x = d.get(\"x\")\n#\t\tself._y = d.get(\"y\")\n#\t\tself._z = d.get(\"z\")\n#\n#\tdef _to_dict(self):\n#\t\t\"\"\"\n#\t\tReturn a dictionary with values and errors.\n#\t\t\"\"\"\n#\t\td = {}\n#\t\tif self._error is not None:\n#\t\t\td[\"error\"] = self._error\n#\t\tif self._x is not None:\n#\t\t\td[\"x\"] = self._x\n#\t\tif self._y is not None:\n#\t\t\td[\"", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tpass\n\t\tcontext.value[self.name] = {\n\t\t\t\t\"x\": self.x,\n\t\t\t\t\"y\": self.y,\n\t\t\t\t\"z\": self.z,\n\t\t\t\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t#\tcontext.value.update(self.properties)\n\t\t#\tcontext.error.update(self.properties)\n\t#\tif self._fields:\n\t#\t\tcontext.error[\"x_low\"] = self._fields\n\t#\t\tcontext.error[\"x_high\"] = self._fields\n\t#\t\tcontext.error[\"y_low\"] = self._fields\n\t#\t\tcontext.error[\"y_high\"] = self._fields\n\t#\t\tcontext.error[\"z_low\"] = self._fields\n\t#\t\tcontext.error[\"z_high\"] = self._fields\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t# Remove duplicates (and keep order)\n\tself.fields = list(set(self.fields))\n\t# Update context\n\tcontext.add_value(\"graph_type\", self.name)\n\tcontext.add_value(\"graph_fields\", self.fields)\n\tcontext.add_value(\"graph_fields_count\", len(self.fields))\n\tcontext.add_value(\"graph_fields_joined\", \", \".join(self.fields))\n\tcontext.add_value(\"graph_labels\", self.labels)\n\tcontext.add_value(\"graph_labels_count\", len(self.labels))\n\tcontext.add_value(\"graph_labels_joined\", \", \".join(self.labels))\n\tcontext.add_value(\"graph_lines_count\", len(self.lines))\n\tif self.fields:\n\t\tcontext.add_value(\"graph_fields_first\", self.fields[0])\n\t\tcontext.add_value(\"graph_fields_last\", self.fields[-1])\n\tif self.labels:\n\t\tcontext.add_value(\"graph_labels_first\", self.labels[0])\n\t\tcontext.add_value(\"graph_labels_last\", self.labels[-1])\n\tif self.lines:\n\t\tcontext.add_value(\"graph_lines_first\", self.lines[0", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tif self.error is not None:\n\t\t\tself.error.update_context(context)\n\t\tif self.error_E_low is not None:\n\t\t\tself.error_E_low.update_context(context)\n\t\tif self.error_E_high is not None:\n\t\t\tself.error_E_high.update_context(context)\n\t\tif self.error_t_low is not None:\n\t\t\tself.error_t_low.update_context(context)\n\t\tif self.error_t_high is not None:\n\t\t\tself.error_t_high.update_context(context)\n\t\tif self.error_z_low is not None:\n\t\t\tself.error_z_low.update_context(context)\n\t\tif self.error_z_high is not None:\n\t\t\tself.error_z_high.update_context(context)\n\t\tif self.error_y_low is not None:\n\t\t\tself.error_y_low.update_context(context)\n\t\tif self.error_y_high is not None:\n\t\t\tself.error_y_high.update_context(context)\n\t\tif self.error_x_low is not None:\n\t\t\tself.error_x_low.update_context(context)\n\t\tif self.error_x_", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tcontext.update(self._context)\n\t\t# context.values.update(self._values)\n\t\t# context.error.update(self._error)\n\n\tdef _get_error_low(self):\n\t\t\"\"\"Return the lowest error value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn min(self._error.values())\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_error_high(self):\n\t\t\"\"\"Return the highest error value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn max(self._error.values())\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_x_low(self):\n\t\t\"\"\"Return the lowest x value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn min(self._values[self._x_axis_name])\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_x_high(self):\n\t\t\"\"\"Return the highest x value in this graph.\"\"\"\n\t\ttry:\n\t\t\treturn max(self._values[self._x_axis_name])\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tdef _get_x_unit(self):\n\t\t\"\"\"Return the unit for the x axis.\"\"\"\n\t\treturn self._x_unit\n\n\tdef _get_y_low(self):\n\t\t\"\"\"Return the lowest y value in this graph.\"\"\"\n\t\ttry:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t#TODO: this should not be necessary here,\n\t\t# but it is, because in _get_context()\n\t\t# we add values to context.value,\n\t\t# and _get_context() is called before\n\t\t# _update_context().\n\t\t# We should reimplement it,\n\t\t# so that we add to context.value\n\t\t# only at the end of the update cycle.\n\t\tassert isinstance(context, dict)\n\t\tif \"value\" not in context:\n\t\t\tcontext[\"value\"] = {}\n\t\tif \"error\" not in context:\n\t\t\tcontext[\"error\"] = {}\n\t\tfor key in self.fields:\n\t\t\tif key not in context[\"value\"]:\n\t\t\t\tcontext[\"value\"][key] = []\n\t\t\tif key not in context[\"error\"]:\n\t\t\t\tcontext[\"error\"][key] = []\n\t\t\tif len(self.fields[key]) > 0:\n\t\t\t\t#TODO: self.fields[key] might be a list of\n\t\t\t\t# lists, but context[\"value\"][key] is a list.\n\t\t\t\t# The code should be more general.\n\t\t\t\tcontext[\"value\"][key].append(self.fields[key][-1])\n\t\t\tif len(self.errors[key]) > 0:\n\t\t\t\tcontext[\"error\"][key].append(self.errors[key][-1])\n\t\tif len(self.errors", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tcontext.value.update(self.props)\n\t\tfor x in [\"error\", \"value\", \"x\", \"y\", \"z\"]:\n\t\t\tif x in self.props:\n\t\t\t\tcontext.error.append(x)\n\t\t\t\tcontext.value[x] = self.props[x]\n\t\t\tif x in context.error:\n\t\t\t\tcontext.error.remove(x)\n\t\tif self.x is not None:\n\t\t\tcontext.error.append(\"x\")\n\t\t\tcontext.value[\"x\"] = self.x\n\t\tif self.y is not None:\n\t\t\tcontext.error.append(\"y\")\n\t\t\tcontext.value[\"y\"] = self.y\n\t\tif self.z is not None:\n\t\t\tcontext.error.append(\"z\")\n\t\t\tcontext.value[\"z\"] = self.z\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\n#\tprint \"Total area: \", np.sum(bins) * (edges[1] - edges[0])\n\treturn np.sum(bins) * (edges[1] - edges[0])\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn np.sum(np.abs(np.diff(bins)) * edges[:-1])\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\tsum = 0\n\t\tfor i in range(len(bins)):\n\t\t\tsum += bins[i] * (edges[i+1] - edges[i])\n\t\t\treturn sum\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn edges[len(edges)-1] * bins[len(bins)-1] / 2\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t \n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += (edges[i+1] - edges[i]) * (bins[i] + bins[i+1]) / 2\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn np.sum((edges[1:] - edges[:-1]) * bins)\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn np.sum(np.multiply(bins, np.diff(edges)))\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn sum(bins) * (edges[1] - edges[0])\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\n#\tprint \"Total area: \", np.sum(bins) * (edges[1] - edges[0])\n\treturn np.sum(bins) * (edges[1] - edges[0])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn np.sum(np.abs(np.diff(bins)) * edges[:-1])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\tsum = 0\n\t\tfor i in range(len(bins)):\n\t\t\tsum += bins[i] * (edges[i+1] - edges[i])\n\t\t\treturn sum\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn edges[len(edges)-1] * bins[len(bins)-1] / 2\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t \n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += (edges[i+1] - edges[i]) * (bins[i] + bins[i+1]) / 2\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn np.sum((edges[1:] - edges[:-1]) * bins)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn np.sum(np.multiply(bins, np.diff(edges)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\treturn sum(bins) * (edges[1] - edges[0])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\timport sqlite3\nimport sys\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n\treturn True\n\t", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n\t# Check if seq starts with \"F\"\n\tif not seq.startswith(\"F\"):\n\t\treturn False\n\n\t# Check if seq contains only digits\n\tif not seq[1:].isdigit():\n\t\treturn False\n\n\treturn True\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n\tif seq is None:\n\t\treturn False\n\n\tif type(seq) == FillRequestSeq:\n\t\treturn True\n\tif type(seq) == list:\n\t\tif len(seq) == 0:\n\t\t\treturn False\n\t\tif type(seq[0]) == FillRequestSeq:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturn False\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t \n\treturn seq in [FILL_REQUEST_SEQ_OPEN_ORDER, FILL_REQUEST_SEQ_FILLED_ORDER]\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\timport sqlite3\nimport sys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n\treturn True\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n\t# Check if seq starts with \"F\"\n\tif not seq.startswith(\"F\"):\n\t\treturn False\n\n\t# Check if seq contains only digits\n\tif not seq[1:].isdigit():\n\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n\tif seq is None:\n\t\treturn False\n\n\tif type(seq) == FillRequestSeq:\n\t\treturn True\n\tif type(seq) == list:\n\t\tif len(seq) == 0:\n\t\t\treturn False\n\t\tif type(seq[0]) == FillRequestSeq:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t \n\treturn seq in [FILL_REQUEST_SEQ_OPEN_ORDER, FILL_REQUEST_SEQ_FILLED_ORDER]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill_request') and hasattr(obj,'request')\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t1/0\n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill') and hasattr(obj,'request'):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\thas_fill = hasattr(obj, 'fill')\n\thas_request = hasattr(obj,'request')\n\treturn has_fill and has_request\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill_request') and hasattr(obj,'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t1/0\n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'fill') and hasattr(obj,'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill') and hasattr(obj,'request'):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t \n\thas_fill = hasattr(obj, 'fill')\n\thas_request = hasattr(obj,'request')\n\treturn has_fill and has_request\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t\n#\tif hasattr(obj, 'run'):\n#\t\treturn True\n#\telse:\n#\t\treturn False\n\treturn hasattr(obj, 'run')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\t# return len(inspect.getmembers(obj, inspect.ismethod)) == 1\n\treturn len(inspect.getmembers(obj, inspect.isroutine)) == 2\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, \"run\") and callable(obj.run)\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t\n#\tif hasattr(obj, 'run'):\n#\t\treturn True\n#\telse:\n#\t\treturn False\n\treturn hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\t# return len(inspect.getmembers(obj, inspect.ismethod)) == 1\n\treturn len(inspect.getmembers(obj, inspect.isroutine)) == 2\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, \"run\") and callable(obj.run)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\t \n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\thas_fill_method = hasattr(obj, 'fill') and callable(obj.fill)\n\thas_compute_method = hasattr(obj, 'compute') and callable(obj.compute)\n\treturn has_fill_method and has_compute_method\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\n\treturn obj.__class__.fill!= None and obj.__class__.compute!= None\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\n\tif not hasattr(obj, 'fill'):\n\t\treturn False\n\tif not hasattr(obj, 'compute'):\n\t\treturn False\n\treturn True\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill'):\n\t\tif callable(obj.fill):\n\t\t\tif hasattr(obj, 'compute'):\n\t\t\t\tif callable(obj.compute):\n\t\t\t\t\treturn True\n\treturn False\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t/src/pylib/pylib/utils/file.py\nimport os\nimport json\nimport numpy as np\nimport shutil\nfrom collections import OrderedDict\nfrom pylib.utils.log import get_logger\n\nlogger = get_logger(__name__)\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill'):\n\t\tif hasattr(obj, 'compute'):\n\t\t\treturn True\n\treturn False\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\telse:\n\t\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\thas_fill_method = hasattr(obj, 'fill') and callable(obj.fill)\n\thas_compute_method = hasattr(obj, 'compute') and callable(obj.compute)\n\treturn has_fill_method and has_compute_method\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\n\treturn obj.__class__.fill!= None and obj.__class__.compute!= None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\n\tif not hasattr(obj, 'fill'):\n\t\treturn False\n\tif not hasattr(obj, 'compute'):\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill'):\n\t\tif callable(obj.fill):\n\t\t\tif hasattr(obj, 'compute'):\n\t\t\t\tif callable(obj.compute):\n\t\t\t\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t/src/pylib/pylib/utils/file.py\nimport os\nimport json\nimport numpy as np\nimport shutil\nfrom collections import OrderedDict\nfrom pylib.utils.log import get_logger\n\nlogger = get_logger(__name__)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill'):\n\t\tif hasattr(obj, 'compute'):\n\t\t\treturn True\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t \n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\tif level == -1:\n\t\t\tlevel = len(d1.keys())\n\t\td3 = {}\n\t\tfor k in d1.keys():\n\t\t\tif type(d1[k]) is dict:\n\t\t\t\tif d1[k].keys()!= d2[k].keys():\n\t\t\t\t\td3[k] = difference(d1[k], d2[k], level-1)\n\t\t\telse:\n\t\t\t\tif d1[k]!= d2[k]:\n\t\t\t\t\td3[k] = d1[k]\n\t\treturn d3\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t \n\tresult = copy.deepcopy(d1)\n\tfor key, value in d2.items():\n\t\tif isinstance(value, collections.Mapping):\n\t\t\tvalue_diff = difference(value, d1.get(key, {}), level=level+1)\n\t\t\tif len(value_diff) > 0:\n\t\t\t\tresult[key] = value_diff\n\t\t\telse:\n\t\t\t\tdel result[key]\n\t\telse:\n\t\t\tif value == d1.get(key, None):\n\t\t\t\tdel result[key]\n\treturn result\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\td = {}\n\t\tfor k, v in d1.items():\n\t\t\t\tif k not in d2:\n\t\t\t\t\t\td[k] = v\n\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\t\t\t\tif level >= 0:\n\t\t\t\t\t\t\t\t\t\td[k] = difference(v, d2[k], level - 1)\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\td[k] = v\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tif v!= d2[k]:\n\t\t\t\t\t\t\t\t\t\td[k] = v\n\t\treturn d\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\tretDict = {}\n\t\tfor key in d1:\n\t\t\tif key in d2:\n\t\t\t\tif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n\t\t\t\t\tretDict[key] = difference(d1[key], d2[key], level=level+1)\n\t\t\t\telse:\n\t\t\t\t\tif d1[key]!= d2[key]:\n\t\t\t\t\t\tretDict[key] = (d1[key], d2[key])\n\t\t\telse:\n\t\t\t\tretDict[key] = d1[key]\n\t\treturn retDict\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\n\n\td = {}\n\n\tfor key in d1.keys():\n\t\tif not key in d2.keys():\n\t\t\td[key] = d1[key]\n\n\treturn d\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\td = d1.copy()\n\t\td.update(d2)\n\t\treturn dict( (k, d[k]) for k in d1.iterkeys() if k not in d2.iterkeys() )\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\td = d1.copy()\n\t\td.update(dict((k, v) for k, v in d2.iteritems() if k in d and v == d[k]))\n\t\treturn d\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\n\n\tif level == -1:\n\t\tlevel = len(d1)-1\n\tif level < 0:\n\t\treturn d1\n\treturn dict([(k, difference(v1, v2, level - 1))\n\t\tfor k, v1 in d1.iteritems()\n\t\tfor k, v2 in d2.iteritems()\n\t\tif k not in d2 or v1!= v2])\n\nif __name__ == '__main__':\n\td1 = {'a': 1, 'b': 2, 'c': {'d': 1, 'e': 2}}\n\td2 = {'a': 1, 'b': 2, 'c': {'d': 1, 'f': 3}}\n\tprint d1\n\tprint d2\n\tprint difference(d1, d2)\n/python/python2.6/python-2.6.6/Tools/scripts/test/test_parser.py\nimport unittest\r\nimport test.test_support\r\nimport sys\r\n\r"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\tif level == -1:\n\t\t\tlevel = len(d1.keys())\n\t\td3 = {}\n\t\tfor k in d1.keys():\n\t\t\tif type(d1[k]) is dict:\n\t\t\t\tif d1[k].keys()!= d2[k].keys():\n\t\t\t\t\td3[k] = difference(d1[k], d2[k], level-1)\n\t\t\telse:\n\t\t\t\tif d1[k]!= d2[k]:\n\t\t\t\t\td3[k] = d1[k]\n\t\treturn d3\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t \n\tresult = copy.deepcopy(d1)\n\tfor key, value in d2.items():\n\t\tif isinstance(value, collections.Mapping):\n\t\t\tvalue_diff = difference(value, d1.get(key, {}), level=level+1)\n\t\t\tif len(value_diff) > 0:\n\t\t\t\tresult[key] = value_diff\n\t\t\telse:\n\t\t\t\tdel result[key]\n\t\telse:\n\t\t\tif value == d1.get(key, None):\n\t\t\t\tdel result[key]\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\td = {}\n\t\tfor k, v in d1.items():\n\t\t\t\tif k not in d2:\n\t\t\t\t\t\td[k] = v\n\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\t\t\t\tif level >= 0:\n\t\t\t\t\t\t\t\t\t\td[k] = difference(v, d2[k], level - 1)\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\td[k] = v\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tif v!= d2[k]:\n\t\t\t\t\t\t\t\t\t\td[k] = v\n\t\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\tretDict = {}\n\t\tfor key in d1:\n\t\t\tif key in d2:\n\t\t\t\tif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n\t\t\t\t\tretDict[key] = difference(d1[key], d2[key], level=level+1)\n\t\t\t\telse:\n\t\t\t\t\tif d1[key]!= d2[key]:\n\t\t\t\t\t\tretDict[key] = (d1[key], d2[key])\n\t\t\telse:\n\t\t\t\tretDict[key] = d1[key]\n\t\treturn retDict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\n\n\td = {}\n\n\tfor key in d1.keys():\n\t\tif not key in d2.keys():\n\t\t\td[key] = d1[key]\n\n\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\td = d1.copy()\n\t\td.update(d2)\n\t\treturn dict( (k, d[k]) for k in d1.iterkeys() if k not in d2.iterkeys() )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\td = d1.copy()\n\t\td.update(dict((k, v) for k, v in d2.iteritems() if k in d and v == d[k]))\n\t\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\n\n\tif level == -1:\n\t\tlevel = len(d1)-1\n\tif level < 0:\n\t\treturn d1\n\treturn dict([(k, difference(v1, v2, level - 1))\n\t\tfor k, v1 in d1.iteritems()\n\t\tfor k, v2 in d2.iteritems()\n\t\tif k not in d2 or v1!= v2])\n\nif __name__ == '__main__':\n\td1 = {'a': 1, 'b': 2, 'c': {'d': 1, 'e': 2}}\n\td2 = {'a': 1, 'b': 2, 'c': {'d': 1, 'f': 3}}\n\tprint d1\n\tprint d2\n\tprint difference(d1, d2)\n/python/python2.6/python-2.6.6/Tools/scripts/test/test_parser.py\nimport unittest\r\nimport test.test_support\r\nimport sys\r\n\r", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < 0:\n\t\t\traise ValueError(\"Negative coordinate\")\n\t\tif coord >= self.size:\n\t\t\treturn\n\t\tself.data[coord] += weight\n\n\tdef fill_hist(self, hist, weight=1):\n\t\t\"\"\"\n\t\tFill histogram with the contents of *hist* with *weight*.\n\nThe *hist* histogram may have different size, but the coordinates must\nbe inside the histogram edges.\n\t\t\"\"\"\n\t\tfor coord in hist.data:\n\t\t\tself.fill(coord, weight * hist.data[coord])\n\n\tdef fill_coord(self, coord, count):\n\t\t\"\"\"\n\t\tFill histogram at *coord* with the given *count*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\t\tif coord < 0:\n\t\t\traise ValueError(\"Negative coordinate\")\n\t\tif coord >= self.size:\n\t\t\treturn\n\t\tself.data[coord] += count\n\n\tdef fill_array(self, array):\n\t\t\"\"\"\n\t\tFill histogram with values in *array*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\t\tfor coord in array:\n\t\t\tself.fill(coord)\n\n\tdef fill_hist_array(self, hist, array):\n\t\t\"\"\"\n\t\tFill histogram with values from *hist* and *array*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\t", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < self.min:\n\t\t\tcoord = self.min\n\t\telif coord > self.max:\n\t\t\tcoord = self.max\n\t\tself.bins[coord] += weight\n\n\tdef normalize(self):\n\t\"\"\"\n\tNormalize the histogram to 1.\n\nThe histogram will have the same shape and edges, but all bin contents will be divided by the total sum.\n\t\"\"\"\n\t\ttotal = sum(self.bins)\n\t\tif total > 0:\n\t\t\tself.bins = [v/float(total) for v in self.bins]\n\n\tdef find_nearest(self, coord):\n\t\"\"\"\n\tFind the bin nearest to the given *coord*.\n\nReturns a tuple (*bin_idx*, *bin_value*).\n\t\"\"\"\n\t\tidx = int((coord - self.min) / self.step)\n\t\treturn (idx, self.bins[idx])\n\n\tdef find_nearest_x(self, coord):\n\t\"\"\"\n\tFind the bin nearest to the given *coord*.\n\nReturns a tuple (*bin_x*, *bin_value*).\n\t\"\"\"\n\t\tidx, value = self.find_nearest(coord)\n\t\treturn (self.min + (idx + 0.5)*self.step, value)\n\n\tdef find_nearest_y(self, coord):\n\t\"\"\"\n\t", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif self._outside(coord):\n\t\t\treturn\n\t\telse:\n\t\t\tself.values[self.index(coord)] += weight\n\n\tdef index(self, coord):\n\t\t\"\"\"\nReturn the index corresponding to *coord* in the flattened histogram.\n\t\t\"\"\"\n\t\treturn ((self.shape[0] - 1) * coord[0]) + (coord[1] - 1)\n\n\tdef outside(self, coord):\n\t\t\"\"\"\nReturn True if *coord* is outside the histogram edges.\n\t\"\"\"\n\t\treturn self._outside(coord)\n#!/bin/bash\n\n#SBATCH -t 1-00:00:00\n#SBATCH -N 1\n#SBATCH -c 4\n#SBATCH -p normal\n#SBATCH --mem=2000\n#SBATCH -o /home/scott/python/output/output_%j.out\n#SBATCH -e /home/scott/python/output/output_%j.err\n#SBATCH --mail-user=\n#SBATCH --mail-type=END\n\nmodule load anaconda3\n\n# Launch the job\npython /home/scott/python/jobs.py\n#ifndef _STAC_H\n#define _STAC_H\n\n#define STAC_NULL -1\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.width:\n\t\t\treturn\n\t\tif coord[1] < 0 or coord[1] >= self.height:\n\t\t\treturn\n\t\tself.data[coord[1]][coord[0]] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[1] < 0 or coord[0] >= self.width or coord[1] >= self.height:\n\t\t\treturn\n\t\tself.counts[coord[0] + coord[1] * self.width] += weight\n\t\treturn\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < self.min or coord > self.max:\n\t\t\treturn\n\t\tself.data[coord - self.min] += weight\n\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tcoord = self.coord(coord)\n\t\tif not self.check(coord):\n\t\t\treturn\n\t\tself.data[coord] += weight\n\n\tdef coord(self, x):\n\t\t\"\"\"\n\tConvert *x* to the histogram coordinate.\n\n\t*x* can be a sequence of coordinates, a 1D array, or a 2D array.\n\tReturns an array of coordinates.\n\t\"\"\"\n\t\tx = np.asarray(x)\n\t\tif x.ndim == 0:\n\t\t\tx = x[np.newaxis]\n\t\tif x.ndim not in (1, 2):\n\t\t\traise ValueError(\"Invalid number of dimensions: %d\" % x.ndim)\n\t\tif x.shape[1]!= self.ndim:\n\t\t\traise ValueError(\"Invalid number of dimensions: %d\" % x.shape[1])\n\n\t\tif x.ndim == 2:\n\t\t\tx = np.round(self.scale * (x - self.min) / self.delta).astype(int)\n\t\t\tx = np.clip(x, 0, self.shape - 1)\n\t\telse:\n\t\t\tx = np.clip(x, 0, self.shape - 1).astype(int)\n\n\t\treturn x\n\n\tdef coord_to_index(self, coord):\n\t\t\"\"\"\n\tConvert *coord*", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.dimensions[0]:\n\t\t\treturn\n\t\tif coord[1] < 0 or coord[1] >= self.dimensions[1]:\n\t\t\treturn\n\t\tself.histogram[coord[0]][coord[1]] += weight\n\n\n\tdef fill_circle(self, coord, radius, weight=1):\n\t\"\"\"\n\tFill a circle of *radius* centered at *coord* with the given *weight*.\n\t\"\"\"\n\t\tfor x in xrange(-radius, radius + 1):\n\t\t\tfor y in xrange(-radius, radius + 1):\n\t\t\t\tif x * x + y * y <= radius * radius:\n\t\t\t\t\tself.fill(coord, weight)\n\n\n\tdef get_histogram(self):\n\t\"\"\"\n\tReturn the histogram as a 2D array.\n\t\"\"\"\n\t\treturn self.histogram\n\n\n\tdef get_histogram_as_image(self):\n\t\"\"\"\n\tReturn the histogram as a PIL image.\n\t\"\"\"\n\t\t# Create PIL image\n\t\twidth, height = self.dimensions\n\t\timage = Image.new(\"L\", (width, height), 0)\n\t\tpixels = image.load()\n\n\t\t# Convert histogram to pixel values\n\t\tmin_val = min(self.histogram)\n\t\tmax_val = max(self.histogram)\n\t\tdiff =", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < self.min or coord > self.max:\n\t\t\treturn\n\t\tself.bins[self.bin_index(coord)] += weight\n\t#end def\n#end class\n\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.size[0]:\n\t\t\treturn\n\t\tif coord[1] < 0 or coord[1] >= self.size[1]:\n\t\t\treturn\n\t\tself.data[coord[0]][coord[1]] += weight\n\n\tdef get(self, coord):\n\t\"\"\"\n\tGet value at *coord*.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.size[0]:\n\t\t\treturn 0\n\t\tif coord[1] < 0 or coord[1] >= self.size[1]:\n\t\t\treturn 0\n\t\treturn self.data[coord[0]][coord[1]]\n\n\tdef __add__(self, other):\n\t\"\"\"\n\tAdd this histogram to another.\n\t\"\"\"\n\t\tif self.size!= other.size:\n\t\t\traise Exception('Histograms must be of the same size')\n\n\t\tresult = Histogram(self.size)\n\t\tfor i in range(self.size[0]):\n\t\t\tfor j in range(self.size[1]):\n\t\t\t\tresult.data[i][j] = self.data[i][j] + other.data[i][j]\n\n\t\treturn result\n\n\tdef normalize(self):\n\t\"\"\"\n\tNormalize this histogram to have a sum of 1.\n\t\"\"\""], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < 0:\n\t\t\traise ValueError(\"Negative coordinate\")\n\t\tif coord >= self.size:\n\t\t\treturn\n\t\tself.data[coord] += weight\n\n\tdef fill_hist(self, hist, weight=1):\n\t\t\"\"\"\n\t\tFill histogram with the contents of *hist* with *weight*.\n\nThe *hist* histogram may have different size, but the coordinates must\nbe inside the histogram edges.\n\t\t\"\"\"\n\t\tfor coord in hist.data:\n\t\t\tself.fill(coord, weight * hist.data[coord])\n\n\tdef fill_coord(self, coord, count):\n\t\t\"\"\"\n\t\tFill histogram at *coord* with the given *count*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\t\tif coord < 0:\n\t\t\traise ValueError(\"Negative coordinate\")\n\t\tif coord >= self.size:\n\t\t\treturn\n\t\tself.data[coord] += count\n\n\tdef fill_array(self, array):\n\t\t\"\"\"\n\t\tFill histogram with values in *array*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\t\tfor coord in array:\n\t\t\tself.fill(coord)\n\n\tdef fill_hist_array(self, hist, array):\n\t\t\"\"\"\n\t\tFill histogram with values from *hist* and *array*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < self.min:\n\t\t\tcoord = self.min\n\t\telif coord > self.max:\n\t\t\tcoord = self.max\n\t\tself.bins[coord] += weight\n\n\tdef normalize(self):\n\t\"\"\"\n\tNormalize the histogram to 1.\n\nThe histogram will have the same shape and edges, but all bin contents will be divided by the total sum.\n\t\"\"\"\n\t\ttotal = sum(self.bins)\n\t\tif total > 0:\n\t\t\tself.bins = [v/float(total) for v in self.bins]\n\n\tdef find_nearest(self, coord):\n\t\"\"\"\n\tFind the bin nearest to the given *coord*.\n\nReturns a tuple (*bin_idx*, *bin_value*).\n\t\"\"\"\n\t\tidx = int((coord - self.min) / self.step)\n\t\treturn (idx, self.bins[idx])\n\n\tdef find_nearest_x(self, coord):\n\t\"\"\"\n\tFind the bin nearest to the given *coord*.\n\nReturns a tuple (*bin_x*, *bin_value*).\n\t\"\"\"\n\t\tidx, value = self.find_nearest(coord)\n\t\treturn (self.min + (idx + 0.5)*self.step, value)\n\n\tdef find_nearest_y(self, coord):\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif self._outside(coord):\n\t\t\treturn\n\t\telse:\n\t\t\tself.values[self.index(coord)] += weight\n\n\tdef index(self, coord):\n\t\t\"\"\"\nReturn the index corresponding to *coord* in the flattened histogram.\n\t\t\"\"\"\n\t\treturn ((self.shape[0] - 1) * coord[0]) + (coord[1] - 1)\n\n\tdef outside(self, coord):\n\t\t\"\"\"\nReturn True if *coord* is outside the histogram edges.\n\t\"\"\"\n\t\treturn self._outside(coord)\n#!/bin/bash\n\n#SBATCH -t 1-00:00:00\n#SBATCH -N 1\n#SBATCH -c 4\n#SBATCH -p normal\n#SBATCH --mem=2000\n#SBATCH -o /home/scott/python/output/output_%j.out\n#SBATCH -e /home/scott/python/output/output_%j.err\n#SBATCH --mail-user=\n#SBATCH --mail-type=END\n\nmodule load anaconda3\n\n# Launch the job\npython /home/scott/python/jobs.py\n#ifndef _STAC_H\n#define _STAC_H\n\n#define STAC_NULL -1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.width:\n\t\t\treturn\n\t\tif coord[1] < 0 or coord[1] >= self.height:\n\t\t\treturn\n\t\tself.data[coord[1]][coord[0]] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[1] < 0 or coord[0] >= self.width or coord[1] >= self.height:\n\t\t\treturn\n\t\tself.counts[coord[0] + coord[1] * self.width] += weight\n\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < self.min or coord > self.max:\n\t\t\treturn\n\t\tself.data[coord - self.min] += weight\n\n\tdef __str__(self):\n\t\treturn \"%s: %s\" % (self.__class__.__name__, self.data)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tcoord = self.coord(coord)\n\t\tif not self.check(coord):\n\t\t\treturn\n\t\tself.data[coord] += weight\n\n\tdef coord(self, x):\n\t\t\"\"\"\n\tConvert *x* to the histogram coordinate.\n\n\t*x* can be a sequence of coordinates, a 1D array, or a 2D array.\n\tReturns an array of coordinates.\n\t\"\"\"\n\t\tx = np.asarray(x)\n\t\tif x.ndim == 0:\n\t\t\tx = x[np.newaxis]\n\t\tif x.ndim not in (1, 2):\n\t\t\traise ValueError(\"Invalid number of dimensions: %d\" % x.ndim)\n\t\tif x.shape[1]!= self.ndim:\n\t\t\traise ValueError(\"Invalid number of dimensions: %d\" % x.shape[1])\n\n\t\tif x.ndim == 2:\n\t\t\tx = np.round(self.scale * (x - self.min) / self.delta).astype(int)\n\t\t\tx = np.clip(x, 0, self.shape - 1)\n\t\telse:\n\t\t\tx = np.clip(x, 0, self.shape - 1).astype(int)\n\n\t\treturn x\n\n\tdef coord_to_index(self, coord):\n\t\t\"\"\"\n\tConvert *coord*", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.dimensions[0]:\n\t\t\treturn\n\t\tif coord[1] < 0 or coord[1] >= self.dimensions[1]:\n\t\t\treturn\n\t\tself.histogram[coord[0]][coord[1]] += weight\n\n\n\tdef fill_circle(self, coord, radius, weight=1):\n\t\"\"\"\n\tFill a circle of *radius* centered at *coord* with the given *weight*.\n\t\"\"\"\n\t\tfor x in xrange(-radius, radius + 1):\n\t\t\tfor y in xrange(-radius, radius + 1):\n\t\t\t\tif x * x + y * y <= radius * radius:\n\t\t\t\t\tself.fill(coord, weight)\n\n\n\tdef get_histogram(self):\n\t\"\"\"\n\tReturn the histogram as a 2D array.\n\t\"\"\"\n\t\treturn self.histogram\n\n\n\tdef get_histogram_as_image(self):\n\t\"\"\"\n\tReturn the histogram as a PIL image.\n\t\"\"\"\n\t\t# Create PIL image\n\t\twidth, height = self.dimensions\n\t\timage = Image.new(\"L\", (width, height), 0)\n\t\tpixels = image.load()\n\n\t\t# Convert histogram to pixel values\n\t\tmin_val = min(self.histogram)\n\t\tmax_val = max(self.histogram)\n\t\tdiff =", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord < self.min or coord > self.max:\n\t\t\treturn\n\t\tself.bins[self.bin_index(coord)] += weight\n\t#end def\n#end class\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.size[0]:\n\t\t\treturn\n\t\tif coord[1] < 0 or coord[1] >= self.size[1]:\n\t\t\treturn\n\t\tself.data[coord[0]][coord[1]] += weight\n\n\tdef get(self, coord):\n\t\"\"\"\n\tGet value at *coord*.\n\t\"\"\"\n\t\tif coord[0] < 0 or coord[0] >= self.size[0]:\n\t\t\treturn 0\n\t\tif coord[1] < 0 or coord[1] >= self.size[1]:\n\t\t\treturn 0\n\t\treturn self.data[coord[0]][coord[1]]\n\n\tdef __add__(self, other):\n\t\"\"\"\n\tAdd this histogram to another.\n\t\"\"\"\n\t\tif self.size!= other.size:\n\t\t\traise Exception('Histograms must be of the same size')\n\n\t\tresult = Histogram(self.size)\n\t\tfor i in range(self.size[0]):\n\t\t\tfor j in range(self.size[1]):\n\t\t\t\tresult.data[i][j] = self.data[i][j] + other.data[i][j]\n\n\t\treturn result\n\n\tdef normalize(self):\n\t\"\"\"\n\tNormalize this histogram to have a sum of 1.\n\t\"\"\"", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t \n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tif not isinstance(labels, dict):\n\t\t\traise TypeError('labels must be of type dict, not {}'.format(type(labels).__name__))\n\t\t\n\t\tfor k, v in labels.iteritems():\n\t\t\tvalidate_key(k)\n\t\t\tvalidate_value(v)\n\t\treturn labels\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tfor key in labels:\n\t\tvalidate_key(key)\n\t\tvalidate_value(labels[key])\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tfor key, value in labels.items():\n\t\t\tif not validate_key(key):\n\t\t\t\traise ValueError(\"Invalid key: \" + key)\n\t\t\tif not validate_value(value):\n\t\t\t\traise ValueError(\"Invalid value: \" + value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t \n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\t\t\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\t# Check that labels are a dictionary\n\tif not isinstance(labels, dict):\n\t\traise TypeError('Labels must be a dictionary.')\n\n\t# Check that each key and value are valid\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\n\t# Check that the key set and value set are equal\n\tif len(labels)!= len(set(labels.items())):\n\t\traise ValueError('The given labels are not unique.')\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t \n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tfor key, value in labels.items():\n\t\t\tvalidate_key(key)\n\t\t\tvalidate_value(value)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t \n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tif not isinstance(labels, dict):\n\t\t\traise TypeError('labels must be of type dict, not {}'.format(type(labels).__name__))\n\t\t\n\t\tfor k, v in labels.iteritems():\n\t\t\tvalidate_key(k)\n\t\t\tvalidate_value(v)\n\t\treturn labels\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tfor key in labels:\n\t\tvalidate_key(key)\n\t\tvalidate_value(labels[key])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tfor key, value in labels.items():\n\t\t\tif not validate_key(key):\n\t\t\t\traise ValueError(\"Invalid key: \" + key)\n\t\t\tif not validate_value(value):\n\t\t\t\traise ValueError(\"Invalid value: \" + value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t \n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\t# Check that labels are a dictionary\n\tif not isinstance(labels, dict):\n\t\traise TypeError('Labels must be a dictionary.')\n\n\t# Check that each key and value are valid\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\n\t# Check that the key set and value set are equal\n\tif len(labels)!= len(set(labels.items())):\n\t\traise ValueError('The given labels are not unique.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t \n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\tfor key, value in labels.items():\n\t\t\tvalidate_key(key)\n\t\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\t# Define the regex for the resource name\n\treturn re.compile(\"^[a-z][a-z0-9-]*[a-z0-9]$\")\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\n\treturn {\n\t\t\"krake_role\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_user\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_group\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_group\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_user\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_role\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_arn\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_name\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_statement_id\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"krake\",\n    version=\"0.1\",\n    packages=find_packages(),\n    install_requires=[\n        \"pyyaml\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"krake = krake.krake:main\",\n        ],\n    },\n)\nimport os\nimport sys\nimport yaml\nimport argparse\nfrom. import util\nfrom. import config\nfrom. import template\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\treturn [re.compile(r'^[0-9a-zA-Z-]+$')]\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t \n\treturn re.compile('^[a-zA-Z0-9_]+$')\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\t\"\"\"\n\t#TODO: Validate that the arguments are the same type\n\t#TODO: Validate that the arguments are correct\n\t#TODO: Validate that the arguments are in the correct order (or that the order is not important)\n\t\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t \n\treturn re.compile(\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\treturn {\n\t\t\t'id': r\"^([A-Za-z0-9\\-\\._~]+)$\",\n\t\t\t'name': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'url': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'path': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'slug': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'description': r\"^([A-Za-z0-9\\-_]+)$\"\n\t\t}\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n#\treturn re.compile(\"^[a-zA-Z0-9_]+$\")\n\treturn re.compile(\"^[a-zA-Z0-9_]+$\")\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\t# Define the regex for the resource name\n\treturn re.compile(\"^[a-z][a-z0-9-]*[a-z0-9]$\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\n\treturn {\n\t\t\"krake_role\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_user\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_group\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_group\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_user\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_role\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_arn\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_name\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t\"krake_policy_statement_id\": r\"^[a-zA-Z][a-zA-Z0-9_-]*$\",\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"krake\",\n    version=\"0.1\",\n    packages=find_packages(),\n    install_requires=[\n        \"pyyaml\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"krake = krake.krake:main\",\n        ],\n    },\n)\nimport os\nimport sys\nimport yaml\nimport argparse\nfrom. import util\nfrom. import config\nfrom. import template\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\treturn [re.compile(r'^[0-9a-zA-Z-]+$')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t \n\treturn re.compile('^[a-zA-Z0-9_]+$')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\t\"\"\"\n\t#TODO: Validate that the arguments are the same type\n\t#TODO: Validate that the arguments are correct\n\t#TODO: Validate that the arguments are in the correct order (or that the order is not important)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t \n\treturn re.compile(\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\treturn {\n\t\t\t'id': r\"^([A-Za-z0-9\\-\\._~]+)$\",\n\t\t\t'name': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'url': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'path': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'slug': r\"^([A-Za-z0-9\\-_]+)$\",\n\t\t\t'description': r\"^([A-Za-z0-9\\-_]+)$\"\n\t\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n#\treturn re.compile(\"^[a-zA-Z0-9_]+$\")\n\treturn re.compile(\"^[a-zA-Z0-9_]+$\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\tif not re.match(regex, value):\n\t\t\traise ValueError\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\t# Get the regular expression for the given value.\n\tregex = REGEX_FOR_VALUES[value]\n\n\t# Get the value as a string without whitespace.\n\tvalue_str = str(value).strip()\n\n\t# If the value fails to match the regular expression, raise an error.\n\tif not re.fullmatch(regex, value_str):\n\t\traise ValueError(f\"Invalid value: {value_str}\")\n\n\treturn value_str\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\tprint(\"Hello world!\")\n/18.12.2017.py\n# -*- coding: utf-8 -*-\n\nimport random\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif not re.match(value, re.compile(regex)):\n\t\traise ValidationError(u'Please enter a valid %s.' % name)\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\tpattern = REGEX_MAP[value.tag]\n\t\tif pattern is None:\n\t\t\traise ValueError(\"No validation regex defined for tag %s\" % value.tag)\n\t\tif not pattern.match(value):\n\t\t\traise ValueError(\"Invalid value for tag %s: %s\" % (value.tag, value))\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n#\tprint(\"Validating value: \" + value)\n\t\n\tif value == None:\n\t\treturn True\n\telse:\n\t\treturn re.match(REGEX_NUMBER, value)!= None\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif not re.match(REGEX_EMAIL, value):\n\t\traise ValidationError(\n\t\t\t_('%(value)s is not a valid email address'),\n\t\t\tparams={'value': value},\n\t\t)\n\n\treturn value\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif value == None:\n\t\treturn True\n\t\n\t# Check if the value is valid against the regular expression\n\tif re.match(value_regexes[value], value) == None:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif not re.match(value, REGEX_EMAIL):\n\t\treturn \"Not a valid email address\"\n\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\tif not re.match(regex, value):\n\t\t\traise ValueError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\t# Get the regular expression for the given value.\n\tregex = REGEX_FOR_VALUES[value]\n\n\t# Get the value as a string without whitespace.\n\tvalue_str = str(value).strip()\n\n\t# If the value fails to match the regular expression, raise an error.\n\tif not re.fullmatch(regex, value_str):\n\t\traise ValueError(f\"Invalid value: {value_str}\")\n\n\treturn value_str\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\tprint(\"Hello world!\")\n/18.12.2017.py\n# -*- coding: utf-8 -*-\n\nimport random\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif not re.match(value, re.compile(regex)):\n\t\traise ValidationError(u'Please enter a valid %s.' % name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\tpattern = REGEX_MAP[value.tag]\n\t\tif pattern is None:\n\t\t\traise ValueError(\"No validation regex defined for tag %s\" % value.tag)\n\t\tif not pattern.match(value):\n\t\t\traise ValueError(\"Invalid value for tag %s: %s\" % (value.tag, value))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n#\tprint(\"Validating value: \" + value)\n\t\n\tif value == None:\n\t\treturn True\n\telse:\n\t\treturn re.match(REGEX_NUMBER, value)!= None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif not re.match(REGEX_EMAIL, value):\n\t\traise ValidationError(\n\t\t\t_('%(value)s is not a valid email address'),\n\t\t\tparams={'value': value},\n\t\t)\n\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif value == None:\n\t\treturn True\n\t\n\t# Check if the value is valid against the regular expression\n\tif re.match(value_regexes[value], value) == None:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\tif not re.match(value, REGEX_EMAIL):\n\t\treturn \"Not a valid email address\"\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\t# Check if the length is correct\n\tif len(key)!= 16:\n\t\treturn False\n\n\t# Check if the key is alphanumeric\n\tif not re.match(\"[a-zA-Z0-9]+$\", key):\n\t\treturn False\n\n\treturn True\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\treturn re.match(KEY_REGEX, key)\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\treturn re.match(KEY_PATTERN, key) is not None\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\t#TODO: Define the regular expression here\n\tregex =...\n\treturn re.match(regex, key) is not None\n/README.md\n# n\npython\npython code examples and exercises\n\n## The exercises\n\n* [AVL tree](avl-tree)\n* [Bloom filter](bloom-filter)\n* [Binary search tree](binary-search-tree)\n* [Bubble sort](bubble-sort)\n* [Cuckoo hash table](cuckoo-hash-table)\n* [Dijkstra's algorithm](dijkstra)\n* [Hamming distance](hamming-distance)\n* [Heap sort](heap-sort)\n* [Hash table](hash-table)\n* [Insertion sort](insertion-sort)\n* [KNN classifier](knn-classifier)\n* [Largest sum subarray](largest-sum-subarray)\n* [Linear search](linear-search)\n* [Longest common subsequence](longest-common-subsequence)\n* [Merge sort](merge-sort)\n* [Minimum spanning tree](minimum-spanning-tree)\n* [Nearest neighbor classifier](nearest-neighbor-classifier)\n* [Percolation](percolation)\n* [Priority queue](priority-queue)\n* [Quicksort](quicksort)\n* [Radix sort", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\t# The regular expression to match any valid key\n\tkey_re = re.compile(r\"^[a-zA-Z_][a-zA-Z_0-9]*$\")\n\n\t# If the regular expression doesn't match, return False\n\tif not key_re.match(key):\n\t\treturn False\n\treturn True\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\treturn re.match(KEY_REGEXP, key) is not None\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\t# Check if the length is correct\n\tif len(key)!= 16:\n\t\treturn False\n\n\t# Check if the key is alphanumeric\n\tif not re.match(\"[a-zA-Z0-9]+$\", key):\n\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\treturn re.match(KEY_REGEX, key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\n\treturn re.match(KEY_PATTERN, key) is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\t#TODO: Define the regular expression here\n\tregex =...\n\treturn re.match(regex, key) is not None\n/README.md\n# n\npython\npython code examples and exercises\n\n## The exercises\n\n* [AVL tree](avl-tree)\n* [Bloom filter](bloom-filter)\n* [Binary search tree](binary-search-tree)\n* [Bubble sort](bubble-sort)\n* [Cuckoo hash table](cuckoo-hash-table)\n* [Dijkstra's algorithm](dijkstra)\n* [Hamming distance](hamming-distance)\n* [Heap sort](heap-sort)\n* [Hash table](hash-table)\n* [Insertion sort](insertion-sort)\n* [KNN classifier](knn-classifier)\n* [Largest sum subarray](largest-sum-subarray)\n* [Linear search](linear-search)\n* [Longest common subsequence](longest-common-subsequence)\n* [Merge sort](merge-sort)\n* [Minimum spanning tree](minimum-spanning-tree)\n* [Nearest neighbor classifier](nearest-neighbor-classifier)\n* [Percolation](percolation)\n* [Priority queue](priority-queue)\n* [Quicksort](quicksort)\n* [Radix sort", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\t# The regular expression to match any valid key\n\tkey_re = re.compile(r\"^[a-zA-Z_][a-zA-Z_0-9]*$\")\n\n\t# If the regular expression doesn't match, return False\n\tif not key_re.match(key):\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\treturn re.match(KEY_REGEXP, key) is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t \n\tschema_dict = dict()\n\tfor key in manifest_dict:\n\t\tif key == 'app':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'config':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'build':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key =='resources':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'run':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'test':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'deployments':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'events':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'webhooks':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'labels':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'annotations':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'owner':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == '", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\tschema_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tval = manifest_dict[key]\n\t\tif isinstance(val, dict):\n\t\t\tif not first_level:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(val, first_level=True)\n\t\t\telse:\n\t\t\t\tschema_dict[key] = {}\n\t\telif isinstance(val, list):\n\t\t\tif not first_level:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(val[0], first_level=True)\n\t\t\telse:\n\t\t\t\tschema_dict[key] = []\n\t\telse:\n\t\t\tif not first_level:\n\t\t\t\tschema_dict[key] = val\n\t\t\telse:\n\t\t\t\tschema_dict[key] = None\n\treturn schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t \n\tgenerated_dict = {}\n\tfor key in manifest_dict:\n\t\tvalue = manifest_dict[key]\n\n\t\tif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tgenerated_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\t\telse:\n\t\t\t\tgenerated_dict[key] = generate_default_observer_schema_dict(value)\n\n\t\telif isinstance(value, list):\n\t\t\tgenerated_dict[key] = []\n\n\t\telse:\n\t\t\tgenerated_dict[key] = value\n\n\treturn generated_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tschema_dict = {}\n\tfor key in manifest_dict:\n\t\tif first_level:\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telse:\n\t\t\tif type(manifest_dict[key]) is dict:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], True)\n\t\t\telif type(manifest_dict[key]) is list:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_list(manifest_dict[key])\n\t\t\telse:\n\t\t\t\tschema_dict[key] = manifest_dict[key]\n\treturn schema_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tif first_level:\n\t\tschema_dict = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"path\": {\n\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\"description\": \"The path to the file or directory that is being monitored.\"\n\t\t\t\t},\n\t\t\t\t\"recursive\": {\n\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\"description\": \"If True, the monitor will recursively monitor all subdirectories.\"\n\t\t\t\t},\n\t\t\t\t\"events\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"create\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is created in the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"delete\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is deleted from the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"modify\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is modified in the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"move\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is moved into or out of the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"attribute_change", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tnew_dict = {}\n\n\t# first level keys are the keys in the first level of the manifest json file.\n\t# for the first level, only dict and list are allowed.\n\tif first_level:\n\t\tif not isinstance(manifest_dict, dict) and not isinstance(manifest_dict, list):\n\t\t\traise TypeError(\"The first level dictionary should only contain dict and list.\")\n\n\t\tif isinstance(manifest_dict, dict):\n\t\t\tfor key in manifest_dict.keys():\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=False)\n\t\telif isinstance(manifest_dict, list):\n\t\t\tfor item in manifest_dict:\n\t\t\t\tnew_dict.append(generate_default_observer_schema_dict(item, first_level=False))\n\t# non-first level keys are the keys in the manifest json file that are not in the first level.\n\t# for the non-first level, only dict are allowed.\n\telse:\n\t\tif not isinstance(manifest_dict, dict):\n\t\t\traise TypeError(\"The non-first level dictionary should only contain dict.\")\n\n\t\tfor key in manifest_dict.keys():\n\t\t\tif key == \"$schema\":\n\t\t\t\tcontinue\n\t\t\telif key == \"description\":\n\t\t\t\tnew_dict[key] = manifest_dict[key", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tif first_level:\n\t\tnew_dict = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"description\": \"Root object of the schema\",\n\t\t\t\"properties\": {},\n\t\t\t\"required\": []\n\t\t}\n\telse:\n\t\tnew_dict = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"description\": \"Root object of the schema\",\n\t\t\t\"properties\": {},\n\t\t\t\"required\": []\n\t\t}\n\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[\"properties\"][key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"array\", \"items\": generate_default_observer_schema_dict(value[0], first_level=True)}\n\t\telse:\n\t\t\tif isinstance(value, str) and value.lower() == \"str\":\n\t\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"string\"}\n\t\t\telif isinstance(value, str) and value.lower() == \"int\":\n\t\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"integer\"}\n\t\t\telif isinstance(value, str) and value.lower() == \"bool\":\n\t\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"boolean\"}\n\t\t\t", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t \n\t# Create a new empty dictionary to store the new schema\n\tnew_dict = {}\n\n\t# Iterate through all the keys in the manifest_dict\n\tfor key, value in manifest_dict.iteritems():\n\t\t# If the value is a dict, then recursively call this function\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\t\t# If the value is a list, then append the \"list\" keyword\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = \"list\"\n\t\t# If the value is a string, then append the \"string\" keyword\n\t\telif isinstance(value, str):\n\t\t\tnew_dict[key] = \"string\"\n\t\t# If the value is an integer, then append the \"integer\" keyword\n\t\telif isinstance(value, int):\n\t\t\tnew_dict[key] = \"integer\"\n\t\t# If the value is a boolean, then append the \"boolean\" keyword\n\t\telif isinstance(value, bool):\n\t\t\tnew_dict[key] = \"boolean\"\n\t\t# If the value is None, then append the \"null\" keyword\n\t\telif value is None:\n\t\t\tnew_dict[key] = \"null\"\n\t\t# If the value is a float, then append the \"double\"", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tnew_dictionary = {}\n\tfor key, value in manifest_dict.iteritems():\n\t\tif first_level:\n\t\t\tif type(value) is dict:\n\t\t\t\tnew_dictionary[key] = generate_default_observer_schema_dict(value, False)\n\t\t\telse:\n\t\t\t\tif type(value) is list:\n\t\t\t\t\tnew_dictionary[key] = value\n\t\t\t\telse:\n\t\t\t\t\tnew_dictionary[key] = default_value_dict[key]\n\t\telse:\n\t\t\tif type(value) is dict:\n\t\t\t\tnew_dictionary[key] = generate_default_observer_schema_dict(value, False)\n\t\t\telse:\n\t\t\t\tif type(value) is list:\n\t\t\t\t\tnew_dictionary[key] = value\n\t\t\t\telse:\n\t\t\t\t\tnew_dictionary[key] = default_value_dict[key]\n\n\treturn new_dictionary\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\t# The schema dict to be returned\n\tobserver_schema_dict = {\n\t\t\"id\": \"observer\",\n\t\t\"type\": \"object\",\n\t\t\"properties\": {\n\t\t\t\"name\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"description\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"type\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"enum\": [\n\t\t\t\t\t\"observer\",\n\t\t\t\t\t\"external\"\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"location\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"status\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"enum\": [\n\t\t\t\t\t\"active\",\n\t\t\t\t\t\"inactive\"\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"statusReason\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"contactInfo\": {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"contactName\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"minLength\": 1\n\t\t\t\t\t},\n\t\t\t\t\t\"contactEmail\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"format\": \"email\"\n\t\t\t\t\t},\n\t\t\t\t\t\"contactPhone\": {\n\t\t\t\t\t\t\"type\": \"string"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t \n\tschema_dict = dict()\n\tfor key in manifest_dict:\n\t\tif key == 'app':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'config':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'build':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key =='resources':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'run':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'test':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'deployments':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'events':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'webhooks':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'labels':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'annotations':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == 'owner':\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telif key == '", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\tschema_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tval = manifest_dict[key]\n\t\tif isinstance(val, dict):\n\t\t\tif not first_level:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(val, first_level=True)\n\t\t\telse:\n\t\t\t\tschema_dict[key] = {}\n\t\telif isinstance(val, list):\n\t\t\tif not first_level:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(val[0], first_level=True)\n\t\t\telse:\n\t\t\t\tschema_dict[key] = []\n\t\telse:\n\t\t\tif not first_level:\n\t\t\t\tschema_dict[key] = val\n\t\t\telse:\n\t\t\t\tschema_dict[key] = None\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t \n\tgenerated_dict = {}\n\tfor key in manifest_dict:\n\t\tvalue = manifest_dict[key]\n\n\t\tif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tgenerated_dict[key] = generate_default_observer_schema_dict(value, first_level=False)\n\t\t\telse:\n\t\t\t\tgenerated_dict[key] = generate_default_observer_schema_dict(value)\n\n\t\telif isinstance(value, list):\n\t\t\tgenerated_dict[key] = []\n\n\t\telse:\n\t\t\tgenerated_dict[key] = value\n\n\treturn generated_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tschema_dict = {}\n\tfor key in manifest_dict:\n\t\tif first_level:\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\telse:\n\t\t\tif type(manifest_dict[key]) is dict:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], True)\n\t\t\telif type(manifest_dict[key]) is list:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_list(manifest_dict[key])\n\t\t\telse:\n\t\t\t\tschema_dict[key] = manifest_dict[key]\n\treturn schema_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tif first_level:\n\t\tschema_dict = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"path\": {\n\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\"description\": \"The path to the file or directory that is being monitored.\"\n\t\t\t\t},\n\t\t\t\t\"recursive\": {\n\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\"description\": \"If True, the monitor will recursively monitor all subdirectories.\"\n\t\t\t\t},\n\t\t\t\t\"events\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"create\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is created in the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"delete\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is deleted from the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"modify\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is modified in the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"move\": {\n\t\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\t\"description\": \"If True, the monitor will send an event when a file is moved into or out of the monitored directory.\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"attribute_change", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tnew_dict = {}\n\n\t# first level keys are the keys in the first level of the manifest json file.\n\t# for the first level, only dict and list are allowed.\n\tif first_level:\n\t\tif not isinstance(manifest_dict, dict) and not isinstance(manifest_dict, list):\n\t\t\traise TypeError(\"The first level dictionary should only contain dict and list.\")\n\n\t\tif isinstance(manifest_dict, dict):\n\t\t\tfor key in manifest_dict.keys():\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level=False)\n\t\telif isinstance(manifest_dict, list):\n\t\t\tfor item in manifest_dict:\n\t\t\t\tnew_dict.append(generate_default_observer_schema_dict(item, first_level=False))\n\t# non-first level keys are the keys in the manifest json file that are not in the first level.\n\t# for the non-first level, only dict are allowed.\n\telse:\n\t\tif not isinstance(manifest_dict, dict):\n\t\t\traise TypeError(\"The non-first level dictionary should only contain dict.\")\n\n\t\tfor key in manifest_dict.keys():\n\t\t\tif key == \"$schema\":\n\t\t\t\tcontinue\n\t\t\telif key == \"description\":\n\t\t\t\tnew_dict[key] = manifest_dict[key", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tif first_level:\n\t\tnew_dict = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"description\": \"Root object of the schema\",\n\t\t\t\"properties\": {},\n\t\t\t\"required\": []\n\t\t}\n\telse:\n\t\tnew_dict = {\n\t\t\t\"type\": \"object\",\n\t\t\t\"description\": \"Root object of the schema\",\n\t\t\t\"properties\": {},\n\t\t\t\"required\": []\n\t\t}\n\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[\"properties\"][key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"array\", \"items\": generate_default_observer_schema_dict(value[0], first_level=True)}\n\t\telse:\n\t\t\tif isinstance(value, str) and value.lower() == \"str\":\n\t\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"string\"}\n\t\t\telif isinstance(value, str) and value.lower() == \"int\":\n\t\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"integer\"}\n\t\t\telif isinstance(value, str) and value.lower() == \"bool\":\n\t\t\t\tnew_dict[\"properties\"][key] = {\"type\": \"boolean\"}\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t \n\t# Create a new empty dictionary to store the new schema\n\tnew_dict = {}\n\n\t# Iterate through all the keys in the manifest_dict\n\tfor key, value in manifest_dict.iteritems():\n\t\t# If the value is a dict, then recursively call this function\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\t\t# If the value is a list, then append the \"list\" keyword\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = \"list\"\n\t\t# If the value is a string, then append the \"string\" keyword\n\t\telif isinstance(value, str):\n\t\t\tnew_dict[key] = \"string\"\n\t\t# If the value is an integer, then append the \"integer\" keyword\n\t\telif isinstance(value, int):\n\t\t\tnew_dict[key] = \"integer\"\n\t\t# If the value is a boolean, then append the \"boolean\" keyword\n\t\telif isinstance(value, bool):\n\t\t\tnew_dict[key] = \"boolean\"\n\t\t# If the value is None, then append the \"null\" keyword\n\t\telif value is None:\n\t\t\tnew_dict[key] = \"null\"\n\t\t# If the value is a float, then append the \"double\"", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\tnew_dictionary = {}\n\tfor key, value in manifest_dict.iteritems():\n\t\tif first_level:\n\t\t\tif type(value) is dict:\n\t\t\t\tnew_dictionary[key] = generate_default_observer_schema_dict(value, False)\n\t\t\telse:\n\t\t\t\tif type(value) is list:\n\t\t\t\t\tnew_dictionary[key] = value\n\t\t\t\telse:\n\t\t\t\t\tnew_dictionary[key] = default_value_dict[key]\n\t\telse:\n\t\t\tif type(value) is dict:\n\t\t\t\tnew_dictionary[key] = generate_default_observer_schema_dict(value, False)\n\t\t\telse:\n\t\t\t\tif type(value) is list:\n\t\t\t\t\tnew_dictionary[key] = value\n\t\t\t\telse:\n\t\t\t\t\tnew_dictionary[key] = default_value_dict[key]\n\n\treturn new_dictionary\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\n\t# The schema dict to be returned\n\tobserver_schema_dict = {\n\t\t\"id\": \"observer\",\n\t\t\"type\": \"object\",\n\t\t\"properties\": {\n\t\t\t\"name\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"description\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"type\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"enum\": [\n\t\t\t\t\t\"observer\",\n\t\t\t\t\t\"external\"\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"location\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"status\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"enum\": [\n\t\t\t\t\t\"active\",\n\t\t\t\t\t\"inactive\"\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"statusReason\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"minLength\": 1\n\t\t\t},\n\t\t\t\"contactInfo\": {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"contactName\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"minLength\": 1\n\t\t\t\t\t},\n\t\t\t\t\t\"contactEmail\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"format\": \"email\"\n\t\t\t\t\t},\n\t\t\t\t\t\"contactPhone\": {\n\t\t\t\t\t\t\"type\": \"string", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tif len(last_applied_manifest) < len(observer_schema):\n\t\tfor field_name in observer_schema[len(last_applied_manifest):]:\n\t\t\tlast_applied_manifest.append(None)\n\n\tfor i in range(len(response)):\n\t\tif len(last_applied_manifest) <= i:\n\t\t\tlast_applied_manifest.append(None)\n\t\tif last_applied_manifest[i] is None:\n\t\t\tlast_applied_manifest[i] = response[i]\n\t\telse:\n\t\t\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest[i], observer_schema[i], response[i])\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor observed_field in observer_schema:\n\t\t\tif observed_field in last_applied_manifest:\n\t\t\t\tcontinue\n\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tpass\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema:\n\t\t\tif len(field.get(\"fields\", [])) > 0:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest, field.get(\"fields\", []), response)\n\t\t\telse:\n\t\t\t\tif field.get(\"name\") not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field.get(\"name\")] = response.get(field.get(\"name\"), \"\")\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tfor field in observer_schema:\n\t\tif field.get(\"type\") == \"object\":\n\t\t\tif field.get(\"name\") == \"spec\":\n\t\t\t\tlast_applied_manifest.setdefault(\"spec\", {})\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[\"spec\"], field[\"properties\"], response\n\t\t\t\t)\n\t\t\telif field.get(\"name\") == \"status\":\n\t\t\t\tlast_applied_manifest.setdefault(\"status\", {})\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[\"status\"], field[\"properties\"], response\n\t\t\t\t)\n\t\t\telif field.get(\"name\") == \"metadata\":\n\t\t\t\tlast_applied_manifest.setdefault(\"metadata\", {})\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[\"metadata\"], field[\"properties\"], response\n\t\t\t\t)\n\t\t\telif field.get(\"type\") == \"array\":\n\t\t\t\tlast_applied_manifest.setdefault(field.get(\"name\"), [])\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field.get(\"name\")], field[\"items\"], response\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest.setdefault(field.get(\"name\"), None)\n\t\telif field.get(\"", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# Check if the field is a list (like a list of objects)\n\tif isinstance(observer_schema, list):\n\t\tsub_observer_schema = observer_schema[0]\n\t\t# If so, initialize the field in last_applied_manifest\n\t\tif not last_applied_manifest.get(sub_observer_schema.get(\"name\")):\n\t\t\tlast_applied_manifest[sub_observer_schema.get(\"name\")] = []\n\t\tif not response.get(\"items\"):\n\t\t\treturn\n\t\tfor item in response.get(\"items\"):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[sub_observer_schema.get(\"name\")],\n\t\t\t\tsub_observer_schema,\n\t\t\t\titem,\n\t\t\t)\n\t\treturn\n\n\t# Check if the field is a dictionary (like a dict of objects)\n\tif isinstance(observer_schema, dict):\n\t\tfor key, value in observer_schema.items():\n\t\t\t# If so, initialize the field in last_applied_manifest\n\t\t\tif not last_applied_manifest.get(key):\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\t# Check if the field is a list (like a list of objects)\n\t\t\tif isinstance(value, list):\n\t\t\t\tsub_observer_schema = value[0]\n\t\t\t\t# If", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema:\n\t\t\tif field.get(\"type\") == \"object\":\n\t\t\t\t# if the field is an object, we call recursively this function\n\t\t\t\t# to update the partial manifest\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest.get(field.get(\"key\")),\n\t\t\t\t\tfield.get(\"properties\"),\n\t\t\t\t\tresponse.get(field.get(\"key\")),\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# if the field is not an object, we initialize its value\n\t\t\t\t# in the manifest if it is not already present\n\t\t\t\tif not last_applied_manifest.get(field.get(\"key\")):\n\t\t\t\t\tlast_applied_manifest[field.get(\"key\")] = response.get(\n\t\t\t\t\t\tfield.get(\"key\")\n\t\t\t\t\t)\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tif len(observer_schema) == 0:\n\t\treturn\n\n\tcurrent_observed_field = observer_schema[0]\n\tif len(response) == 0:\n\t\treturn\n\tcurrent_response_field = response[0]\n\n\tif current_response_field.get(\"kind\") is not None:\n\t\tif current_response_field.get(\"kind\") == \"List\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field.get(\"items\")\n\t\t\t)\n\t\telif current_response_field.get(\"kind\") == \"ManifestList\":\n\t\t\tupdate_last_applied_manifest_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field\n\t\t\t)\n\t\telif current_response_field.get(\"kind\") == \"Object\":\n\t\t\tupdate_last_applied_manifest_object_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field\n\t\t\t)\n\t\telif current_response_field.get(\"kind\") == \"Status\":\n\t\t\tupdate_last_applied_manifest_status_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field\n\t\t\t)\n\t\treturn\n\n\tif current", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# print('in update_last_applied_manifest_list_from_resp')\n\tfor field in observer_schema:\n\t\tif field['name'] in last_applied_manifest:\n\t\t\tif'sub_observer_schema' in field:\n\t\t\t\t# print('in update_last_applied_manifest_list_from_resp', field['name'])\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field['name']],\n\t\t\t\t\tfield['sub_observer_schema'],\n\t\t\t\t\tresponse[field['name']])\n\t\telse:\n\t\t\tlast_applied_manifest[field['name']] = default_value(\n\t\t\t\tfield['type'])\n\t\t\tif'sub_observer_schema' in field:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field['name']],\n\t\t\t\t\tfield['sub_observer_schema'],\n\t\t\t\t\tresponse[field['name']])\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# get the current observed fields\n\tfields = observer_schema[0][\"fields\"]\n\tfor field in fields:\n\t\tfield_name = field[\"name\"]\n\t\tif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t# now go through each list items\n\tif \"items\" in response:\n\t\tfor item in response[\"items\"]:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema[1:], item\n\t\t\t)\n/examples/kubernetes_to_prometheus/kubernetes_to_prometheus.py\n# Copyright 2019. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tif len(last_applied_manifest) < len(observer_schema):\n\t\tfor field_name in observer_schema[len(last_applied_manifest):]:\n\t\t\tlast_applied_manifest.append(None)\n\n\tfor i in range(len(response)):\n\t\tif len(last_applied_manifest) <= i:\n\t\t\tlast_applied_manifest.append(None)\n\t\tif last_applied_manifest[i] is None:\n\t\t\tlast_applied_manifest[i] = response[i]\n\t\telse:\n\t\t\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest[i], observer_schema[i], response[i])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor observed_field in observer_schema:\n\t\t\tif observed_field in last_applied_manifest:\n\t\t\t\tcontinue\n\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema:\n\t\t\tif len(field.get(\"fields\", [])) > 0:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest, field.get(\"fields\", []), response)\n\t\t\telse:\n\t\t\t\tif field.get(\"name\") not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field.get(\"name\")] = response.get(field.get(\"name\"), \"\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tfor field in observer_schema:\n\t\tif field.get(\"type\") == \"object\":\n\t\t\tif field.get(\"name\") == \"spec\":\n\t\t\t\tlast_applied_manifest.setdefault(\"spec\", {})\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[\"spec\"], field[\"properties\"], response\n\t\t\t\t)\n\t\t\telif field.get(\"name\") == \"status\":\n\t\t\t\tlast_applied_manifest.setdefault(\"status\", {})\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[\"status\"], field[\"properties\"], response\n\t\t\t\t)\n\t\t\telif field.get(\"name\") == \"metadata\":\n\t\t\t\tlast_applied_manifest.setdefault(\"metadata\", {})\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[\"metadata\"], field[\"properties\"], response\n\t\t\t\t)\n\t\t\telif field.get(\"type\") == \"array\":\n\t\t\t\tlast_applied_manifest.setdefault(field.get(\"name\"), [])\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field.get(\"name\")], field[\"items\"], response\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest.setdefault(field.get(\"name\"), None)\n\t\telif field.get(\"", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# Check if the field is a list (like a list of objects)\n\tif isinstance(observer_schema, list):\n\t\tsub_observer_schema = observer_schema[0]\n\t\t# If so, initialize the field in last_applied_manifest\n\t\tif not last_applied_manifest.get(sub_observer_schema.get(\"name\")):\n\t\t\tlast_applied_manifest[sub_observer_schema.get(\"name\")] = []\n\t\tif not response.get(\"items\"):\n\t\t\treturn\n\t\tfor item in response.get(\"items\"):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[sub_observer_schema.get(\"name\")],\n\t\t\t\tsub_observer_schema,\n\t\t\t\titem,\n\t\t\t)\n\t\treturn\n\n\t# Check if the field is a dictionary (like a dict of objects)\n\tif isinstance(observer_schema, dict):\n\t\tfor key, value in observer_schema.items():\n\t\t\t# If so, initialize the field in last_applied_manifest\n\t\t\tif not last_applied_manifest.get(key):\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\t# Check if the field is a list (like a list of objects)\n\t\t\tif isinstance(value, list):\n\t\t\t\tsub_observer_schema = value[0]\n\t\t\t\t# If", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema:\n\t\t\tif field.get(\"type\") == \"object\":\n\t\t\t\t# if the field is an object, we call recursively this function\n\t\t\t\t# to update the partial manifest\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest.get(field.get(\"key\")),\n\t\t\t\t\tfield.get(\"properties\"),\n\t\t\t\t\tresponse.get(field.get(\"key\")),\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# if the field is not an object, we initialize its value\n\t\t\t\t# in the manifest if it is not already present\n\t\t\t\tif not last_applied_manifest.get(field.get(\"key\")):\n\t\t\t\t\tlast_applied_manifest[field.get(\"key\")] = response.get(\n\t\t\t\t\t\tfield.get(\"key\")\n\t\t\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tif len(observer_schema) == 0:\n\t\treturn\n\n\tcurrent_observed_field = observer_schema[0]\n\tif len(response) == 0:\n\t\treturn\n\tcurrent_response_field = response[0]\n\n\tif current_response_field.get(\"kind\") is not None:\n\t\tif current_response_field.get(\"kind\") == \"List\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field.get(\"items\")\n\t\t\t)\n\t\telif current_response_field.get(\"kind\") == \"ManifestList\":\n\t\t\tupdate_last_applied_manifest_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field\n\t\t\t)\n\t\telif current_response_field.get(\"kind\") == \"Object\":\n\t\t\tupdate_last_applied_manifest_object_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field\n\t\t\t)\n\t\telif current_response_field.get(\"kind\") == \"Status\":\n\t\t\tupdate_last_applied_manifest_status_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, current_response_field\n\t\t\t)\n\t\treturn\n\n\tif current", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# print('in update_last_applied_manifest_list_from_resp')\n\tfor field in observer_schema:\n\t\tif field['name'] in last_applied_manifest:\n\t\t\tif'sub_observer_schema' in field:\n\t\t\t\t# print('in update_last_applied_manifest_list_from_resp', field['name'])\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field['name']],\n\t\t\t\t\tfield['sub_observer_schema'],\n\t\t\t\t\tresponse[field['name']])\n\t\telse:\n\t\t\tlast_applied_manifest[field['name']] = default_value(\n\t\t\t\tfield['type'])\n\t\t\tif'sub_observer_schema' in field:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field['name']],\n\t\t\t\t\tfield['sub_observer_schema'],\n\t\t\t\t\tresponse[field['name']])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# get the current observed fields\n\tfields = observer_schema[0][\"fields\"]\n\tfor field in fields:\n\t\tfield_name = field[\"name\"]\n\t\tif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t# now go through each list items\n\tif \"items\" in response:\n\t\tfor item in response[\"items\"]:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema[1:], item\n\t\t\t)\n/examples/kubernetes_to_prometheus/kubernetes_to_prometheus.py\n# Copyright 2019. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema['fields']:\n\t\t\tfield_name = field['name']\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tif field_name in response:\n\t\t\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\t\t\telse:\n\t\t\t\t\traise KeyError(\n\t\t\t\t\t\t\"field {} not found in response\".format(field_name)\n\t\t\t\t\t)\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tif isinstance(response, list):\n\t\t\tfor item in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest,\n\t\t\t\t\tobserver_schema,\n\t\t\t\t\titem,\n\t\t\t\t)\n\t\telif isinstance(response, dict):\n\t\t\tfor field_name in observer_schema[\"fields\"]:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\t\tif field_name not in response:\n\t\t\t\t\tcontinue\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tobserver_schema[\"fields\"][field_name],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema[\"observedFields\"]:\n\t\t\tif field not in last_applied_manifest:\n\t\t\t\tif \"default\" in field:\n\t\t\t\t\tlast_applied_manifest[field] = field[\"default\"]\n\t\t\t\telse:\n\t\t\t\t\tif \"apiVersion\" in response:\n\t\t\t\t\t\tfield_name = field[\"name\"]\n\t\t\t\t\t\tapi_version = response[\"apiVersion\"]\n\t\t\t\t\t\tname = response[\"metadata\"][\"name\"]\n\t\t\t\t\t\tif field_name == \"metadata.name\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = name\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"metadata.namespace\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"metadata\"][\n\t\t\t\t\t\t\t\t\"namespace\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"metadata.uid\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"metadata\"][\n\t\t\t\t\t\t\t\t\"uid\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"kind\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"kind\"]\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"apiVersion\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = api_version\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"metadata.creationTimestamp\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"metadata\"][\n\t\t\t\t\t\t\t\t\"creationTimestamp\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tcontinue", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema[\"observedFields\"]:\n\t\t\tif field in last_applied_manifest:\n\t\t\t\tcontinue\n\t\t\tif field in response:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor f in observer_schema[\"observed\"]:\n\t\t\tif f not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[f] = response[f]\n\t\t\t\tcontinue\n\t\t\t# if the field is a list, we go through the items\n\t\t\tif isinstance(last_applied_manifest[f], list):\n\t\t\t\tfor i, item in enumerate(last_applied_manifest[f]):\n\t\t\t\t\tif f+\"s\" in response:\n\t\t\t\t\t\tif item not in response[f+\"s\"]:\n\t\t\t\t\t\t\tlast_applied_manifest[f][i] = response[f+\"s\"][item]\n\t\t\t\t\telse:\n\t\t\t\t\t\tlast_applied_manifest[f][i] = response[f][i]\n\t\t\t# if the field is a dict, we go through the nested fields\n\t\t\telif isinstance(last_applied_manifest[f], dict):\n\t\t\t\tfor k in last_applied_manifest[f]:\n\t\t\t\t\tif k in response:\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[f],\n\t\t\t\t\t\t\tobserver_schema[\"observed\"][k],\n\t\t\t\t\t\t\tresponse[k])\n\t\t\t\t\telse:\n\t\t\t\t\t\tlast_applied_manifest[f][k] = response[k]\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[f] = response[f]\n/docs/source/generated/argo_workflows.v", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor key, value in observer_schema.items():\n\t\t\tif value.get('observed', ''):\n\t\t\t\ttry:\n\t\t\t\t\tlast_applied_manifest[key] = response[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\n\t\tfor key, value in observer_schema.items():\n\t\t\tif value.get('items', ''):\n\t\t\t\ttry:\n\t\t\t\t\tfor item in response.get(key, []):\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[key],\n\t\t\t\t\t\t\tobserver_schema[key]['items'],\n\t\t\t\t\t\t\titem\n\t\t\t\t\t\t)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field_name in observer_schema.keys():\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tif field_name not in response:\n\t\t\t\traise KeyError(f\"{field_name} is not in {response}\")\n\t\t\tif isinstance(observer_schema[field_name], dict):\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tobserver_schema[field_name],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tfor field_name, field_schema in observer_schema.items():\n\t\tif field_name not in last_applied_manifest:\n\t\t\tif field_schema.get(\"type\") == \"object\":\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = field_schema[\"default_value\"]\n\t\tif field_schema.get(\"type\") == \"object\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field_schema[\"schema\"], response[field_name]\n\t\t\t)\n\t\telse:\n\t\t\tif field_name not in response:\n\t\t\t\traise KeyError(field_name)\n\t\t\tif field_schema.get(\"type\") == \"list\":\n\t\t\t\tfor i, item in enumerate(response[field_name]):\n\t\t\t\t\tif not isinstance(item, dict):\n\t\t\t\t\t\traise KeyError(field_name)\n\t\t\t\t\tif i >= len(last_applied_manifest[field_name]):\n\t\t\t\t\t\tlast_applied_manifest[field_name].append({})\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field_name][i],\n\t\t\t\t\t\tfield_schema[\"schema\"],\n\t\t\t\t\t\titem,\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field, value in response.items():\n\t\t\tif field in observer_schema:\n\t\t\t\t# This field is observed, we need to update its value in\n\t\t\t\t# last_applied_manifest\n\t\t\t\tlast_applied_manifest[field] = value\n\t\t\t\tif isinstance(observer_schema[field], dict):\n\t\t\t\t\t# This field is a dict, we need to recursively call this\n\t\t\t\t\t# function to update its value in last_applied_manifest\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\tvalue,\n\t\t\t\t\t)\n\t\t\t\telif isinstance(observer_schema[field], list):\n\t\t\t\t\t# This field is a list, we need to recursively call this\n\t\t\t\t\t# function to update its value in last_applied_manifest\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\tvalue,\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t# This field is not a dict or a list, we can safely update\n\t\t\t\t\t# its value in last_applied_manifest\n\t\t\t\t\tlast_applied_manifest[field] = value\n\t\t\telse:\n\t\t\t\t# This field is not observed, we can safely update its value in\n\t\t\t\t# last_applied_manifest\n\t\t\t\tlast_applied_manifest[", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field_name, field in observer_schema.items():\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tif field[\"type\"] == \"dict\":\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tfield[\"schema\"],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n\t\t\telif field[\"type\"] == \"list\":\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tfield[\"schema\"],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema['fields']:\n\t\t\tfield_name = field['name']\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tif field_name in response:\n\t\t\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\t\t\telse:\n\t\t\t\t\traise KeyError(\n\t\t\t\t\t\t\"field {} not found in response\".format(field_name)\n\t\t\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tif isinstance(response, list):\n\t\t\tfor item in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest,\n\t\t\t\t\tobserver_schema,\n\t\t\t\t\titem,\n\t\t\t\t)\n\t\telif isinstance(response, dict):\n\t\t\tfor field_name in observer_schema[\"fields\"]:\n\t\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\t\tif field_name not in response:\n\t\t\t\t\tcontinue\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tobserver_schema[\"fields\"][field_name],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema[\"observedFields\"]:\n\t\t\tif field not in last_applied_manifest:\n\t\t\t\tif \"default\" in field:\n\t\t\t\t\tlast_applied_manifest[field] = field[\"default\"]\n\t\t\t\telse:\n\t\t\t\t\tif \"apiVersion\" in response:\n\t\t\t\t\t\tfield_name = field[\"name\"]\n\t\t\t\t\t\tapi_version = response[\"apiVersion\"]\n\t\t\t\t\t\tname = response[\"metadata\"][\"name\"]\n\t\t\t\t\t\tif field_name == \"metadata.name\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = name\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"metadata.namespace\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"metadata\"][\n\t\t\t\t\t\t\t\t\"namespace\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"metadata.uid\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"metadata\"][\n\t\t\t\t\t\t\t\t\"uid\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"kind\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"kind\"]\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"apiVersion\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = api_version\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif field_name == \"metadata.creationTimestamp\":\n\t\t\t\t\t\t\tlast_applied_manifest[field_name] = response[\"metadata\"][\n\t\t\t\t\t\t\t\t\"creationTimestamp\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tcontinue", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in observer_schema[\"observedFields\"]:\n\t\t\tif field in last_applied_manifest:\n\t\t\t\tcontinue\n\t\t\tif field in response:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor f in observer_schema[\"observed\"]:\n\t\t\tif f not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[f] = response[f]\n\t\t\t\tcontinue\n\t\t\t# if the field is a list, we go through the items\n\t\t\tif isinstance(last_applied_manifest[f], list):\n\t\t\t\tfor i, item in enumerate(last_applied_manifest[f]):\n\t\t\t\t\tif f+\"s\" in response:\n\t\t\t\t\t\tif item not in response[f+\"s\"]:\n\t\t\t\t\t\t\tlast_applied_manifest[f][i] = response[f+\"s\"][item]\n\t\t\t\t\telse:\n\t\t\t\t\t\tlast_applied_manifest[f][i] = response[f][i]\n\t\t\t# if the field is a dict, we go through the nested fields\n\t\t\telif isinstance(last_applied_manifest[f], dict):\n\t\t\t\tfor k in last_applied_manifest[f]:\n\t\t\t\t\tif k in response:\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[f],\n\t\t\t\t\t\t\tobserver_schema[\"observed\"][k],\n\t\t\t\t\t\t\tresponse[k])\n\t\t\t\t\telse:\n\t\t\t\t\t\tlast_applied_manifest[f][k] = response[k]\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[f] = response[f]\n/docs/source/generated/argo_workflows.v", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor key, value in observer_schema.items():\n\t\t\tif value.get('observed', ''):\n\t\t\t\ttry:\n\t\t\t\t\tlast_applied_manifest[key] = response[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\n\t\tfor key, value in observer_schema.items():\n\t\t\tif value.get('items', ''):\n\t\t\t\ttry:\n\t\t\t\t\tfor item in response.get(key, []):\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[key],\n\t\t\t\t\t\t\tobserver_schema[key]['items'],\n\t\t\t\t\t\t\titem\n\t\t\t\t\t\t)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field_name in observer_schema.keys():\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tif field_name not in response:\n\t\t\t\traise KeyError(f\"{field_name} is not in {response}\")\n\t\t\tif isinstance(observer_schema[field_name], dict):\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tobserver_schema[field_name],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tfor field_name, field_schema in observer_schema.items():\n\t\tif field_name not in last_applied_manifest:\n\t\t\tif field_schema.get(\"type\") == \"object\":\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = field_schema[\"default_value\"]\n\t\tif field_schema.get(\"type\") == \"object\":\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field_schema[\"schema\"], response[field_name]\n\t\t\t)\n\t\telse:\n\t\t\tif field_name not in response:\n\t\t\t\traise KeyError(field_name)\n\t\t\tif field_schema.get(\"type\") == \"list\":\n\t\t\t\tfor i, item in enumerate(response[field_name]):\n\t\t\t\t\tif not isinstance(item, dict):\n\t\t\t\t\t\traise KeyError(field_name)\n\t\t\t\t\tif i >= len(last_applied_manifest[field_name]):\n\t\t\t\t\t\tlast_applied_manifest[field_name].append({})\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field_name][i],\n\t\t\t\t\t\tfield_schema[\"schema\"],\n\t\t\t\t\t\titem,\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field, value in response.items():\n\t\t\tif field in observer_schema:\n\t\t\t\t# This field is observed, we need to update its value in\n\t\t\t\t# last_applied_manifest\n\t\t\t\tlast_applied_manifest[field] = value\n\t\t\t\tif isinstance(observer_schema[field], dict):\n\t\t\t\t\t# This field is a dict, we need to recursively call this\n\t\t\t\t\t# function to update its value in last_applied_manifest\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\tvalue,\n\t\t\t\t\t)\n\t\t\t\telif isinstance(observer_schema[field], list):\n\t\t\t\t\t# This field is a list, we need to recursively call this\n\t\t\t\t\t# function to update its value in last_applied_manifest\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\tvalue,\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t# This field is not a dict or a list, we can safely update\n\t\t\t\t\t# its value in last_applied_manifest\n\t\t\t\t\tlast_applied_manifest[field] = value\n\t\t\telse:\n\t\t\t\t# This field is not observed, we can safely update its value in\n\t\t\t\t# last_applied_manifest\n\t\t\t\tlast_applied_manifest[", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field_name, field in observer_schema.items():\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tif field[\"type\"] == \"dict\":\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tfield[\"schema\"],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n\t\t\telif field[\"type\"] == \"list\":\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\tfield[\"schema\"],\n\t\t\t\t\tresponse[field_name],\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field_name] = response[field_name]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t \n\t# TODO: This function should be called \"generate_default_observer_schema\"\n\t# TODO: Instead of \"generate_default_observer_schema\" we should call \"generate_default_observer_schema_for_resource\"\n\t# TODO: Then we should call this function for each resource in the manifest.\n\n\t# Get the manifest\n\tmanifest = app.get_manifest()\n\t\n\t# Get the \"observer schema\"\n\tobserver_schema = app.get_observer_schema()\n\t\n\t# Loop through the manifest and create an observer schema for each resource\n\tfor resource in manifest.get_resources():\n\n\t\t# Get the resource name\n\t\tresource_name = resource.get_name()\n\t\t\n\t\t# Get the resource schema\n\t\tresource_schema = resource.get_schema()\n\t\t\n\t\t# Get the resource category\n\t\tresource_category = resource.get_category()\n\t\t\n\t\t# If the resource category is \"cluster\" then we don't need to generate an observer schema for this resource\n\t\tif resource_category == \"cluster\":\n\t\t\n\t\t\t# Skip this resource\n\t\t\tcontinue\n\t\t\n\t\t# Get the observer schema for this resource\n\t\tobserver_schema_for_resource = observer_schema.get_observer_schema(resource_name)\n\t\t\n\t\t# If the observer schema for this resource is not defined then we need to generate the default observer schema\n\t\tif observer_schema_for_resource", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\n\tfor manifest in app.spec.manifest:\n\t\tschema_name = 'default_observer_schema_' + manifest['kind']\n\t\tschema_name = schema_name.lower()\n\t\tschema_name = re.sub(r'\\W+', '_', schema_name)\n\t\tschema_name = schema_name.replace('__', '_')\n\n\t\tif schema_name in app.status.observer.schemas:\n\t\t\tcontinue\n\n\t\tschema_name_exists = False\n\t\tfor schema in app.status.observer.schemas:\n\t\t\tif schema['name'] == schema_name:\n\t\t\t\tschema_name_exists = True\n\t\t\t\tbreak\n\n\t\tif schema_name_exists:\n\t\t\tcontinue\n\n\t\tschema = {}\n\t\tschema['name'] = schema_name\n\t\tschema['kind'] = manifest['kind']\n\t\tschema['fields'] = manifest['fields']\n\t\tschema['type'] = 'object'\n\n\t\tapp.status.observer.schemas.append(schema)\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\n\tprint(\"Generating default observer schema...\")\n\tprint(\"-----------------------------------\")\n\tprint()\n\n\t# The manifest is organized by Kubernetes resource type.\n\t# Each resource type has a list of objects.\n\t# Each object has a list of fields.\n\tmanifest = get_manifest()\n\n\tfor kind, objects in manifest.items():\n\t\tfor obj in objects:\n\t\t\tprint(f\"generating schema for {kind}.{obj['name']}...\")\n\t\t\t\n\t\t\tif 'observer-schema' in obj:\n\t\t\t\tcontinue\n\n\t\t\t# Add a default observer schema to the object declaration\n\t\t\tobj['observer-schema'] = {\n\t\t\t\t\"id\": f\"{app}.{kind}.{obj['name']}\",\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"kind\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"enum\": [kind],\n\t\t\t\t\t\t\"const\": kind\n\t\t\t\t\t},\n\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"const\": obj['name']\n\t\t\t\t\t},\n\t\t\t\t\t\"version\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"const\": obj['version']\n\t\t\t\t\t},\n\t\t\t\t\t\"namespace\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"const\": obj['namespace']\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor prop in obj['properties']:\n\t\t\t\tobj['observer-schema']['properties'][prop]", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\n\tfor manifest in app.spec.manifest:\n\t\tif manifest.spec.observer is not None:\n\t\t\tcontinue\n\n\t\tschema = generate_default_observer_schema_for_manifest(manifest)\n\t\tif schema is None:\n\t\t\tcontinue\n\n\t\tmanifest.spec.observer = schema\n\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tojects/\n# Projects\n\nThis directory contains all the projects that are deployed with the [k8s-operator](https://github.com/k8s-operator/k8s-operator) project.\n\n## Current Projects\n\n- [ArgoCD](https://argoproj.github.io/argo-cd/)\n- [Flux](https://fluxcd.io/)\n- [Helm](https://helm.sh/)\n- [Istio](https://istio.io/)\n- [Kiali](https://kiali.io/)\n- [Kubediff](https://kubediff.io/)\n- [Kustomize](https://kustomize.io/)\n- [Open Policy Agent](https://www.openpolicyagent.org/)\n- [Skaffold](https://skaffold.dev/)\n- [Strimzi](https://strimzi.io/)\n- [Weave Cloud](https://www.weave.works/primport json\nimport os\nimport pathlib\nimport subprocess\nimport sys\nimport yaml\nfrom typing import Any, List, Union\nfrom argparse import ArgumentParser\nfrom collections import OrderedDict\nfrom kubernetes import client, config\nfrom kubernetes.client import Configuration\nfrom kubernetes.config.kube_config import KubeConfigLoader, KubeConfigLoaderException, KubeConfigMissingException\nfrom kubernetes.config.", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n#\tresources = app.spec.manifest.resources\n#\t\n#\tfor resource in resources:\n#\t\tif not resource.observer.schema:\n#\t\t\tschema = {}\n#\t\t\tschema['type'] = 'object'\n#\t\t\tschema['properties'] = {}\n#\t\t\t\n#\t\t\tif resource.kind == 'Pod':\n#\t\t\t\tschema['properties']['phase'] = {'type':'string'}\n#\t\t\t\t\n#\t\t\tresource.observer.schema = schema\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t \n\t# TODO: This function should be called \"generate_default_observer_schema\"\n\t# TODO: Instead of \"generate_default_observer_schema\" we should call \"generate_default_observer_schema_for_resource\"\n\t# TODO: Then we should call this function for each resource in the manifest.\n\n\t# Get the manifest\n\tmanifest = app.get_manifest()\n\t\n\t# Get the \"observer schema\"\n\tobserver_schema = app.get_observer_schema()\n\t\n\t# Loop through the manifest and create an observer schema for each resource\n\tfor resource in manifest.get_resources():\n\n\t\t# Get the resource name\n\t\tresource_name = resource.get_name()\n\t\t\n\t\t# Get the resource schema\n\t\tresource_schema = resource.get_schema()\n\t\t\n\t\t# Get the resource category\n\t\tresource_category = resource.get_category()\n\t\t\n\t\t# If the resource category is \"cluster\" then we don't need to generate an observer schema for this resource\n\t\tif resource_category == \"cluster\":\n\t\t\n\t\t\t# Skip this resource\n\t\t\tcontinue\n\t\t\n\t\t# Get the observer schema for this resource\n\t\tobserver_schema_for_resource = observer_schema.get_observer_schema(resource_name)\n\t\t\n\t\t# If the observer schema for this resource is not defined then we need to generate the default observer schema\n\t\tif observer_schema_for_resource", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\n\tfor manifest in app.spec.manifest:\n\t\tschema_name = 'default_observer_schema_' + manifest['kind']\n\t\tschema_name = schema_name.lower()\n\t\tschema_name = re.sub(r'\\W+', '_', schema_name)\n\t\tschema_name = schema_name.replace('__', '_')\n\n\t\tif schema_name in app.status.observer.schemas:\n\t\t\tcontinue\n\n\t\tschema_name_exists = False\n\t\tfor schema in app.status.observer.schemas:\n\t\t\tif schema['name'] == schema_name:\n\t\t\t\tschema_name_exists = True\n\t\t\t\tbreak\n\n\t\tif schema_name_exists:\n\t\t\tcontinue\n\n\t\tschema = {}\n\t\tschema['name'] = schema_name\n\t\tschema['kind'] = manifest['kind']\n\t\tschema['fields'] = manifest['fields']\n\t\tschema['type'] = 'object'\n\n\t\tapp.status.observer.schemas.append(schema)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\n\tprint(\"Generating default observer schema...\")\n\tprint(\"-----------------------------------\")\n\tprint()\n\n\t# The manifest is organized by Kubernetes resource type.\n\t# Each resource type has a list of objects.\n\t# Each object has a list of fields.\n\tmanifest = get_manifest()\n\n\tfor kind, objects in manifest.items():\n\t\tfor obj in objects:\n\t\t\tprint(f\"generating schema for {kind}.{obj['name']}...\")\n\t\t\t\n\t\t\tif 'observer-schema' in obj:\n\t\t\t\tcontinue\n\n\t\t\t# Add a default observer schema to the object declaration\n\t\t\tobj['observer-schema'] = {\n\t\t\t\t\"id\": f\"{app}.{kind}.{obj['name']}\",\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"kind\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"enum\": [kind],\n\t\t\t\t\t\t\"const\": kind\n\t\t\t\t\t},\n\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"const\": obj['name']\n\t\t\t\t\t},\n\t\t\t\t\t\"version\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"const\": obj['version']\n\t\t\t\t\t},\n\t\t\t\t\t\"namespace\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"const\": obj['namespace']\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor prop in obj['properties']:\n\t\t\t\tobj['observer-schema']['properties'][prop]", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\n\tfor manifest in app.spec.manifest:\n\t\tif manifest.spec.observer is not None:\n\t\t\tcontinue\n\n\t\tschema = generate_default_observer_schema_for_manifest(manifest)\n\t\tif schema is None:\n\t\t\tcontinue\n\n\t\tmanifest.spec.observer = schema\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tojects/\n# Projects\n\nThis directory contains all the projects that are deployed with the [k8s-operator](https://github.com/k8s-operator/k8s-operator) project.\n\n## Current Projects\n\n- [ArgoCD](https://argoproj.github.io/argo-cd/)\n- [Flux](https://fluxcd.io/)\n- [Helm](https://helm.sh/)\n- [Istio](https://istio.io/)\n- [Kiali](https://kiali.io/)\n- [Kubediff](https://kubediff.io/)\n- [Kustomize](https://kustomize.io/)\n- [Open Policy Agent](https://www.openpolicyagent.org/)\n- [Skaffold](https://skaffold.dev/)\n- [Strimzi](https://strimzi.io/)\n- [Weave Cloud](https://www.weave.works/primport json\nimport os\nimport pathlib\nimport subprocess\nimport sys\nimport yaml\nfrom typing import Any, List, Union\nfrom argparse import ArgumentParser\nfrom collections import OrderedDict\nfrom kubernetes import client, config\nfrom kubernetes.client import Configuration\nfrom kubernetes.config.kube_config import KubeConfigLoader, KubeConfigLoaderException, KubeConfigMissingException\nfrom kubernetes.config.", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n#\tresources = app.spec.manifest.resources\n#\t\n#\tfor resource in resources:\n#\t\tif not resource.observer.schema:\n#\t\t\tschema = {}\n#\t\t\tschema['type'] = 'object'\n#\t\t\tschema['properties'] = {}\n#\t\t\t\n#\t\t\tif resource.kind == 'Pod':\n#\t\t\t\tschema['properties']['phase'] = {'type':'string'}\n#\t\t\t\t\n#\t\t\tresource.observer.schema = schema\n", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params), params\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tif isinstance(sql, bytes):\n\t\t\tsql = sql.decode(\"utf-8\")\n\n\t\tif not isinstance(sql, str):\n\t\t\traise TypeError(\"sql argument must be str\")\n\n\t\tif params is None:\n\t\t\tparams = {}\n\n\t\tif not isinstance(params, collections.abc.Mapping):\n\t\t\tparams = dict(enumerate(params))\n\n\t\tif isinstance(params, collections.abc.Mapping):\n\t\t\tparams = [params]\n\n\t\treturn (\n\t\t\tself._converter.convert(sql),\n\t\t\tparams,\n\t\t)\n```\n\n# `sqlalchemy.dialects.mysql.base.MySQLDialect_pymysql`\n\n```python", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tparams = self._converter.convert(sql, params)\n\t\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tconverter = self._converter\n\t\tconverted_sql, converted_params = converter.convert(sql, params)\n\t\treturn (converted_sql, converted_params)\n\n# #############################################################################", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tif self._converter is None:\n\t\t\treturn sql, params\n\t\treturn self._converter.convert(sql, params)\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tnew_sql, new_params = self._converter.convert(sql, params)\n\t\treturn new_sql, new_params\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.format(sql, params)\n\n\tdef convert(self, value: Any) -> Any:\n\t\t\"\"\"\n\t\tConvert value using self._converter.convert\n\t\t\"\"\"\n\t\treturn self._converter.convert(value)\n\n\tdef adapt(self, value: Any) -> Any:\n\t\t\"\"\"\n\t\tAdapt value using self._converter.adapt\n\t\t\"\"\"\n\t\treturn self._converter.adapt(value)\n\n\tdef enquote(self, value: Any) -> Any:\n\t\t\"\"\"\n\t\tQuote value using self._converter.enquote\n\t\t\"\"\"\n\t\treturn self._converter.enquote(value)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params), params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tif isinstance(sql, bytes):\n\t\t\tsql = sql.decode(\"utf-8\")\n\n\t\tif not isinstance(sql, str):\n\t\t\traise TypeError(\"sql argument must be str\")\n\n\t\tif params is None:\n\t\t\tparams = {}\n\n\t\tif not isinstance(params, collections.abc.Mapping):\n\t\t\tparams = dict(enumerate(params))\n\n\t\tif isinstance(params, collections.abc.Mapping):\n\t\t\tparams = [params]\n\n\t\treturn (\n\t\t\tself._converter.convert(sql),\n\t\t\tparams,\n\t\t)\n```\n\n# `sqlalchemy.dialects.mysql.base.MySQLDialect_pymysql`\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tparams = self._converter.convert(sql, params)\n\t\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tconverter = self._converter\n\t\tconverted_sql, converted_params = converter.convert(sql, params)\n\t\treturn (converted_sql, converted_params)\n\n# #############################################################################", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tif self._converter is None:\n\t\t\treturn sql, params\n\t\treturn self._converter.convert(sql, params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\tnew_sql, new_params = self._converter.convert(sql, params)\n\t\treturn new_sql, new_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.convert(sql, params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\t\treturn self._converter.format(sql, params)\n\n\tdef convert(self, value: Any) -> Any:\n\t\t\"\"\"\n\t\tConvert value using self._converter.convert\n\t\t\"\"\"\n\t\treturn self._converter.convert(value)\n\n\tdef adapt(self, value: Any) -> Any:\n\t\t\"\"\"\n\t\tAdapt value using self._converter.adapt\n\t\t\"\"\"\n\t\treturn self._converter.adapt(value)\n\n\tdef enquote(self, value: Any) -> Any:\n\t\t\"\"\"\n\t\tQuote value using self._converter.enquote\n\t\t\"\"\"\n\t\treturn self._converter.enquote(value)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif self._converter is None:\n\t\t\traise ValueError(\n\t\t\t\t\"The converter was not specified in constructor, \"\n\t\t\t\t\"so it cannot be used to convert the parameters\"\n\t\t\t)\n\t\treturn self._converter.convert_many(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif many_params is None:\n\t\t\tmany_params = []\n\t\telif not isinstance(many_params, (list, tuple, set)):\n\t\t\tmany_params = [many_params]\n\t\tif type(many_params[0]) is dict:\n\t\t\tmany_params = [tuple(d.values()) for d in many_params]\n\t\treturn self._converter.convert_many(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tsql, many_params = self._converter.convert_many(sql, many_params)\n\t\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif many_params:\n\t\t\tif isinstance(many_params[0], dict):\n\t\t\t\tmany_params = [dict(p) for p in many_params]\n\t\t\telse:\n\t\t\t\tmany_params = list(many_params)\n\t\treturn self._converter.convert_many(sql, many_params)\nlf._columns_to_update(data)\n\t\tif not data:\n\t\t\treturn\n\n\t\tfor k, v in data.items():\n\t\t\tif isinstance(v, list):\n\t\t\t\tv = tuple(v)\n\t\t\tif isinstance(v, tuple):\n\t\t\t\tif isinstance(v[0], dict):\n\t\t\t\t\tv = tuple(v)\n\t\t\t\telif v[0] == v[-1]:\n\t\t\t\t\tv = v[0]\n\t\t\tdata[k] = v\n\n\t\tself._query = self._query.filter(self._query.c.id == self._id)\n\t\tself._query = self._query.update(data)\n\t\tself._query = self._query.returning(self._query.c)\n\t\tself._query = self._query.execute()\n\t\tself._query = self._query.fetchone()\n\t\tself._columns_to_update(self._query)\n\n\tdef delete(self) -> None:\n\t\t\"\"\"\n\t\tDelete current object\n\t\t\"\"\"\n\t\tself._query =", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif not isinstance(many_params, collections.Iterable) or isinstance(many_params, str):\n\t\t\traise TypeError(f\"many_params must be an iterable of sequences, not {type(many_params)}\")\n\t\tif isinstance(many_params, collections.abc.Mapping):\n\t\t\tmany_params = [many_params]\n\t\treturn self._converter.convert_many(sql, many_params)\n\t```\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif not many_params:\n\t\t\treturn sql, many_params\n\t\tconverter = self._converter\n\t\tparams = []\n\t\tfor i, params in enumerate(many_params):\n\t\t\tparams = list(params)\n\t\t\tsql, params = converter.convert_many(sql, params)\n\t\t\tmany_params[i] = params\n\t\treturn sql, many_params\n\n\tdef formatparams(\n\t\tself, sql: AnyStr, params: Optional[Dict[Union[str, int], Any]]\n\t) -> Tuple[AnyStr, Optional[Dict[Union[str, int], Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._converter.convert_one\n\t\t\"\"\"\n\t\tif params is None:\n\t\t\treturn sql, params\n\t\tconverter = self._converter\n\t\tsql, params = converter.convert_one(sql, params)\n\t\treturn sql, params\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif self._converter is None:\n\t\t\traise ValueError(\n\t\t\t\t\"The converter was not specified in constructor, \"\n\t\t\t\t\"so it cannot be used to convert the parameters\"\n\t\t\t)\n\t\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif many_params is None:\n\t\t\tmany_params = []\n\t\telif not isinstance(many_params, (list, tuple, set)):\n\t\t\tmany_params = [many_params]\n\t\tif type(many_params[0]) is dict:\n\t\t\tmany_params = [tuple(d.values()) for d in many_params]\n\t\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tsql, many_params = self._converter.convert_many(sql, many_params)\n\t\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif many_params:\n\t\t\tif isinstance(many_params[0], dict):\n\t\t\t\tmany_params = [dict(p) for p in many_params]\n\t\t\telse:\n\t\t\t\tmany_params = list(many_params)\n\t\treturn self._converter.convert_many(sql, many_params)\nlf._columns_to_update(data)\n\t\tif not data:\n\t\t\treturn\n\n\t\tfor k, v in data.items():\n\t\t\tif isinstance(v, list):\n\t\t\t\tv = tuple(v)\n\t\t\tif isinstance(v, tuple):\n\t\t\t\tif isinstance(v[0], dict):\n\t\t\t\t\tv = tuple(v)\n\t\t\t\telif v[0] == v[-1]:\n\t\t\t\t\tv = v[0]\n\t\t\tdata[k] = v\n\n\t\tself._query = self._query.filter(self._query.c.id == self._id)\n\t\tself._query = self._query.update(data)\n\t\tself._query = self._query.returning(self._query.c)\n\t\tself._query = self._query.execute()\n\t\tself._query = self._query.fetchone()\n\t\tself._columns_to_update(self._query)\n\n\tdef delete(self) -> None:\n\t\t\"\"\"\n\t\tDelete current object\n\t\t\"\"\"\n\t\tself._query =", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif not isinstance(many_params, collections.Iterable) or isinstance(many_params, str):\n\t\t\traise TypeError(f\"many_params must be an iterable of sequences, not {type(many_params)}\")\n\t\tif isinstance(many_params, collections.abc.Mapping):\n\t\t\tmany_params = [many_params]\n\t\treturn self._converter.convert_many(sql, many_params)\n\t```\n\t\"\"\"\n\t\treturn self._converter.convert_many(sql, many_params)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\t\tif not many_params:\n\t\t\treturn sql, many_params\n\t\tconverter = self._converter\n\t\tparams = []\n\t\tfor i, params in enumerate(many_params):\n\t\t\tparams = list(params)\n\t\t\tsql, params = converter.convert_many(sql, params)\n\t\t\tmany_params[i] = params\n\t\treturn sql, many_params\n\n\tdef formatparams(\n\t\tself, sql: AnyStr, params: Optional[Dict[Union[str, int], Any]]\n\t) -> Tuple[AnyStr, Optional[Dict[Union[str, int], Any]]]:\n\t\t\"\"\"\n\t\tConvert sql using self._converter.convert_one\n\t\t\"\"\"\n\t\tif params is None:\n\t\t\treturn sql, params\n\t\tconverter = self._converter\n\t\tsql, params = converter.convert_one(sql, params)\n\t\treturn sql, params\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn os.path.isdir(path) and exists(path + \"/OCFL_1.0/0/root/version/1\")\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\t# TODO: replace with real implementation\n\treturn self.exists(path)\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn path in self.objects or path == self.root\n\t", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn self.get_type(path) == \"OCFL Object\"\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\tif not path.startswith(self.root):\n\t\t\treturn False\n\t\tif path == self.root and self.root_valid:\n\t\t\treturn True\n\t\tif path.startswith(self.root + '/'):\n\t\t\tif os.path.isdir(path):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telif os.path.isdir(path):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn os.path.isdir(path) and \\\n\t\t\tos.path.isfile(os.path.join(path, '0=ocfl_1.0')) and \\\n\t\t\tos.path.isfile(os.path.join(path, '0=ocfl_1.0/inventory.json')) and \\\n\t\t\tos.path.isfile(os.path.join(path, '0=ocfl_1.0/VERSION'))\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\t\treturn False\n\t\tif not os.path.isdir(path):\n\t\t\treturn False\n\t\tif _ocfl_version(path) is None:\n\t\t\treturn False\n\t\treturn True\n/requirements.txt\n# OCFLpy requirements file\n#\n# This file is autogenerated by pip-compile\n# To update, run:\n#\n#    pip-compile --output-file requirements.txt requirements.in\n#\ncertifi==2018.1.18\t# via requests\nchardet==3.0.4\t# via requests\nidna==2.6\t# via requests\nrequests==2.18.4\t# via ocflpy\nurllib3==1.24.2\t# via requests\n/pyfs/ocfl.py\nimport os\nimport json\nfrom ocfl.object_layout import ObjectLayout\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\n        if not path.endswith(\"/\"):\n            path += \"/\"\n        if not path.startswith(\"/\"):\n            path = \"/\" + path\n\n        # If we're not at a top-level OCFL object, let the parent validate\n        if not path.endswith(self.id + \"/\"):\n            return self._parent.validate(path)\n\n        return True\n\n    def validate_digest(self, digest):\n        \"\"\"\n        Returns True if digest is a valid digest for this object, False\n        otherwise.\n        \"\"\"\n        return _validate_digest(digest, self.algorithm)\n\n    def validate_path(self, path):\n        \"\"\"\n        Returns True if path is valid, False otherwise.\n        \"\"\"\n        if not path.endswith(\"/\"):\n            path += \"/\"\n        if not path.startswith(\"/\"):\n            path = \"/\" + path\n\n        # If we're not at a top-level OCFL object, let the parent validate\n        if not path.endswith(self.id + \"/\"):\n            return self._parent.validate_path(path)\n\n        return True\n\n    def validate_type(self, type):\n        \"\"\"\n        Returns True if type is a valid type for this object, False otherwise.\n        \"\"\"\n        return _validate_type(type)\n\n    def validate_version(self, version):\n        \"\"\"\n        Returns True if version is a valid version for this object, False\n        otherwise.\n        \"\"\"\n        return _validate_version(version)\n\n    def validate_location(self,", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn False\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn os.path.isdir(path) and exists(path + \"/OCFL_1.0/0/root/version/1\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\t# TODO: replace with real implementation\n\treturn self.exists(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn path in self.objects or path == self.root\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn self.get_type(path) == \"OCFL Object\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\tif not path.startswith(self.root):\n\t\t\treturn False\n\t\tif path == self.root and self.root_valid:\n\t\t\treturn True\n\t\tif path.startswith(self.root + '/'):\n\t\t\tif os.path.isdir(path):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telif os.path.isdir(path):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn os.path.isdir(path) and \\\n\t\t\tos.path.isfile(os.path.join(path, '0=ocfl_1.0')) and \\\n\t\t\tos.path.isfile(os.path.join(path, '0=ocfl_1.0/inventory.json')) and \\\n\t\t\tos.path.isfile(os.path.join(path, '0=ocfl_1.0/VERSION'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\t\treturn False\n\t\tif not os.path.isdir(path):\n\t\t\treturn False\n\t\tif _ocfl_version(path) is None:\n\t\t\treturn False\n\t\treturn True\n/requirements.txt\n# OCFLpy requirements file\n#\n# This file is autogenerated by pip-compile\n# To update, run:\n#\n#    pip-compile --output-file requirements.txt requirements.in\n#\ncertifi==2018.1.18\t# via requests\nchardet==3.0.4\t# via requests\nidna==2.6\t# via requests\nrequests==2.18.4\t# via ocflpy\nurllib3==1.24.2\t# via requests\n/pyfs/ocfl.py\nimport os\nimport json\nfrom ocfl.object_layout import ObjectLayout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\n        if not path.endswith(\"/\"):\n            path += \"/\"\n        if not path.startswith(\"/\"):\n            path = \"/\" + path\n\n        # If we're not at a top-level OCFL object, let the parent validate\n        if not path.endswith(self.id + \"/\"):\n            return self._parent.validate(path)\n\n        return True\n\n    def validate_digest(self, digest):\n        \"\"\"\n        Returns True if digest is a valid digest for this object, False\n        otherwise.\n        \"\"\"\n        return _validate_digest(digest, self.algorithm)\n\n    def validate_path(self, path):\n        \"\"\"\n        Returns True if path is valid, False otherwise.\n        \"\"\"\n        if not path.endswith(\"/\"):\n            path += \"/\"\n        if not path.startswith(\"/\"):\n            path = \"/\" + path\n\n        # If we're not at a top-level OCFL object, let the parent validate\n        if not path.endswith(self.id + \"/\"):\n            return self._parent.validate_path(path)\n\n        return True\n\n    def validate_type(self, type):\n        \"\"\"\n        Returns True if type is a valid type for this object, False otherwise.\n        \"\"\"\n        return _validate_type(type)\n\n    def validate_version(self, version):\n        \"\"\"\n        Returns True if version is a valid version for this object, False\n        otherwise.\n        \"\"\"\n        return _validate_version(version)\n\n    def validate_location(self,", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix + ', '.join([str(m) for m in sorted(self.messages)])\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn '\\n'.join(prefix + str(item) for item in sorted(self.messages))\n\t\texcept:\n\t\t\treturn\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn '\\n'.join(str(item) for item in sorted(self.messages))\n\t\texcept:\n\t\t\treturn\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix +''.join(str(msg) for msg in sorted(self.messages))\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n        return ''.join(map(lambda x: prefix + str(x) + \"\\n\", sorted(self.messages)))\n\t", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn '\\n'.join('%s%s' % (prefix, m) for m in sorted(self.messages, reverse=True))\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n        ret = ''\n        for m in sorted(self.messages):\n            ret += prefix + m + '\\n'\n        return ret\n\n    def status(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n        ret = ''\n        for m in sorted(self.messages):\n            ret += prefix + m + '\\n'\n        return ret\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\tresult = ''\n\t\tfor m in self.messages:\n\t\t\tresult += prefix + str(m) + '\\n'\n\t\treturn result\n\n\tdef get_status(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\tresult = ''\n\t\tfor m in self.messages:\n\t\t\tresult += prefix + m.get_status() + '\\n'\n\t\treturn result\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\tstr = ''\n\t\tfor i in range(len(self.messages)):\n\t\t\tif i < len(self.messages) - 1:\n\t\t\t\tstr += prefix + str(self.messages[i]) + '\\n'\n\t\t\telse:\n\t\t\t\tstr += prefix + str(self.messages[i])\n\t\treturn str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix + self.prefix + self.sep.join(sorted(self.messages))\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix + self.messages[0] +'' + self.messages[1] +'' + self.messages[2]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix + ', '.join([str(m) for m in sorted(self.messages)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn '\\n'.join(prefix + str(item) for item in sorted(self.messages))\n\t\texcept:\n\t\t\treturn\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn '\\n'.join(str(item) for item in sorted(self.messages))\n\t\texcept:\n\t\t\treturn\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix +''.join(str(msg) for msg in sorted(self.messages))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n        return ''.join(map(lambda x: prefix + str(x) + \"\\n\", sorted(self.messages)))\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn '\\n'.join('%s%s' % (prefix, m) for m in sorted(self.messages, reverse=True))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n        ret = ''\n        for m in sorted(self.messages):\n            ret += prefix + m + '\\n'\n        return ret\n\n    def status(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n        ret = ''\n        for m in sorted(self.messages):\n            ret += prefix + m + '\\n'\n        return ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\tresult = ''\n\t\tfor m in self.messages:\n\t\t\tresult += prefix + str(m) + '\\n'\n\t\treturn result\n\n\tdef get_status(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\tresult = ''\n\t\tfor m in self.messages:\n\t\t\tresult += prefix + m.get_status() + '\\n'\n\t\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\tstr = ''\n\t\tfor i in range(len(self.messages)):\n\t\t\tif i < len(self.messages) - 1:\n\t\t\t\tstr += prefix + str(self.messages[i]) + '\\n'\n\t\t\telse:\n\t\t\t\tstr += prefix + str(self.messages[i])\n\t\treturn str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix + self.prefix + self.sep.join(sorted(self.messages))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\treturn prefix + self.messages[0] +'' + self.messages[1] +'' + self.messages[2]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.log.status_str()\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(Task, self).__init__(*args, **kwargs)\n\n\t\t# Check for task_name in kwargs\n\t\tif not self.task_name:\n\t\t\tself.task_name = self.name()\n\t\tself.task_name = self.task_name.upper()\n\n\t\t# Check for task_id in kwargs\n\t\tif not self.task_id:\n\t\t\tself.task_id = self.name()\n\t\tself.task_id = self.task_id.upper()\n\n\t\t# Check for task_id in kwargs\n\t\tif not self.task_key:\n\t\t\tself.task_key = self.name()\n\t\tself.task_key = self.task_key.upper()\n\n\t\tself.log.append('Task.__init__')\n\t\tself.log.append(self.task_name)\n\t\tself.log.append(self.task_id)\n\t\tself.log.append(self.task_key)\n\n\t\t# Check for task_status in kwargs\n\t\tif not self.task_status:\n\t\t\tself.task_status = 'CREATED'\n\t\tself.log.append('task_status: {}'.format(self.task_status))\n\n\t\t# Check for task_status", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix=prefix)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.log.status_str()\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix+self.log.status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix)\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\n        # Return self.log.status_str with optional prefix\n        return prefix + self.log.status_str()\n\n\n#******************************************************************************", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\tstatus_str = self.log.status_str()\n\t\tif prefix:\n\t\t\tstatus_str = prefix +'' + status_str\n\t\treturn status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.log.status_str()\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix=prefix)\n\n\tdef warn(self, msg):\n\t\t\"\"\"\n\t\tLog message to self.log.warn, with optional prefix.\n\t\t\"\"\"\n\t\tself.log.warn(msg)\n\n\tdef error(self, msg):\n\t\t\"\"\"\n\t\tLog message to self.log.error, with optional prefix.\n\t\t\"\"\"\n\t\tself.log.error(msg)\n\n\tdef fatal(self, msg):\n\t\t\"\"\"\n\t\tLog message to self.log.fatal, with optional prefix.\n\t\t\"\"\"\n\t\tself.log.fatal(msg)\n\n\tdef is_log_level(self, level):\n\t\t\"\"\"\n\t\tReturn True if log level is set to level or higher.\n\t\t\"\"\"\n\t\treturn self.log.is_log_level(level)\n\n\tdef set_log_level(self, level, prefix=''):\n\t\t\"\"\"\n\t\tSet the log level to level (0-5), with optional prefix.\n\t\t\"\"\"\n\t\tself.log.set_log_level(level, prefix=prefix)\n\n\tdef log_level_str(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn string representation of the current log level, with optional prefix.\n\t\t\"\"\"\n\t\treturn self.log.log_level_str(prefix=prefix)\n\n\tdef set_log_file("], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.log.status_str()\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(Task, self).__init__(*args, **kwargs)\n\n\t\t# Check for task_name in kwargs\n\t\tif not self.task_name:\n\t\t\tself.task_name = self.name()\n\t\tself.task_name = self.task_name.upper()\n\n\t\t# Check for task_id in kwargs\n\t\tif not self.task_id:\n\t\t\tself.task_id = self.name()\n\t\tself.task_id = self.task_id.upper()\n\n\t\t# Check for task_id in kwargs\n\t\tif not self.task_key:\n\t\t\tself.task_key = self.name()\n\t\tself.task_key = self.task_key.upper()\n\n\t\tself.log.append('Task.__init__')\n\t\tself.log.append(self.task_name)\n\t\tself.log.append(self.task_id)\n\t\tself.log.append(self.task_key)\n\n\t\t# Check for task_status in kwargs\n\t\tif not self.task_status:\n\t\t\tself.task_status = 'CREATED'\n\t\tself.log.append('task_status: {}'.format(self.task_status))\n\n\t\t# Check for task_status", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix=prefix)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.log.status_str()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix+self.log.status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\n        # Return self.log.status_str with optional prefix\n        return prefix + self.log.status_str()\n\n\n#******************************************************************************", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\tstatus_str = self.log.status_str()\n\t\tif prefix:\n\t\t\tstatus_str = prefix +'' + status_str\n\t\treturn status_str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.log.status_str()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\treturn self.log.status_str(prefix=prefix)\n\n\tdef warn(self, msg):\n\t\t\"\"\"\n\t\tLog message to self.log.warn, with optional prefix.\n\t\t\"\"\"\n\t\tself.log.warn(msg)\n\n\tdef error(self, msg):\n\t\t\"\"\"\n\t\tLog message to self.log.error, with optional prefix.\n\t\t\"\"\"\n\t\tself.log.error(msg)\n\n\tdef fatal(self, msg):\n\t\t\"\"\"\n\t\tLog message to self.log.fatal, with optional prefix.\n\t\t\"\"\"\n\t\tself.log.fatal(msg)\n\n\tdef is_log_level(self, level):\n\t\t\"\"\"\n\t\tReturn True if log level is set to level or higher.\n\t\t\"\"\"\n\t\treturn self.log.is_log_level(level)\n\n\tdef set_log_level(self, level, prefix=''):\n\t\t\"\"\"\n\t\tSet the log level to level (0-5), with optional prefix.\n\t\t\"\"\"\n\t\tself.log.set_log_level(level, prefix=prefix)\n\n\tdef log_level_str(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn string representation of the current log level, with optional prefix.\n\t\t\"\"\"\n\t\treturn self.log.log_level_str(prefix=prefix)\n\n\tdef set_log_file(", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n\n    def get_identifier_info(self, identifier):\n\t\"\"\"\n\tReturn information about the identifier, including any associated metadata.\n\t\"\"\"\n\t\n        return None\n\n    def get_identifier_metadata(self, identifier):\n\t\"\"\"\n\tReturn any associated metadata for the identifier.\n\t\"\"\"\n\t\n        return None\n\n    def _get_url_path(self, identifier, format):\n\t\"\"\"\n\tReturn a path string to use for a URL for the given identifier and format.\n\tIn this base implementation, return None.\n\t\"\"\"\n\t\n        return None\n\n    def get_url(self, identifier, format=None, session=None):\n\t\"\"\"\n\tReturn a URL from the identifier.\n\t\"\"\"\n\t\n        # Check for a format\n        if format is None:\n            format = 'text'\n\n        # Check for a session\n        if session is None:\n            session = get_current_session()\n\n        # Get the URL path\n        url_path = self._get_url_path(identifier, format)\n\n        if url_path is None:\n            raise IOError(\"No URL path found for %s/%s\" % (identifier, format))\n\n        # Build the URL\n        url = '%s/%s' % (self.url, url_path)\n\n        return url\n\n    def get_path(self, identifier, format=None, session=None):\n\t\"\"\"\n\tReturn the path of", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n#\treturn True\n\treturn False\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True \n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n\t", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t \n\t# TODO: implement this method\n\t\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n        \n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n\n    def get_identifier_info(self, identifier):\n\t\"\"\"\n\tReturn information about the identifier, including any associated metadata.\n\t\"\"\"\n\t\n        return None\n\n    def get_identifier_metadata(self, identifier):\n\t\"\"\"\n\tReturn any associated metadata for the identifier.\n\t\"\"\"\n\t\n        return None\n\n    def _get_url_path(self, identifier, format):\n\t\"\"\"\n\tReturn a path string to use for a URL for the given identifier and format.\n\tIn this base implementation, return None.\n\t\"\"\"\n\t\n        return None\n\n    def get_url(self, identifier, format=None, session=None):\n\t\"\"\"\n\tReturn a URL from the identifier.\n\t\"\"\"\n\t\n        # Check for a format\n        if format is None:\n            format = 'text'\n\n        # Check for a session\n        if session is None:\n            session = get_current_session()\n\n        # Get the URL path\n        url_path = self._get_url_path(identifier, format)\n\n        if url_path is None:\n            raise IOError(\"No URL path found for %s/%s\" % (identifier, format))\n\n        # Build the URL\n        url = '%s/%s' % (self.url, url_path)\n\n        return url\n\n    def get_path(self, identifier, format=None, session=None):\n\t\"\"\"\n\tReturn the path of", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n#\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True \n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t \n\t# TODO: implement this method\n\t\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n        return True\n        \n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tself.inventory = inventory\n\t\tself.spec_version = self.get_spec_version(extract_spec_version)\n\t\tself.result = self.run_tests()\n\t\treturn self.result\n\n\tdef get_spec_version(self, extract_spec_version):\n\t\t\"\"\"\n\t\tGet the spec version to use for validation. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\ttype = self.inventory.get('type')\n\t\t\ttry:\n\t\t\t\tspec_version = SpecVersion(type)\n\t\t\texcept ValueError:\n\t\t\t\tlogging.warning('Inventory type {} is not valid'.format(type))\n\t\t\t\tspec_version = self.spec_version\n\t\t\telse:\n\t\t\t\treturn spec_version\n\t\treturn self.spec_version\n\n\tdef run_tests(self):\n\t\t\"\"\"\n\t\tRun the tests for the given specification version. (D)\n\t\t\"\"\"\n\t\tresult = {}\n\t\tif self.spec_version.major == 1:\n\t\t\tresult = self.run_tests_1()\n\t\telif self.spec_version.major == 2:\n\t\t\tresult", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif not issubclass(inventory.__class__, Inventory):\n\t\t\traise TypeError(\"inventory must be a subclass of Inventory\")\n\t\tif extract_spec_version:\n\t\t\ttry:\n\t\t\t\tspec_version = inventory.get_type()\n\t\t\t\tif not spec_version in self.spec_versions:\n\t\t\t\t\traise ValueError(\"spec_version: {0} is not a valid specification version\".format(spec_version))\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\telse:\n\t\t\tspec_version = self.spec_version\n\t\tif spec_version == \"1.0\":\n\t\t\tif not inventory.get_name() or not inventory.get_description():\n\t\t\t\traise ValueError(\"Name and Description are required fields in specification version 1.0\")\n\t\t\tif not inventory.get_owner() or not inventory.get_owner().get_id() or not inventory.get_owner().get_name():\n\t\t\t\traise ValueError(\"Owner ID and Name are required fields in specification version 1.0\")\n\t\t\tif not inventory.get_owner().get_id() or not inventory.get_owner().get_name():\n\t\t\t\traise ValueError(\"Owner ID and Name are required fields in specification version 1.0\")\n\t\t\tif inventory.get_owner().get_type()!= \"person\":\n\t\t\t\traise ValueError(\"Owner type must be 'person' in specification version 1.0\")\n\t\t\t", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\tspec_version = self.get_spec_version(inventory)\n\t\telse:\n\t\t\tspec_version = self.spec_version\n\n\t\t# do some validation\n\t\tif inventory['type'] not in self.valid_types:\n\t\t\tprint \"Type not valid.\"\n\t\t\treturn False\n\n\t\telif inventory['type'] =='system':\n\t\t\ttry:\n\t\t\t\tself.validate_system(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"System validation failed with exception: %s\" % str(e)\n\t\t\t\treturn False\n\t\telif inventory['type'] == 'chassis':\n\t\t\ttry:\n\t\t\t\tself.validate_chassis(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"Chassis validation failed with exception: %s\" % str(e)\n\t\t\t\treturn False\n\t\telif inventory['type'] == 'blade':\n\t\t\ttry:\n\t\t\t\tself.validate_blade(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"Blade validation failed with exception: %s\" % str(e)\n\t\t\t\treturn False\n\t\telif inventory['type'] =='slot':\n\t\t\ttry:\n\t\t\t\tself.validate_slot(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"Slot validation failed with exception: %s\" % str", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tself.spec_version = inventory['type']\n\t\telse:\n\t\t\tself.spec_version = self.spec_version\n\n\t\t# The following will only run for the current version of the specification\n\t\tif self.spec_version == '1.0':\n\n\t\t\tif 'name' in inventory:\n\t\t\t\tself.validate_name(inventory['name'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A name must be given for the inventory')\n\n\t\t\tif'version' in inventory:\n\t\t\t\tself.validate_version(inventory['version'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A version must be given for the inventory')\n\n\t\t\tif 'description' in inventory:\n\t\t\t\tself.validate_description(inventory['description'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A description must be given for the inventory')\n\n\t\t\tif 'keywords' in inventory:\n\t\t\t\tself.validate_keywords(inventory['keywords'])\n\t\t\telse:\n\t\t\t\traise ValidationError('Keywords must be given for the inventory')\n\n\t\t\tif 'date' in inventory:\n\t\t\t\tself.validate_date(inventory['date'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A date must be given for the inventory')\n\n\t\t\tif 'authors' in inventory:\n\t\t\t\tself.validate_authors(inventory['authors'])\n\t\t\telse:\n\t\t\t\traise ValidationError('Authors", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_file(self, inventory_file):\n\t\"\"\"\n\tValidate a given inventory file. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_string(self, inventory_string):\n\t\"\"\"\n\tValidate a given inventory string. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_url(self, inventory_url):\n\t\"\"\"\n\tValidate a given inventory URL. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_zip_file(self, inventory_zip_file):\n\t\"\"\"\n\tValidate a given inventory zip file. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_zip_string(self, inventory_zip_string):\n\t\"\"\"\n\tValidate a given inventory zip string. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_zip_url(self, inventory_zip_url):\n\t\"\"\"\n\tValidate a given inventory zip URL. (D)\n\t\"\"\"\n\t\tpass\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n# (E)\n\traise NotImplemented\n\n# (F)\n\traise NotImplemented\n\n# (G)\n\traise NotImplemented\n\n# (H)\n\traise NotImplemented\n\n# (I)\n\traise NotImplemented\n\n# (J)\n\traise NotImplemented\n\n# (K)\n\traise NotImplemented\n\n# (L)\n\traise NotImplemented\n\n# (M)\n\traise NotImplemented\n\n# (N)\n\traise NotImplemented\n\n# (O)\n\traise NotImplemented\n\n# (P)\n\traise NotImplemented\n\n# (Q)\n\traise NotImplemented\n\n# (R)\n\traise NotImplemented\n\n# (S)\n\traise NotImplemented\n\n# (T)\n\traise NotImplemented\n\n# (U)\n\traise NotImplemented\n\n# (V)\n\traise NotImplemented\n\n# (W)\n\traise NotImplemented\n\n# (X)\n\traise NotImplemented\n\n# (Y)\n\traise NotImplemented\n\n# (Z)\n\traise NotImplemented\n\n# ([)\n\traise NotImplemented\n\n# (])\n\traise NotImplemented\n\n# (\\)\n\traise NotImplemented\n\n# (^)\n\traise NotImplemented\n\n# (_1)\n\traise NotImplemented\n\n# (_2)\n\traise NotImplemented\n\n# (_3)\n\traise NotImplemented\n\n# (_4)\n\traise NotImplemented\n\n# (_5", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n        # First, let's see if we have a specification version in this inventory\n\tif extract_spec_version:\n\t\tself.spec_version = inventory.get_specification_version()\n\telif self.spec_version == None:\n\t\traise Exception(\"No specification version set in validator!\")\n\n\tif self.spec_version == '1.0':\n\t\tself.validate_1_0(inventory)\n\telif self.spec_version == '1.1':\n\t\tself.validate_1_1(inventory)\n\telse:\n\t\traise Exception('Unknown specification version: %s' % self.spec_version)\n\n    # Validate an inventory based on the 1.0 specification\n    def validate_1_0(self, inventory):\n\t\"\"\"\n\tValidate a given inventory. (D)\n\t\"\"\"\n\n\t# Make sure we have a specification version\n\tself.spec_version = inventory.get_specification_version()\n\n\t# Make sure we have the required fields\n\tself.validate_required_fields(inventory)\n\n\t# Make sure we have a valid format for the description\n\tself.validate_description(inventory)\n\n\t# Make sure we have a valid format for the version\n\tself.validate_version(inventory)\n\n\t# Make sure we have a valid format for the created\n\tself.validate_created(inventory)\n\n\t#", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif not self.validate_inventory(inventory):\n\t\t\treturn False\n\n\t\t# TODO: validate that the inventory is valid for the type\n\t\t# TODO: validate that the inventory is valid for the version\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\tif 'type' in inventory:\n\t\t\t\tself.spec_version = inventory['type']\n\t\t\telse:\n\t\t\t\tself.spec_version = self.spec_version\n\n\t\tif self.spec_version == 'v1.0':\n\t\t\treturn self.validate_v1_0(inventory)\n\t\telif self.spec_version == 'v1.1':\n\t\t\treturn self.validate_v1_1(inventory)\n\t\telif self.spec_version == 'v1.2':\n\t\t\treturn self.validate_v1_2(inventory)\n\t\telif self.spec_version == 'v1.3':\n\t\t\treturn self.validate_v1_3(inventory)\n\t\telif self.spec_version == 'v1.4':\n\t\t\treturn self.validate_v1_4(inventory)\n\t\telif self.spec_version == 'v1.5':\n\t\t\treturn self.validate_v1_5(inventory)\n\t\telse:\n\t\t\traise ValueError(\"spec_version {} is not valid\".format(self.spec_version))\n\n\tdef validate_v1_0(self, inventory):\n\t\t\"\"\"\n\t\tValidate a given inventory using the v1.0 specification.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Check to see if the inventory is a dict\n\t\t\tif not isinstance"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tself.inventory = inventory\n\t\tself.spec_version = self.get_spec_version(extract_spec_version)\n\t\tself.result = self.run_tests()\n\t\treturn self.result\n\n\tdef get_spec_version(self, extract_spec_version):\n\t\t\"\"\"\n\t\tGet the spec version to use for validation. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\ttype = self.inventory.get('type')\n\t\t\ttry:\n\t\t\t\tspec_version = SpecVersion(type)\n\t\t\texcept ValueError:\n\t\t\t\tlogging.warning('Inventory type {} is not valid'.format(type))\n\t\t\t\tspec_version = self.spec_version\n\t\t\telse:\n\t\t\t\treturn spec_version\n\t\treturn self.spec_version\n\n\tdef run_tests(self):\n\t\t\"\"\"\n\t\tRun the tests for the given specification version. (D)\n\t\t\"\"\"\n\t\tresult = {}\n\t\tif self.spec_version.major == 1:\n\t\t\tresult = self.run_tests_1()\n\t\telif self.spec_version.major == 2:\n\t\t\tresult", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif not issubclass(inventory.__class__, Inventory):\n\t\t\traise TypeError(\"inventory must be a subclass of Inventory\")\n\t\tif extract_spec_version:\n\t\t\ttry:\n\t\t\t\tspec_version = inventory.get_type()\n\t\t\t\tif not spec_version in self.spec_versions:\n\t\t\t\t\traise ValueError(\"spec_version: {0} is not a valid specification version\".format(spec_version))\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\telse:\n\t\t\tspec_version = self.spec_version\n\t\tif spec_version == \"1.0\":\n\t\t\tif not inventory.get_name() or not inventory.get_description():\n\t\t\t\traise ValueError(\"Name and Description are required fields in specification version 1.0\")\n\t\t\tif not inventory.get_owner() or not inventory.get_owner().get_id() or not inventory.get_owner().get_name():\n\t\t\t\traise ValueError(\"Owner ID and Name are required fields in specification version 1.0\")\n\t\t\tif not inventory.get_owner().get_id() or not inventory.get_owner().get_name():\n\t\t\t\traise ValueError(\"Owner ID and Name are required fields in specification version 1.0\")\n\t\t\tif inventory.get_owner().get_type()!= \"person\":\n\t\t\t\traise ValueError(\"Owner type must be 'person' in specification version 1.0\")\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\tspec_version = self.get_spec_version(inventory)\n\t\telse:\n\t\t\tspec_version = self.spec_version\n\n\t\t# do some validation\n\t\tif inventory['type'] not in self.valid_types:\n\t\t\tprint \"Type not valid.\"\n\t\t\treturn False\n\n\t\telif inventory['type'] =='system':\n\t\t\ttry:\n\t\t\t\tself.validate_system(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"System validation failed with exception: %s\" % str(e)\n\t\t\t\treturn False\n\t\telif inventory['type'] == 'chassis':\n\t\t\ttry:\n\t\t\t\tself.validate_chassis(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"Chassis validation failed with exception: %s\" % str(e)\n\t\t\t\treturn False\n\t\telif inventory['type'] == 'blade':\n\t\t\ttry:\n\t\t\t\tself.validate_blade(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"Blade validation failed with exception: %s\" % str(e)\n\t\t\t\treturn False\n\t\telif inventory['type'] =='slot':\n\t\t\ttry:\n\t\t\t\tself.validate_slot(inventory, spec_version)\n\t\t\texcept Exception, e:\n\t\t\t\tprint \"Slot validation failed with exception: %s\" % str", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tself.spec_version = inventory['type']\n\t\telse:\n\t\t\tself.spec_version = self.spec_version\n\n\t\t# The following will only run for the current version of the specification\n\t\tif self.spec_version == '1.0':\n\n\t\t\tif 'name' in inventory:\n\t\t\t\tself.validate_name(inventory['name'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A name must be given for the inventory')\n\n\t\t\tif'version' in inventory:\n\t\t\t\tself.validate_version(inventory['version'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A version must be given for the inventory')\n\n\t\t\tif 'description' in inventory:\n\t\t\t\tself.validate_description(inventory['description'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A description must be given for the inventory')\n\n\t\t\tif 'keywords' in inventory:\n\t\t\t\tself.validate_keywords(inventory['keywords'])\n\t\t\telse:\n\t\t\t\traise ValidationError('Keywords must be given for the inventory')\n\n\t\t\tif 'date' in inventory:\n\t\t\t\tself.validate_date(inventory['date'])\n\t\t\telse:\n\t\t\t\traise ValidationError('A date must be given for the inventory')\n\n\t\t\tif 'authors' in inventory:\n\t\t\t\tself.validate_authors(inventory['authors'])\n\t\t\telse:\n\t\t\t\traise ValidationError('Authors", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_file(self, inventory_file):\n\t\"\"\"\n\tValidate a given inventory file. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_string(self, inventory_string):\n\t\"\"\"\n\tValidate a given inventory string. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_url(self, inventory_url):\n\t\"\"\"\n\tValidate a given inventory URL. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_zip_file(self, inventory_zip_file):\n\t\"\"\"\n\tValidate a given inventory zip file. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_zip_string(self, inventory_zip_string):\n\t\"\"\"\n\tValidate a given inventory zip string. (D)\n\t\"\"\"\n\t\tpass\n\n\tdef validate_from_zip_url(self, inventory_zip_url):\n\t\"\"\"\n\tValidate a given inventory zip URL. (D)\n\t\"\"\"\n\t\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n# (E)\n\traise NotImplemented\n\n# (F)\n\traise NotImplemented\n\n# (G)\n\traise NotImplemented\n\n# (H)\n\traise NotImplemented\n\n# (I)\n\traise NotImplemented\n\n# (J)\n\traise NotImplemented\n\n# (K)\n\traise NotImplemented\n\n# (L)\n\traise NotImplemented\n\n# (M)\n\traise NotImplemented\n\n# (N)\n\traise NotImplemented\n\n# (O)\n\traise NotImplemented\n\n# (P)\n\traise NotImplemented\n\n# (Q)\n\traise NotImplemented\n\n# (R)\n\traise NotImplemented\n\n# (S)\n\traise NotImplemented\n\n# (T)\n\traise NotImplemented\n\n# (U)\n\traise NotImplemented\n\n# (V)\n\traise NotImplemented\n\n# (W)\n\traise NotImplemented\n\n# (X)\n\traise NotImplemented\n\n# (Y)\n\traise NotImplemented\n\n# (Z)\n\traise NotImplemented\n\n# ([)\n\traise NotImplemented\n\n# (])\n\traise NotImplemented\n\n# (\\)\n\traise NotImplemented\n\n# (^)\n\traise NotImplemented\n\n# (_1)\n\traise NotImplemented\n\n# (_2)\n\traise NotImplemented\n\n# (_3)\n\traise NotImplemented\n\n# (_4)\n\traise NotImplemented\n\n# (_5", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n        # First, let's see if we have a specification version in this inventory\n\tif extract_spec_version:\n\t\tself.spec_version = inventory.get_specification_version()\n\telif self.spec_version == None:\n\t\traise Exception(\"No specification version set in validator!\")\n\n\tif self.spec_version == '1.0':\n\t\tself.validate_1_0(inventory)\n\telif self.spec_version == '1.1':\n\t\tself.validate_1_1(inventory)\n\telse:\n\t\traise Exception('Unknown specification version: %s' % self.spec_version)\n\n    # Validate an inventory based on the 1.0 specification\n    def validate_1_0(self, inventory):\n\t\"\"\"\n\tValidate a given inventory. (D)\n\t\"\"\"\n\n\t# Make sure we have a specification version\n\tself.spec_version = inventory.get_specification_version()\n\n\t# Make sure we have the required fields\n\tself.validate_required_fields(inventory)\n\n\t# Make sure we have a valid format for the description\n\tself.validate_description(inventory)\n\n\t# Make sure we have a valid format for the version\n\tself.validate_version(inventory)\n\n\t# Make sure we have a valid format for the created\n\tself.validate_created(inventory)\n\n\t#", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif not self.validate_inventory(inventory):\n\t\t\treturn False\n\n\t\t# TODO: validate that the inventory is valid for the type\n\t\t# TODO: validate that the inventory is valid for the version\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t# TODO: validate that the inventory has the right number of entries\n\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\tif 'type' in inventory:\n\t\t\t\tself.spec_version = inventory['type']\n\t\t\telse:\n\t\t\t\tself.spec_version = self.spec_version\n\n\t\tif self.spec_version == 'v1.0':\n\t\t\treturn self.validate_v1_0(inventory)\n\t\telif self.spec_version == 'v1.1':\n\t\t\treturn self.validate_v1_1(inventory)\n\t\telif self.spec_version == 'v1.2':\n\t\t\treturn self.validate_v1_2(inventory)\n\t\telif self.spec_version == 'v1.3':\n\t\t\treturn self.validate_v1_3(inventory)\n\t\telif self.spec_version == 'v1.4':\n\t\t\treturn self.validate_v1_4(inventory)\n\t\telif self.spec_version == 'v1.5':\n\t\t\treturn self.validate_v1_5(inventory)\n\t\telse:\n\t\t\traise ValueError(\"spec_version {} is not valid\".format(self.spec_version))\n\n\tdef validate_v1_0(self, inventory):\n\t\t\"\"\"\n\t\tValidate a given inventory using the v1.0 specification.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Check to see if the inventory is a dict\n\t\t\tif not isinstance", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor image in manifest_files:\n\t\t\tfor digest in manifest_files[image]:\n\t\t\t\tif digest not in digests_used:\n\t\t\t\t\treturn error(\"Digest \" + digest + \" in manifest file \" + image + \" is not used.\")\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t\tself.error(f\"Digest {digest} used in manifest but not present in repo\")\n\t\tfor digest, manifest_file in manifest_files.items():\n\t\t\tif digest not in digests_used:\n\t\t\t\tself.error(f\"Digest {digest} in repo but not used in manifest\")\n\t\treturn\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\twith open(manifest_file, \"r\") as manifest:\n\t\t\t\tfor line in manifest:\n\t\t\t\t\tif line.startswith(\"sha256\"):\n\t\t\t\t\t\tdigest = line.split(\" \")[0]\n\t\t\t\t\t\tif digest not in digests_used:\n\t\t\t\t\t\t\terror(\"Digest %s is in the manifest but not used. Please fix the manifest.\" % digest)\n\treturn\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\n        for manifest_file in manifest_files:\n            with open(manifest_file) as mf:\n                for line in mf:\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    if line.startswith('SHA256'):\n                        digest = line.split()[0]\n                        digest_name = digest[5:]\n                        if digest_name not in digests_used:\n                            self.error('Digest'+ digest_name +'not used in digest list.')\n                    elif line.startswith('SHA1'):\n                        digest = line.split()[0]\n                        digest_name = digest[4:]\n                        if digest_name not in digests_used:\n                            self.error('Digest'+ digest_name +'not used in digest list.')\n                    elif line.startswith('MD5'):\n                        digest = line.split()[0]\n                        digest_name = digest[3:]\n                        if digest_name not in digests_used:\n                            self.error('Digest'+ digest_name +'not used in digest list.')\n\n    def add_digests_to_digest_list(self, digests_used, digest_list, digest, digest_type):\n        \"\"\"\n        Add digests to digest list.\n        \"\"\"\n\n        for index, digest_name in enumerate(digests_used):\n            if digest_name == digest:\n                if digest_list[index]!= digest:\n                    self", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\n        # Check that all digests are present\n        for i in manifest_files:\n                if i.get(\"digest\") is not None:\n                        if i.get(\"digest\") not in digests_used:\n                                self.error(\"Digest %s used in manifest but not present in file list\" % i.get(\"digest\"))\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tif not digests_used:\n\t\t\traise Exception('No digests used in this repository')\n\t\tfor digest in digests_used:\n\t\t\tif not digest in manifest_files:\n\t\t\t\traise Exception('Digest'+ digest +'is not present in the manifest')\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t\tself.error(\"digest {} not found in manifest\".format(digest), \"digest\")\n\t\tfor manifest_file in manifest_files:\n\t\t\tif manifest_file not in digests_used:\n\t\t\t\tself.error(\"digest {} not used in manifest\".format(manifest_file), \"digest\")\n\tdef check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t\tself.error(\"digest {} not found in manifest\".format(digest), \"digest\")\n\t\tfor manifest_file in manifest_files:\n\t\t\tif manifest_file not in digests_used:\n\t\t\t\tself.error(\"digest {} not used in manifest\".format(manifest_file), \"digest\")\n\tdef check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif not digest in manifest_files:\n\t\t\t\tself.error(\"Digest not found in manifest: %s\" % digest)\n\t\treturn\n\n\tdef check_digests_not_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are not needed are not used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in manifest_files:\n\t\t\tif digest in digests_used:\n\t\t\t\tself.error(\"Digest is in manifest but also used: %s\" % digest)\n\t\treturn\n\n\tdef check_digests_used_not_in_manifest(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are used are not in the manifest. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif not digest in manifest_files:\n\t\t\t\tself.error(\"Digest in used list is not in manifest: %s\" % digest)\n\t\treturn\n\n\tdef check_digests_used_in_manifest_no_duplicates(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are used are not in the manifest. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digest", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor mf in manifest_files:\n\t\t\tif mf.name == \"digests\":\n\t\t\t\tfor digest in mf.digests:\n\t\t\t\t\tif digest.digest not in digests_used:\n\t\t\t\t\t\tself.error(f\"Digest {digest.digest} used in {mf.name} in {manifest_files[0].name} but not in any other manifest.\")\n\n\tdef check_digests_not_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are not needed are not present. Return error() in the class.\n\t\"\"\"\n\t\tfor mf in manifest_files:\n\t\t\tif mf.name == \"digests\":\n\t\t\t\tfor digest in mf.digests:\n\t\t\t\t\tif digest.digest not in digests_used:\n\t\t\t\t\t\tself.error(f\"Digest {digest.digest} used in {mf.name} in {manifest_files[0].name} but not in any other manifest.\")\n\n\tdef check_manifest_files_have_same_digests(self, manifest_files):\n\t\"\"\"\n\tCheck all manifest files are consistent with each other. Return error() in the class.\n\t\"\"\"\n\t\tdigests_used = set()\n\t\tfor mf in manifest_files:\n\t\t\tif mf.name == \"digests\":\n\t\t\t\tfor digest in mf."], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor image in manifest_files:\n\t\t\tfor digest in manifest_files[image]:\n\t\t\t\tif digest not in digests_used:\n\t\t\t\t\treturn error(\"Digest \" + digest + \" in manifest file \" + image + \" is not used.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t\tself.error(f\"Digest {digest} used in manifest but not present in repo\")\n\t\tfor digest, manifest_file in manifest_files.items():\n\t\t\tif digest not in digests_used:\n\t\t\t\tself.error(f\"Digest {digest} in repo but not used in manifest\")\n\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\twith open(manifest_file, \"r\") as manifest:\n\t\t\t\tfor line in manifest:\n\t\t\t\t\tif line.startswith(\"sha256\"):\n\t\t\t\t\t\tdigest = line.split(\" \")[0]\n\t\t\t\t\t\tif digest not in digests_used:\n\t\t\t\t\t\t\terror(\"Digest %s is in the manifest but not used. Please fix the manifest.\" % digest)\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\n        for manifest_file in manifest_files:\n            with open(manifest_file) as mf:\n                for line in mf:\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    if line.startswith('SHA256'):\n                        digest = line.split()[0]\n                        digest_name = digest[5:]\n                        if digest_name not in digests_used:\n                            self.error('Digest'+ digest_name +'not used in digest list.')\n                    elif line.startswith('SHA1'):\n                        digest = line.split()[0]\n                        digest_name = digest[4:]\n                        if digest_name not in digests_used:\n                            self.error('Digest'+ digest_name +'not used in digest list.')\n                    elif line.startswith('MD5'):\n                        digest = line.split()[0]\n                        digest_name = digest[3:]\n                        if digest_name not in digests_used:\n                            self.error('Digest'+ digest_name +'not used in digest list.')\n\n    def add_digests_to_digest_list(self, digests_used, digest_list, digest, digest_type):\n        \"\"\"\n        Add digests to digest list.\n        \"\"\"\n\n        for index, digest_name in enumerate(digests_used):\n            if digest_name == digest:\n                if digest_list[index]!= digest:\n                    self", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\n        # Check that all digests are present\n        for i in manifest_files:\n                if i.get(\"digest\") is not None:\n                        if i.get(\"digest\") not in digests_used:\n                                self.error(\"Digest %s used in manifest but not present in file list\" % i.get(\"digest\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tif not digests_used:\n\t\t\traise Exception('No digests used in this repository')\n\t\tfor digest in digests_used:\n\t\t\tif not digest in manifest_files:\n\t\t\t\traise Exception('Digest'+ digest +'is not present in the manifest')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t\tself.error(\"digest {} not found in manifest\".format(digest), \"digest\")\n\t\tfor manifest_file in manifest_files:\n\t\t\tif manifest_file not in digests_used:\n\t\t\t\tself.error(\"digest {} not used in manifest\".format(manifest_file), \"digest\")\n\tdef check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t\tself.error(\"digest {} not found in manifest\".format(digest), \"digest\")\n\t\tfor manifest_file in manifest_files:\n\t\t\tif manifest_file not in digests_used:\n\t\t\t\tself.error(\"digest {} not used in manifest\".format(manifest_file), \"digest\")\n\tdef check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif digest not in manifest_files:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif not digest in manifest_files:\n\t\t\t\tself.error(\"Digest not found in manifest: %s\" % digest)\n\t\treturn\n\n\tdef check_digests_not_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are not needed are not used. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in manifest_files:\n\t\t\tif digest in digests_used:\n\t\t\t\tself.error(\"Digest is in manifest but also used: %s\" % digest)\n\t\treturn\n\n\tdef check_digests_used_not_in_manifest(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are used are not in the manifest. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digests_used:\n\t\t\tif not digest in manifest_files:\n\t\t\t\tself.error(\"Digest in used list is not in manifest: %s\" % digest)\n\t\treturn\n\n\tdef check_digests_used_in_manifest_no_duplicates(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are used are not in the manifest. Return error() in the class.\n\t\"\"\"\n\t\tfor digest in digest", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\t\tfor mf in manifest_files:\n\t\t\tif mf.name == \"digests\":\n\t\t\t\tfor digest in mf.digests:\n\t\t\t\t\tif digest.digest not in digests_used:\n\t\t\t\t\t\tself.error(f\"Digest {digest.digest} used in {mf.name} in {manifest_files[0].name} but not in any other manifest.\")\n\n\tdef check_digests_not_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are not needed are not present. Return error() in the class.\n\t\"\"\"\n\t\tfor mf in manifest_files:\n\t\t\tif mf.name == \"digests\":\n\t\t\t\tfor digest in mf.digests:\n\t\t\t\t\tif digest.digest not in digests_used:\n\t\t\t\t\t\tself.error(f\"Digest {digest.digest} used in {mf.name} in {manifest_files[0].name} but not in any other manifest.\")\n\n\tdef check_manifest_files_have_same_digests(self, manifest_files):\n\t\"\"\"\n\tCheck all manifest files are consistent with each other. Return error() in the class.\n\t\"\"\"\n\t\tdigests_used = set()\n\t\tfor mf in manifest_files:\n\t\t\tif mf.name == \"digests\":\n\t\t\t\tfor digest in mf.", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\tif prior.version < self.version:\n\t\t\traise ValueError(\"The prior inventory is not a valid prior version of the current inventory object.\")\n\tdef validate_as_prior_version_strict(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Raise an exception if this is not the case.\n\t\"\"\"\n\t\tif prior.version > self.version:\n\t\t\traise ValueError(\"The prior inventory is not a valid prior version of the current inventory object.\")\n\tdef validate_as_post_version(self, post):\n\t\"\"\"\n\tCheck that post is a valid post version of the current inventory object. The input variable post is also expected to be an InventoryValidator object and both self and post inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\tif post.version < self.version:\n\t\t\traise ValueError(\"The post inventory is not a valid post version of the current inventory object.\")\n\tdef validate_as_post_version_strict(self, post):\n\t\"\"\"\n\tCheck that post is a valid post version of the current inventory object. The input variable post is also expected to be an InventoryValidator object and both self and post invent", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\treturn self.error()\n\n\tdef validate_as_next_version(self, next):\n\t\"\"\"\n\tCheck that next is a valid next version of the current inventory object. The input variable next is also expected to be an InventoryValidator object and both self and next inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\treturn self.error()\n\n\tdef validate_as_same_version(self, same):\n\t\"\"\"\n\tCheck that same is a valid same version of the current inventory object. The input variable same is also expected to be an InventoryValidator object and both self and same inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\treturn self.error()\n\n\tdef check_consistency(self):\n\t\t\"\"\"\n\t\tCheck that the inventory object is internally consistent. Return an error code if the inventory is inconsistent, or self.error() if the inventory is consistent.\n\t\t\"\"\"\n\t\treturn self.error()\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\traise NotImplementedError\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\n\tif not isinstance(prior, InventoryValidator):\n\t\traise ValueError(\"validate_as_prior_version: prior is not an InventoryValidator object\")\n\n\tif prior.get_version() > self.get_version():\n\t\traise ValueError(\"validate_as_prior_version: prior is a newer version than the current inventory\")\n\tif prior.get_version() < self.get_version():\n\t\traise ValueError(\"validate_as_prior_version: prior is an older version than the current inventory\")\n\treturn\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n#\treturn True\n\tif not isinstance(prior, InventoryValidator):\n\t\tself.error(\"Invalid prior inventory passed to validate_as_prior_version.\")\n\t\treturn False\n\tif not prior.is_valid():\n\t\tself.error(\"Invalid prior inventory passed to validate_as_prior_version.\")\n\t\treturn False\n\tif prior.get_filename()!= self.get_filename():\n\t\tself.error(\"Prior inventory has wrong file.\")\n\t\treturn False\n\tif prior.get_version() > self.get_version():\n\t\tself.error(\"Prior inventory has wrong version.\")\n\t\treturn False\n\tif prior.get_schema_version()!= self.get_schema_version():\n\t\tself.error(\"Prior inventory has wrong schema version.\")\n\t\treturn False\n\tif prior.get_date() > self.get_date():\n\t\tself.error(\"Prior inventory has wrong date.\")\n\t\treturn False\n\tif prior.get_date_modified() > self.get_date_modified():\n\t\tself.error(\"Prior inventory has wrong date modified.\")\n\t\treturn False\n\treturn True\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\t# Check that prior is a valid prior version of self.\n\t\tif self.version >= prior.version:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn 1\n\n\tdef validate_as_latest_version(self, latest):\n\t\t\"\"\"\n\t\tCheck that latest is a valid latest version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\t\"\"\"\n\t\t# Check that latest is a valid latest version of self.\n\t\tif self.version <= latest.version:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn 1\n\n\tdef print_inventory(self):\n\t\t\"\"\"\n\t\tPrint out the inventory.\n\t\t\"\"\"\n\t\tprint \"Inventory Version: \", self.version\n\t\tprint self.inventory\n\n\tdef print_inventory_as_string(self):\n\t\t\"\"\"\n\t\tReturn a string representation of the inventory.\n\t\t\"\"\"\n\t\toutput = \"\"\n\t\toutput += \"Inventory Version: \" + str(self.version) + \"\\n\"\n\t\toutput += self.inventory\n\t\treturn output\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\tif prior.version < self.version:\n\t\t\traise ValueError(\"The prior inventory is not a valid prior version of the current inventory object.\")\n\tdef validate_as_prior_version_strict(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Raise an exception if this is not the case.\n\t\"\"\"\n\t\tif prior.version > self.version:\n\t\t\traise ValueError(\"The prior inventory is not a valid prior version of the current inventory object.\")\n\tdef validate_as_post_version(self, post):\n\t\"\"\"\n\tCheck that post is a valid post version of the current inventory object. The input variable post is also expected to be an InventoryValidator object and both self and post inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\tif post.version < self.version:\n\t\t\traise ValueError(\"The post inventory is not a valid post version of the current inventory object.\")\n\tdef validate_as_post_version_strict(self, post):\n\t\"\"\"\n\tCheck that post is a valid post version of the current inventory object. The input variable post is also expected to be an InventoryValidator object and both self and post invent", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\treturn self.error()\n\n\tdef validate_as_next_version(self, next):\n\t\"\"\"\n\tCheck that next is a valid next version of the current inventory object. The input variable next is also expected to be an InventoryValidator object and both self and next inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\treturn self.error()\n\n\tdef validate_as_same_version(self, same):\n\t\"\"\"\n\tCheck that same is a valid same version of the current inventory object. The input variable same is also expected to be an InventoryValidator object and both self and same inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\treturn self.error()\n\n\tdef check_consistency(self):\n\t\t\"\"\"\n\t\tCheck that the inventory object is internally consistent. Return an error code if the inventory is inconsistent, or self.error() if the inventory is consistent.\n\t\t\"\"\"\n\t\treturn self.error()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\n\tif not isinstance(prior, InventoryValidator):\n\t\traise ValueError(\"validate_as_prior_version: prior is not an InventoryValidator object\")\n\n\tif prior.get_version() > self.get_version():\n\t\traise ValueError(\"validate_as_prior_version: prior is a newer version than the current inventory\")\n\tif prior.get_version() < self.get_version():\n\t\traise ValueError(\"validate_as_prior_version: prior is an older version than the current inventory\")\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n#\treturn True\n\tif not isinstance(prior, InventoryValidator):\n\t\tself.error(\"Invalid prior inventory passed to validate_as_prior_version.\")\n\t\treturn False\n\tif not prior.is_valid():\n\t\tself.error(\"Invalid prior inventory passed to validate_as_prior_version.\")\n\t\treturn False\n\tif prior.get_filename()!= self.get_filename():\n\t\tself.error(\"Prior inventory has wrong file.\")\n\t\treturn False\n\tif prior.get_version() > self.get_version():\n\t\tself.error(\"Prior inventory has wrong version.\")\n\t\treturn False\n\tif prior.get_schema_version()!= self.get_schema_version():\n\t\tself.error(\"Prior inventory has wrong schema version.\")\n\t\treturn False\n\tif prior.get_date() > self.get_date():\n\t\tself.error(\"Prior inventory has wrong date.\")\n\t\treturn False\n\tif prior.get_date_modified() > self.get_date_modified():\n\t\tself.error(\"Prior inventory has wrong date modified.\")\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\t# Check that prior is a valid prior version of self.\n\t\tif self.version >= prior.version:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn 1\n\n\tdef validate_as_latest_version(self, latest):\n\t\t\"\"\"\n\t\tCheck that latest is a valid latest version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\t\"\"\"\n\t\t# Check that latest is a valid latest version of self.\n\t\tif self.version <= latest.version:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn 1\n\n\tdef print_inventory(self):\n\t\t\"\"\"\n\t\tPrint out the inventory.\n\t\t\"\"\"\n\t\tprint \"Inventory Version: \", self.version\n\t\tprint self.inventory\n\n\tdef print_inventory_as_string(self):\n\t\t\"\"\"\n\t\tReturn a string representation of the inventory.\n\t\t\"\"\"\n\t\toutput = \"\"\n\t\toutput += \"Inventory Version: \" + str(self.version) + \"\\n\"\n\t\toutput += self.inventory\n\t\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n#\tlogical_paths = {}\n#\tfor host, host_data in inventory.items():\n#\t\tlogical_paths[host] = {}\n#\t\tfor state, state_data in host_data.items():\n#\t\t\tlogical_paths[host][state] = os.path.join(version, host, state + \".yml\")\n#\treturn logical_paths\n\tlogical_paths = {}\n\tfor host, host_data in inventory.items():\n\t\tfor state, state_data in host_data.items():\n\t\t\tlogical_paths[state] = os.path.join(version, host, state + \".yml\")\n\treturn logical_paths\n\t\n\t", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n\n\tlogical_path_map = {}\n\tif version == 0:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 1:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 2:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 3:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 4:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 5:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 6:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 7:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tstates_dir = inventory[version][\"dir_states\"]\n\tstates_base = inventory[version][\"base_states\"]\n\tstates_files = inventory[version][\"files_states\"]\n\tstates_file_types = inventory[version][\"file_types\"]\n\n\tlogical_path_map = {}\n\tfor state_file_type in states_file_types:\n\t\tlogical_path_map[state_file_type] = {}\n\t\tfor state_file in states_files[state_file_type]:\n\t\t\tlogical_path_map[state_file_type][state_file] = os.path.join(states_dir[state_file_type], states_base[state_file_type], state_file)\n\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\tlogical_path_map = {}\n\t\tfor server_name in inventory:\n\t\t\t# get the file path of the state file\n\t\t\tserver_path = os.path.join(inventory[server_name]['path'],'server.sls')\n\t\t\t# get the file path of the sls file\n\t\t\tlogical_path_map[server_path] = inventory[server_name]['files'][version]['logical']\n\t\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n\n\tlogical_path_map = dict()\n\tfor host in inventory:\n\t\tfor group in inventory[host]:\n\t\t\tif group == \"children\":\n\t\t\t\tfor child_host in inventory[host][group]:\n\t\t\t\t\thost_path = os.path.join(version, child_host)\n\t\t\t\t\tlogical_path_map[child_host] = host_path\n\n\treturn logical_path_map\n\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tpath_map = {}\n\tbase_path = \"%s/inventory/%s/roles\" % (inventory, version)\n\n\tfor role in os.listdir(base_path):\n\t\trole_path = \"%s/%s\" % (base_path, role)\n\t\tif os.path.isdir(role_path):\n\t\t\tpath_map[role] = {}\n\t\t\tfor state in os.listdir(role_path):\n\t\t\t\tstate_path = \"%s/%s\" % (role_path, state)\n\t\t\t\tif os.path.isdir(state_path):\n\t\t\t\t\tpath_map[role][state] = {}\n\t\t\t\t\tfor file in os.listdir(state_path):\n\t\t\t\t\t\tif file.endswith(\".sls\"):\n\t\t\t\t\t\t\tpath_map[role][state][file] = \"%s/%s\" % (state_path, file)\n\t\n\treturn path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tstate_path_dict = {}\n\tfor hostname, host_info in inventory.items():\n\t\tif version in host_info['facts']['ansible_facts']:\n\t\t\tstate_path_dict[hostname] = host_info['facts']['ansible_facts'][version]['state_path']\n\t\telse:\n\t\t\tstate_path_dict[hostname] = ''\n\treturn state_path_dict\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n        logical_path_map = {}\n\tfor filename in inventory:\n\t\tlogical_path_map[filename] = get_logical_path(filename, version)\n\t\t\n\t\t# print logical_path_map[filename]\n\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tlogical_path_map = {}\n\tfor inventory_item in inventory:\n\t\tif inventory_item['type'] == 'file':\n\t\t\tlogical_path_map[inventory_item['path']] = get_path_version(inventory_item, version)\n\t\telif inventory_item['type'] == 'dir':\n\t\t\tdir_content = get_directory_content(inventory_item, version)\n\t\t\tfor file_path in dir_content:\n\t\t\t\tlogical_path_map[file_path] = get_path_version(dir_content[file_path], version)\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tlogical_path_map = {}\n\tfor module in inventory.keys():\n\t\tlogical_path_map[module] = {}\n\t\tfor state in inventory[module].keys():\n\t\t\tif state =='meta':\n\t\t\t\tcontinue\n\t\t\tlogical_path_map[module][state] = '/'.join([module, state, version])\n\treturn logical_path_map\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n#\tlogical_paths = {}\n#\tfor host, host_data in inventory.items():\n#\t\tlogical_paths[host] = {}\n#\t\tfor state, state_data in host_data.items():\n#\t\t\tlogical_paths[host][state] = os.path.join(version, host, state + \".yml\")\n#\treturn logical_paths\n\tlogical_paths = {}\n\tfor host, host_data in inventory.items():\n\t\tfor state, state_data in host_data.items():\n\t\t\tlogical_paths[state] = os.path.join(version, host, state + \".yml\")\n\treturn logical_paths\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n\n\tlogical_path_map = {}\n\tif version == 0:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 1:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 2:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 3:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 4:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 5:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 6:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif version == 7:\n\t\tfor item in inventory.items():\n\t\t\tlogical_path_map[item[0]] = item[1]['path']\n\telif", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tstates_dir = inventory[version][\"dir_states\"]\n\tstates_base = inventory[version][\"base_states\"]\n\tstates_files = inventory[version][\"files_states\"]\n\tstates_file_types = inventory[version][\"file_types\"]\n\n\tlogical_path_map = {}\n\tfor state_file_type in states_file_types:\n\t\tlogical_path_map[state_file_type] = {}\n\t\tfor state_file in states_files[state_file_type]:\n\t\t\tlogical_path_map[state_file_type][state_file] = os.path.join(states_dir[state_file_type], states_base[state_file_type], state_file)\n\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\tlogical_path_map = {}\n\t\tfor server_name in inventory:\n\t\t\t# get the file path of the state file\n\t\t\tserver_path = os.path.join(inventory[server_name]['path'],'server.sls')\n\t\t\t# get the file path of the sls file\n\t\t\tlogical_path_map[server_path] = inventory[server_name]['files'][version]['logical']\n\t\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n\n\tlogical_path_map = dict()\n\tfor host in inventory:\n\t\tfor group in inventory[host]:\n\t\t\tif group == \"children\":\n\t\t\t\tfor child_host in inventory[host][group]:\n\t\t\t\t\thost_path = os.path.join(version, child_host)\n\t\t\t\t\tlogical_path_map[child_host] = host_path\n\n\treturn logical_path_map\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tpath_map = {}\n\tbase_path = \"%s/inventory/%s/roles\" % (inventory, version)\n\n\tfor role in os.listdir(base_path):\n\t\trole_path = \"%s/%s\" % (base_path, role)\n\t\tif os.path.isdir(role_path):\n\t\t\tpath_map[role] = {}\n\t\t\tfor state in os.listdir(role_path):\n\t\t\t\tstate_path = \"%s/%s\" % (role_path, state)\n\t\t\t\tif os.path.isdir(state_path):\n\t\t\t\t\tpath_map[role][state] = {}\n\t\t\t\t\tfor file in os.listdir(state_path):\n\t\t\t\t\t\tif file.endswith(\".sls\"):\n\t\t\t\t\t\t\tpath_map[role][state][file] = \"%s/%s\" % (state_path, file)\n\t\n\treturn path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tstate_path_dict = {}\n\tfor hostname, host_info in inventory.items():\n\t\tif version in host_info['facts']['ansible_facts']:\n\t\t\tstate_path_dict[hostname] = host_info['facts']['ansible_facts'][version]['state_path']\n\t\telse:\n\t\t\tstate_path_dict[hostname] = ''\n\treturn state_path_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t\n        logical_path_map = {}\n\tfor filename in inventory:\n\t\tlogical_path_map[filename] = get_logical_path(filename, version)\n\t\t\n\t\t# print logical_path_map[filename]\n\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tlogical_path_map = {}\n\tfor inventory_item in inventory:\n\t\tif inventory_item['type'] == 'file':\n\t\t\tlogical_path_map[inventory_item['path']] = get_path_version(inventory_item, version)\n\t\telif inventory_item['type'] == 'dir':\n\t\t\tdir_content = get_directory_content(inventory_item, version)\n\t\t\tfor file_path in dir_content:\n\t\t\t\tlogical_path_map[file_path] = get_path_version(dir_content[file_path], version)\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\t \n\tlogical_path_map = {}\n\tfor module in inventory.keys():\n\t\tlogical_path_map[module] = {}\n\t\tfor state in inventory[module].keys():\n\t\t\tif state =='meta':\n\t\t\t\tcontinue\n\t\t\tlogical_path_map[module][state] = '/'.join([module, state, version])\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\n\tfixity_list = fixity['fixity']\n\tfor fixity_block in fixity_list:\n\t\tfile_name = fixity_block['file']\n\t\tif file_name not in manifest_files:\n\t\t\tself.error_list.append(file_name)\n\n\treturn self.error_list\n/README.md\n# py-inventory\nAn inventory validation tool for the Dryad Data Repository\n\n## Installation\n\n### Dependencies\n\n1. Python 2.7\n2. lxml\n\n### Installation of lxml\n\n#### Ubuntu\n\n```\nsudo apt-get install python-lxml\n```\n\n## Usage\n\n```\npython inventory.py <inventory_file> <manifest_file>\n```\n\n## Examples\n\n### Validate an inventory file\n\n```\npython inventory.py inventory.xml manifest.csv\n```\n\n### Validate an inventory file with a custom error message\n\n```\npython inventory.py inventory.xml manifest.csv \"Custom Message\"\n```\n\n## Error Messages\n\n### Errors\n\n| Error | Description |\n| --- | --- |\n| [1000] | Cannot read manifest file |\n| [1001] | Manifest file is not a CSV |\n| [1002] | Manifest file is empty |\n| [1003]", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tpass\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tif fixity is None:\n\t\t\treturn error('fixity field is missing from inventory')\n\t\tif not isinstance(fixity, list):\n\t\t\treturn error('fixity field is not a list')\n\t\tfor f in fixity:\n\t\t\tif not isinstance(f, dict):\n\t\t\t\treturn error('fixity block is not a list of dicts')\n\t\t\tif 'type' not in f:\n\t\t\t\treturn error('fixity block is missing type')\n\t\t\telif f['type'] not in self.fixity_types:\n\t\t\t\treturn error('fixity block has unknown type')\n\t\t\tif 'path' not in f:\n\t\t\t\treturn error('fixity block is missing path')\n\t\t\telif f['path'] not in manifest_files:\n\t\t\t\treturn error('fixity block references file not in manifest')\n\"\"\"\n\tdef validate_checksum_type(self, checksum):\n\t\t\"\"\"\n\t\tValidate checksum type. Make sure that we are not using an unsupported checksum type. Return error() in the class.\n\t\t\"\"\"\n\t\tif 'type' not in checksum:\n\t\t\treturn error('checksum field is missing type')\n\t\telif checksum['type'] not in self.checksum_types:\n\t\t\treturn error('checksum field has unknown type')\n\"\"\"\n\tdef validate_checksum(self, checksum, manifest_files):\n\t\t\"\"\"\n\t\tValidate checksum block in inventory. Check the", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tif not \"files\" in fixity:\n\t\t\tself.error(\"fixity block does not contain 'files' field\")\n\t\t\treturn\n\t\tif not \"hash\" in fixity:\n\t\t\tself.error(\"fixity block does not contain 'hash' field\")\n\t\t\treturn\n\t\tif not \"size\" in fixity:\n\t\t\tself.error(\"fixity block does not contain'size' field\")\n\t\t\treturn\n\t\t# iterate over the files in the fixity block and make sure they are in the manifest\n\t\tfor file in fixity[\"files\"]:\n\t\t\tif not file in manifest_files:\n\t\t\t\tself.error(\"file \" + file + \" is not in the manifest\")\n\t\t\t\treturn\n\t\t# make sure that the fixity hash is a valid hash\n\t\tif not fixity[\"hash\"] in self.hashes:\n\t\t\tself.error(\"invalid hash \" + fixity[\"hash\"])\n\t\t\treturn\n\t\t# check that the fixity block has a valid size\n\t\ttry:\n\t\t\tif not self.is_valid_size(fixity[\"size\"]):\n\t\t\t\tself.error(\"invalid size \" + fixity[\"size\"])\n\t\t\t\treturn\n\t\texcept:\n\t\t\tself.error(\"invalid size \" + fixity[\"size\"])\n\t\t\treturn\n\n\tdef validate_manifest(self, manifest):\n\t\t\"\"\"\n\t\tValidate manifest block in inventory. Check the structure of", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\t# TODO\n\n#TODO: Check for duplicate files in manifest\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tif fixity[file] == None:\n\t\t\t\tself.error(\"Missing fixity for file: \" + file)\n\t\t\t\treturn\n\n\t\treturn True\n\n\tdef validate_inventory(self, inventory, manifest_files):\n\t\"\"\"\n\tValidate inventory block in inventory. Check the structure of the inventory block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tif inventory[file] == None:\n\t\t\t\tself.error(\"Missing inventory for file: \" + file)\n\t\t\t\treturn\n\n\t\treturn True\n\n\tdef validate_manifest(self, manifest):\n\t\"\"\"\n\tValidate manifest block in inventory. Check the structure of the manifest block and makes sure that only files listed in the inventory are referenced. Return error() in the class.\n\t\"\"\"\n\t\tfor file in manifest:\n\t\t\tif manifest[file] == None:\n\t\t\t\tself.error(\"Missing manifest for file: \" + file)\n\t\t\t\treturn\n\n\t\treturn True\n\n\tdef validate(self):\n\t\"\"\"\n\tValidate all blocks in inventory. Return error() in the class.\n\t\"\"\"\n\t\t# check for required blocks\n\t\tif \"inventory\" not in self.inventory:\n\t\t\tself.error(\"Missing inventory block\")\n\t\t\treturn\n\t\tif \"fixity\" not in self", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tself.validate_fixity_structure(fixity)\n\t\tself.validate_fixity_files(fixity, manifest_files)\n\n\tdef validate_fixity_structure(self, fixity):\n\t\t\"\"\"\n\t\tCheck fixity block structure:\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t\"\"\"\n\t\tif not isinstance(fixity, list):\n\t\t\tprint \"Fixity block should be a list\"\n\t\t\tself.error()\n\t\tif len(fixity)!= 3:\n\t\t\tprint \"Fixity block should contain 3 hashes\"\n\t\t\tself.error()\n\n\tdef validate_fixity_files(self, fixity, manifest_files):\n\t\t\"\"\"\n\t\tCheck fixity block files:\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t\"\"\"\n\t\tif not isinstance(fixity[0], dict):\n\t\t\tprint \"Fixity block should be a list of hashes\"\n\t\t\tself.error()\n\t\tif not isinstance(fixity[1], dict):\n\t\t\tprint \"Fixity block should be a list of hashes\"\n\t\t\tself.error()\n\t\tif", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\t# Check structure\n\t\tif not isinstance(fixity, dict):\n\t\t\tself.error(\"invalid fixity block\")\n\t\t\treturn\n\n\t\t# Check the fixity method\n\t\tif not \"method\" in fixity:\n\t\t\tself.error(\"fixity method is missing\")\n\t\t\treturn\n\t\tif not fixity[\"method\"] in self.fixity_methods:\n\t\t\tself.error(\"invalid fixity method: %s\" % fixity[\"method\"])\n\t\t\treturn\n\n\t\t# Check the fixity files\n\t\tif not \"files\" in fixity:\n\t\t\tself.error(\"fixity files is missing\")\n\t\t\treturn\n\t\tfor file in fixity[\"files\"]:\n\t\t\tif not file in manifest_files:\n\t\t\t\tself.error(\"file %s is not in the manifest\" %(file))\n\t\t\t\treturn\n\n\tdef validate_inventory(self):\n\t\t\"\"\"\n\t\tValidate the inventory.\n\t\t\"\"\"\n\t\t# Make sure we have a valid inventory\n\n\t\tif not isinstance(self.inventory, dict):\n\t\t\tself.error(\"inventory is missing\")\n\t\t\treturn\n\n\t\t# Validate the manifest\n\t\tself.validate_manifest()\n\n\t\t# Validate the version\n\t\tself.validate_version()\n\n\t\t# Validate the fixity\n\t\tself.validate_fixity()\n/tests/test_inventory_parser.py\nimport unittest\nimport os\nimport sys\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n        # Validate fixity block\n        if 'fixity' not in self.inventory:\n            return self.error('Fixity block not found')\n\n        fixity = self.inventory['fixity']\n\n        if 'type' not in fixity:\n            return self.error('Fixity type not found')\n\n        elif fixity['type'] not in ['md5','sha1']:\n            return self.error('Fixity type {} is not supported'.format(fixity['type']))\n\n        elif 'algorithm' not in fixity:\n            return self.error('Fixity algorithm not found')\n\n        elif fixity['algorithm'] not in ['SHA-1', 'SHA-256', 'MD5', 'SHA-512']:\n            return self.error('Fixity algorithm {} is not supported'.format(fixity['algorithm']))\n\n        elif 'digest' not in fixity:\n            return self.error('Fixity digest not found')\n\n        elif fixity['type'] =='md5' and len(fixity['digest'])!= 32:\n            return self.error('MD5 digest should be 32 characters long')\n\n        elif fixity['type'] =='sha1' and len(fixity['digest'])!= 40:\n            return self.error('SHA1 digest should be 40 characters long')\n\n        elif fixity['type'] =='sha256' and len(fix"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\n\tfixity_list = fixity['fixity']\n\tfor fixity_block in fixity_list:\n\t\tfile_name = fixity_block['file']\n\t\tif file_name not in manifest_files:\n\t\t\tself.error_list.append(file_name)\n\n\treturn self.error_list\n/README.md\n# py-inventory\nAn inventory validation tool for the Dryad Data Repository\n\n## Installation\n\n### Dependencies\n\n1. Python 2.7\n2. lxml\n\n### Installation of lxml\n\n#### Ubuntu\n\n```\nsudo apt-get install python-lxml\n```\n\n## Usage\n\n```\npython inventory.py <inventory_file> <manifest_file>\n```\n\n## Examples\n\n### Validate an inventory file\n\n```\npython inventory.py inventory.xml manifest.csv\n```\n\n### Validate an inventory file with a custom error message\n\n```\npython inventory.py inventory.xml manifest.csv \"Custom Message\"\n```\n\n## Error Messages\n\n### Errors\n\n| Error | Description |\n| --- | --- |\n| [1000] | Cannot read manifest file |\n| [1001] | Manifest file is not a CSV |\n| [1002] | Manifest file is empty |\n| [1003]", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tif fixity is None:\n\t\t\treturn error('fixity field is missing from inventory')\n\t\tif not isinstance(fixity, list):\n\t\t\treturn error('fixity field is not a list')\n\t\tfor f in fixity:\n\t\t\tif not isinstance(f, dict):\n\t\t\t\treturn error('fixity block is not a list of dicts')\n\t\t\tif 'type' not in f:\n\t\t\t\treturn error('fixity block is missing type')\n\t\t\telif f['type'] not in self.fixity_types:\n\t\t\t\treturn error('fixity block has unknown type')\n\t\t\tif 'path' not in f:\n\t\t\t\treturn error('fixity block is missing path')\n\t\t\telif f['path'] not in manifest_files:\n\t\t\t\treturn error('fixity block references file not in manifest')\n\"\"\"\n\tdef validate_checksum_type(self, checksum):\n\t\t\"\"\"\n\t\tValidate checksum type. Make sure that we are not using an unsupported checksum type. Return error() in the class.\n\t\t\"\"\"\n\t\tif 'type' not in checksum:\n\t\t\treturn error('checksum field is missing type')\n\t\telif checksum['type'] not in self.checksum_types:\n\t\t\treturn error('checksum field has unknown type')\n\"\"\"\n\tdef validate_checksum(self, checksum, manifest_files):\n\t\t\"\"\"\n\t\tValidate checksum block in inventory. Check the", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tif not \"files\" in fixity:\n\t\t\tself.error(\"fixity block does not contain 'files' field\")\n\t\t\treturn\n\t\tif not \"hash\" in fixity:\n\t\t\tself.error(\"fixity block does not contain 'hash' field\")\n\t\t\treturn\n\t\tif not \"size\" in fixity:\n\t\t\tself.error(\"fixity block does not contain'size' field\")\n\t\t\treturn\n\t\t# iterate over the files in the fixity block and make sure they are in the manifest\n\t\tfor file in fixity[\"files\"]:\n\t\t\tif not file in manifest_files:\n\t\t\t\tself.error(\"file \" + file + \" is not in the manifest\")\n\t\t\t\treturn\n\t\t# make sure that the fixity hash is a valid hash\n\t\tif not fixity[\"hash\"] in self.hashes:\n\t\t\tself.error(\"invalid hash \" + fixity[\"hash\"])\n\t\t\treturn\n\t\t# check that the fixity block has a valid size\n\t\ttry:\n\t\t\tif not self.is_valid_size(fixity[\"size\"]):\n\t\t\t\tself.error(\"invalid size \" + fixity[\"size\"])\n\t\t\t\treturn\n\t\texcept:\n\t\t\tself.error(\"invalid size \" + fixity[\"size\"])\n\t\t\treturn\n\n\tdef validate_manifest(self, manifest):\n\t\t\"\"\"\n\t\tValidate manifest block in inventory. Check the structure of", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\t# TODO\n\n#TODO: Check for duplicate files in manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tif fixity[file] == None:\n\t\t\t\tself.error(\"Missing fixity for file: \" + file)\n\t\t\t\treturn\n\n\t\treturn True\n\n\tdef validate_inventory(self, inventory, manifest_files):\n\t\"\"\"\n\tValidate inventory block in inventory. Check the structure of the inventory block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tif inventory[file] == None:\n\t\t\t\tself.error(\"Missing inventory for file: \" + file)\n\t\t\t\treturn\n\n\t\treturn True\n\n\tdef validate_manifest(self, manifest):\n\t\"\"\"\n\tValidate manifest block in inventory. Check the structure of the manifest block and makes sure that only files listed in the inventory are referenced. Return error() in the class.\n\t\"\"\"\n\t\tfor file in manifest:\n\t\t\tif manifest[file] == None:\n\t\t\t\tself.error(\"Missing manifest for file: \" + file)\n\t\t\t\treturn\n\n\t\treturn True\n\n\tdef validate(self):\n\t\"\"\"\n\tValidate all blocks in inventory. Return error() in the class.\n\t\"\"\"\n\t\t# check for required blocks\n\t\tif \"inventory\" not in self.inventory:\n\t\t\tself.error(\"Missing inventory block\")\n\t\t\treturn\n\t\tif \"fixity\" not in self", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\tself.validate_fixity_structure(fixity)\n\t\tself.validate_fixity_files(fixity, manifest_files)\n\n\tdef validate_fixity_structure(self, fixity):\n\t\t\"\"\"\n\t\tCheck fixity block structure:\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t\"\"\"\n\t\tif not isinstance(fixity, list):\n\t\t\tprint \"Fixity block should be a list\"\n\t\t\tself.error()\n\t\tif len(fixity)!= 3:\n\t\t\tprint \"Fixity block should contain 3 hashes\"\n\t\t\tself.error()\n\n\tdef validate_fixity_files(self, fixity, manifest_files):\n\t\t\"\"\"\n\t\tCheck fixity block files:\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t- fixity block should only contain a list of hashes\n\t\t\"\"\"\n\t\tif not isinstance(fixity[0], dict):\n\t\t\tprint \"Fixity block should be a list of hashes\"\n\t\t\tself.error()\n\t\tif not isinstance(fixity[1], dict):\n\t\t\tprint \"Fixity block should be a list of hashes\"\n\t\t\tself.error()\n\t\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\t# Check structure\n\t\tif not isinstance(fixity, dict):\n\t\t\tself.error(\"invalid fixity block\")\n\t\t\treturn\n\n\t\t# Check the fixity method\n\t\tif not \"method\" in fixity:\n\t\t\tself.error(\"fixity method is missing\")\n\t\t\treturn\n\t\tif not fixity[\"method\"] in self.fixity_methods:\n\t\t\tself.error(\"invalid fixity method: %s\" % fixity[\"method\"])\n\t\t\treturn\n\n\t\t# Check the fixity files\n\t\tif not \"files\" in fixity:\n\t\t\tself.error(\"fixity files is missing\")\n\t\t\treturn\n\t\tfor file in fixity[\"files\"]:\n\t\t\tif not file in manifest_files:\n\t\t\t\tself.error(\"file %s is not in the manifest\" %(file))\n\t\t\t\treturn\n\n\tdef validate_inventory(self):\n\t\t\"\"\"\n\t\tValidate the inventory.\n\t\t\"\"\"\n\t\t# Make sure we have a valid inventory\n\n\t\tif not isinstance(self.inventory, dict):\n\t\t\tself.error(\"inventory is missing\")\n\t\t\treturn\n\n\t\t# Validate the manifest\n\t\tself.validate_manifest()\n\n\t\t# Validate the version\n\t\tself.validate_version()\n\n\t\t# Validate the fixity\n\t\tself.validate_fixity()\n/tests/test_inventory_parser.py\nimport unittest\nimport os\nimport sys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n        # Validate fixity block\n        if 'fixity' not in self.inventory:\n            return self.error('Fixity block not found')\n\n        fixity = self.inventory['fixity']\n\n        if 'type' not in fixity:\n            return self.error('Fixity type not found')\n\n        elif fixity['type'] not in ['md5','sha1']:\n            return self.error('Fixity type {} is not supported'.format(fixity['type']))\n\n        elif 'algorithm' not in fixity:\n            return self.error('Fixity algorithm not found')\n\n        elif fixity['algorithm'] not in ['SHA-1', 'SHA-256', 'MD5', 'SHA-512']:\n            return self.error('Fixity algorithm {} is not supported'.format(fixity['algorithm']))\n\n        elif 'digest' not in fixity:\n            return self.error('Fixity digest not found')\n\n        elif fixity['type'] =='md5' and len(fixity['digest'])!= 32:\n            return self.error('MD5 digest should be 32 characters long')\n\n        elif fixity['type'] =='sha1' and len(fixity['digest'])!= 40:\n            return self.error('SHA1 digest should be 40 characters long')\n\n        elif fixity['type'] =='sha256' and len(fix", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "_id": "62b463153879012d19481498", "code": ["def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\treturn [f for f in listdir(path) if isfile(join(path, f))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn [f for f in listdir(path) if isfile(join(path, f))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\t\t\treturn []\n\t\t\t\treturn os.listdir(path)\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn glob.glob(path + \"/*\")\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\tfiles = os.listdir(path)\n\t\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\tfile_list = []\n\t\tfile_list.append(os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n\t\tfile_list = sorted(file_list)\n\t\treturn file_list\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\tfile_list = []\n\tfor file_name in os.listdir(path):\n\t\tfile_list.append(file_name)\n\t \n\treturn file_list\n\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\treturn [f for f in listdir(path) if isfile(join(path, f))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn os.listdir(path)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\treturn [f for f in listdir(path) if isfile(join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn [f for f in listdir(path) if isfile(join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\t\t\treturn []\n\t\t\t\treturn os.listdir(path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn glob.glob(path + \"/*\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\tfiles = os.listdir(path)\n\t\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\tfile_list = []\n\t\tfile_list.append(os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n\t\tfile_list = sorted(file_list)\n\t\treturn file_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\tfile_list = []\n\tfor file_name in os.listdir(path):\n\t\tfile_list.append(file_name)\n\t \n\treturn file_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\treturn [f for f in listdir(path) if isfile(join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t \n\treturn os.listdir(path)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\tgroups = {}\n\t\tfor xml in xmls:\n\t\t\tgroups[xml] = []\n\t\tfor file in files:\n\t\t\tfor xml in xmls:\n\t\t\t\tif get_file_extension(file) == get_file_extension(xml):\n\t\t\t\t\tgroups[xml].append(file)\n\t\t\t\t\tbreak\n\t\treturn groups\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\n\n\tdata = {}\n\tfor xml in xmls:\n\t\tpath = os.path.join(source, xml)\n\t\tdata[xml] = _read_xml(path)\n\n\tfor xml in files:\n\t\tpath = os.path.join(source, xml)\n\t\tdata[xml] = _read_xml(path)\n\n\treturn data\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t \n\txmls = [xml.split(os.sep)[-1] for xml in xmls]\n\tfiles = [file.split(os.sep)[-1] for file in files]\n\tgroup_files = {}\n\n\tfor xml_file in xmls:\n\t\txml_file_name = xml_file.split('.')[0]\n\t\tgroup_files[xml_file_name] = []\n\t\tfor file in files:\n\t\t\tif file.startswith(xml_file_name):\n\t\t\t\tgroup_files[xml_file_name].append(file)\n\n\tfor xml_file_name, files in group_files.iteritems():\n\t\tgroup_files[xml_file_name] = [os.path.join(source, file) for file in files]\n\n\treturn group_files\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t \n\tfiles_in_xml = {}\n\tfor xml in xmls:\n\t\tfiles_in_xml[xml] = []\n\t\n\tfor file in files:\n\t\txml_name = file.filename.replace(source, '')\n\t\tfiles_in_xml[xml_name].append(file)\n\t\n\treturn files_in_xml\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\n\n\tdata = {}\n\txml_to_files = defaultdict(list)\n\txml_to_files_count = defaultdict(int)\n\t\n\n\tfor xml in xmls:\n\t\txml_name = os.path.basename(xml)\n\t\txml_to_files[xml_name].append(source)\n\t\txml_to_files_count[xml_name] += 1\n\n\tfor file in files:\n\t\txml_name = os.path.basename(file)\n\t\txml_to_files[xml_name].append(file)\n\t\txml_to_files_count[xml_name] += 1\n\n\tfor xml_name in xml_to_files:\n\t\txml_to_files[xml_name] = list(set(xml_to_files[xml_name]))\n\t\tif len(xml_to_files[xml_name]) == 1:\n\t\t\tdata[xml_name] = xml_to_files[xml_name][0]\n\t\telse:\n\t\t\tdata[xml_name] = xml_to_files[xml_name]\n\n\treturn data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\txml_filenames = set(map(lambda x: x.filename, xmls))\n\t\tgrouped = {}\n\t\tfor xml_filename in xml_filenames:\n\t\t\tgrouped[xml_filename] = []\n\t\tfor f in files:\n\t\t\ttry:\n\t\t\t\txml_filename = f.xml.filename\n\t\t\t\tgrouped[xml_filename].append(f)\n\t\t\texcept Exception as e:\n\t\t\t\tpass\n\t\treturn grouped\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t/python/python2/t_xml_parser.py\n#!/usr/bin/python\n\nimport os, sys\nimport xml.dom.minidom\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\tprint(\"Grouping files by xml...\")\n\t\tdata = {}\n\t\tfor xml in xmls:\n\t\t\tdata[xml] = []\n\t\tfor xml in xmls:\n\t\t\tfor file in files:\n\t\t\t\tif xml.lower() in file.lower():\n\t\t\t\t\tdata[xml].append(file)\n\t\treturn data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t \n\tfiles_dict = {}\n\tfor xml in xmls:\n\t\tfiles_dict[xml] = []\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif xml in file:\n\t\t\t\tfiles_dict[xml].append(file)\n\n\treturn files_dict\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\tfor xml in xmls:\n\t\t\tfiles[xml] = []\n\tfor file in files:\n\t\tfiles[file.get('xml')].append(file)\n\treturn files\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\tgroups = {}\n\t\tfor xml in xmls:\n\t\t\tgroups[xml] = []\n\t\tfor file in files:\n\t\t\tfor xml in xmls:\n\t\t\t\tif get_file_extension(file) == get_file_extension(xml):\n\t\t\t\t\tgroups[xml].append(file)\n\t\t\t\t\tbreak\n\t\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\n\n\tdata = {}\n\tfor xml in xmls:\n\t\tpath = os.path.join(source, xml)\n\t\tdata[xml] = _read_xml(path)\n\n\tfor xml in files:\n\t\tpath = os.path.join(source, xml)\n\t\tdata[xml] = _read_xml(path)\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t \n\txmls = [xml.split(os.sep)[-1] for xml in xmls]\n\tfiles = [file.split(os.sep)[-1] for file in files]\n\tgroup_files = {}\n\n\tfor xml_file in xmls:\n\t\txml_file_name = xml_file.split('.')[0]\n\t\tgroup_files[xml_file_name] = []\n\t\tfor file in files:\n\t\t\tif file.startswith(xml_file_name):\n\t\t\t\tgroup_files[xml_file_name].append(file)\n\n\tfor xml_file_name, files in group_files.iteritems():\n\t\tgroup_files[xml_file_name] = [os.path.join(source, file) for file in files]\n\n\treturn group_files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t \n\tfiles_in_xml = {}\n\tfor xml in xmls:\n\t\tfiles_in_xml[xml] = []\n\t\n\tfor file in files:\n\t\txml_name = file.filename.replace(source, '')\n\t\tfiles_in_xml[xml_name].append(file)\n\t\n\treturn files_in_xml\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\n\n\tdata = {}\n\txml_to_files = defaultdict(list)\n\txml_to_files_count = defaultdict(int)\n\t\n\n\tfor xml in xmls:\n\t\txml_name = os.path.basename(xml)\n\t\txml_to_files[xml_name].append(source)\n\t\txml_to_files_count[xml_name] += 1\n\n\tfor file in files:\n\t\txml_name = os.path.basename(file)\n\t\txml_to_files[xml_name].append(file)\n\t\txml_to_files_count[xml_name] += 1\n\n\tfor xml_name in xml_to_files:\n\t\txml_to_files[xml_name] = list(set(xml_to_files[xml_name]))\n\t\tif len(xml_to_files[xml_name]) == 1:\n\t\t\tdata[xml_name] = xml_to_files[xml_name][0]\n\t\telse:\n\t\t\tdata[xml_name] = xml_to_files[xml_name]\n\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\txml_filenames = set(map(lambda x: x.filename, xmls))\n\t\tgrouped = {}\n\t\tfor xml_filename in xml_filenames:\n\t\t\tgrouped[xml_filename] = []\n\t\tfor f in files:\n\t\t\ttry:\n\t\t\t\txml_filename = f.xml.filename\n\t\t\t\tgrouped[xml_filename].append(f)\n\t\t\texcept Exception as e:\n\t\t\t\tpass\n\t\treturn grouped\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t/python/python2/t_xml_parser.py\n#!/usr/bin/python\n\nimport os, sys\nimport xml.dom.minidom\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\tprint(\"Grouping files by xml...\")\n\t\tdata = {}\n\t\tfor xml in xmls:\n\t\t\tdata[xml] = []\n\t\tfor xml in xmls:\n\t\t\tfor file in files:\n\t\t\t\tif xml.lower() in file.lower():\n\t\t\t\t\tdata[xml].append(file)\n\t\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t \n\tfiles_dict = {}\n\tfor xml in xmls:\n\t\tfiles_dict[xml] = []\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif xml in file:\n\t\t\t\tfiles_dict[xml].append(file)\n\n\treturn files_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\tfor xml in xmls:\n\t\t\tfiles[xml] = []\n\tfor file in files:\n\t\tfiles[file.get('xml')].append(file)\n\treturn files\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\treturn basename(file_path).startswith(prefix + \"-\") or basename(file_path).startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\tif os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n#TODO: implement this\n\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n#\tif os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\"):\n#\t\treturn True\n#\telse:\n#\t\treturn False\n\t\n\t\n\t# Get the basename of the file\n\tfile_basename = os.path.basename(file_path)\n\t\n\t# If the basename matches the prefix plus \".\" or the prefix plus \"-\", return True\n\tif file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\treturn os.path.basename(file_path).startswith(prefix)\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\tif prefix + \"-\" in file_path.split(\"/\")[-1] or prefix + \".\" in file_path.split(\"/\")[-1]:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix+\"-\") or file_name.startswith(prefix+\".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix + \"-\") or file_name.startswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\treturn file_path.startswith(prefix) or file_path.startswith(prefix + \".\") or file_path.startswith(prefix + \"-\")\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\treturn basename(file_path).startswith(prefix + \"-\") or basename(file_path).startswith(prefix + \".\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\tif os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n#TODO: implement this\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n#\tif os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\"):\n#\t\treturn True\n#\telse:\n#\t\treturn False\n\t\n\t\n\t# Get the basename of the file\n\tfile_basename = os.path.basename(file_path)\n\t\n\t# If the basename matches the prefix plus \".\" or the prefix plus \"-\", return True\n\tif file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\treturn os.path.basename(file_path).startswith(prefix)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\tif prefix + \"-\" in file_path.split(\"/\")[-1] or prefix + \".\" in file_path.split(\"/\")[-1]:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix+\"-\") or file_name.startswith(prefix+\".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix + \"-\") or file_name.startswith(prefix + \".\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\t\n\n\treturn file_path.startswith(prefix) or file_path.startswith(prefix + \".\") or file_path.startswith(prefix + \"-\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\n#\tprint(files)\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tyield file\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\treturn [f for f in files if f.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\n\n\tmatching_files = []\n\tfor index, file in enumerate(files):\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files\n\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\t# Return all the files that match the given prefix\n\treturn [f for f in files if f.startswith(prefix)]\n\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\n\n\treturn [ file for file in files if file.startswith(prefix)]\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t \n\treturn [f for f in files if f.startswith(prefix)]\n\t", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\n#\tprint(files)\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tyield file\n", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\treturn [f for f in files if f.startswith(prefix)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\n\n\tmatching_files = []\n\tfor index, file in enumerate(files):\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\t# Return all the files that match the given prefix\n\treturn [f for f in files if f.startswith(prefix)]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t\n\n\treturn [ file for file in files if file.startswith(prefix)]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t \n\treturn [f for f in files if f.startswith(prefix)]\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t# Load all files in the folder.\n\tfiles = listdir(folder)\n\tprint \"Total files: {}\".format(len(files))\n\t\n\t# Find all xml files in the folder.\n\txml_files = _find_xml_files(files)\n\t\n\t# Group all xml files by their filenames.\n\txml_groups = _group_files_by_xml_filename(xml_files)\n\t\n\t# Iterate over the xml groups.\n\tfor xml_group in xml_groups:\n\t\t\n\t\t# Group files by their filenames.\n\t\tother_files = [f for f in xml_files if f!= xml_group]\n\t\tother_groups = _group_files_by_xml_filename(other_files)\n\t\t\n\t\tprint \"Group size: {}\".format(len(xml_group))\n\t\t\n\t\t# Iterate over the other groups.\n\t\tfor other_group in other_groups:\n\t\t\tprint \"Other group size: {}\".format(len(other_group))\n\t\t\tprint \"Other group names: {}\".format(other_group)\n\t\t\tbreak\n\t\t\t\n\t\tbreak\n\t\n\t# Return the xml groups.\n\treturn xml_groups\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tgroup_files = _group_files_by_xml_filename(folder)\n\tfor xml_filename, group_files in group_files:\n\t\tprint xml_filename\n\t\tprint group_files\n\t\tprint\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\t# Find all XML files in the given folder\n\tfiles = _get_xml_files_in_folder(folder)\n\t\n\tfiles_grouped = _group_files_by_xml_filename(files)\n\t\n\treturn files_grouped\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tprint \"Exploring folder '%s'...\" % folder\n\tfiles_path = os.path.join(folder, '*.xml')\n\txml_files = glob.glob(files_path)\n\txml_files_by_xml_filename = {}\n\tfor xml_file in xml_files:\n\t\txml_filename = os.path.basename(xml_file)\n\t\txml_files_by_xml_filename[xml_filename] = xml_file\n\tprint \"Found %d xml files\" % len(xml_files)\n\n\txml_files_by_group = _group_files_by_xml_filename(xml_files_by_xml_filename)\n\treturn xml_files_by_group\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\tif len(os.listdir(folder)) == 0:\n\t\treturn {}\n\n\tfiles = os.listdir(folder)\n\n\t# Create a list of all files in the folder\n\txml_files = [f for f in files if f.endswith(\".xml\")]\n\t# Create a list of all files in the folder\n\tcsv_files = [f for f in files if f.endswith(\".csv\")]\n\n\t# Group files by XML file name\n\txml_groups = _group_files_by_xml_filename(xml_files)\n\n\t# Create a dict of all the groups\n\tgroups = {}\n\n\t# For each group:\n\tfor xml_group in xml_groups:\n\n\t\t# Create the group\n\t\tgroup = Group(xml_group)\n\n\t\t# Add it to the dict\n\t\tgroups[xml_group] = group\n\n\t\t# Add each file to the group\n\t\tfor file in xml_groups[xml_group]:\n\t\t\tgroup.add(file)\n\n\t\t# If there is a csv file with the same name, add it to the group\n\t\tif xml_group+\".csv\" in csv_files:\n\t\t\tgroup.add(xml_group+\".csv\")\n\n\t\t# If there is a csv file with the same name but a different extension, add it to the group\n\t\tif xml_group+\".txt\" in csv_files:", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t#group_by_xml_filename = _group_files_by_xml_filename(folder)\n\t#print(group_by_xml_filename)\n\t#return\n\n\t#files_in_folder = sorted(os.listdir(folder))\n\n\t#for xml_filename in files_in_folder:\n\t#\tif xml_filename.endswith(\".xml\"):\n\t#\t\tprint(xml_filename)\n\t#\t\tcontinue\n\t#\telse:\n\t#\t\tprint(xml_filename)\n\n\t#group_by_xml_filename = _group_files_by_xml_filename(folder)\n\t#for xml_filename in group_by_xml_filename:\n\t#\tprint(xml_filename)\n\t#\tprint(group_by_xml_filename[xml_filename])\n\t#\tprint(\"\\n\")\n\n\n\t#print(group_by_xml_filename)\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\t# First, find all the.xml files in the given folder.\n\txml_files = glob.glob(os.path.join(folder, '*.xml'))\n\txml_files.sort()\n\t\n\t# Now group the files by xml filename.\n\t# The result is a list of lists.\n\t# The inner lists are the files with the same name.\n\tgroups = _group_files_by_xml_filename(xml_files)\n\t\n\treturn groups\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t# TODO: If this is a folder of XML files, group them by folder.\n\t# TODO: If this is a folder of files, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\treturn None\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t# Make sure there is a _filter_xml_files(folder) file.\n\tfilter = _filter_xml_files(folder)\n\t\n\t# Make sure the filter is correct.\n\tif filter!= None:\n\t\tprint \"Using filter: \" + filter\n\telse:\n\t\tprint \"No filter found.\"\n\n\t# Group files by using the filter.\n\tgroups = _group_files_by_xml_filename(folder, filter)\n\n\t# Show the groups.\n\tfor group in groups:\n\t\tprint \"Group: \" + group\n\t\tfor file in groups[group]:\n\t\t\tprint \"  \" + file\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t # TODO: Check if folder is empty.\n\t# TODO: Check if folder is valid.\n\tfiles = os.listdir(folder)\n\tgroups = _group_files_by_xml_filename(files)\n\treturn groups\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t# Load all files in the folder.\n\tfiles = listdir(folder)\n\tprint \"Total files: {}\".format(len(files))\n\t\n\t# Find all xml files in the folder.\n\txml_files = _find_xml_files(files)\n\t\n\t# Group all xml files by their filenames.\n\txml_groups = _group_files_by_xml_filename(xml_files)\n\t\n\t# Iterate over the xml groups.\n\tfor xml_group in xml_groups:\n\t\t\n\t\t# Group files by their filenames.\n\t\tother_files = [f for f in xml_files if f!= xml_group]\n\t\tother_groups = _group_files_by_xml_filename(other_files)\n\t\t\n\t\tprint \"Group size: {}\".format(len(xml_group))\n\t\t\n\t\t# Iterate over the other groups.\n\t\tfor other_group in other_groups:\n\t\t\tprint \"Other group size: {}\".format(len(other_group))\n\t\t\tprint \"Other group names: {}\".format(other_group)\n\t\t\tbreak\n\t\t\t\n\t\tbreak\n\t\n\t# Return the xml groups.\n\treturn xml_groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tgroup_files = _group_files_by_xml_filename(folder)\n\tfor xml_filename, group_files in group_files:\n\t\tprint xml_filename\n\t\tprint group_files\n\t\tprint\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\t# Find all XML files in the given folder\n\tfiles = _get_xml_files_in_folder(folder)\n\t\n\tfiles_grouped = _group_files_by_xml_filename(files)\n\t\n\treturn files_grouped\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tprint \"Exploring folder '%s'...\" % folder\n\tfiles_path = os.path.join(folder, '*.xml')\n\txml_files = glob.glob(files_path)\n\txml_files_by_xml_filename = {}\n\tfor xml_file in xml_files:\n\t\txml_filename = os.path.basename(xml_file)\n\t\txml_files_by_xml_filename[xml_filename] = xml_file\n\tprint \"Found %d xml files\" % len(xml_files)\n\n\txml_files_by_group = _group_files_by_xml_filename(xml_files_by_xml_filename)\n\treturn xml_files_by_group\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\tif len(os.listdir(folder)) == 0:\n\t\treturn {}\n\n\tfiles = os.listdir(folder)\n\n\t# Create a list of all files in the folder\n\txml_files = [f for f in files if f.endswith(\".xml\")]\n\t# Create a list of all files in the folder\n\tcsv_files = [f for f in files if f.endswith(\".csv\")]\n\n\t# Group files by XML file name\n\txml_groups = _group_files_by_xml_filename(xml_files)\n\n\t# Create a dict of all the groups\n\tgroups = {}\n\n\t# For each group:\n\tfor xml_group in xml_groups:\n\n\t\t# Create the group\n\t\tgroup = Group(xml_group)\n\n\t\t# Add it to the dict\n\t\tgroups[xml_group] = group\n\n\t\t# Add each file to the group\n\t\tfor file in xml_groups[xml_group]:\n\t\t\tgroup.add(file)\n\n\t\t# If there is a csv file with the same name, add it to the group\n\t\tif xml_group+\".csv\" in csv_files:\n\t\t\tgroup.add(xml_group+\".csv\")\n\n\t\t# If there is a csv file with the same name but a different extension, add it to the group\n\t\tif xml_group+\".txt\" in csv_files:", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t#group_by_xml_filename = _group_files_by_xml_filename(folder)\n\t#print(group_by_xml_filename)\n\t#return\n\n\t#files_in_folder = sorted(os.listdir(folder))\n\n\t#for xml_filename in files_in_folder:\n\t#\tif xml_filename.endswith(\".xml\"):\n\t#\t\tprint(xml_filename)\n\t#\t\tcontinue\n\t#\telse:\n\t#\t\tprint(xml_filename)\n\n\t#group_by_xml_filename = _group_files_by_xml_filename(folder)\n\t#for xml_filename in group_by_xml_filename:\n\t#\tprint(xml_filename)\n\t#\tprint(group_by_xml_filename[xml_filename])\n\t#\tprint(\"\\n\")\n\n\n\t#print(group_by_xml_filename)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\t# First, find all the.xml files in the given folder.\n\txml_files = glob.glob(os.path.join(folder, '*.xml'))\n\txml_files.sort()\n\t\n\t# Now group the files by xml filename.\n\t# The result is a list of lists.\n\t# The inner lists are the files with the same name.\n\tgroups = _group_files_by_xml_filename(xml_files)\n\t\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t# TODO: If this is a folder of XML files, group them by folder.\n\t# TODO: If this is a folder of files, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\n\t# TODO: If this is a folder of folders, group them by name.\n\t# TODO: If this is a folder of folders, group them by name.\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\t# Make sure there is a _filter_xml_files(folder) file.\n\tfilter = _filter_xml_files(folder)\n\t\n\t# Make sure the filter is correct.\n\tif filter!= None:\n\t\tprint \"Using filter: \" + filter\n\telse:\n\t\tprint \"No filter found.\"\n\n\t# Group files by using the filter.\n\tgroups = _group_files_by_xml_filename(folder, filter)\n\n\t# Show the groups.\n\tfor group in groups:\n\t\tprint \"Group: \" + group\n\t\tfor file in groups[group]:\n\t\t\tprint \"  \" + file\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t # TODO: Check if folder is empty.\n\t# TODO: Check if folder is valid.\n\tfiles = os.listdir(folder)\n\tgroups = _group_files_by_xml_filename(files)\n\treturn groups\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t \n\tif not file_path.endswith(prefix):\n\t\treturn None\n\n\tif file_path.endswith(\"xml\"):\n\t\treturn None\n\telif file_path.endswith(\"pdf\"):\n\t\treturn dict(component_id=file_path.split(\"/\")[-2], file_path=file_path)\n\telse:\n\t\treturn dict(component_id=file_path.split(\"/\")[-2], file_path=file_path, ftype=file_path[-4:])\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t \n\t# file_path = file_path.replace(\"\\\\\", \"/\")\n\t# print(\"file_path\", file_path)\n\t# print(\"prefix\", prefix)\n\t# print(\"os.path.splitext(file_path)[1]\", os.path.splitext(file_path)[1])\n\t# print(\"os.path.splitext(file_path)[1].lower()\", os.path.splitext(file_path)[1].lower())\n\t# print(\"os.path.splitext(file_path)[1].lower() in ['.pdf']\", os.path.splitext(file_path)[1].lower() in ['.pdf'])\n\t# print(\"os.path.splitext(file_path)[1].lower() not in ['.xml']\", os.path.splitext(file_path)[1].lower() not in ['.xml'])\n\t# print(\"file_path.startswith(prefix)\", file_path.startswith(prefix))\n\t# print(\"os.path.splitext(file_path)[1].lower() not in ['.xml'] and file_path.startswith(prefix)\", os.path.splitext(file_path)[1].lower() not in ['.xml'] and file_path.startswith(prefix))\n\t# print(\"file_path.startswith(prefix) and os.path.splitext(file_path)[1].lower", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\twith open(file_path, 'r') as f:\n\t\tcontent = f.read()\n\n\tif len(content) == 0:\n\t\treturn None\n\n\tfile_name = os.path.basename(file_path)\n\n\tif file_name.startswith(prefix):\n\t\tfile_name = file_name[len(prefix):]\n\t\tif file_name.startswith('-'):\n\t\t\tfile_name = file_name[1:]\n\n\t\tfile_name = file_name.replace('.pdf', '')\n\t\tfile_name = file_name.replace('.xml', '')\n\t\tfile_name = file_name.replace('.doc', '')\n\t\tfile_name = file_name.replace('.docx', '')\n\t\tfile_name = file_name.replace('.xls', '')\n\t\tfile_name = file_name.replace('.xlsx', '')\n\t\tfile_name = file_name.replace('.ppt', '')\n\t\tfile_name = file_name.replace('.pptx', '')\n\t\tfile_name = file_name.replace('.htm', '')\n\t\tfile_name = file_name.replace('.html', '')\n\t\tfile_name = file_name.replace('.txt', '')\n\t\tfile_name = file_name.replace('.csv', '')\n\t\tfile_name = file_name.replace('.zip', '')\n\t\t", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif file_path.endswith(\".pdf\"):\n\t\treturn {\"component_id\": prefix, \"file_path\": file_path}\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\tprint(file_path)\n\treturn {\"component_id\": prefix, \"file_path\": file_path, \"ftype\": \"txt\"}\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif file_path.endswith(\".pdf\"):\n\t\treturn {prefix: file_path}\n\telif file_path.endswith(\".xml\"):\n\t\treturn None\n\telse:\n\t\ttry:\n\t\t\tfile_type = magic.from_file(file_path, mime=True)\n\t\texcept:\n\t\t\treturn None\n\t\tif \"pdf\" in file_type:\n\t\t\treturn {prefix: file_path}\n\t\telse:\n\t\t\treturn {prefix: file_path, \"ftype\": file_type}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\tif not os.path.exists(file_path):\n\t\t\treturn None\n\t\t# check the file type\n\t\tftype = file_type(file_path)\n\t\tif ftype!= \"pdf\":\n\t\t\treturn None\n\t\t# check the file prefix\n\t\tif not os.path.basename(file_path).startswith(prefix):\n\t\t\treturn None\n\t\t# get the component id\n\t\tcomponent_id = os.path.basename(file_path).split(\"_\")[1]\n\t\tif not component_id.startswith(\"C\"):\n\t\t\treturn None\n\t\t# return\n\t\treturn dict(component_id=component_id, file_path=file_path, ftype=ftype)\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif not prefix in file_path:\n\t\treturn None\n\n\tif file_path[-3:] == \"xml\":\n\t\treturn None\n\n\tpdf_match = re.match(r'(.*)' + prefix + r'(\\d+\\.pdf)', file_path)\n\tif pdf_match:\n\t\treturn {\n\t\t\t\"component_id\": pdf_match.group(2),\n\t\t\t\"file_path\": file_path\n\t\t}\n\n\treturn {\n\t\t\"component_id\": \"\",\n\t\t\"file_path\": file_path,\n\t\t\"ftype\": \"misc\"\n\t}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif os.path.isfile(file_path) and file_path.endswith(prefix):\n\t\tif prefix == \"pdf\":\n\t\t\tftypes = [\"pdf\", \"xml\"]\n\t\t\tftype = None\n\t\t\tfor ftype in ftypes:\n\t\t\t\tftype_file_path = file_path + \"_\" + ftype + \".pdf\"\n\t\t\t\tif os.path.isfile(ftype_file_path):\n\t\t\t\t\treturn {\"component_id\": os.path.splitext(os.path.basename(file_path))[0], \"file_path\": file_path, \"ftype\": ftype, \"file_type\": ftype_file_path}\n\t\t\treturn {\"component_id\": os.path.splitext(os.path.basename(file_path))[0], \"file_path\": file_path}\n\t\telif prefix == \"xml\":\n\t\t\treturn {\"component_id\": os.path.splitext(os.path.basename(file_path))[0], \"file_path\": file_path}\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n        if file_path.endswith(\".xml\"):\n            return None\n\n\tif not file_path.startswith(prefix):\n\t    return None\n\n\tftype = file_path.split(\".\")[-1]\n\tif ftype == \"pdf\":\n\t    return {\"component_id\": prefix, \"file_path\": file_path}\n\telse:\n\t    return {\"component_id\": prefix, \"file_path\": file_path, \"ftype\": ftype, \"file_path\": file_path}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t \n\t# Get the file type\n\tfile_type = os.path.splitext(file_path)[-1]\n\tfile_type = file_type.lower()\n\n\t# Return None if the file is not the expected type\n\tif not file_type == prefix:\n\t\treturn None\n\n\t# Return None if the file is xml\n\tif file_type == _XML_EXT:\n\t\treturn None\n\n\t# Return dict with the key of component_id, file_path if the type of the file is \"pdf\"\n\tif file_type == _PDF_EXT:\n\t\treturn {\n\t\t\t_COMPONENT_ID: os.path.basename(file_path),\n\t\t\t_FILE_PATH: file_path\n\t\t}\n\n\t# Return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\"\n\treturn {\n\t\t_COMPONENT_ID: os.path.basename(file_path),\n\t\t_FILE_PATH: file_path,\n\t\t_FTYPE: file_type\n\t}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t \n\tif not file_path.endswith(prefix):\n\t\treturn None\n\n\tif file_path.endswith(\"xml\"):\n\t\treturn None\n\telif file_path.endswith(\"pdf\"):\n\t\treturn dict(component_id=file_path.split(\"/\")[-2], file_path=file_path)\n\telse:\n\t\treturn dict(component_id=file_path.split(\"/\")[-2], file_path=file_path, ftype=file_path[-4:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t \n\t# file_path = file_path.replace(\"\\\\\", \"/\")\n\t# print(\"file_path\", file_path)\n\t# print(\"prefix\", prefix)\n\t# print(\"os.path.splitext(file_path)[1]\", os.path.splitext(file_path)[1])\n\t# print(\"os.path.splitext(file_path)[1].lower()\", os.path.splitext(file_path)[1].lower())\n\t# print(\"os.path.splitext(file_path)[1].lower() in ['.pdf']\", os.path.splitext(file_path)[1].lower() in ['.pdf'])\n\t# print(\"os.path.splitext(file_path)[1].lower() not in ['.xml']\", os.path.splitext(file_path)[1].lower() not in ['.xml'])\n\t# print(\"file_path.startswith(prefix)\", file_path.startswith(prefix))\n\t# print(\"os.path.splitext(file_path)[1].lower() not in ['.xml'] and file_path.startswith(prefix)\", os.path.splitext(file_path)[1].lower() not in ['.xml'] and file_path.startswith(prefix))\n\t# print(\"file_path.startswith(prefix) and os.path.splitext(file_path)[1].lower", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\twith open(file_path, 'r') as f:\n\t\tcontent = f.read()\n\n\tif len(content) == 0:\n\t\treturn None\n\n\tfile_name = os.path.basename(file_path)\n\n\tif file_name.startswith(prefix):\n\t\tfile_name = file_name[len(prefix):]\n\t\tif file_name.startswith('-'):\n\t\t\tfile_name = file_name[1:]\n\n\t\tfile_name = file_name.replace('.pdf', '')\n\t\tfile_name = file_name.replace('.xml', '')\n\t\tfile_name = file_name.replace('.doc', '')\n\t\tfile_name = file_name.replace('.docx', '')\n\t\tfile_name = file_name.replace('.xls', '')\n\t\tfile_name = file_name.replace('.xlsx', '')\n\t\tfile_name = file_name.replace('.ppt', '')\n\t\tfile_name = file_name.replace('.pptx', '')\n\t\tfile_name = file_name.replace('.htm', '')\n\t\tfile_name = file_name.replace('.html', '')\n\t\tfile_name = file_name.replace('.txt', '')\n\t\tfile_name = file_name.replace('.csv', '')\n\t\tfile_name = file_name.replace('.zip', '')\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif file_path.endswith(\".pdf\"):\n\t\treturn {\"component_id\": prefix, \"file_path\": file_path}\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\tprint(file_path)\n\treturn {\"component_id\": prefix, \"file_path\": file_path, \"ftype\": \"txt\"}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif file_path.endswith(\".pdf\"):\n\t\treturn {prefix: file_path}\n\telif file_path.endswith(\".xml\"):\n\t\treturn None\n\telse:\n\t\ttry:\n\t\t\tfile_type = magic.from_file(file_path, mime=True)\n\t\texcept:\n\t\t\treturn None\n\t\tif \"pdf\" in file_type:\n\t\t\treturn {prefix: file_path}\n\t\telse:\n\t\t\treturn {prefix: file_path, \"ftype\": file_type}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\tif not os.path.exists(file_path):\n\t\t\treturn None\n\t\t# check the file type\n\t\tftype = file_type(file_path)\n\t\tif ftype!= \"pdf\":\n\t\t\treturn None\n\t\t# check the file prefix\n\t\tif not os.path.basename(file_path).startswith(prefix):\n\t\t\treturn None\n\t\t# get the component id\n\t\tcomponent_id = os.path.basename(file_path).split(\"_\")[1]\n\t\tif not component_id.startswith(\"C\"):\n\t\t\treturn None\n\t\t# return\n\t\treturn dict(component_id=component_id, file_path=file_path, ftype=ftype)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif not prefix in file_path:\n\t\treturn None\n\n\tif file_path[-3:] == \"xml\":\n\t\treturn None\n\n\tpdf_match = re.match(r'(.*)' + prefix + r'(\\d+\\.pdf)', file_path)\n\tif pdf_match:\n\t\treturn {\n\t\t\t\"component_id\": pdf_match.group(2),\n\t\t\t\"file_path\": file_path\n\t\t}\n\n\treturn {\n\t\t\"component_id\": \"\",\n\t\t\"file_path\": file_path,\n\t\t\"ftype\": \"misc\"\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\n\tif os.path.isfile(file_path) and file_path.endswith(prefix):\n\t\tif prefix == \"pdf\":\n\t\t\tftypes = [\"pdf\", \"xml\"]\n\t\t\tftype = None\n\t\t\tfor ftype in ftypes:\n\t\t\t\tftype_file_path = file_path + \"_\" + ftype + \".pdf\"\n\t\t\t\tif os.path.isfile(ftype_file_path):\n\t\t\t\t\treturn {\"component_id\": os.path.splitext(os.path.basename(file_path))[0], \"file_path\": file_path, \"ftype\": ftype, \"file_type\": ftype_file_path}\n\t\t\treturn {\"component_id\": os.path.splitext(os.path.basename(file_path))[0], \"file_path\": file_path}\n\t\telif prefix == \"xml\":\n\t\t\treturn {\"component_id\": os.path.splitext(os.path.basename(file_path))[0], \"file_path\": file_path}\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n        if file_path.endswith(\".xml\"):\n            return None\n\n\tif not file_path.startswith(prefix):\n\t    return None\n\n\tftype = file_path.split(\".\")[-1]\n\tif ftype == \"pdf\":\n\t    return {\"component_id\": prefix, \"file_path\": file_path}\n\telse:\n\t    return {\"component_id\": prefix, \"file_path\": file_path, \"ftype\": ftype, \"file_path\": file_path}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t \n\t# Get the file type\n\tfile_type = os.path.splitext(file_path)[-1]\n\tfile_type = file_type.lower()\n\n\t# Return None if the file is not the expected type\n\tif not file_type == prefix:\n\t\treturn None\n\n\t# Return None if the file is xml\n\tif file_type == _XML_EXT:\n\t\treturn None\n\n\t# Return dict with the key of component_id, file_path if the type of the file is \"pdf\"\n\tif file_type == _PDF_EXT:\n\t\treturn {\n\t\t\t_COMPONENT_ID: os.path.basename(file_path),\n\t\t\t_FILE_PATH: file_path\n\t\t}\n\n\t# Return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\"\n\treturn {\n\t\t_COMPONENT_ID: os.path.basename(file_path),\n\t\t_FILE_PATH: file_path,\n\t\t_FTYPE: file_type\n\t}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n\n\tdef filepath(self, lang):\n\t\t\"\"\"\n\t\tReturns the path of the file in the language given.\n\t\t\"\"\"\n\t\treturn self._renditions[lang]\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\n        if lang not in self._renditions:\n            self._renditions[lang] = file_path\n\n        return self._renditions[lang]\n\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n\t\n\tdef filepath(self, lang):\n\t\"\"\"\n\tReturn the filepath for the given language.\n\t\"\"\"\n\treturn self._renditions[lang]\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\t#lang is a string, file_path is a string, both of which are defined in the class.\n\t\t#self is the class object, which is implied when calling the method.\n\t\tself._renditions[lang] = file_path\n\n\tdef read(self, lang):\n\t\t\"\"\"\n\t\tRead the file identified by \"lang\" in _renditions and return its contents.\n\t\t\"\"\"\n\t\t#lang is a string, defined in the class.\n\t\t#self is the class object, which is implied when calling the method.\n\t\twith open(self._renditions[lang], \"r\") as f:\n\t\t\treturn f.read()\n\n\tdef read_rendition(self, lang):\n\t\t\"\"\"\n\t\tRead the file identified by \"lang\" in _renditions and return its contents, printing a warning if the file is not found.\n\t\t\"\"\"\n\t\t#lang is a string, defined in the class.\n\t\t#self is the class object, which is implied when calling the method.\n\t\ttry:\n\t\t\twith open(self._renditions[lang], \"r\") as f:\n\t\t\t\treturn f.read()\n\t\texcept KeyError:\n\t\t\tprint(f\"WARNING: No rendition of {self.name} in {lang}\")\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn the class name and the name of the book in a string", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n\n\tdef filepath(self, lang):\n\t\t\"\"\"\n\t\tReturns the path of the file in the language given.\n\t\t\"\"\"\n\t\treturn self._renditions[lang]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\n        if lang not in self._renditions:\n            self._renditions[lang] = file_path\n\n        return self._renditions[lang]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n\t\n\tdef filepath(self, lang):\n\t\"\"\"\n\tReturn the filepath for the given language.\n\t\"\"\"\n\treturn self._renditions[lang]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\t#lang is a string, file_path is a string, both of which are defined in the class.\n\t\t#self is the class object, which is implied when calling the method.\n\t\tself._renditions[lang] = file_path\n\n\tdef read(self, lang):\n\t\t\"\"\"\n\t\tRead the file identified by \"lang\" in _renditions and return its contents.\n\t\t\"\"\"\n\t\t#lang is a string, defined in the class.\n\t\t#self is the class object, which is implied when calling the method.\n\t\twith open(self._renditions[lang], \"r\") as f:\n\t\t\treturn f.read()\n\n\tdef read_rendition(self, lang):\n\t\t\"\"\"\n\t\tRead the file identified by \"lang\" in _renditions and return its contents, printing a warning if the file is not found.\n\t\t\"\"\"\n\t\t#lang is a string, defined in the class.\n\t\t#self is the class object, which is implied when calling the method.\n\t\ttry:\n\t\t\twith open(self._renditions[lang], \"r\") as f:\n\t\t\t\treturn f.read()\n\t\texcept KeyError:\n\t\t\tprint(f\"WARNING: No rendition of {self.name} in {lang}\")\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn the class name and the name of the book in a string", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\t\tself._renditions[lang] = file_path\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\n\tdef get_asset(self, basename):\n\t\"\"\"\n\tReturns the filepath for a given \"basename\"\n\t\"\"\"\n\t\treturn self._assets[basename]\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tsetattr(self, basename, file_path)\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\t\treturn self\n\n\tdef filepath(self, basename):\n\t\"\"\"\n\tReturn the filepath of the specified asset.\n\t\"\"\"\n\t\treturn self._assets[basename]\n\n\tdef __getattr__(self, attrname):\n\t\"\"\"\n\tReturn the filepath of the specified attribute name.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._assets[attrname]\n\t\texcept KeyError:\n\t\t\traise AttributeError(attrname)\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturn a string of the class.\n\t\"\"\"\n\t\treturn self.__class__.__name__ + \"(%s)\" % self._assets\n\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\t\n\tdef remove_asset(self, basename):\n\t\"\"\"\n\tRemove the filepath from _assets in the class.\n\t\"\"\"\n\t\tdel self._assets[basename]\n\t\n\tdef filepath(self, basename):\n\t\"\"\"\n\tReturn the filepath of a file.\n\t\"\"\"\n\t\treturn self._assets[basename]\n\n\tdef read(self, basename):\n\t\"\"\"\n\tReturn the content of a file.\n\t\"\"\"\n\t\treturn self._assets[basename].read()\n\n\tdef write(self, basename, content):\n\t\"\"\"\n\tWrite content to a file.\n\t\"\"\"\n\t\tself._assets[basename].write(content)\n\"\"\"\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\tdef filepath(self, basename):\n\t\"\"\"\n\tGet the filepath of a asset in _assets.\n\t\"\"\"\n\t\treturn self._assets[basename]\n/README.md\n# Pygame Game Engine\n\n### Version 0.1.0\n\nThis is a game engine designed to be used as a base for pygame games.\n\n**This is under development and still needs a lot of work.**\n\n## Features\n\n- Asset managment\n- A basic entity system\n- A basic animation system\n\n## Dependencies\n\n- Python 3.4.4\n- Pygame 1.9.2\n\n## Current state\n\nThe current state of the engine is:\n\n- Working on the asset manager\n- Working on the entity system\n- Working on the animation system\n\n## Usage\n\nThe engine is designed to be used in a way similar to the [pygame tutorial](http://www.pygame.org/docs/tut/PygameIntro.html).\n\n### Import the engine\n\n```python\nimport pygame_game_engine\n```\n\n### Initialize the engine\n\n```python\nengine = pygame_game_engine.Engine()\n```\n\n### Add assets\n\n```python\nengine.add_asset(\"background\", \"assets/background.png\")\n```\n\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\t\treturn self._assets\n\n\tdef filepath(self, basename):\n\t\"\"\"\n\tPass the basename of the file you want to access.\n\t\"\"\"\n\t\t\tif basename in self._assets:\n\t\t\t\treturn self._assets[basename]\n\t\t\telse:\n\t\t\t\treturn self._assets[basename]\n\n\tdef get_asset(self, basename):\n\t\"\"\"\n\tPass the basename of the file you want to access.\n\t\"\"\"\n\t\t\tif basename in self._assets:\n\t\t\t\treturn self._assets[basename]\n\t\t\telse:\n\t\t\t\treturn self._assets[basename]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\n\tdef get_asset(self, basename):\n\t\"\"\"\n\tReturns the filepath for a given \"basename\"\n\t\"\"\"\n\t\treturn self._assets[basename]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tsetattr(self, basename, file_path)\n\t\tself._assets[basename] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\t\treturn self\n\n\tdef filepath(self, basename):\n\t\"\"\"\n\tReturn the filepath of the specified asset.\n\t\"\"\"\n\t\treturn self._assets[basename]\n\n\tdef __getattr__(self, attrname):\n\t\"\"\"\n\tReturn the filepath of the specified attribute name.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._assets[attrname]\n\t\texcept KeyError:\n\t\t\traise AttributeError(attrname)\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturn a string of the class.\n\t\"\"\"\n\t\treturn self.__class__.__name__ + \"(%s)\" % self._assets\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\t\n\tdef remove_asset(self, basename):\n\t\"\"\"\n\tRemove the filepath from _assets in the class.\n\t\"\"\"\n\t\tdel self._assets[basename]\n\t\n\tdef filepath(self, basename):\n\t\"\"\"\n\tReturn the filepath of a file.\n\t\"\"\"\n\t\treturn self._assets[basename]\n\n\tdef read(self, basename):\n\t\"\"\"\n\tReturn the content of a file.\n\t\"\"\"\n\t\treturn self._assets[basename].read()\n\n\tdef write(self, basename, content):\n\t\"\"\"\n\tWrite content to a file.\n\t\"\"\"\n\t\tself._assets[basename].write(content)\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\tdef filepath(self, basename):\n\t\"\"\"\n\tGet the filepath of a asset in _assets.\n\t\"\"\"\n\t\treturn self._assets[basename]\n/README.md\n# Pygame Game Engine\n\n### Version 0.1.0\n\nThis is a game engine designed to be used as a base for pygame games.\n\n**This is under development and still needs a lot of work.**\n\n## Features\n\n- Asset managment\n- A basic entity system\n- A basic animation system\n\n## Dependencies\n\n- Python 3.4.4\n- Pygame 1.9.2\n\n## Current state\n\nThe current state of the engine is:\n\n- Working on the asset manager\n- Working on the entity system\n- Working on the animation system\n\n## Usage\n\nThe engine is designed to be used in a way similar to the [pygame tutorial](http://www.pygame.org/docs/tut/PygameIntro.html).\n\n### Import the engine\n\n```python\nimport pygame_game_engine\n```\n\n### Initialize the engine\n\n```python\nengine = pygame_game_engine.Engine()\n```\n\n### Add assets\n\n```python\nengine.add_asset(\"background\", \"assets/background.png\")\n```\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\tself._assets[basename] = file_path\n\t\treturn self._assets\n\n\tdef filepath(self, basename):\n\t\"\"\"\n\tPass the basename of the file you want to access.\n\t\"\"\"\n\t\t\tif basename in self._assets:\n\t\t\t\treturn self._assets[basename]\n\t\t\telse:\n\t\t\t\treturn self._assets[basename]\n\n\tdef get_asset(self, basename):\n\t\"\"\"\n\tPass the basename of the file you want to access.\n\t\"\"\"\n\t\t\tif basename in self._assets:\n\t\t\t\treturn self._assets[basename]\n\t\t\telse:\n\t\t\t\treturn self._assets[basename]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\t# extract the root of the zip file\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tzip_root = os.path.dirname(zf.namelist()[0])\n\t# group the files by the XML filename\n\treturn _group_files_by_xml_filename(path_join(zip_root, \"*\"))\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\twith zipfile.ZipFile(zip_path, mode='r') as archive:\n\t\treturn _group_files_by_xml_filename(archive.namelist())\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\t#Get the list of files in the zip\n\t\tfile_list = zip_file.namelist()\n\t\t#Remove the path and only keep the file name\n\t\tfile_list = [os.path.basename(file) for file in file_list]\n\t\t#Group the files by their xml filename\n\t\tgroup_files_by_xml_filename = _group_files_by_xml_filename(file_list)\n\t\t#Return the dictionary\n\t\treturn group_files_by_xml_filename\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\twith ZipFile(zip_path, 'r') as zip_file:\n\t\t\t# Get the XML file name\n\t\t\txml_filename = _get_xml_filename(zip_file)\n\n\t\t\t# Group the files by XML file name\n\t\t\tgroups = _group_files_by_xml_filename(zip_file, xml_filename)\n\n\t\t\t# For each XML file name, process the files\n\t\t\tfor xml_filename, files in groups.items():\n\t\t\t\t# Get the XML path\n\t\t\t\txml_path = _get_xml_path(zip_path, xml_filename)\n\n\t\t\t\t# Process the files\n\t\t\t\t_process_files(files, xml_path)\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tif zip_path.endswith(\".zip\"):\n\t\t# zips are just files\n\t\treturn _group_files_by_xml_filename(zip_path)\n\telse:\n\t\t# for directory, collect the files from subdirectories.\n\t\tfiles = []\n\t\tfor file in os.listdir(zip_path):\n\t\t\tfile_path = os.path.join(zip_path, file)\n\t\t\tif os.path.isdir(file_path):\n\t\t\t\tfiles += _explore_zipfile(file_path)\n\t\t\telse:\n\t\t\t\tfiles.append(file_path)\n\t\treturn _group_files_by_xml_filename(files)\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\t# Use the zipfile library to open the zip archive.\n\tzip_file = zipfile.ZipFile(zip_path)\n\t\n\t# Iterate over the zip file's contents.\n\tfor file_name in zip_file.namelist():\n\t\n\t\t# We only want.xml files.\n\t\tif file_name.endswith('.xml'):\n\t\t\n\t\t\t# Get the file's data.\n\t\t\tdata = zip_file.read(file_name)\n\t\t\t\n\t\t\t# Create a file object for the data.\n\t\t\tfile_obj = BytesIO(data)\n\t\t\t\n\t\t\t# Call _group_files_by_xml_filename with the data.\n\t\t\t_group_files_by_xml_filename(file_obj)\n\t\n\t# Close the zip file.\n\tzip_file.close()\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tzip_handle = ZipFile(zip_path)\n\texcept IOError, e:\n\t\traise Exception(\"Could not open zip file %s: %s\" % (zip_path, str(e)))\n\n\tmanifest_entry = zip_handle.getinfo('META-INF/MANIFEST.MF')\n\tmanifest_xml = zip_handle.read(manifest_entry)\n\tmanifest_doc = minidom.parseString(manifest_xml)\n\n\txml_filename_pattern = re.compile('(.*)\\\\.xml$')\n\n\t# Group files by xml filename\n\tfiles_by_xml_filename = _group_files_by_xml_filename(zip_handle, manifest_doc)\n\n\treturn files_by_xml_filename\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tprint(\"Exploring %s\" % zip_path)\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\t# get the names of the files in the zip\n\t\tfor name in zf.namelist():\n\t\t\t# get the xml filename from the file name\n\t\t\txml_filename = _get_xml_filename(name)\n\t\t\t# get the file extension from the file name\n\t\t\tfile_extension = _get_file_extension(name)\n\n\t\t\t# if the zip contains a file with an xml filename\n\t\t\tif xml_filename:\n\t\t\t\t# get the file content\n\t\t\t\tfile_content = zf.read(name)\n\n\t\t\t\t# if the zip contains a file with an xml filename\n\t\t\t\t# and the file extension is xml\n\t\t\t\tif file_extension == \"xml\":\n\t\t\t\t\t# get the xml document from the file content\n\t\t\t\t\txml_document = _get_xml_document(file_content)\n\n\t\t\t\t\t# if the xml document is valid\n\t\t\t\t\tif xml_document:\n\n\t\t\t\t\t\t# get the xml element from the xml document\n\t\t\t\t\t\txml_element = _get_xml_element(xml_document)\n\n\t\t\t\t\t\t# if the xml element is valid\n\t\t\t\t\t\tif xml_element:\n\n\t\t\t\t\t\t\t# get the xml element tag\n\t\t\t\t\t\t\txml_element_tag = _get_xml_element_tag(xml_element)\n\n\t\t\t\t\t\t\t# if", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\twith zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n\t\t# Iterate through the zip file\n\t\tfor filename in zip_ref.namelist():\n\t\t\tif filename.endswith('.xml'):\n\t\t\t\txml_filename = os.path.basename(filename)\n\t\t\t\txml_filepath = os.path.join('zip_files', xml_filename)\n\t\t\t\t# Extract the xml file to the 'xml_files' directory\n\t\t\t\tzip_ref.extract(filename, 'xml_files')\n\n\t\t\t\t# Read the xml file\n\t\t\t\txml_file = ET.parse(xml_filepath)\n\t\t\t\txml_root = xml_file.getroot()\n\n\t\t\t\t# Group the files by the given xml filename\n\t\t\t\tgroup_filename = _group_files_by_xml_filename(xml_root)\n\t\t\t\txml_file.write(group_filename)\n\t\t\t\tos.remove(xml_filepath)\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tfile_groupings = {}\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor filename in zf.namelist():\n\t\t\txml_filename = _get_xml_filename(filename)\n\t\t\tif xml_filename in file_groupings:\n\t\t\t\tfile_groupings[xml_filename].append(filename)\n\t\t\telse:\n\t\t\t\tfile_groupings[xml_filename] = [filename]\n\treturn file_groupings\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\t# extract the root of the zip file\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tzip_root = os.path.dirname(zf.namelist()[0])\n\t# group the files by the XML filename\n\treturn _group_files_by_xml_filename(path_join(zip_root, \"*\"))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\twith zipfile.ZipFile(zip_path, mode='r') as archive:\n\t\treturn _group_files_by_xml_filename(archive.namelist())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\t#Get the list of files in the zip\n\t\tfile_list = zip_file.namelist()\n\t\t#Remove the path and only keep the file name\n\t\tfile_list = [os.path.basename(file) for file in file_list]\n\t\t#Group the files by their xml filename\n\t\tgroup_files_by_xml_filename = _group_files_by_xml_filename(file_list)\n\t\t#Return the dictionary\n\t\treturn group_files_by_xml_filename\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\twith ZipFile(zip_path, 'r') as zip_file:\n\t\t\t# Get the XML file name\n\t\t\txml_filename = _get_xml_filename(zip_file)\n\n\t\t\t# Group the files by XML file name\n\t\t\tgroups = _group_files_by_xml_filename(zip_file, xml_filename)\n\n\t\t\t# For each XML file name, process the files\n\t\t\tfor xml_filename, files in groups.items():\n\t\t\t\t# Get the XML path\n\t\t\t\txml_path = _get_xml_path(zip_path, xml_filename)\n\n\t\t\t\t# Process the files\n\t\t\t\t_process_files(files, xml_path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tif zip_path.endswith(\".zip\"):\n\t\t# zips are just files\n\t\treturn _group_files_by_xml_filename(zip_path)\n\telse:\n\t\t# for directory, collect the files from subdirectories.\n\t\tfiles = []\n\t\tfor file in os.listdir(zip_path):\n\t\t\tfile_path = os.path.join(zip_path, file)\n\t\t\tif os.path.isdir(file_path):\n\t\t\t\tfiles += _explore_zipfile(file_path)\n\t\t\telse:\n\t\t\t\tfiles.append(file_path)\n\t\treturn _group_files_by_xml_filename(files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\t# Use the zipfile library to open the zip archive.\n\tzip_file = zipfile.ZipFile(zip_path)\n\t\n\t# Iterate over the zip file's contents.\n\tfor file_name in zip_file.namelist():\n\t\n\t\t# We only want.xml files.\n\t\tif file_name.endswith('.xml'):\n\t\t\n\t\t\t# Get the file's data.\n\t\t\tdata = zip_file.read(file_name)\n\t\t\t\n\t\t\t# Create a file object for the data.\n\t\t\tfile_obj = BytesIO(data)\n\t\t\t\n\t\t\t# Call _group_files_by_xml_filename with the data.\n\t\t\t_group_files_by_xml_filename(file_obj)\n\t\n\t# Close the zip file.\n\tzip_file.close()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tzip_handle = ZipFile(zip_path)\n\texcept IOError, e:\n\t\traise Exception(\"Could not open zip file %s: %s\" % (zip_path, str(e)))\n\n\tmanifest_entry = zip_handle.getinfo('META-INF/MANIFEST.MF')\n\tmanifest_xml = zip_handle.read(manifest_entry)\n\tmanifest_doc = minidom.parseString(manifest_xml)\n\n\txml_filename_pattern = re.compile('(.*)\\\\.xml$')\n\n\t# Group files by xml filename\n\tfiles_by_xml_filename = _group_files_by_xml_filename(zip_handle, manifest_doc)\n\n\treturn files_by_xml_filename\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tprint(\"Exploring %s\" % zip_path)\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\t# get the names of the files in the zip\n\t\tfor name in zf.namelist():\n\t\t\t# get the xml filename from the file name\n\t\t\txml_filename = _get_xml_filename(name)\n\t\t\t# get the file extension from the file name\n\t\t\tfile_extension = _get_file_extension(name)\n\n\t\t\t# if the zip contains a file with an xml filename\n\t\t\tif xml_filename:\n\t\t\t\t# get the file content\n\t\t\t\tfile_content = zf.read(name)\n\n\t\t\t\t# if the zip contains a file with an xml filename\n\t\t\t\t# and the file extension is xml\n\t\t\t\tif file_extension == \"xml\":\n\t\t\t\t\t# get the xml document from the file content\n\t\t\t\t\txml_document = _get_xml_document(file_content)\n\n\t\t\t\t\t# if the xml document is valid\n\t\t\t\t\tif xml_document:\n\n\t\t\t\t\t\t# get the xml element from the xml document\n\t\t\t\t\t\txml_element = _get_xml_element(xml_document)\n\n\t\t\t\t\t\t# if the xml element is valid\n\t\t\t\t\t\tif xml_element:\n\n\t\t\t\t\t\t\t# get the xml element tag\n\t\t\t\t\t\t\txml_element_tag = _get_xml_element_tag(xml_element)\n\n\t\t\t\t\t\t\t# if", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\twith zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n\t\t# Iterate through the zip file\n\t\tfor filename in zip_ref.namelist():\n\t\t\tif filename.endswith('.xml'):\n\t\t\t\txml_filename = os.path.basename(filename)\n\t\t\t\txml_filepath = os.path.join('zip_files', xml_filename)\n\t\t\t\t# Extract the xml file to the 'xml_files' directory\n\t\t\t\tzip_ref.extract(filename, 'xml_files')\n\n\t\t\t\t# Read the xml file\n\t\t\t\txml_file = ET.parse(xml_filepath)\n\t\t\t\txml_root = xml_file.getroot()\n\n\t\t\t\t# Group the files by the given xml filename\n\t\t\t\tgroup_filename = _group_files_by_xml_filename(xml_root)\n\t\t\t\txml_file.write(group_filename)\n\t\t\t\tos.remove(xml_filepath)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\t \n\tfile_groupings = {}\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor filename in zf.namelist():\n\t\t\txml_filename = _get_xml_filename(filename)\n\t\t\tif xml_filename in file_groupings:\n\t\t\t\tfile_groupings[xml_filename].append(filename)\n\t\t\telse:\n\t\t\t\tfile_groupings[xml_filename] = [filename]\n\treturn file_groupings\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\tzip_files_list = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_files:\n\t\tzip_files_list.append(zip_files.namelist())\n\treturn zip_files_list\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t \n\tz = zipfile.ZipFile(zip_path)\n\treturn z.namelist()\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\tzip_file = zipfile.ZipFile(zip_path)\n\t# print(zip_file.namelist())\n\treturn zip_file.namelist()\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n \tzip = zipfile.ZipFile(zip_path)\n\treturn zip.namelist()\n\t", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n    \tzip_file = zipfile.ZipFile(zip_path)\n\t\n\treturn zip_file.namelist()\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\tzf = zipfile.ZipFile(zip_path)\n\treturn zf.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\t\treturn zip_file.namelist()\n\t\t", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\tfor file_name in zip_file.namelist():\n\t\t\tyield file_name\n/scripts/convert_images.py\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport logging\n\nfrom scripts.utils import get_image_from_zipfile\nfrom scripts.utils import files_list_from_zipfile\nfrom scripts.utils import get_image_file_path\nfrom scripts.utils import get_image_file_extension\nfrom scripts.utils import convert_image_file\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\tzip_files_list = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_files:\n\t\tzip_files_list.append(zip_files.namelist())\n\treturn zip_files_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t \n\tz = zipfile.ZipFile(zip_path)\n\treturn z.namelist()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\tzip_file = zipfile.ZipFile(zip_path)\n\t# print(zip_file.namelist())\n\treturn zip_file.namelist()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n \tzip = zipfile.ZipFile(zip_path)\n\treturn zip.namelist()\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n    \tzip_file = zipfile.ZipFile(zip_path)\n\t\n\treturn zip_file.namelist()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\tzf = zipfile.ZipFile(zip_path)\n\treturn zf.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\t\treturn zip_file.namelist()\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t\n\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\tfor file_name in zip_file.namelist():\n\t\t\tyield file_name\n/scripts/convert_images.py\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport logging\n\nfrom scripts.utils import get_image_from_zipfile\nfrom scripts.utils import files_list_from_zipfile\nfrom scripts.utils import get_image_file_path\nfrom scripts.utils import get_image_file_extension\nfrom scripts.utils import convert_image_file\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n\n\treturn content.replace(\"w:st=\", \"w-st=\")\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n\n\treturn content.replace('w:st=', 'w-st=')\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\tcontent = content.replace(\"w:pStyle=\", \"w-pStyle=\")\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\treturn content.replace('w:st=', 'w-st=')\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n#\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\treturn re.sub(r\"w:st\\=\", \"w-st=\", content)\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\tcontent = re.sub(r'(w:st=\")(\\w+)(\")', r'\\1\\2\\3', content)\n\treturn content\n\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n        return content.replace('w:st=', 'w-st=')\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\treturn re.sub(r'w:st=', r'w-st=', content)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n\n\treturn content.replace(\"w:st=\", \"w-st=\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n\n\treturn content.replace('w:st=', 'w-st=')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\tcontent = content.replace(\"w:pStyle=\", \"w-pStyle=\")\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\treturn content.replace('w:st=', 'w-st=')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n#\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\treturn re.sub(r\"w:st\\=\", \"w-st=\", content)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\tcontent = re.sub(r'(w:st=\")(\\w+)(\")', r'\\1\\2\\3', content)\n\treturn content\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t\n        return content.replace('w:st=', 'w-st=')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\t \n\treturn re.sub(r'w:st=', r'w-st=', content)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t/README.md\n# Wikidata-Scraper\n\nThis is a scraper for the Wikidata dump and is currently used to create a\nmapping between Wikidata and the [Library of Congress Subject Headings](https://www.loc.gov/catdir/cpso/subjectheadings/main.html).\n\n## Requirements\n\nThis scraper requires Python 3.4 and the [BeautifulSoup](https://pypi.python.org/pypi/beautifulsoup4) library.\n\n## Usage\n\nThe scraper can be run on the command line, with the following options:\n\n```\nusage: scraper.py [-h] [--dump-dir DIR] [--wikidata-file FILENAME]\n                  [--loc-file FILENAME]\n\nScrape Wikidata dump for LCSH mappings\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --dump-dir DIR        Directory containing Wikidata dump\n  --wikidata-file FILENAME\n                        File to save Wikidata output to\n  --loc-file FILENAME  File to save LCSH output to\n```\n\nWhen run, the script will create a new file `wikidata_lcsh_mapping.csv` in the same directory as the script. This file contains two columns: Wikidata ID and LCSH ID.\n\n## Output\n\nThe output", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tdate = _get_node(node, pubdate_xpath)\n\t\tif date:\n\t\t\treturn date\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tvalue = node.xpath(pubdate_xpath)\n\t\tif value:\n\t\t\treturn value[0]\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\n```\n\nNow we can write the main function.\n\n```python", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\nfor node in nodes:\n\tif node.tag == \"article\":\n\t\tpass\n\telif node.tag == \"title\":\n\t\tpass\n\telif node.tag == \"date\":\n\t\tpass\n\telif node.tag == \"pubdate\":\n\t\tpass\n\telif node.tag == \"author\":\n\t\tpass\n\telif node.tag == \"byline\":\n\t\tpass\n\telif node.tag == \"url\":\n\t\tpass\n\telif node.tag == \"link\":\n\t\tpass\n\telif node.tag == \"description\":\n\t\tpass\n\telif node.tag == \"body\":\n\t\tpass\n\telif node.tag == \"p\":\n\t\tpass\n\telif node.tag == \"img\":\n\t\tpass\n\telif node.tag == \"caption\":\n\t\tpass\n\telif node.tag == \"related\":\n\t\tpass\n\telif node.tag == \"related_story\":\n\t\tpass\n\telif node.tag == \"related_link\":\n\t\tpass\n\telif node.tag == \"related_photo\":\n\t\tpass\n\telif node.tag == \"related_video\":\n\t\tpass\n\telif node.tag == \"related_audio\":\n\t\tpass\n\telif node.tag == \"related_video\":\n\t\tpass\n\telif node.tag == \"related_quote\":\n\t\tpass\n\telif node.", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor xpath in pubdate_xpaths:\n\t\tpubdate_node = node.xpath(xpath)\n\t\tif len(pubdate_node) > 0:\n\t\t\treturn pubdate_node[0]\n\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0].strip()\n\t\t\tbreak\n\t\texcept:\n\t\t\tpubdate = None\n\treturn pubdate\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif match:\n\t\t\treturn match[0].strip()\n\t\telse:\n\t\t\treturn None\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t/README.md\n# Wikidata-Scraper\n\nThis is a scraper for the Wikidata dump and is currently used to create a\nmapping between Wikidata and the [Library of Congress Subject Headings](https://www.loc.gov/catdir/cpso/subjectheadings/main.html).\n\n## Requirements\n\nThis scraper requires Python 3.4 and the [BeautifulSoup](https://pypi.python.org/pypi/beautifulsoup4) library.\n\n## Usage\n\nThe scraper can be run on the command line, with the following options:\n\n```\nusage: scraper.py [-h] [--dump-dir DIR] [--wikidata-file FILENAME]\n                  [--loc-file FILENAME]\n\nScrape Wikidata dump for LCSH mappings\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --dump-dir DIR        Directory containing Wikidata dump\n  --wikidata-file FILENAME\n                        File to save Wikidata output to\n  --loc-file FILENAME  File to save LCSH output to\n```\n\nWhen run, the script will create a new file `wikidata_lcsh_mapping.csv` in the same directory as the script. This file contains two columns: Wikidata ID and LCSH ID.\n\n## Output\n\nThe output", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tdate = _get_node(node, pubdate_xpath)\n\t\tif date:\n\t\t\treturn date\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tvalue = node.xpath(pubdate_xpath)\n\t\tif value:\n\t\t\treturn value[0]\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\n```\n\nNow we can write the main function.\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\nfor node in nodes:\n\tif node.tag == \"article\":\n\t\tpass\n\telif node.tag == \"title\":\n\t\tpass\n\telif node.tag == \"date\":\n\t\tpass\n\telif node.tag == \"pubdate\":\n\t\tpass\n\telif node.tag == \"author\":\n\t\tpass\n\telif node.tag == \"byline\":\n\t\tpass\n\telif node.tag == \"url\":\n\t\tpass\n\telif node.tag == \"link\":\n\t\tpass\n\telif node.tag == \"description\":\n\t\tpass\n\telif node.tag == \"body\":\n\t\tpass\n\telif node.tag == \"p\":\n\t\tpass\n\telif node.tag == \"img\":\n\t\tpass\n\telif node.tag == \"caption\":\n\t\tpass\n\telif node.tag == \"related\":\n\t\tpass\n\telif node.tag == \"related_story\":\n\t\tpass\n\telif node.tag == \"related_link\":\n\t\tpass\n\telif node.tag == \"related_photo\":\n\t\tpass\n\telif node.tag == \"related_video\":\n\t\tpass\n\telif node.tag == \"related_audio\":\n\t\tpass\n\telif node.tag == \"related_video\":\n\t\tpass\n\telif node.tag == \"related_quote\":\n\t\tpass\n\telif node.", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor xpath in pubdate_xpaths:\n\t\tpubdate_node = node.xpath(xpath)\n\t\tif len(pubdate_node) > 0:\n\t\t\treturn pubdate_node[0]\n\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t \n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0].strip()\n\t\t\tbreak\n\t\texcept:\n\t\t\tpubdate = None\n\treturn pubdate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\t\tfor xpath in pubdate_xpaths:\n\t\tmatch = node.xpath(xpath)\n\t\tif match:\n\t\t\treturn match[0].strip()\n\t\telse:\n\t\t\treturn None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t_number_pattern = re.compile(r'^(\\d+)(\\w)?')\n\t_sup_pattern = re.compile(r'^\\((\\d+)\\)$')\n\n\tnumber = _number_pattern.search(issue.text)\n\tsup = _sup_pattern.search(issue.text)\n\n\tif number:\n\t\tnumber = number.group(1)\n\t\tif len(number) == 1:\n\t\t\tnumber = '0' + number\n\telse:\n\t\tnumber = None\n\n\tif sup:\n\t\tsup = sup.group(1)\n\telse:\n\t\tsup = None\n\n\treturn number, sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\t#Get the number and supplement from the issue.\n\t\t#The number and supplement are stored in the issue element.\n\t\t#The number and supplement are separated by a period.\n\t\t#The number and supplement are both strings.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\t# This is a bit of a hack to work around a bug with regex in python\n\t\t# where a pattern like \"\\d[0-9]*\" does not match the full length of the\n\t\t# string \"\\d111222333\", and so we have to do it manually.  The exact\n\t\t# regex is:\n\t\t#\n\t\t# ^(?P<number>^[0-9]*)(?P<sup>[0-9]+)$\n\t\t#\n\t\t# I have not found a way to make the regex match the full string, and\n\t\t# so this is the best I can do.\n\t\t#\n\t\t# In the future, we should probably move the regex matching to the\n\t\t# database, rather than doing it here.\n\t\t#\n\t\tfirst_match = re.match(r'^(?P<number>^[0-9]*)(?P<sup>[0-9]+)$', issue)\n\t\tif first_match:\n\t\t\tnumber = first_match.group('number')\n\t\t\tsup = first_match.group('sup')\n\t\t\treturn number, sup\n\t\telse:\n\t\t\treturn None, None\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t \n\t# the number should be the first <b> element in the contents, if it has a'sup' element as its sibling, \n\t# then it should be the sup, otherwise it should be the b.\n\tnumber_b = issue.find('b')\n\tif number_b:\n\t\tnumber = number_b.text\n\t\t# extract sup\n\t\tsup = number_b.find('sup')\n\t\tif sup:\n\t\t\tnumber = sup.text\n\telse:\n\t\tnumber = ''\n\t# extract sup\n\tsup = issue.find('sup')\n\tif sup:\n\t\tsuppl = sup.text\n\telse:\n\t\tsuppl = ''\n\treturn number, suppl\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\ttry:\n\t\t\tnumber = int(issue.contents[0].contents[0].strip())\n\t\texcept:\n\t\t\tnumber = None\n\n\t\ttry:\n\t\t\tsuppl = issue.contents[0].contents[1].strip()\n\t\texcept:\n\t\t\tsuppl = None\n\n\t\treturn number, suppl\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t# First extract the issue number\n\tmatch = re.search(r'([0-9]+)', issue)\n\tif match:\n\t\tnumber = match.group(1)\n\telse:\n\t\tnumber = None\n\n\t# Now extract the supplement\n\tmatch = re.search(r'([0-9]+)', issue)\n\tif match:\n\t\tsup = match.group(1)\n\telse:\n\t\tsup = None\n\n\treturn number, sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t# if the first four characters of the element are digits,\n\t# then we assume that this is a number\n\tnumber = None\n\tif issue[0:4].isdigit():\n\t\tnumber = issue[0:4]\n\t\tissue = issue[4:]\n\n\t# if the first four characters of the element are digits\n\t# and a hyphen, then we assume that this is a number\n\t# followed by a supplement (e.g. 12-1)\n\tif issue[0:4].isdigit():\n\t\tif issue[4] == '-':\n\t\t\tnumber = issue[0:4]\n\t\t\tissue = issue[5:]\n\t\t\tsup = issue[0]\n\t\t\tissue = issue[1:]\n\t\telse:\n\t\t\tsup = issue[0]\n\t\t\tissue = issue[1:]\n\n\t# if the first three characters of the element are digits\n\t# and a hyphen, then we assume that this is a supplement\n\t# (e.g. 2-3)\n\telif issue[0:3].isdigit():\n\t\tif issue[3] == '-':\n\t\t\tsup = issue[0:3]\n\t\t\tissue = issue[4:]\n\n\t# if the first two characters of the element are digits\n\t# and a hyphen, then we assume that this is a supplement\n\t# (e.", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t \n\tnumber = None\n\tsupplment = None\n\tif issue.find('number') is not None:\n\t\tnumber = issue.find('number').text\n\tif issue.find('supplment') is not None:\n\t\tsupplment = issue.find('supplment').text\n\treturn number, supplment\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\t# check for number and sup\n\tnumber_and_sup = issue.xpath('.//number |.//sup')\n\tif not number_and_sup:\n\t\treturn None, None\n\n\t# number\n\tif len(number_and_sup) == 1:\n\t\treturn number_and_sup[0].text, None\n\t\n\t# sup\n\tif len(number_and_sup) == 2:\n\t\treturn number_and_sup[0].text, number_and_sup[1].text\n\n\traise ValueError(f'Issue has more than 2 number/sup elements, found {len(number_and_sup)}')\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t# find number and sup\n\tnumber_sup_elements = issue.findall('.//{http://www.tei-c.org/ns/1.0}number[@type=\"issue\"]|{http://www.tei-c.org/ns/1.0}number[@type=\"supplement\"]'.format(NS))\n\tif len(number_sup_elements) == 2:\n\t\tnumber_element = number_sup_elements[0]\n\t\tsup_element = number_sup_elements[1]\n\t\tnumber = number_element.text\n\t\tsup = sup_element.text\n\telif len(number_sup_elements) == 1:\n\t\tnumber_element = number_sup_elements[0]\n\t\tnumber = number_element.text\n\t\tsup = None\n\telif len(number_sup_elements) == 0:\n\t\tnumber = None\n\t\tsup = None\n\telse:\n\t\traise ValueError(\"Multiple number/supplement elements.\")\n\n\treturn number, sup\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t_number_pattern = re.compile(r'^(\\d+)(\\w)?')\n\t_sup_pattern = re.compile(r'^\\((\\d+)\\)$')\n\n\tnumber = _number_pattern.search(issue.text)\n\tsup = _sup_pattern.search(issue.text)\n\n\tif number:\n\t\tnumber = number.group(1)\n\t\tif len(number) == 1:\n\t\t\tnumber = '0' + number\n\telse:\n\t\tnumber = None\n\n\tif sup:\n\t\tsup = sup.group(1)\n\telse:\n\t\tsup = None\n\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\t#Get the number and supplement from the issue.\n\t\t#The number and supplement are stored in the issue element.\n\t\t#The number and supplement are separated by a period.\n\t\t#The number and supplement are both strings.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for the first issue element.\n\t\t#TODO: this only works for", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\t# This is a bit of a hack to work around a bug with regex in python\n\t\t# where a pattern like \"\\d[0-9]*\" does not match the full length of the\n\t\t# string \"\\d111222333\", and so we have to do it manually.  The exact\n\t\t# regex is:\n\t\t#\n\t\t# ^(?P<number>^[0-9]*)(?P<sup>[0-9]+)$\n\t\t#\n\t\t# I have not found a way to make the regex match the full string, and\n\t\t# so this is the best I can do.\n\t\t#\n\t\t# In the future, we should probably move the regex matching to the\n\t\t# database, rather than doing it here.\n\t\t#\n\t\tfirst_match = re.match(r'^(?P<number>^[0-9]*)(?P<sup>[0-9]+)$', issue)\n\t\tif first_match:\n\t\t\tnumber = first_match.group('number')\n\t\t\tsup = first_match.group('sup')\n\t\t\treturn number, sup\n\t\telse:\n\t\t\treturn None, None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t \n\t# the number should be the first <b> element in the contents, if it has a'sup' element as its sibling, \n\t# then it should be the sup, otherwise it should be the b.\n\tnumber_b = issue.find('b')\n\tif number_b:\n\t\tnumber = number_b.text\n\t\t# extract sup\n\t\tsup = number_b.find('sup')\n\t\tif sup:\n\t\t\tnumber = sup.text\n\telse:\n\t\tnumber = ''\n\t# extract sup\n\tsup = issue.find('sup')\n\tif sup:\n\t\tsuppl = sup.text\n\telse:\n\t\tsuppl = ''\n\treturn number, suppl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\ttry:\n\t\t\tnumber = int(issue.contents[0].contents[0].strip())\n\t\texcept:\n\t\t\tnumber = None\n\n\t\ttry:\n\t\t\tsuppl = issue.contents[0].contents[1].strip()\n\t\texcept:\n\t\t\tsuppl = None\n\n\t\treturn number, suppl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t# First extract the issue number\n\tmatch = re.search(r'([0-9]+)', issue)\n\tif match:\n\t\tnumber = match.group(1)\n\telse:\n\t\tnumber = None\n\n\t# Now extract the supplement\n\tmatch = re.search(r'([0-9]+)', issue)\n\tif match:\n\t\tsup = match.group(1)\n\telse:\n\t\tsup = None\n\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t# if the first four characters of the element are digits,\n\t# then we assume that this is a number\n\tnumber = None\n\tif issue[0:4].isdigit():\n\t\tnumber = issue[0:4]\n\t\tissue = issue[4:]\n\n\t# if the first four characters of the element are digits\n\t# and a hyphen, then we assume that this is a number\n\t# followed by a supplement (e.g. 12-1)\n\tif issue[0:4].isdigit():\n\t\tif issue[4] == '-':\n\t\t\tnumber = issue[0:4]\n\t\t\tissue = issue[5:]\n\t\t\tsup = issue[0]\n\t\t\tissue = issue[1:]\n\t\telse:\n\t\t\tsup = issue[0]\n\t\t\tissue = issue[1:]\n\n\t# if the first three characters of the element are digits\n\t# and a hyphen, then we assume that this is a supplement\n\t# (e.g. 2-3)\n\telif issue[0:3].isdigit():\n\t\tif issue[3] == '-':\n\t\t\tsup = issue[0:3]\n\t\t\tissue = issue[4:]\n\n\t# if the first two characters of the element are digits\n\t# and a hyphen, then we assume that this is a supplement\n\t# (e.", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t \n\tnumber = None\n\tsupplment = None\n\tif issue.find('number') is not None:\n\t\tnumber = issue.find('number').text\n\tif issue.find('supplment') is not None:\n\t\tsupplment = issue.find('supplment').text\n\treturn number, supplment\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\t# check for number and sup\n\tnumber_and_sup = issue.xpath('.//number |.//sup')\n\tif not number_and_sup:\n\t\treturn None, None\n\n\t# number\n\tif len(number_and_sup) == 1:\n\t\treturn number_and_sup[0].text, None\n\t\n\t# sup\n\tif len(number_and_sup) == 2:\n\t\treturn number_and_sup[0].text, number_and_sup[1].text\n\n\traise ValueError(f'Issue has more than 2 number/sup elements, found {len(number_and_sup)}')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\n\t# find number and sup\n\tnumber_sup_elements = issue.findall('.//{http://www.tei-c.org/ns/1.0}number[@type=\"issue\"]|{http://www.tei-c.org/ns/1.0}number[@type=\"supplement\"]'.format(NS))\n\tif len(number_sup_elements) == 2:\n\t\tnumber_element = number_sup_elements[0]\n\t\tsup_element = number_sup_elements[1]\n\t\tnumber = number_element.text\n\t\tsup = sup_element.text\n\telif len(number_sup_elements) == 1:\n\t\tnumber_element = number_sup_elements[0]\n\t\tnumber = number_element.text\n\t\tsup = None\n\telif len(number_sup_elements) == 0:\n\t\tnumber = None\n\t\tsup = None\n\telse:\n\t\traise ValueError(\"Multiple number/supplement elements.\")\n\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.obj}\"\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\tif debug:\n\t\t\tdebug_details = f\", _debug__index={self._debug__index}, _debug__parent_index={self._debug__parent_index}\"\n\t\telse:\n\t\t\tdebug_details = \"\"\n\t\tif isinstance(self.obj, str):\n\t\t\tobj = f\"'{self.obj}'\"\n\t\telse:\n\t\t\tobj = repr(self.obj)\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}, {obj})\"\n\n\tdef __str__(self):\n\t\treturn self.pretty()\n\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{self.obj})\"\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\tif debug:\n\t\t\treturn self.pretty(debug=True)\n\t\telse:\n\t\t\treturn self.pretty(debug=False)\n\n\t# def __str__(self):\n\t# \treturn f\"{self.__class__.__name__}({self.obj})\"\n\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, self.__class__):\n\t\t\treturn NotImplemented\n\t\treturn self.obj == other.obj\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __hash__(self):\n\t\treturn hash(self.obj)\n\n\tdef __iter__(self):\n\t\treturn iter(self.obj)\n\n\tdef __len__(self):\n\t\treturn len(self.obj)\n\n\tdef __getitem__(self, index):\n\t\treturn self.__class__(self.obj[index])\n\n\tdef __contains__(self, item):\n\t\treturn item in self.obj\n\n\tdef __add__(self, other):\n\t\treturn self.__class__(self.obj + other.obj)\n\n\tdef __radd__(self, other):\n\t\treturn self + self.__class__(other)\n\n\tdef __sub__(self, other):\n\t\treturn self.__class__(self.obj - other.obj)\n\n\tdef __r", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self, obj):\n\t\t\"\"\"\n\t\tInitialize self.\n\t\t\"\"\"\n\t\tsuper().__init__(obj)\n\t\"\"\"\n\t\"\"\"\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__ + repr(self.obj)\n\t\"\"\"\n\t\"\"\"\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__ + str(self.obj)\n\t\"\"\"\n\t\"\"\"\n\tdef pretty(self, indent=0, debug=False):\n\t\t\"\"\"\n\t\tReturn a pretty formatted representation of self.\n\t\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({repr(self.obj)})\"\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self, obj):\n\t\t\"\"\"\n\t\tInitialize self.\n\t\t\"\"\"\n\t\tsuper().__init__(obj)\n\t\"\"\"\n\t\"\"\"\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__ + repr(self.obj)\n\t\"\"\"\n\t\"\"\"\n\t", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\traise NotImplementedError()\n\tdef __str__(self):\n\t\treturn self.pretty()\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn str(self)\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn self.__class__.__name__ + \"(\" + self.obj.__class__.__name__ + \")\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\tif self.obj is None: return (\" \" * indent) + f\"{self.__class__.__name__}('None')\"\n\t\tif isinstance(self.obj, str): return (\" \" * indent) + f\"{self.__class__.__name__}('{self.obj}')\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({self.obj})\"\n\tdef __str__(self):\n\t\treturn self.pretty()\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\tdef __hash__(self):\n\t\treturn hash(self.obj)\n\tdef __eq__(self, other):\n\t\tif isinstance(other, str):\n\t\t\treturn self.obj == other\n\t\tif isinstance(other, int):\n\t\t\treturn self.obj == other\n\t\treturn self.obj == other.obj\n\tdef __ne__(self, other):\n\t\treturn not self == other\n\tdef __lt__(self, other):\n\t\treturn self.obj < other.obj\n\tdef __le__(self, other):\n\t\treturn self.obj <= other.obj\n\tdef __gt__(self, other):\n\t\treturn self.obj > other.obj\n\tdef __ge__(self, other):\n\t\treturn self.obj >= other.obj\n\tdef __add__(self, other):\n\t\treturn self.obj + other."], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.obj}\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\tif debug:\n\t\t\tdebug_details = f\", _debug__index={self._debug__index}, _debug__parent_index={self._debug__parent_index}\"\n\t\telse:\n\t\t\tdebug_details = \"\"\n\t\tif isinstance(self.obj, str):\n\t\t\tobj = f\"'{self.obj}'\"\n\t\telse:\n\t\t\tobj = repr(self.obj)\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}, {obj})\"\n\n\tdef __str__(self):\n\t\treturn self.pretty()\n\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{self.obj})\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\tif debug:\n\t\t\treturn self.pretty(debug=True)\n\t\telse:\n\t\t\treturn self.pretty(debug=False)\n\n\t# def __str__(self):\n\t# \treturn f\"{self.__class__.__name__}({self.obj})\"\n\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, self.__class__):\n\t\t\treturn NotImplemented\n\t\treturn self.obj == other.obj\n\n\tdef __ne__(self, other):\n\t\treturn not self.__eq__(other)\n\n\tdef __hash__(self):\n\t\treturn hash(self.obj)\n\n\tdef __iter__(self):\n\t\treturn iter(self.obj)\n\n\tdef __len__(self):\n\t\treturn len(self.obj)\n\n\tdef __getitem__(self, index):\n\t\treturn self.__class__(self.obj[index])\n\n\tdef __contains__(self, item):\n\t\treturn item in self.obj\n\n\tdef __add__(self, other):\n\t\treturn self.__class__(self.obj + other.obj)\n\n\tdef __radd__(self, other):\n\t\treturn self + self.__class__(other)\n\n\tdef __sub__(self, other):\n\t\treturn self.__class__(self.obj - other.obj)\n\n\tdef __r", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self, obj):\n\t\t\"\"\"\n\t\tInitialize self.\n\t\t\"\"\"\n\t\tsuper().__init__(obj)\n\t\"\"\"\n\t\"\"\"\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__ + repr(self.obj)\n\t\"\"\"\n\t\"\"\"\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__ + str(self.obj)\n\t\"\"\"\n\t\"\"\"\n\tdef pretty(self, indent=0, debug=False):\n\t\t\"\"\"\n\t\tReturn a pretty formatted representation of self.\n\t\t\"\"\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({repr(self.obj)})\"\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self, obj):\n\t\t\"\"\"\n\t\tInitialize self.\n\t\t\"\"\"\n\t\tsuper().__init__(obj)\n\t\"\"\"\n\t\"\"\"\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\n\t\t\"\"\"\n\t\treturn self.__class__.__name__ + repr(self.obj)\n\t\"\"\"\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\traise NotImplementedError()\n\tdef __str__(self):\n\t\treturn self.pretty()\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn str(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\treturn self.__class__.__name__ + \"(\" + self.obj.__class__.__name__ + \")\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\t\tif self.obj is None: return (\" \" * indent) + f\"{self.__class__.__name__}('None')\"\n\t\tif isinstance(self.obj, str): return (\" \" * indent) + f\"{self.__class__.__name__}('{self.obj}')\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({self.obj})\"\n\tdef __str__(self):\n\t\treturn self.pretty()\n\tdef __repr__(self):\n\t\treturn self.pretty()\n\tdef __hash__(self):\n\t\treturn hash(self.obj)\n\tdef __eq__(self, other):\n\t\tif isinstance(other, str):\n\t\t\treturn self.obj == other\n\t\tif isinstance(other, int):\n\t\t\treturn self.obj == other\n\t\treturn self.obj == other.obj\n\tdef __ne__(self, other):\n\t\treturn not self == other\n\tdef __lt__(self, other):\n\t\treturn self.obj < other.obj\n\tdef __le__(self, other):\n\t\treturn self.obj <= other.obj\n\tdef __gt__(self, other):\n\t\treturn self.obj > other.obj\n\tdef __ge__(self, other):\n\t\treturn self.obj >= other.obj\n\tdef __add__(self, other):\n\t\treturn self.obj + other.", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tdef absorb(expr):\n\t\t\tif expr.op == '+':\n\t\t\t\treturn expr.args[0]\n\t\t\tif expr.op == '*':\n\t\t\t\treturn expr.args[0]\n\t\t\tif expr.op == '-':\n\t\t\t\te1, e2 = expr.args\n\t\t\t\tif e1.op == '*':\n\t\t\t\t\tif e1.args[1] == e2:\n\t\t\t\t\t\treturn e1.args[0]\n\t\t\t\tif e1.op == '+':\n\t\t\t\t\tif e1.args[0] == e2:\n\t\t\t\t\t\treturn e1.args[1]\n\t\t\treturn expr\n\t\treturn [absorb(expr) for expr in args]\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tresult = []\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, Expression):\n\t\t\t\tif arg.operator == 'absorb':\n\t\t\t\t\tresult.extend(arg.args)\n\t\t\t\telse:\n\t\t\t\t\tresult.append(arg)\n\t\t\telse:\n\t\t\t\tresult.append(arg)\n\t\treturn result\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\treturn self.apply_method('absorb', args)\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\treturn list(absorb_sequence(args))\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tif len(args) == 1:\n\t\t\treturn args\n\t\telse:\n\t\t\tfor i in range(1, len(args)):\n\t\t\t\tif args[i].op == args[i-1].op:\n\t\t\t\t\targs[i-1] = args[i-1] + args[i]\n\t\t\t\t\tdel args[i]\n\t\t\t\t\treturn self.absorb(args)\n\t\t\telse:\n\t\t\t\treturn args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\treturn [self.absorb_one(x) for x in args]\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb(result[i])\n\t\treturn result\n\n\tdef absorb_first(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb_first(result[i])\n\t\treturn result\n\n\tdef absorb_last(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb_last(result[i])\n\t\treturn result\n\n\tdef absorb_first_last(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb_first_last(result[i])\n\t\treturn result\n\n\tdef absorb_first_last(self, args):", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tres = []\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, Mul) and len(arg.args) == 2 and arg.args[0] == arg.args[1]:\n\t\t\t\tres.append(arg.args[0])\n\t\t\telse:\n\t\t\t\tres.append(arg)\n\t\treturn res\n\t\n\tdef flatten(self, args):\n\t\t\"\"\"\n\t\tGiven an `args` sequence of expressions, return a new list of expression applying flatten.\n\t\t\"\"\"\n\t\tres = []\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, Add):\n\t\t\t\tres.extend(arg.args)\n\t\t\telse:\n\t\t\t\tres.append(arg)\n\t\treturn res\n\t\n\tdef _eval_simplify(self, expr):\n\t\t\"\"\"\n\t\tSimplify an expression.\n\t\t\"\"\"\n\t\texpr = self.remove_abs_mul(expr)\n\t\texpr = self.simplify_abs(expr)\n\t\texpr = self.add_mul_one(expr)\n\t\texpr = self.simplify_abs(expr)\n\t\texpr = self.remove_zero(expr)\n\t\texpr = self.remove_one(expr)\n\t\texpr = self.remove_zero(expr)\n\t\texpr = self.remove_one(expr)\n\t\texpr = self.flatten(expr)\n\t\texpr = self.remove_mul_one"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tdef absorb(expr):\n\t\t\tif expr.op == '+':\n\t\t\t\treturn expr.args[0]\n\t\t\tif expr.op == '*':\n\t\t\t\treturn expr.args[0]\n\t\t\tif expr.op == '-':\n\t\t\t\te1, e2 = expr.args\n\t\t\t\tif e1.op == '*':\n\t\t\t\t\tif e1.args[1] == e2:\n\t\t\t\t\t\treturn e1.args[0]\n\t\t\t\tif e1.op == '+':\n\t\t\t\t\tif e1.args[0] == e2:\n\t\t\t\t\t\treturn e1.args[1]\n\t\t\treturn expr\n\t\treturn [absorb(expr) for expr in args]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tresult = []\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, Expression):\n\t\t\t\tif arg.operator == 'absorb':\n\t\t\t\t\tresult.extend(arg.args)\n\t\t\t\telse:\n\t\t\t\t\tresult.append(arg)\n\t\t\telse:\n\t\t\t\tresult.append(arg)\n\t\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\treturn self.apply_method('absorb', args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\treturn list(absorb_sequence(args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tif len(args) == 1:\n\t\t\treturn args\n\t\telse:\n\t\t\tfor i in range(1, len(args)):\n\t\t\t\tif args[i].op == args[i-1].op:\n\t\t\t\t\targs[i-1] = args[i-1] + args[i]\n\t\t\t\t\tdel args[i]\n\t\t\t\t\treturn self.absorb(args)\n\t\t\telse:\n\t\t\t\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\treturn [self.absorb_one(x) for x in args]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb(result[i])\n\t\treturn result\n\n\tdef absorb_first(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb_first(result[i])\n\t\treturn result\n\n\tdef absorb_last(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb_last(result[i])\n\t\treturn result\n\n\tdef absorb_first_last(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions applying absorption.\n\t\"\"\"\n\t\tresult = self.list[:]\n\t\tfor i, a in enumerate(args):\n\t\t\tresult[i] = a.absorb_first_last(result[i])\n\t\treturn result\n\n\tdef absorb_first_last(self, args):", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\t\tres = []\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, Mul) and len(arg.args) == 2 and arg.args[0] == arg.args[1]:\n\t\t\t\tres.append(arg.args[0])\n\t\t\telse:\n\t\t\t\tres.append(arg)\n\t\treturn res\n\t\n\tdef flatten(self, args):\n\t\t\"\"\"\n\t\tGiven an `args` sequence of expressions, return a new list of expression applying flatten.\n\t\t\"\"\"\n\t\tres = []\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, Add):\n\t\t\t\tres.extend(arg.args)\n\t\t\telse:\n\t\t\t\tres.append(arg)\n\t\treturn res\n\t\n\tdef _eval_simplify(self, expr):\n\t\t\"\"\"\n\t\tSimplify an expression.\n\t\t\"\"\"\n\t\texpr = self.remove_abs_mul(expr)\n\t\texpr = self.simplify_abs(expr)\n\t\texpr = self.add_mul_one(expr)\n\t\texpr = self.simplify_abs(expr)\n\t\texpr = self.remove_zero(expr)\n\t\texpr = self.remove_one(expr)\n\t\texpr = self.remove_zero(expr)\n\t\texpr = self.remove_one(expr)\n\t\texpr = self.flatten(expr)\n\t\texpr = self.remove_mul_one", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tself.registry[hook] += [func]\n\t\t\treturn func\n\t\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tif hook not in self.hooks:\n\t\t\t\tself.hooks[hook] = []\n\t\t\tself.hooks[hook].append(func)\n\t\treturn wrapper\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(function):\n\t\t\tself.__add_hook(hook, function)\n\t\t\treturn function\n\t\treturn wrapper\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tt(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tGet the class of the target, that is, the type of the object.\n\t\t\"\"\"\n\t\t/pypi_install_script/tornado-3.2.1.tar/setup.py\n# uncompyle6 version 3.7.4\n# Python bytecode 2.7 (62211)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: /home/fapinski/source/tornadoweb/tornado/setup.py\n# Compiled at: 2014-10-27 17:07:21\nimport os\nimport sys\nimport time\nimport subprocess\nimport setuptools\nfrom distutils import log\nfrom distutils.core import setup\nfrom distutils.command.build import build\nfrom distutils.command.install import install\nfrom distutils.command.sdist import sdist\nfrom distutils.command.install_data import install_data\nfrom distutils.command.clean import clean\nfrom distutils.command.build_ext import build_ext\nfrom distutils.extension import Extension\nfrom distutils", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tself.add_hook(hook, handler)\n\t\t\treturn handler\n\t\treturn wrapper\n\t\n\t\n\tdef off(self, hook):\n\t\"\"\"\n\tReturn a decorator function to remove a handler from the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tself.remove_hook(hook, handler)\n\t\t\treturn handler\n\t\treturn wrapper\n\t\n\t\n\tdef once(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\tThe handler is only executed once.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tself.add_hook(hook, handler, once=True)\n\t\t\treturn handler\n\t\treturn wrapper\n\t\n\t\n\tdef get_hooks(self):\n\t\"\"\"\n\tReturn a dictionary of all hooks in the registry in the class.\n\t\"\"\"\n\t\treturn self.hooks\n\t\n\t\n\tdef get_hook_handlers(self, hook):\n\t\"\"\"\n\tReturn a list of handlers for the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\treturn self.hooks.get(hook, [])\n\t\n\t\n\tdef add_hook(self, hook, handler, once=False):\n\t\"\"\"\n\tAdd a new handler for the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tif hook not", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\"\"\"\n\t\t\tAdd the decorated function to the registry in the class.\n\t\t\t\"\"\"\n\t\t\t\tregistry = getattr(self, '_registry', {})\n\t\t\t\tregistry[hook] = registry.get(hook, []) + [func]\n\t\t\t\tsetattr(self, '_registry', registry)\n\t\t\t\treturn func\n\t\t\treturn decorator\n\t\treturn self.on\n\n\tdef off(self, hook):\n\t\"\"\"\n\tReturn a decorator function to remove a handler from the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\"\"\"\n\t\t\tRemove the decorated function from the registry in the class.\n\t\t\t\"\"\"\n\t\t\t\tregistry = getattr(self, '_registry', {})\n\t\t\t\tregistry[hook] = [f for f in registry[hook] if f!= func]\n\t\t\t\tsetattr(self, '_registry', registry)\n\t\t\t\treturn func\n\t\t\treturn decorator\n\t\treturn self.off\n\n\tdef emit(self, hook, *args, **kwargs):\n\t\"\"\"\n\tCall the functions in the registry for the \"hook\" in the class.\n\t\"\"\"\n\t\tregistry = getattr(self, '_registry', {})\n\t\tfor func in registry.get(hook, []):\n\t\t\tfunc(*args, **kwargs)\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tself.registry.setdefault(hook, []).append(func)\n\t\t\treturn func\n\t\treturn decorator\n\n\tdef off(self, hook):\n\t\"\"\"\n\tReturn a decorator function to remove a handler from the registry.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tself.registry[hook].remove(func)\n\t\t\treturn func\n\t\treturn decorator\n\n\tdef trigger(self, hook, **kwargs):\n\t\"\"\"\n\tCall all the handlers for a given hook.\n\t\"\"\"\n\t\tfor func in self.registry.get(hook, []):\n\t\t\tfunc(**kwargs)\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef add_handler(f):\n\t\t\tself.registry.add(hook, f)\n\t\t\treturn f\n\t\treturn add_handler\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(fn):\n\t\t\tself.registry[hook].append(fn)\n\t\t\treturn fn\n\t\treturn decorator\n\n\tdef off(self, hook):\n\t\"\"\"\n\tRemove a handler from the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\treturn self.registry.pop(hook)\n\n\tdef call(self, hook, *args, **kwargs):\n\t\"\"\"\n\tCall all handlers for a given hook.\n\t\"\"\"\n\t\tif not self.registry:\n\t\t\treturn\n\t\tfor fn in self.registry[hook]:\n\t\t\tfn(*args, **kwargs)\n/README.md\n# python-hook\na simple hook implementation in python\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(handler):\n\t\t\tself.registry[hook].append(handler)\n\t\t\treturn handler\n\t\treturn decorator\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tself.registry[hook] += [func]\n\t\t\treturn func\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tif hook not in self.hooks:\n\t\t\t\tself.hooks[hook] = []\n\t\t\tself.hooks[hook].append(func)\n\t\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(function):\n\t\t\tself.__add_hook(hook, function)\n\t\t\treturn function\n\t\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tt(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tGet the class of the target, that is, the type of the object.\n\t\t\"\"\"\n\t\t/pypi_install_script/tornado-3.2.1.tar/setup.py\n# uncompyle6 version 3.7.4\n# Python bytecode 2.7 (62211)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: /home/fapinski/source/tornadoweb/tornado/setup.py\n# Compiled at: 2014-10-27 17:07:21\nimport os\nimport sys\nimport time\nimport subprocess\nimport setuptools\nfrom distutils import log\nfrom distutils.core import setup\nfrom distutils.command.build import build\nfrom distutils.command.install import install\nfrom distutils.command.sdist import sdist\nfrom distutils.command.install_data import install_data\nfrom distutils.command.clean import clean\nfrom distutils.command.build_ext import build_ext\nfrom distutils.extension import Extension\nfrom distutils", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tself.add_hook(hook, handler)\n\t\t\treturn handler\n\t\treturn wrapper\n\t\n\t\n\tdef off(self, hook):\n\t\"\"\"\n\tReturn a decorator function to remove a handler from the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tself.remove_hook(hook, handler)\n\t\t\treturn handler\n\t\treturn wrapper\n\t\n\t\n\tdef once(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\tThe handler is only executed once.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tself.add_hook(hook, handler, once=True)\n\t\t\treturn handler\n\t\treturn wrapper\n\t\n\t\n\tdef get_hooks(self):\n\t\"\"\"\n\tReturn a dictionary of all hooks in the registry in the class.\n\t\"\"\"\n\t\treturn self.hooks\n\t\n\t\n\tdef get_hook_handlers(self, hook):\n\t\"\"\"\n\tReturn a list of handlers for the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\treturn self.hooks.get(hook, [])\n\t\n\t\n\tdef add_hook(self, hook, handler, once=False):\n\t\"\"\"\n\tAdd a new handler for the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tif hook not", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\"\"\"\n\t\t\tAdd the decorated function to the registry in the class.\n\t\t\t\"\"\"\n\t\t\t\tregistry = getattr(self, '_registry', {})\n\t\t\t\tregistry[hook] = registry.get(hook, []) + [func]\n\t\t\t\tsetattr(self, '_registry', registry)\n\t\t\t\treturn func\n\t\t\treturn decorator\n\t\treturn self.on\n\n\tdef off(self, hook):\n\t\"\"\"\n\tReturn a decorator function to remove a handler from the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\t\"\"\"\n\t\t\tRemove the decorated function from the registry in the class.\n\t\t\t\"\"\"\n\t\t\t\tregistry = getattr(self, '_registry', {})\n\t\t\t\tregistry[hook] = [f for f in registry[hook] if f!= func]\n\t\t\t\tsetattr(self, '_registry', registry)\n\t\t\t\treturn func\n\t\t\treturn decorator\n\t\treturn self.off\n\n\tdef emit(self, hook, *args, **kwargs):\n\t\"\"\"\n\tCall the functions in the registry for the \"hook\" in the class.\n\t\"\"\"\n\t\tregistry = getattr(self, '_registry', {})\n\t\tfor func in registry.get(hook, []):\n\t\t\tfunc(*args, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tself.registry.setdefault(hook, []).append(func)\n\t\t\treturn func\n\t\treturn decorator\n\n\tdef off(self, hook):\n\t\"\"\"\n\tReturn a decorator function to remove a handler from the registry.\n\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tself.registry[hook].remove(func)\n\t\t\treturn func\n\t\treturn decorator\n\n\tdef trigger(self, hook, **kwargs):\n\t\"\"\"\n\tCall all the handlers for a given hook.\n\t\"\"\"\n\t\tfor func in self.registry.get(hook, []):\n\t\t\tfunc(**kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef add_handler(f):\n\t\t\tself.registry.add(hook, f)\n\t\t\treturn f\n\t\treturn add_handler\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(fn):\n\t\t\tself.registry[hook].append(fn)\n\t\t\treturn fn\n\t\treturn decorator\n\n\tdef off(self, hook):\n\t\"\"\"\n\tRemove a handler from the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\treturn self.registry.pop(hook)\n\n\tdef call(self, hook, *args, **kwargs):\n\t\"\"\"\n\tCall all handlers for a given hook.\n\t\"\"\"\n\t\tif not self.registry:\n\t\t\treturn\n\t\tfor fn in self.registry[hook]:\n\t\t\tfn(*args, **kwargs)\n/README.md\n# python-hook\na simple hook implementation in python\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\t\tdef decorator(handler):\n\t\t\tself.registry[hook].append(handler)\n\t\t\treturn handler\n\t\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"enabled\": False\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"type\": \"db\",\n\t\t\t\t\"db\": {\n\t\t\t\t\t\"driver\": \"mysql\",\n\t\t\t\t\t\"host\": \"localhost\",\n\t\t\t\t\t\"port\": 3306,\n\t\t\t\t\t\"database\": \"kong\",\n\t\t\t\t\t\"username\": user,\n\t\t\t\t\t\"password\": \"\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"type\": \"db\",\n\t\t\t\t\"db\": {\n\t\t\t\t\t\"driver\": \"mysql\",\n\t\t\t\t\t\"host\": \"localhost\",\n\t\t\t\t\t\"port\": 3306,\n\t\t\t\t\t\"database\": \"kong\",\n\t\t\t\t\t\"username\": user,\n\t\t\t\t\t\"password\": \"\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"ip\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"path\": \"/docs\",\n\t\t\t\t\"path_by_tag\": True\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"info\",\n\t\t\t\t\"console\": True,\n\t\t\t\t\"path\": \"/tmp/kong.log\",\n\t\t\t\t\"format\": \"%{time} %{level} %{message}\"\n\t\t\t}\n\t", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"log\": {\n\t\t\t\"path\": \"\",\n\t\t\t\"name\": \"\",\n\t\t\t\"level\": \"\",\n\t\t\t\"format\": \"\",\n\t\t\t\"console\": True,\n\t\t\t\"stream\": True,\n\t\t\t\"file\": True\n\t\t}\n\t}\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t \n\treturn {\n\t\t\"tls\": {\n\t\t\t\"certificate-authority-data\": CA_DATA,\n\t\t\t\"client-certificate-data\": user_cert_data(user),\n\t\t\t\"client-key-data\": user_key_data(user)\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"Certificate\",\n\t\t\t\"certificate-authority-data\": CA_DATA\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"mode\": \"AlwaysAllow\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"servers\": [\n\t\t\t\t\"https://{}:{}\".format(etcd_host, etcd_port)\n\t\t\t],\n\t\t\t\"tls-server-name\": \"etcd.service.consul\",\n\t\t\t\"tls-ca-file\": CA_FILE\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"base-url\": \"https://docs.portworx.com\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\"\n\t\t}\n\t}\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"cert\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"certs/server.pem\"),\n\t\t\t\t\"key\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"certs/server-key.pem\"),\n\t\t\t\t\"ca_cert\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"certs/ca.pem\")\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"type\": \"basic\",\n\t\t\t\t\"basic\": {\n\t\t\t\t\t\"users\": [user]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"type\": \"rbac\",\n\t\t\t\t\"rbac\": {\n\t\t\t\t\t\"users\": [user],\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"version\": True,\n\t\t\t\t\"info\": True,\n\t\t\t\t\"swagger\": True,\n\t\t\t\t\"ui\": True,\n\t\t\t\t\"ui_path\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"swaggerui\")\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"INFO\",\n\t\t\t\t\"format\": \"%(", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"enabled\": False,\n\t\t\t\t\"cert\": os.path.join(user.home, \".kube\", \"certs\", \"cert.pem\"),\n\t\t\t\t\"key\": os.path.join(user.home, \".kube\", \"certs\", \"key.pem\")\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"strategy\": \"x509\",\n\t\t\t\t\"jwt-secret-name\": \"default-token-secret\",\n\t\t\t\t\"requestheader-client-ca-file\": os.path.join(user.home, \".kube\", \"certs\", \"ca.pem\")\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"mode\": \"AlwaysAllow\",\n\t\t\t\t\"webhook-config-file\": \"\"\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"2\"\n\t\t\t}\n\t\t}\n\n\t\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\treturn {\n\t\t\t\"tls\": {\n\t\t\t\t\"enabled\": False,\n\t\t\t\t\"key\": \"/path/to/key\",\n\t\t\t\t\"cert\": \"/path/to/cert\",\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"user\": user,\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port,\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"DEBUG\",\n\t\t\t\t\"file\": \"access.log\",\n\t\t\t},\n\t\t}\n\t\t\nif __name__ == \"__main__\":\n\tmain()\n```\n\n## 4.2.2 \u8fd0\u884c\n\n```\npython3 main.py\n```\n\n/docs/01.\u8fd0\u7ef4/17.Nginx/03.Nginx \u90e8\u7f72.md\n---\ntitle: Nginx \u90e8\u7f72\ndate: 2021-03-15 16:40:39\npermalink: /pages/f7e996/\ncategories:\n  - \u8fd0\u7ef4\n  - Nginx\ntags:\n  - \n---\n\n## 1. \u4e0b\u8f7d Nginx\n\n### 1.1. ", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t    \"tls\": {\n\t\t        \"cert_file\": \"client.crt\",\n\t\t        \"key_file\": \"client.key\",\n\t\t        \"trusted_ca_file\": \"ca.crt\"\n\t\t    },\n\t\t    \"authentication\": {\n\t\t        \"type\": \"token\",\n\t\t        \"token_file\": \"token.txt\"\n\t\t    },\n\t\t    \"authorization\": {\n\t\t        \"type\": \"simple\",\n\t\t        \"permissions\": [\n\t\t            {\"user\": user, \"role\": \"admin\"}\n\t\t        ]\n\t\t    },\n\t\t    \"etcd\": {\n\t\t        \"host\": etcd_host,\n\t\t        \"port\": etcd_port\n\t\t    },\n\t\t    \"docs\": {\n\t\t        \"info\": {\n\t\t            \"title\": \"My API\",\n\t\t            \"description\": \"This is my API\"\n\t\t        },\n\t\t        \"swagger\": \"2.0\",\n\t\t        \"ui\": False\n\t\t    },\n\t\t    \"log\": {\n\t\t        \"level\": \"DEBUG\",\n\t\t        \"handlers\": [\n\t\t            {\n\t\t                \"type\": \"console\",\n\t\t                \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\t\t            }\n\t\t        ]\n\t\t    }\n\t\t}\n\t\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t \n\tconfig = {\n\t\t\"tls\": base_tls_config(user),\n\t\t\"authentication\": base_authentication_config(user),\n\t\t\"authorization\": base_authorization_config(user),\n\t\t\"etcd\": base_etcd_config(etcd_host, etcd_port)\n\t}\n\n\tif user == \"k8s\":\n\t\tconfig[\"docs\"] = base_docs_config(user)\n\n\treturn config\n/test/test_etcd_config.py\nimport unittest\nfrom unittest.mock import MagicMock\n\nfrom.base import base_config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tlogger.info(\"Creating configuration...\")\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"cert\": \"/Users/jimmywang/Desktop/certs/etcd/server.crt\",\n\t\t\t\t\"key\": \"/Users/jimmywang/Desktop/certs/etcd/server.key\",\n\t\t\t\t\"ca\": \"/Users/jimmywang/Desktop/certs/etcd/ca.crt\"\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"strategy\": \"x509\",\n\t\t\t\t\"trust_ca_file\": \"/Users/jimmywang/Desktop/certs/etcd/ca.crt\"\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"permission\": \"admin\"\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port,\n\t\t\t\t\"prefix\": \"etcdctl\"\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"host\": \"localhost\",\n\t\t\t\t\"port\": 8080\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"info\"\n\t\t\t}\n\t\t}\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\n\n\tconfig = {}\n\n\t# The TLS configuration\n\tconfig[\"tls\"] = {}\n\tconfig[\"tls\"][\"certificate\"] = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"server.crt\")\n\tconfig[\"tls\"][\"key\"] = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"server.key\")\n\n\t# The authentication configuration\n\tconfig[\"authentication\"] = {}\n\tconfig[\"authentication\"][\"provider\"] = \"x509\"\n\tconfig[\"authentication\"][\"client_ca_file\"] = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"ca.crt\")\n\n\t# The authorization configuration\n\tconfig[\"authorization\"] = {}\n\tconfig[\"authorization\"][\"mode\"] = \"unrestricted\"\n\n\t# The etcd configuration\n\tconfig[\"etcd\"] = {}\n\tconfig[\"etcd\"][\"host\"] = etcd_host\n\tconfig[\"etcd\"][\"port\"] = etcd_port\n\n\t# The documentation configuration\n\tconfig[\"docs\"] = {}\n\tconfig[\"docs\"][\"version\"] = \"1.0\"\n\tconfig[\"docs\"][\"info\"] = {\n\t\t\"title\" : \"Pet Store\",\n\t\t\"description\" : \"This is a sample server Petstore server.  You can find out more about Swagger at <a href=\\\"http"], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"enabled\": False\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"type\": \"db\",\n\t\t\t\t\"db\": {\n\t\t\t\t\t\"driver\": \"mysql\",\n\t\t\t\t\t\"host\": \"localhost\",\n\t\t\t\t\t\"port\": 3306,\n\t\t\t\t\t\"database\": \"kong\",\n\t\t\t\t\t\"username\": user,\n\t\t\t\t\t\"password\": \"\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"type\": \"db\",\n\t\t\t\t\"db\": {\n\t\t\t\t\t\"driver\": \"mysql\",\n\t\t\t\t\t\"host\": \"localhost\",\n\t\t\t\t\t\"port\": 3306,\n\t\t\t\t\t\"database\": \"kong\",\n\t\t\t\t\t\"username\": user,\n\t\t\t\t\t\"password\": \"\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"ip\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"path\": \"/docs\",\n\t\t\t\t\"path_by_tag\": True\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"info\",\n\t\t\t\t\"console\": True,\n\t\t\t\t\"path\": \"/tmp/kong.log\",\n\t\t\t\t\"format\": \"%{time} %{level} %{message}\"\n\t\t\t}\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"key\": \"\",\n\t\t\t\"cert\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"verify\": True\n\t\t},\n\t\t\"log\": {\n\t\t\t\"path\": \"\",\n\t\t\t\"name\": \"\",\n\t\t\t\"level\": \"\",\n\t\t\t\"format\": \"\",\n\t\t\t\"console\": True,\n\t\t\t\"stream\": True,\n\t\t\t\"file\": True\n\t\t}\n\t}\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t \n\treturn {\n\t\t\"tls\": {\n\t\t\t\"certificate-authority-data\": CA_DATA,\n\t\t\t\"client-certificate-data\": user_cert_data(user),\n\t\t\t\"client-key-data\": user_key_data(user)\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"Certificate\",\n\t\t\t\"certificate-authority-data\": CA_DATA\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"mode\": \"AlwaysAllow\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"servers\": [\n\t\t\t\t\"https://{}:{}\".format(etcd_host, etcd_port)\n\t\t\t],\n\t\t\t\"tls-server-name\": \"etcd.service.consul\",\n\t\t\t\"tls-ca-file\": CA_FILE\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"base-url\": \"https://docs.portworx.com\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\"\n\t\t}\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"cert\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"certs/server.pem\"),\n\t\t\t\t\"key\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"certs/server-key.pem\"),\n\t\t\t\t\"ca_cert\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"certs/ca.pem\")\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"type\": \"basic\",\n\t\t\t\t\"basic\": {\n\t\t\t\t\t\"users\": [user]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"type\": \"rbac\",\n\t\t\t\t\"rbac\": {\n\t\t\t\t\t\"users\": [user],\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"version\": True,\n\t\t\t\t\"info\": True,\n\t\t\t\t\"swagger\": True,\n\t\t\t\t\"ui\": True,\n\t\t\t\t\"ui_path\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"swaggerui\")\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"INFO\",\n\t\t\t\t\"format\": \"%(", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"enabled\": False,\n\t\t\t\t\"cert\": os.path.join(user.home, \".kube\", \"certs\", \"cert.pem\"),\n\t\t\t\t\"key\": os.path.join(user.home, \".kube\", \"certs\", \"key.pem\")\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"strategy\": \"x509\",\n\t\t\t\t\"jwt-secret-name\": \"default-token-secret\",\n\t\t\t\t\"requestheader-client-ca-file\": os.path.join(user.home, \".kube\", \"certs\", \"ca.pem\")\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"mode\": \"AlwaysAllow\",\n\t\t\t\t\"webhook-config-file\": \"\"\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"2\"\n\t\t\t}\n\t\t}\n\n\t\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\treturn {\n\t\t\t\"tls\": {\n\t\t\t\t\"enabled\": False,\n\t\t\t\t\"key\": \"/path/to/key\",\n\t\t\t\t\"cert\": \"/path/to/cert\",\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"user\": user,\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port,\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"DEBUG\",\n\t\t\t\t\"file\": \"access.log\",\n\t\t\t},\n\t\t}\n\t\t\nif __name__ == \"__main__\":\n\tmain()\n```\n\n## 4.2.2 \u8fd0\u884c\n\n```\npython3 main.py\n```\n\n/docs/01.\u8fd0\u7ef4/17.Nginx/03.Nginx \u90e8\u7f72.md\n---\ntitle: Nginx \u90e8\u7f72\ndate: 2021-03-15 16:40:39\npermalink: /pages/f7e996/\ncategories:\n  - \u8fd0\u7ef4\n  - Nginx\ntags:\n  - \n---\n\n## 1. \u4e0b\u8f7d Nginx\n\n### 1.1. ", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tconfig = {\n\t\t    \"tls\": {\n\t\t        \"cert_file\": \"client.crt\",\n\t\t        \"key_file\": \"client.key\",\n\t\t        \"trusted_ca_file\": \"ca.crt\"\n\t\t    },\n\t\t    \"authentication\": {\n\t\t        \"type\": \"token\",\n\t\t        \"token_file\": \"token.txt\"\n\t\t    },\n\t\t    \"authorization\": {\n\t\t        \"type\": \"simple\",\n\t\t        \"permissions\": [\n\t\t            {\"user\": user, \"role\": \"admin\"}\n\t\t        ]\n\t\t    },\n\t\t    \"etcd\": {\n\t\t        \"host\": etcd_host,\n\t\t        \"port\": etcd_port\n\t\t    },\n\t\t    \"docs\": {\n\t\t        \"info\": {\n\t\t            \"title\": \"My API\",\n\t\t            \"description\": \"This is my API\"\n\t\t        },\n\t\t        \"swagger\": \"2.0\",\n\t\t        \"ui\": False\n\t\t    },\n\t\t    \"log\": {\n\t\t        \"level\": \"DEBUG\",\n\t\t        \"handlers\": [\n\t\t            {\n\t\t                \"type\": \"console\",\n\t\t                \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\t\t            }\n\t\t        ]\n\t\t    }\n\t\t}\n\t\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t \n\tconfig = {\n\t\t\"tls\": base_tls_config(user),\n\t\t\"authentication\": base_authentication_config(user),\n\t\t\"authorization\": base_authorization_config(user),\n\t\t\"etcd\": base_etcd_config(etcd_host, etcd_port)\n\t}\n\n\tif user == \"k8s\":\n\t\tconfig[\"docs\"] = base_docs_config(user)\n\n\treturn config\n/test/test_etcd_config.py\nimport unittest\nfrom unittest.mock import MagicMock\n\nfrom.base import base_config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\tlogger.info(\"Creating configuration...\")\n\t\tconfig = {\n\t\t\t\"tls\": {\n\t\t\t\t\"cert\": \"/Users/jimmywang/Desktop/certs/etcd/server.crt\",\n\t\t\t\t\"key\": \"/Users/jimmywang/Desktop/certs/etcd/server.key\",\n\t\t\t\t\"ca\": \"/Users/jimmywang/Desktop/certs/etcd/ca.crt\"\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"strategy\": \"x509\",\n\t\t\t\t\"trust_ca_file\": \"/Users/jimmywang/Desktop/certs/etcd/ca.crt\"\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"permission\": \"admin\"\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port,\n\t\t\t\t\"prefix\": \"etcdctl\"\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"host\": \"localhost\",\n\t\t\t\t\"port\": 8080\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"info\"\n\t\t\t}\n\t\t}\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\n\n\tconfig = {}\n\n\t# The TLS configuration\n\tconfig[\"tls\"] = {}\n\tconfig[\"tls\"][\"certificate\"] = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"server.crt\")\n\tconfig[\"tls\"][\"key\"] = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"server.key\")\n\n\t# The authentication configuration\n\tconfig[\"authentication\"] = {}\n\tconfig[\"authentication\"][\"provider\"] = \"x509\"\n\tconfig[\"authentication\"][\"client_ca_file\"] = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"ca.crt\")\n\n\t# The authorization configuration\n\tconfig[\"authorization\"] = {}\n\tconfig[\"authorization\"][\"mode\"] = \"unrestricted\"\n\n\t# The etcd configuration\n\tconfig[\"etcd\"] = {}\n\tconfig[\"etcd\"][\"host\"] = etcd_host\n\tconfig[\"etcd\"][\"port\"] = etcd_port\n\n\t# The documentation configuration\n\tconfig[\"docs\"] = {}\n\tconfig[\"docs\"][\"version\"] = \"1.0\"\n\tconfig[\"docs\"][\"info\"] = {\n\t\t\"title\" : \"Pet Store\",\n\t\t\"description\" : \"This is a sample server Petstore server.  You can find out more about Swagger at <a href=\\\"http", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\ttz = self.tzinfo\n\t\tif tz is None:\n\t\t\treturn dt\n\t\tdt = tz.normalize(dt)\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\treturn tz.fromutc(dt)\n\t\n\tdef _toutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in UTC.\n\t\"\"\"\n\t\ttz = self.tzinfo\n\t\tif tz is None:\n\t\t\treturn dt\n\t\tdt = tz.normalize(dt)\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\treturn tz.fromutc(dt)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\t# We can't use.astimezone() since it only works on naive datetimes.\n\t\tdt = dt.replace(tzinfo=self)\n\t\treturn dt.astimezone(self.other)\n\n```\n\nSo I'm not sure what is going on here.\n\nI'm using Python 3.3.2\n\nThanks for any help!username_1: I got something weird when I tried it:\n\n```\n>>> import pytz\n>>> tz1 = pytz.timezone('US/Central')\n>>> tz2 = pytz.timezone('US/Mountain')\n>>> tz1.name == 'US/Central'\nTrue\n>>> tz2.name == 'US/Mountain'\nTrue\n>>> tz1.other == tz2\nTrue\n>>> tz2.other == tz1\nTrue\n\n```\n\nSo I would say that `pytz.timezone('US/Mountain').other` is probably `pytz.timezone('US/Central')`.\nUpvotes: 2 username_2: It appears that the `timezone` objects are immutable, so you can't change a `timezone` object's `other` attribute.\n\nFrom the [docs](http://pytz.sourceforge.net/#timezone):\n\n> \n> Timezone objects are immutable, so you can't change a timezone object's other attribute", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tif not isinstance(dt, datetime):\n\t\t\traise TypeError('Expected a datetime')\n\t\tif not isinstance(self, timezone):\n\t\t\traise TypeError('Expected a timezone')\n\n\t\tdt_local = dt.replace(tzinfo=self)\n\t\tdt_utc = dt_local.astimezone(pytz.utc)\n\t\treturn dt_utc\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\n        new_tz = timezone(self.tz)\n        new_dt = new_tz.localize(dt)\n        return new_dt.astimezone(timezone(self.new_tz))\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tif dt is None: return None\n\t\treturn dt.astimezone(self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=self)\n\tdef _tolocal(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a new timezone, calculates a timezone datetime in the original timezone.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=tz.tzutc())\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tlocal_dt = dt.astimezone(pytz.timezone(self.timezone))\n\t\treturn local_dt.replace(tzinfo=pytz.timezone(self.timezone))\n/README.md\n# Timezone-converter\nPython script to convert timezone from one timezone to another.\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\treturn self.__class__(dt.astimezone(self), self)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\treturn dt + self.utc(dt)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError('Timezone datetime required')\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError('Timezone datetime required')\n\t\tfrom_tz = dt.tzinfo\n\t\tto_tz   = self._tz\n\t\tfrom_offset = from_tz.utcoffset(dt)\n\t\tto_offset = to_tz.utcoffset(dt)\n\t\tfrom_dst = from_tz.dst(dt)\n\t\tto_dst = to_tz.dst(dt)\n\t\tif from_offset is None or to_offset is None:\n\t\t\traise ValueError('Timezone datetime required')\n\t\toffset = from_offset - to_offset\n\t\tif from_dst!= to_dst:\n\t\t\toffset -= 1\n\t\treturn dt - offset\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\ttz = self.tzinfo\n\t\tif tz is None:\n\t\t\treturn dt\n\t\tdt = tz.normalize(dt)\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\treturn tz.fromutc(dt)\n\t\n\tdef _toutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in UTC.\n\t\"\"\"\n\t\ttz = self.tzinfo\n\t\tif tz is None:\n\t\t\treturn dt\n\t\tdt = tz.normalize(dt)\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\treturn tz.fromutc(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\t# We can't use.astimezone() since it only works on naive datetimes.\n\t\tdt = dt.replace(tzinfo=self)\n\t\treturn dt.astimezone(self.other)\n\n```\n\nSo I'm not sure what is going on here.\n\nI'm using Python 3.3.2\n\nThanks for any help!username_1: I got something weird when I tried it:\n\n```\n>>> import pytz\n>>> tz1 = pytz.timezone('US/Central')\n>>> tz2 = pytz.timezone('US/Mountain')\n>>> tz1.name == 'US/Central'\nTrue\n>>> tz2.name == 'US/Mountain'\nTrue\n>>> tz1.other == tz2\nTrue\n>>> tz2.other == tz1\nTrue\n\n```\n\nSo I would say that `pytz.timezone('US/Mountain').other` is probably `pytz.timezone('US/Central')`.\nUpvotes: 2 username_2: It appears that the `timezone` objects are immutable, so you can't change a `timezone` object's `other` attribute.\n\nFrom the [docs](http://pytz.sourceforge.net/#timezone):\n\n> \n> Timezone objects are immutable, so you can't change a timezone object's other attribute", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tif not isinstance(dt, datetime):\n\t\t\traise TypeError('Expected a datetime')\n\t\tif not isinstance(self, timezone):\n\t\t\traise TypeError('Expected a timezone')\n\n\t\tdt_local = dt.replace(tzinfo=self)\n\t\tdt_utc = dt_local.astimezone(pytz.utc)\n\t\treturn dt_utc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\n        new_tz = timezone(self.tz)\n        new_dt = new_tz.localize(dt)\n        return new_dt.astimezone(timezone(self.new_tz))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tif dt is None: return None\n\t\treturn dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=self)\n\tdef _tolocal(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a new timezone, calculates a timezone datetime in the original timezone.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=tz.tzutc())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tlocal_dt = dt.astimezone(pytz.timezone(self.timezone))\n\t\treturn local_dt.replace(tzinfo=pytz.timezone(self.timezone))\n/README.md\n# Timezone-converter\nPython script to convert timezone from one timezone to another.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\treturn self.__class__(dt.astimezone(self), self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\treturn dt + self.utc(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError('Timezone datetime required')\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError('Timezone datetime required')\n\t\tfrom_tz = dt.tzinfo\n\t\tto_tz   = self._tz\n\t\tfrom_offset = from_tz.utcoffset(dt)\n\t\tto_offset = to_tz.utcoffset(dt)\n\t\tfrom_dst = from_tz.dst(dt)\n\t\tto_dst = to_tz.dst(dt)\n\t\tif from_offset is None or to_offset is None:\n\t\t\traise ValueError('Timezone datetime required')\n\t\toffset = from_offset - to_offset\n\t\tif from_dst!= to_dst:\n\t\t\toffset -= 1\n\t\treturn dt - offset\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\ttime = self.time\n\t\tself.time = [time[0] / 60, time[1] / 60, time[2] / 60]\n\t\treturn self.time\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns a string representation of a time.\n\t\t\"\"\"\n\t\tnorm = self.normalized()\n\t\treturn \"%s:%s:%s\" % (norm[0], norm[1], norm[2])\n\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\tReturns True if two times are equal, False otherwise.\n\t\t\"\"\"\n\t\treturn self.time == other.time\n\n\tdef __lt__(self, other):\n\t\t\"\"\"\n\t\tReturns True if the first time is less than the second, False otherwise.\n\t\t\"\"\"\n\t\treturn self.time < other.time\n\n\tdef __sub__(self, other):\n\t\t\"\"\"\n\t\tReturns the difference between two times.\n\t\t\"\"\"\n\t\treturn Time(self.time[0] - other.time[0], self.time[1] - other.time[1], self.time[2] - other.time[2])\n\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tfor key in self.units:\n\t\t\tself.units[key] = int(self.units[key])\n\t\treturn self\n\tdef __repr__(self):\n\t\treturn str(self.units)\n\tdef __str__(self):\n\t\treturn \"Time: \" + repr(self)\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, Time):\n\t\t\traise TypeError(\"Cannot compare Time object to \" + str(type(other)))\n\t\treturn self.units == other.units\n\tdef __ne__(self, other):\n\t\treturn not (self == other)\n\tdef __gt__(self, other):\n\t\tif not isinstance(other, Time):\n\t\t\traise TypeError(\"Cannot compare Time object to \" + str(type(other)))\n\t\tif self.units == other.units:\n\t\t\treturn False\n\t\tfor key in self.units:\n\t\t\tif self.units[key] > other.units[key]:\n\t\t\t\treturn True\n\t\t\telif self.units[key] < other.units[key]:\n\t\t\t\treturn False\n\t\treturn True\n\tdef __ge__(self, other):\n\t\treturn self > other or self == other\n\tdef __lt__(self, other):\n\t\treturn not (self >= other)\n\tdef __le__(self, other):\n\t\treturn not (self > other)\n\tdef", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t1 #TODO\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tfor attribute in dir(self):\n\t\t\tvalue = getattr(self, attribute)\n\t\t\ttry:\n\t\t\t\tif len(value) == 2:\n\t\t\t\t\tsetattr(self, attribute, int(value[0]))\n\t\t\t\telif len(value) == 4:\n\t\t\t\t\tsetattr(self, attribute, int(value[0:2]))\n\t\t\t\telif len(value) == 6:\n\t\t\t\t\tsetattr(self, attribute, int(value[0:2]))\n\t\t\texcept:\n\t\t\t\tprint(\"Failed to normalize time unit {}\".format(attribute))\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\treturn self.replace(years=self.years,months=self.months,days=self.days,hours=self.hours,minutes=self.minutes,seconds=self.seconds)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\n        # Find the largest unit of time.\n\tmax_unit = 0\n\tmax_val  = 0\n\tfor unit, val in self.units.items():\n\t\tif val > max_val:\n\t\t\tmax_unit = unit\n\t\t\tmax_val  = val\n\n\t# Normalize everything by the largest unit of time.\n\tfor unit, val in self.units.items():\n\t\tself.units[unit] = val / max_val\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tself.seconds = int(self.seconds)\n\t\tself.minutes = int(self.minutes)\n\t\tself.hours = int(self.hours)\n\t\tself.days = int(self.days)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tself.seconds = int(self.seconds)\n\t\tself.minutes = int(self.minutes)\n\t\tself.hours = int(self.hours)\n\t\tself.days = int(self.days)\n\t\tself.weeks = int(self.weeks)\n\t\tself.months = int(self.months)\n\t\tself.years = int(self.years)\n\n@property", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tif self._normalized:\n\t\t\treturn self\n\n\t\tself._normalized = True\n\n\t\t# Normalize years\n\t\tif self._years:\n\t\t\tif self._months:\n\t\t\t\tself._years += self._months // 12\n\t\t\t\tself._months %= 12\n\t\t\tif self._days:\n\t\t\t\tself._years += self._days // 365\n\t\t\t\tself._days %= 365\n\t\t\tif self._hours:\n\t\t\t\tself._years += self._hours // 8760\n\t\t\t\tself._hours %= 8760\n\t\t\tif self._minutes:\n\t\t\t\tself._years += self._minutes // 525600\n\t\t\t\tself._minutes %= 525600\n\t\t\tif self._seconds:\n\t\t\t\tself._years += self._seconds // 31536000\n\t\t\t\tself._seconds %= 31536000\n\n\t\t# Normalize months\n\t\tif self._months:\n\t\t\tif self._days:\n\t\t\t\tself._months += self._days // 30\n\t\t\t\tself._days %= 30\n\t\t\tif self._hours:\n\t\t\t\tself._months += self._hours // 720\n\t\t\t\tself._hours %= 720\n\t\t\tif self._minutes:\n\t\t\t\tself._months += self._minutes // 432", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tself.weeks = int(self.weeks)\n\t\tself.days = int(self.days)\n\t\tself.hours = int(self.hours)\n\t\tself.minutes = int(self.minutes)\n\t\tself.seconds = int(self.seconds)\n\n\tdef to_seconds(self):\n\t\t\"\"\"\n\t\tConvert all units of time to seconds.\n\t\t\"\"\"\n\t\treturn self.weeks * 7 * 24 * 60 * 60 + self.days * 24 * 60 * 60 + self.hours * 60 * 60 + self.minutes * 60 + self.seconds\n\n\tdef to_string(self):\n\t\t\"\"\"\n\t\tConvert the object to a string.\n\t\t\"\"\"\n\t\treturn str(self.weeks) + \" week(s), \" + str(self.days) + \" day(s), \" + str(self.hours) + \" hour(s), \" + str(self.minutes) + \" minute(s), \" + str(self.seconds) + \" second(s)\"\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tAllow the object to be printed in a human-readable format.\n\t\t\"\"\"\n\t\treturn self.to_string()\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tAllow the object to be"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\ttime = self.time\n\t\tself.time = [time[0] / 60, time[1] / 60, time[2] / 60]\n\t\treturn self.time\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns a string representation of a time.\n\t\t\"\"\"\n\t\tnorm = self.normalized()\n\t\treturn \"%s:%s:%s\" % (norm[0], norm[1], norm[2])\n\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\tReturns True if two times are equal, False otherwise.\n\t\t\"\"\"\n\t\treturn self.time == other.time\n\n\tdef __lt__(self, other):\n\t\t\"\"\"\n\t\tReturns True if the first time is less than the second, False otherwise.\n\t\t\"\"\"\n\t\treturn self.time < other.time\n\n\tdef __sub__(self, other):\n\t\t\"\"\"\n\t\tReturns the difference between two times.\n\t\t\"\"\"\n\t\treturn Time(self.time[0] - other.time[0], self.time[1] - other.time[1], self.time[2] - other.time[2])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tfor key in self.units:\n\t\t\tself.units[key] = int(self.units[key])\n\t\treturn self\n\tdef __repr__(self):\n\t\treturn str(self.units)\n\tdef __str__(self):\n\t\treturn \"Time: \" + repr(self)\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, Time):\n\t\t\traise TypeError(\"Cannot compare Time object to \" + str(type(other)))\n\t\treturn self.units == other.units\n\tdef __ne__(self, other):\n\t\treturn not (self == other)\n\tdef __gt__(self, other):\n\t\tif not isinstance(other, Time):\n\t\t\traise TypeError(\"Cannot compare Time object to \" + str(type(other)))\n\t\tif self.units == other.units:\n\t\t\treturn False\n\t\tfor key in self.units:\n\t\t\tif self.units[key] > other.units[key]:\n\t\t\t\treturn True\n\t\t\telif self.units[key] < other.units[key]:\n\t\t\t\treturn False\n\t\treturn True\n\tdef __ge__(self, other):\n\t\treturn self > other or self == other\n\tdef __lt__(self, other):\n\t\treturn not (self >= other)\n\tdef __le__(self, other):\n\t\treturn not (self > other)\n\tdef", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t1 #TODO\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tfor attribute in dir(self):\n\t\t\tvalue = getattr(self, attribute)\n\t\t\ttry:\n\t\t\t\tif len(value) == 2:\n\t\t\t\t\tsetattr(self, attribute, int(value[0]))\n\t\t\t\telif len(value) == 4:\n\t\t\t\t\tsetattr(self, attribute, int(value[0:2]))\n\t\t\t\telif len(value) == 6:\n\t\t\t\t\tsetattr(self, attribute, int(value[0:2]))\n\t\t\texcept:\n\t\t\t\tprint(\"Failed to normalize time unit {}\".format(attribute))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\treturn self.replace(years=self.years,months=self.months,days=self.days,hours=self.hours,minutes=self.minutes,seconds=self.seconds)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\n        # Find the largest unit of time.\n\tmax_unit = 0\n\tmax_val  = 0\n\tfor unit, val in self.units.items():\n\t\tif val > max_val:\n\t\t\tmax_unit = unit\n\t\t\tmax_val  = val\n\n\t# Normalize everything by the largest unit of time.\n\tfor unit, val in self.units.items():\n\t\tself.units[unit] = val / max_val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tself.seconds = int(self.seconds)\n\t\tself.minutes = int(self.minutes)\n\t\tself.hours = int(self.hours)\n\t\tself.days = int(self.days)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tself.seconds = int(self.seconds)\n\t\tself.minutes = int(self.minutes)\n\t\tself.hours = int(self.hours)\n\t\tself.days = int(self.days)\n\t\tself.weeks = int(self.weeks)\n\t\tself.months = int(self.months)\n\t\tself.years = int(self.years)\n\n@property", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tif self._normalized:\n\t\t\treturn self\n\n\t\tself._normalized = True\n\n\t\t# Normalize years\n\t\tif self._years:\n\t\t\tif self._months:\n\t\t\t\tself._years += self._months // 12\n\t\t\t\tself._months %= 12\n\t\t\tif self._days:\n\t\t\t\tself._years += self._days // 365\n\t\t\t\tself._days %= 365\n\t\t\tif self._hours:\n\t\t\t\tself._years += self._hours // 8760\n\t\t\t\tself._hours %= 8760\n\t\t\tif self._minutes:\n\t\t\t\tself._years += self._minutes // 525600\n\t\t\t\tself._minutes %= 525600\n\t\t\tif self._seconds:\n\t\t\t\tself._years += self._seconds // 31536000\n\t\t\t\tself._seconds %= 31536000\n\n\t\t# Normalize months\n\t\tif self._months:\n\t\t\tif self._days:\n\t\t\t\tself._months += self._days // 30\n\t\t\t\tself._days %= 30\n\t\t\tif self._hours:\n\t\t\t\tself._months += self._hours // 720\n\t\t\t\tself._hours %= 720\n\t\t\tif self._minutes:\n\t\t\t\tself._months += self._minutes // 432", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\t\tself.weeks = int(self.weeks)\n\t\tself.days = int(self.days)\n\t\tself.hours = int(self.hours)\n\t\tself.minutes = int(self.minutes)\n\t\tself.seconds = int(self.seconds)\n\n\tdef to_seconds(self):\n\t\t\"\"\"\n\t\tConvert all units of time to seconds.\n\t\t\"\"\"\n\t\treturn self.weeks * 7 * 24 * 60 * 60 + self.days * 24 * 60 * 60 + self.hours * 60 * 60 + self.minutes * 60 + self.seconds\n\n\tdef to_string(self):\n\t\t\"\"\"\n\t\tConvert the object to a string.\n\t\t\"\"\"\n\t\treturn str(self.weeks) + \" week(s), \" + str(self.days) + \" day(s), \" + str(self.hours) + \" hour(s), \" + str(self.minutes) + \" minute(s), \" + str(self.seconds) + \" second(s)\"\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tAllow the object to be printed in a human-readable format.\n\t\t\"\"\"\n\t\treturn self.to_string()\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tAllow the object to be", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args):\n\t\treturn namefunc(*args).encode('utf-8')\n\treturn wrapper\n\nif sys.version_info.major == 2:\n\ttzname = tzname_in_python2(tzname)\n\tdst = dst_in_python2(dst)\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tWrapper for namefunc\n\t\t\"\"\"\n\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\t\n\treturn wrapper\n\n\n# --- Timezone-aware datetime ---", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kw):\n\t\trv = namefunc(*args, **kw)\n\t\tif isinstance(rv, unicode):\n\t\t\trv = rv.encode('utf-8')\n\t\treturn rv\n\treturn wrapper\n\nif sys.version_info[0] < 3:\n\t# Python 2\n\ttzname = tzname_in_python2(tzname)\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kw):\n\t\tret = namefunc(*args, **kw)\n\t\tif isinstance(ret, unicode):\n\t\t\treturn ret.encode('utf-8')\n\t\telse:\n\t\t\treturn ret\n\treturn wrapper\n\t\nif pytz.version_info < (2013j,):\n\t# Python 2.x\n\ttzname_in_python2(pytz.timezone.zone_names[0])\n\ttzname_in_python2(pytz.timezone.zone_names[1])\n\ttzname_in_python2(pytz.timezone.zone_names[2])\n\ttzname_in_python2(pytz.timezone.zone_names[3])\n\ttzname_in_python2(pytz.timezone.zone_names[4])\n\ttzname_in_python2(pytz.timezone.zone_names[5])\n\ttzname_in_python2(pytz.timezone.zone_names[6])\n\ttzname_in_python2(pytz.timezone.zone_names[7])\n\ttzname_in_python2(pytz.timezone.zone_names[8])\n\ttzname_in_python2(pytz.timezone.zone_names[9])\n\ttzname_in_python2(", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef with_bytestrings(func):\n\t\tdef with_bytestrings_func(*args, **kwargs):\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tif isinstance(result, tuple):\n\t\t\t\treturn tuple([x.encode('utf8') if isinstance(x, unicode) else x for x in result])\n\t\t\telse:\n\t\t\t\treturn result.encode('utf8') if isinstance(result, unicode) else result\n\t\treturn with_bytestrings_func\n\t \n\treturn with_bytestrings(namefunc)\n\t# return namefunc # for debugging\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kwargs):\n\t\tif six.PY2:\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(*args, **kwargs)\n \n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef namefunc_wrapper(tz):\n\t\treturn namefunc(tz).encode('utf-8')\n\treturn namefunc_wrapper\n/src/tzutil/tests/test_tzutil.py\nimport os\nimport sys\nimport unittest\nimport pytz\n\nfrom tzutil.tzutil import tzname_in_python2\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tif sys.version_info[0] == 2:\n\t\tdef wrapper(tzname):\n\t\t\treturn namefunc(tzname).encode('utf-8')\n\t\treturn wrapper\n\treturn namefunc\n ", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kwargs):\n\t\tres = namefunc(*args, **kwargs)\n\t\tif isinstance(res, unicode):\n\t\t\tres = res.encode('utf-8')\n\t\treturn res\n\treturn wrapper\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(loc, format, tz=None):\n\t\tname = namefunc(loc, format, tz)\n\t\tif isinstance(name, unicode):\n\t\t\tname = name.encode('utf-8')\n\t\treturn name\n\treturn wrapper\n\nif __name__ == '__main__':\n\ttz = pytz.timezone('Europe/London')\n\tprint(tz.tzname(datetime.datetime.now()))\n/README.md\n# pytz-tzname\n\nA small hack to get the tzname in pytz to work with Python 2.\n\nThis is a hack that was put together to solve this problem:\nhttps://github.com/stub42/wagtail/issues/3167\n\nThe hack is in the tzname_in_python2 function, which replaces the original tzname function.\n\nIt's not pretty, but it works for the use case here.\n\n## Usage\n\nSimply copy the tzname_in_python2 function into your project and replace the original tzname function with the new function.\n\nFor example:\n\n```python\nimport pytz\nfrom tzname_in_python2 import tzname_in_python2\n\npytz.timezone = tzname_in_python2(pytz.timezone)\n```\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args):\n\t\treturn namefunc(*args).encode('utf-8')\n\treturn wrapper\n\nif sys.version_info.major == 2:\n\ttzname = tzname_in_python2(tzname)\n\tdst = dst_in_python2(dst)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tWrapper for namefunc\n\t\t\"\"\"\n\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\t\n\treturn wrapper\n\n\n# --- Timezone-aware datetime ---", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kw):\n\t\trv = namefunc(*args, **kw)\n\t\tif isinstance(rv, unicode):\n\t\t\trv = rv.encode('utf-8')\n\t\treturn rv\n\treturn wrapper\n\nif sys.version_info[0] < 3:\n\t# Python 2\n\ttzname = tzname_in_python2(tzname)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kw):\n\t\tret = namefunc(*args, **kw)\n\t\tif isinstance(ret, unicode):\n\t\t\treturn ret.encode('utf-8')\n\t\telse:\n\t\t\treturn ret\n\treturn wrapper\n\t\nif pytz.version_info < (2013j,):\n\t# Python 2.x\n\ttzname_in_python2(pytz.timezone.zone_names[0])\n\ttzname_in_python2(pytz.timezone.zone_names[1])\n\ttzname_in_python2(pytz.timezone.zone_names[2])\n\ttzname_in_python2(pytz.timezone.zone_names[3])\n\ttzname_in_python2(pytz.timezone.zone_names[4])\n\ttzname_in_python2(pytz.timezone.zone_names[5])\n\ttzname_in_python2(pytz.timezone.zone_names[6])\n\ttzname_in_python2(pytz.timezone.zone_names[7])\n\ttzname_in_python2(pytz.timezone.zone_names[8])\n\ttzname_in_python2(pytz.timezone.zone_names[9])\n\ttzname_in_python2(", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef with_bytestrings(func):\n\t\tdef with_bytestrings_func(*args, **kwargs):\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tif isinstance(result, tuple):\n\t\t\t\treturn tuple([x.encode('utf8') if isinstance(x, unicode) else x for x in result])\n\t\t\telse:\n\t\t\t\treturn result.encode('utf8') if isinstance(result, unicode) else result\n\t\treturn with_bytestrings_func\n\t \n\treturn with_bytestrings(namefunc)\n\t# return namefunc # for debugging\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kwargs):\n\t\tif six.PY2:\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(*args, **kwargs)\n \n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef namefunc_wrapper(tz):\n\t\treturn namefunc(tz).encode('utf-8')\n\treturn namefunc_wrapper\n/src/tzutil/tests/test_tzutil.py\nimport os\nimport sys\nimport unittest\nimport pytz\n\nfrom tzutil.tzutil import tzname_in_python2\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tif sys.version_info[0] == 2:\n\t\tdef wrapper(tzname):\n\t\t\treturn namefunc(tzname).encode('utf-8')\n\t\treturn wrapper\n\treturn namefunc\n ", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(*args, **kwargs):\n\t\tres = namefunc(*args, **kwargs)\n\t\tif isinstance(res, unicode):\n\t\t\tres = res.encode('utf-8')\n\t\treturn res\n\treturn wrapper\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\t \n\tdef wrapper(loc, format, tz=None):\n\t\tname = namefunc(loc, format, tz)\n\t\tif isinstance(name, unicode):\n\t\t\tname = name.encode('utf-8')\n\t\treturn name\n\treturn wrapper\n\nif __name__ == '__main__':\n\ttz = pytz.timezone('Europe/London')\n\tprint(tz.tzname(datetime.datetime.now()))\n/README.md\n# pytz-tzname\n\nA small hack to get the tzname in pytz to work with Python 2.\n\nThis is a hack that was put together to solve this problem:\nhttps://github.com/stub42/wagtail/issues/3167\n\nThe hack is in the tzname_in_python2 function, which replaces the original tzname function.\n\nIt's not pretty, but it works for the use case here.\n\n## Usage\n\nSimply copy the tzname_in_python2 function into your project and replace the original tzname function with the new function.\n\nFor example:\n\n```python\nimport pytz\nfrom tzname_in_python2 import tzname_in_python2\n\npytz.timezone = tzname_in_python2(pytz.timezone)\n```\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn get_version_info()\n\texcept Exception:\n\t\treturn None\n\n# Returns the version information.", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(os.path.join(os.path.dirname(__file__), \"VERSION\"), \"r\")\n\t\tversion = f.readline().strip()\n\t\tf.close()\n\texcept Exception as e:\n\t\tversion = \"0.0.0\"\n\n\treturn version\n\nsetup(name=\"py-sif-reader\",\n\tversion=get_versions(),\n\tdescription=\"Python SIF Reader\",\n\tauthor=\"\",\n\tauthor_email=\"\",\n\turl=\"https://github.com/kth-se/py-sif-reader\",\n\tpackages=[\"sif_reader\"],\n\tinstall_requires=[\"pandas==0.25.1\"],\n\tclassifiers=[\"Development Status :: 3 - Alpha\", \"License :: OSI Approved :: MIT License\", \"Programming Language :: Python :: 3.6\", \"Topic :: Scientific/Engineering :: Information Analysis\", \"Topic :: Software Development :: Libraries :: Python Modules\"]\n)\n/sif_reader/utils.py\nimport pandas as pd\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\tinfo = subprocess.check_output([\"git\",\"describe\",\"--tags\"])\n\t\tversion = info.strip()\n\texcept Exception as e:\n\t\tprint(e)\n\t\tversion = \"unknown\"\n\treturn version\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\timport pkg_resources\n\texcept ImportError:\n\t\treturn get_default_version()\n\telse:\n\t\tif pkg_resources.resource_exists('py_version_info','version.json'):\n\t\t\treturn pkg_resources.resource_string('py_version_info','version.json')\n\t\telse:\n\t\t\treturn get_default_version()\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(\"version.txt\", \"r\")\n\t\tversion = f.readline().strip()\n\t\tf.close()\n\t\treturn version\n\texcept:\n\t\treturn \"0.1\"\n\nif __name__ == \"__main__\":\n\tmain()/src/main.py\nimport sys\nimport os\nimport configparser\nfrom PyQt5.QtWidgets import QApplication, QMessageBox\nfrom PyQt5.QtGui import QIcon\nimport ui\nfrom core import Core\nimport version\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\tversions = {}\n\tversions[\"system\"] = \"unknown\"\n\t\n\ttry:\n\t\tfp = open(\"/etc/redhat-release\", \"r\")\n\t\tversions[\"system\"] = fp.read().strip()\n\t\tfp.close()\n\texcept:\n\t\tpass\n\t\n\treturn versions\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\t# Obtains the version information from the git repository.\n\t\tversion_info = subprocess.check_output(['git', 'describe', '--tags', '--dirty', '--always']).strip()\n\texcept subprocess.CalledProcessError:\n\t\t# In case the version information cannot be obtained, the default value is returned.\n\t\tversion_info = 'Unknown'\n\n\treturn version_info\n/doc/html/search/all_b.js\nvar searchData=\n[\n  ['main',['main',['../main_8py.html#a0ddf1224851353fc92bfbff6f499fa97',1,'main.py']]],\n  ['main_2epy',['main.py',['../main_8py.html',1,'']]],\n  ['match',['match',['../class_word_guess_game_1_1_word_guess_game.html#a2d44086621f968838f0a892d449d76f8',1,'WordGuessGame::WordGuessGame']]],\n  ['matches',['matches',['../class_word_guess_game_1_1_word_guess_game.html#a9865240117878834c6646236c7", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\treturn {\n\t\t\t'name': 'Nagios',\n\t\t\t'version': '1.0.0',\n\t\t\t'description': 'Nagios Plugin',\n\t\t\t'author': '',\n\t\t\t'copyright': 'Copyright 2015',\n\t\t\t'license': 'GPL',\n\t\t\t'url': 'http://nagios.com',\n\t\t\t'code_url': 'https://github.com/netmania/check_nagios'\n\t\t}\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tversion = __import__('__main__').__version__\n\texcept AttributeError:\n\t\tversion = 'unknown'\n\n\treturn version\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\tversion = get_version()\n\texcept:\n\t\tversion = 'Unknown'\n\t\n\ttry:\n\t\tplatform = get_platform()\n\texcept:\n\t\tplatform = 'Unknown'\n\t\n\ttry:\n\t\tarchitecture = get_architecture()\n\texcept:\n\t\tarchitecture = 'Unknown'\n\t\n\treturn version, platform, architecture\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn get_version_info()\n\texcept Exception:\n\t\treturn None\n\n# Returns the version information.", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(os.path.join(os.path.dirname(__file__), \"VERSION\"), \"r\")\n\t\tversion = f.readline().strip()\n\t\tf.close()\n\texcept Exception as e:\n\t\tversion = \"0.0.0\"\n\n\treturn version\n\nsetup(name=\"py-sif-reader\",\n\tversion=get_versions(),\n\tdescription=\"Python SIF Reader\",\n\tauthor=\"\",\n\tauthor_email=\"\",\n\turl=\"https://github.com/kth-se/py-sif-reader\",\n\tpackages=[\"sif_reader\"],\n\tinstall_requires=[\"pandas==0.25.1\"],\n\tclassifiers=[\"Development Status :: 3 - Alpha\", \"License :: OSI Approved :: MIT License\", \"Programming Language :: Python :: 3.6\", \"Topic :: Scientific/Engineering :: Information Analysis\", \"Topic :: Software Development :: Libraries :: Python Modules\"]\n)\n/sif_reader/utils.py\nimport pandas as pd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\tinfo = subprocess.check_output([\"git\",\"describe\",\"--tags\"])\n\t\tversion = info.strip()\n\texcept Exception as e:\n\t\tprint(e)\n\t\tversion = \"unknown\"\n\treturn version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\timport pkg_resources\n\texcept ImportError:\n\t\treturn get_default_version()\n\telse:\n\t\tif pkg_resources.resource_exists('py_version_info','version.json'):\n\t\t\treturn pkg_resources.resource_string('py_version_info','version.json')\n\t\telse:\n\t\t\treturn get_default_version()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(\"version.txt\", \"r\")\n\t\tversion = f.readline().strip()\n\t\tf.close()\n\t\treturn version\n\texcept:\n\t\treturn \"0.1\"\n\nif __name__ == \"__main__\":\n\tmain()/src/main.py\nimport sys\nimport os\nimport configparser\nfrom PyQt5.QtWidgets import QApplication, QMessageBox\nfrom PyQt5.QtGui import QIcon\nimport ui\nfrom core import Core\nimport version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\tversions = {}\n\tversions[\"system\"] = \"unknown\"\n\t\n\ttry:\n\t\tfp = open(\"/etc/redhat-release\", \"r\")\n\t\tversions[\"system\"] = fp.read().strip()\n\t\tfp.close()\n\texcept:\n\t\tpass\n\t\n\treturn versions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\t# Obtains the version information from the git repository.\n\t\tversion_info = subprocess.check_output(['git', 'describe', '--tags', '--dirty', '--always']).strip()\n\texcept subprocess.CalledProcessError:\n\t\t# In case the version information cannot be obtained, the default value is returned.\n\t\tversion_info = 'Unknown'\n\n\treturn version_info\n/doc/html/search/all_b.js\nvar searchData=\n[\n  ['main',['main',['../main_8py.html#a0ddf1224851353fc92bfbff6f499fa97',1,'main.py']]],\n  ['main_2epy',['main.py',['../main_8py.html',1,'']]],\n  ['match',['match',['../class_word_guess_game_1_1_word_guess_game.html#a2d44086621f968838f0a892d449d76f8',1,'WordGuessGame::WordGuessGame']]],\n  ['matches',['matches',['../class_word_guess_game_1_1_word_guess_game.html#a9865240117878834c6646236c7", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\treturn {\n\t\t\t'name': 'Nagios',\n\t\t\t'version': '1.0.0',\n\t\t\t'description': 'Nagios Plugin',\n\t\t\t'author': '',\n\t\t\t'copyright': 'Copyright 2015',\n\t\t\t'license': 'GPL',\n\t\t\t'url': 'http://nagios.com',\n\t\t\t'code_url': 'https://github.com/netmania/check_nagios'\n\t\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tversion = __import__('__main__').__version__\n\texcept AttributeError:\n\t\tversion = 'unknown'\n\n\treturn version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\t \n\ttry:\n\t\tversion = get_version()\n\texcept:\n\t\tversion = 'Unknown'\n\t\n\ttry:\n\t\tplatform = get_platform()\n\texcept:\n\t\tplatform = 'Unknown'\n\t\n\ttry:\n\t\tarchitecture = get_architecture()\n\texcept:\n\t\tarchitecture = 'Unknown'\n\t\n\treturn version, platform, architecture\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\tfor i in pieces:\n\t\t\trender_piece(i, style)\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\te\n\t\"\"\"\n\tpass\n/pieces/piece.py", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t/README.md\n# poker_game\n\nThis is a poker game.\n\nThis project is inspired by the Udacity's Introduction to Computer Science.\n\nThe aim of this project is to practice the object-oriented programming, including class, inheritance, encapsulation and polymorphism.\n\nThe game is based on the Udacity's poker_template.py.\n\nThe game is still under development and will be updated.\n\nThis project is written in Python 3.5.1.\n/poker_game.py\nimport random\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\n\tif style == \"png\":\n\t\trender_png(pieces)\n\telif style == \"html\":\n\t\trender_html(pieces)\n\telse:\n\t\tprint(\"Style {} not supported.\".format(style))\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t/src/utils.py", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tfrom piece import Piece\nfrom render import render\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\tfor i in pieces:\n\t\t\trender_piece(i, style)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\te\n\t\"\"\"\n\tpass\n/pieces/piece.py", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t/README.md\n# poker_game\n\nThis is a poker game.\n\nThis project is inspired by the Udacity's Introduction to Computer Science.\n\nThe aim of this project is to practice the object-oriented programming, including class, inheritance, encapsulation and polymorphism.\n\nThe game is based on the Udacity's poker_template.py.\n\nThe game is still under development and will be updated.\n\nThis project is written in Python 3.5.1.\n/poker_game.py\nimport random\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\n\tif style == \"png\":\n\t\trender_png(pieces)\n\telif style == \"html\":\n\t\trender_html(pieces)\n\telse:\n\t\tprint(\"Style {} not supported.\".format(style))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t/src/utils.py", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tfrom piece import Piece\nfrom render import render\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\treturn pieces[-1].find(\"+\")!= -1 and \".\" or \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\treturn \".\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\tif ('+' in pieces[-1]):\n\t\t\treturn '.'\n\t\telse:\n\t\t\treturn '+'\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\ttags = pieces[1].split('+')\n\t\tfor tag in tags:\n\t\t\tif tag in ['+', 'p', 'd', 'e']:\n\t\t\t\treturn '.'\n\t\treturn '+'\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\tif pieces[-1] == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\treturn \".\" if any(tag.endswith(\"+\") for tag in pieces[-1]) else \"+\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\tif pieces[-1] == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n        # If pieces is empty, return \".\"\n\tif pieces == []:\n\t\treturn \".\"\n\n\t# Otherwise, return the closet-tag of pieces\n\telse:\n\t\treturn pieces[-1].tag\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\treturn \"+\" if pieces[-1].find(\"+\") > -1 else \".\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\tpieces = pieces.split(\"+\")\n\tif pieces[-1] == \"\":\n\t\tpieces = pieces[:-1]\n\n\tcloset_tag = pieces[-1]\n\tif closet_tag.find(\"+\")!= -1:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\treturn pieces[-1].find(\"+\")!= -1 and \".\" or \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\treturn \".\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\tif ('+' in pieces[-1]):\n\t\t\treturn '.'\n\t\telse:\n\t\t\treturn '+'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\ttags = pieces[1].split('+')\n\t\tfor tag in tags:\n\t\t\tif tag in ['+', 'p', 'd', 'e']:\n\t\t\t\treturn '.'\n\t\treturn '+'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\tif pieces[-1] == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\treturn \".\" if any(tag.endswith(\"+\") for tag in pieces[-1]) else \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\tif pieces[-1] == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n        # If pieces is empty, return \".\"\n\tif pieces == []:\n\t\treturn \".\"\n\n\t# Otherwise, return the closet-tag of pieces\n\telse:\n\t\treturn pieces[-1].tag\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\treturn \"+\" if pieces[-1].find(\"+\") > -1 else \".\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\n\tpieces = pieces.split(\"+\")\n\tif pieces[-1] == \"\":\n\t\tpieces = pieces[:-1]\n\n\tcloset_tag = pieces[-1]\n\tif closet_tag.find(\"+\")!= -1:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t # If this is a list, run each command in turn.\n\tif isinstance(commands, list):\n\t\tfor command in commands:\n\t\t\trun_command(command, args, cwd, verbose, hide_stderr, env)\n\t\treturn\n\t\t\n\tglobal _log_mutex\n\tif verbose:\n\t\t# Print the command and arguments.\n\t\t_log_mutex.acquire()\n\t\tif isinstance(commands, basestring):\n\t\t\tprint \"Running command: %s\" % commands\n\t\telse:\n\t\t\tprint \"Running command: %s %s\" % (commands[0], \" \".join(commands[1:]))\n\t\t_log_mutex.release()\n\n\t# Hide stderr if requested.\n\tsave_stderr = os.dup(2)\n\tdevnull = os.open(os.devnull, os.O_WRONLY)\n\tif hide_stderr:\n\t\tos.dup2(devnull, 2)\n\n\t# Execute the command\n\tif cwd is None:\n\t\tp = sp.Popen(commands, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True, env=env)\n\telse:\n\t\tp = sp.Popen(commands, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True, env=env, cwd=cwd)\n\t", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\tassert isinstance(commands, list), \"argument 'commands' must be a list\"\n\tassert isinstance(args, list), \"argument 'args' must be a list\"\n\n\tif verbose and hide_stderr:\n\t\traise ValueError(\"'verbose' and 'hide_stderr' are mutually exclusive\")\n\n\t# Build our command string.\n\tcmd_list = []\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor cmd in commands:\n\t\tcmd_list.append(cmd)\n\n\t# Append all args.\n\tcmd_list.extend(args)\n\n\t# Create a new environment dict so we can\n\t# silence output.\n\tnew_env = os.environ.copy()\n\tif hide_stderr:\n\t\tnew_env[\"LC_ALL\"] = \"en_US.UTF-8\"\n\t\tnew_env[\"LANG\"] = \"en_US.UTF-8\"\n\t\tnew_env[\"PYTHONIOENCODING\"] = \"UTF-8\"\n\t\tnew_env[\"PYTHONUNBUFFERED\"] = \"1\"\n\n\t# Create the subprocess, redirecting stderr unless requested.\n\ttry:\n\t\tproc = subprocess.Popen(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\texcept OSError as e:\n\t\tif e.errno ==", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n#\tassert isinstance(commands, list)\n\tassert isinstance(args, list)\n\t\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\t\n\tif isinstance(args, str):\n\t\targs = args.split(' ')\n\t\n\t# check and correct usage maybe\n\tif not os.path.exists( commands[0] ) and os.path.exists( commands[0] + '.exe' ):\n\t\tcommands[0] += '.exe'\n\t\n\tassert os.path.exists( commands[0] )\n\t\n\tif verbose:\n\t\tprint(\"Running: \" + str(commands) + \" \" + str(args))\n\t\n\t# build the process\n\tp = Popen(commands + args, cwd=cwd, env=env)\n\t\n\t# get return code, ignore output\n\tretcode = p.wait()\n\tout = \"\"\n\terr = \"\"\n\t\n\tif retcode!= 0:\n\t\tif not hide_stderr:\n\t\t\terr = p.stderr.read()\n\t\traise CalledProcessError(retcode, commands)\n\telse:\n\t\treturncode = 0\n\t\tif not hide_stderr:\n\t\t\terr = p.stdout.read()\n\t\n\treturn returncode, out, err\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t \n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\t# Use shell=True on Windows so we can use pipes, etc.\n\tif sys.platform == 'win32':\n\t\tshell = True\n\telse:\n\t\tshell = False\n\n\tif verbose:\n\t\tprint('\\nRunning: %s' %''.join(commands))\n\n\t# Always call the command with shell=True, even if we're not on Windows.\n\t# This keeps errors from containing un-expanded variables\n\tif env:\n\t\tchild = subprocess.Popen(commands, shell=shell, cwd=cwd, env=env)\n\telse:\n\t\tchild = subprocess.Popen(commands, shell=shell, cwd=cwd)\n\tif hide_stderr:\n\t\tchild.stderr.close()\n\tretcode = child.wait()\n\tif retcode:\n\t\traise Exception('Command \"%s\" returned non-zero exit status %d' % (commands, retcode))\n\treturn retcode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tcommands = list(commands)\n\t\tif not commands:\n\t\t\treturn None\n\t\telif isinstance(commands[0], list):\n\t\t\tprocesses = []\n\t\t\tfor command in commands:\n\t\t\t\tprocesses.append(run_command(command, args, cwd=cwd,\n\t\t\t\t\tverbose=verbose, hide_stderr=hide_stderr, env=env))\n\t\t\treturn processes\n\t\tp = None\n\t\ttry:\n\t\t\tp = Popen(commands, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\t\t\tif verbose:\n\t\t\t\tprint(\"Spawned {0} pid {1} with args {2} and cwd {3}\".format(commands, p.pid, args, cwd))\n\t\t\tstdout, stderr = p.communicate(args)\n\t\t\tif p.returncode!= 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(\"Command {0} returned non-zero exit status {1} and stderr was {2}\".format(commands,\n\t\t\t\t\t\tp.returncode, stderr))\n\t\t\t\traise Exception(\"Non-zero exit status {0} from command {1}\".format(p.returncode, commands))\n\t\t\tif not hide_stderr:\n\t\t\t\tif stderr:\n\t\t\t\t\tprint(stderr)\n\t\t\treturn stdout\n\t\texcept OSError as e:\n\t\t\tif p:\n\t\t\t\tp.kill()\n\t\t\traise e\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\t#if not isinstance(commands, list):\n\t#\tcommands = [commands]\n\n\tfor c in commands:\n\t\tif not isinstance(c, list):\n\t\t\tc = [c]\n\n\tp = Popen(commands, shell=False, cwd=cwd, env=env, stdin=PIPE, stdout=PIPE, stderr=(PIPE if hide_stderr else None))\n\tout, err = p.communicate(args) #@UnusedVariable\n\tif p.returncode!= 0:\n\t\traise Exception(\"non-zero exit status %d for %s\" % (p.returncode, str(commands)))\n\tif verbose:\n\t\tprint(out)\n\treturn out\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t \n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\t\n\t# Validate arguments\n\tassert all(isinstance(command, str) for command in commands)\n\tfor arg in args:\n\t\tassert isinstance(arg, str)\n\t \n\t# Check for existence of required commands\n\tmissing = [command for command in commands if not find_executable(command)]\n\tif missing:\n\t\traise Exception('Missing prerequisites: {0}'.format(', '.join(missing)))\n\t \n\tenv = env or os.environ\n\tlogger = logging.getLogger()\n\tlogger.debug('Running command: {0}'.format(' '.join(commands)))\n\tlogger.debug('Args: {0}'.format(' '.join(args)))\n\tlogger.debug('Working dir: {0}'.format(cwd))\n\tlogger.debug('=======================')\n\ttry:\n\t\tresult = subprocess.Popen(commands, args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n\t\tif verbose:\n\t\t\tfor line in result.stdout:\n\t\t\t\tlogger.info(line.rstrip())\n\t\t\tif result.stdout:\n\t\t\t\tlogger.info('=======================')\n\t\tif hide_stderr:\n\t\t\tfor line in result.stderr:\n\t\t\t\tlogger.warning(line.rstrip())\n\t\t\tif result.", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif not isinstance(commands, list):\n\t\traise TypeError(\"commands is not a list or string\")\n\n\tif not isinstance(args, list):\n\t\traise TypeError(\"args is not a list\")\n\n\t# handle shell form of cmd\n\tif len(commands) == 1:\n\t\tif isinstance(commands[0], str):\n\t\t\tif'' in commands[0] or '\\n' in commands[0]:\n\t\t\t\tcommands[0] =''.join(commands[0].split())\n\t\t\t\tif verbose:\n\t\t\t\t\tprint('run shell: {}'.format(commands[0]))\n\t\t\t\targs =''.join(args)\n\t\t\t\treturn subprocess.check_call(commands[0], shell=True, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint('run shell: {}'.format(' '.join(commands + args)))\n\t\t\t\treturn subprocess.check_call(' '.join(commands + args), shell=True, cwd=cwd, env=env)\n\n\t# handle executable form of cmd\n\tif verbose:\n\t\tprint('run executable: {} {}'.format(commands[0], args))\n\treturn subprocess.check_call((commands[0],) + tuple(args), cwd=cwd, env=env)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t \n\tif isinstance(commands, list):\n\t\tfor command in commands:\n\t\t\trun_command(command, args, cwd, verbose, hide_stderr, env)\n\telse:\n\t\tassert isinstance(commands, str)\n\t\tcmd = shlex.split(commands)\n\t\tassert isinstance(args, list)\n\t\tp = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\t\tinput_ = bytes('\\n'.join(args) + '\\n', 'utf-8')\n\t\tstdout, stderr = p.communicate(input_)\n\t\tif p.returncode!= 0:\n\t\t\tif verbose or hide_stderr:\n\t\t\t\tif not hide_stderr:\n\t\t\t\t\tprint(stderr.decode('utf-8', 'ignore'))\n\t\t\t\traise Exception(stderr.decode('utf-8', 'ignore'))\n\t\t\telse:\n\t\t\t\traise Exception(stderr.decode('utf-8', 'ignore'))\n\t\tif verbose:\n\t\t\tprint(stdout.decode('utf-8', 'ignore'))\n/README.md\n# Citation-Curation\n\n## Summary\n\nThis repository contains the scripts necessary to reproduce the results of the article [Citation-Curation: A Method to Improve the Quality of Citations in Software Engineering Literature](https://doi.org/10.1109", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tif not isinstance(commands, list):\n\t\t\tcommands = [commands]\n\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\n\t\tenv = env or os.environ\n\n\t\tp = None\n\t\ttry:\n\t\t\tp = Popen(commands, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\t\t\tstdout, stderr = p.communicate(b' '.join(args))\n\t\t\tif p.returncode!= 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tif stderr:\n\t\t\t\t\t\tsys.stderr.write(stderr)\n\t\t\t\t\traise CalledProcessError(p.returncode,''.join(commands), output=stdout)\n\t\t\t\telse:\n\t\t\t\t\traise CalledProcessError(p.returncode,''.join(commands))\n\t\t\tif hide_stderr:\n\t\t\t\tstderr = None\n\t\t\treturn stdout, stderr\n\t\texcept OSError as e:\n\t\t\tif verbose:\n\t\t\t\traise CommandExecutionError(e, commands, args)\n\t\t\telse:\n\t\t\t\traise CommandExecutionError(e)\n\t\tfinally:\n\t\t\tif p is not None:\n\t\t\t\tp.stdin.close()\n\t\t\t\tp.stdout.close()\n\t\t\t\tp.stderr.close()\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t # If this is a list, run each command in turn.\n\tif isinstance(commands, list):\n\t\tfor command in commands:\n\t\t\trun_command(command, args, cwd, verbose, hide_stderr, env)\n\t\treturn\n\t\t\n\tglobal _log_mutex\n\tif verbose:\n\t\t# Print the command and arguments.\n\t\t_log_mutex.acquire()\n\t\tif isinstance(commands, basestring):\n\t\t\tprint \"Running command: %s\" % commands\n\t\telse:\n\t\t\tprint \"Running command: %s %s\" % (commands[0], \" \".join(commands[1:]))\n\t\t_log_mutex.release()\n\n\t# Hide stderr if requested.\n\tsave_stderr = os.dup(2)\n\tdevnull = os.open(os.devnull, os.O_WRONLY)\n\tif hide_stderr:\n\t\tos.dup2(devnull, 2)\n\n\t# Execute the command\n\tif cwd is None:\n\t\tp = sp.Popen(commands, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True, env=env)\n\telse:\n\t\tp = sp.Popen(commands, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True, env=env, cwd=cwd)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\tassert isinstance(commands, list), \"argument 'commands' must be a list\"\n\tassert isinstance(args, list), \"argument 'args' must be a list\"\n\n\tif verbose and hide_stderr:\n\t\traise ValueError(\"'verbose' and 'hide_stderr' are mutually exclusive\")\n\n\t# Build our command string.\n\tcmd_list = []\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor cmd in commands:\n\t\tcmd_list.append(cmd)\n\n\t# Append all args.\n\tcmd_list.extend(args)\n\n\t# Create a new environment dict so we can\n\t# silence output.\n\tnew_env = os.environ.copy()\n\tif hide_stderr:\n\t\tnew_env[\"LC_ALL\"] = \"en_US.UTF-8\"\n\t\tnew_env[\"LANG\"] = \"en_US.UTF-8\"\n\t\tnew_env[\"PYTHONIOENCODING\"] = \"UTF-8\"\n\t\tnew_env[\"PYTHONUNBUFFERED\"] = \"1\"\n\n\t# Create the subprocess, redirecting stderr unless requested.\n\ttry:\n\t\tproc = subprocess.Popen(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\texcept OSError as e:\n\t\tif e.errno ==", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n#\tassert isinstance(commands, list)\n\tassert isinstance(args, list)\n\t\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\t\n\tif isinstance(args, str):\n\t\targs = args.split(' ')\n\t\n\t# check and correct usage maybe\n\tif not os.path.exists( commands[0] ) and os.path.exists( commands[0] + '.exe' ):\n\t\tcommands[0] += '.exe'\n\t\n\tassert os.path.exists( commands[0] )\n\t\n\tif verbose:\n\t\tprint(\"Running: \" + str(commands) + \" \" + str(args))\n\t\n\t# build the process\n\tp = Popen(commands + args, cwd=cwd, env=env)\n\t\n\t# get return code, ignore output\n\tretcode = p.wait()\n\tout = \"\"\n\terr = \"\"\n\t\n\tif retcode!= 0:\n\t\tif not hide_stderr:\n\t\t\terr = p.stderr.read()\n\t\traise CalledProcessError(retcode, commands)\n\telse:\n\t\treturncode = 0\n\t\tif not hide_stderr:\n\t\t\terr = p.stdout.read()\n\t\n\treturn returncode, out, err\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t \n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\t# Use shell=True on Windows so we can use pipes, etc.\n\tif sys.platform == 'win32':\n\t\tshell = True\n\telse:\n\t\tshell = False\n\n\tif verbose:\n\t\tprint('\\nRunning: %s' %''.join(commands))\n\n\t# Always call the command with shell=True, even if we're not on Windows.\n\t# This keeps errors from containing un-expanded variables\n\tif env:\n\t\tchild = subprocess.Popen(commands, shell=shell, cwd=cwd, env=env)\n\telse:\n\t\tchild = subprocess.Popen(commands, shell=shell, cwd=cwd)\n\tif hide_stderr:\n\t\tchild.stderr.close()\n\tretcode = child.wait()\n\tif retcode:\n\t\traise Exception('Command \"%s\" returned non-zero exit status %d' % (commands, retcode))\n\treturn retcode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tcommands = list(commands)\n\t\tif not commands:\n\t\t\treturn None\n\t\telif isinstance(commands[0], list):\n\t\t\tprocesses = []\n\t\t\tfor command in commands:\n\t\t\t\tprocesses.append(run_command(command, args, cwd=cwd,\n\t\t\t\t\tverbose=verbose, hide_stderr=hide_stderr, env=env))\n\t\t\treturn processes\n\t\tp = None\n\t\ttry:\n\t\t\tp = Popen(commands, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\t\t\tif verbose:\n\t\t\t\tprint(\"Spawned {0} pid {1} with args {2} and cwd {3}\".format(commands, p.pid, args, cwd))\n\t\t\tstdout, stderr = p.communicate(args)\n\t\t\tif p.returncode!= 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(\"Command {0} returned non-zero exit status {1} and stderr was {2}\".format(commands,\n\t\t\t\t\t\tp.returncode, stderr))\n\t\t\t\traise Exception(\"Non-zero exit status {0} from command {1}\".format(p.returncode, commands))\n\t\t\tif not hide_stderr:\n\t\t\t\tif stderr:\n\t\t\t\t\tprint(stderr)\n\t\t\treturn stdout\n\t\texcept OSError as e:\n\t\t\tif p:\n\t\t\t\tp.kill()\n\t\t\traise e\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\t#if not isinstance(commands, list):\n\t#\tcommands = [commands]\n\n\tfor c in commands:\n\t\tif not isinstance(c, list):\n\t\t\tc = [c]\n\n\tp = Popen(commands, shell=False, cwd=cwd, env=env, stdin=PIPE, stdout=PIPE, stderr=(PIPE if hide_stderr else None))\n\tout, err = p.communicate(args) #@UnusedVariable\n\tif p.returncode!= 0:\n\t\traise Exception(\"non-zero exit status %d for %s\" % (p.returncode, str(commands)))\n\tif verbose:\n\t\tprint(out)\n\treturn out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t \n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\t\n\t# Validate arguments\n\tassert all(isinstance(command, str) for command in commands)\n\tfor arg in args:\n\t\tassert isinstance(arg, str)\n\t \n\t# Check for existence of required commands\n\tmissing = [command for command in commands if not find_executable(command)]\n\tif missing:\n\t\traise Exception('Missing prerequisites: {0}'.format(', '.join(missing)))\n\t \n\tenv = env or os.environ\n\tlogger = logging.getLogger()\n\tlogger.debug('Running command: {0}'.format(' '.join(commands)))\n\tlogger.debug('Args: {0}'.format(' '.join(args)))\n\tlogger.debug('Working dir: {0}'.format(cwd))\n\tlogger.debug('=======================')\n\ttry:\n\t\tresult = subprocess.Popen(commands, args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n\t\tif verbose:\n\t\t\tfor line in result.stdout:\n\t\t\t\tlogger.info(line.rstrip())\n\t\t\tif result.stdout:\n\t\t\t\tlogger.info('=======================')\n\t\tif hide_stderr:\n\t\t\tfor line in result.stderr:\n\t\t\t\tlogger.warning(line.rstrip())\n\t\t\tif result.", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif not isinstance(commands, list):\n\t\traise TypeError(\"commands is not a list or string\")\n\n\tif not isinstance(args, list):\n\t\traise TypeError(\"args is not a list\")\n\n\t# handle shell form of cmd\n\tif len(commands) == 1:\n\t\tif isinstance(commands[0], str):\n\t\t\tif'' in commands[0] or '\\n' in commands[0]:\n\t\t\t\tcommands[0] =''.join(commands[0].split())\n\t\t\t\tif verbose:\n\t\t\t\t\tprint('run shell: {}'.format(commands[0]))\n\t\t\t\targs =''.join(args)\n\t\t\t\treturn subprocess.check_call(commands[0], shell=True, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint('run shell: {}'.format(' '.join(commands + args)))\n\t\t\t\treturn subprocess.check_call(' '.join(commands + args), shell=True, cwd=cwd, env=env)\n\n\t# handle executable form of cmd\n\tif verbose:\n\t\tprint('run executable: {} {}'.format(commands[0], args))\n\treturn subprocess.check_call((commands[0],) + tuple(args), cwd=cwd, env=env)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t \n\tif isinstance(commands, list):\n\t\tfor command in commands:\n\t\t\trun_command(command, args, cwd, verbose, hide_stderr, env)\n\telse:\n\t\tassert isinstance(commands, str)\n\t\tcmd = shlex.split(commands)\n\t\tassert isinstance(args, list)\n\t\tp = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\t\tinput_ = bytes('\\n'.join(args) + '\\n', 'utf-8')\n\t\tstdout, stderr = p.communicate(input_)\n\t\tif p.returncode!= 0:\n\t\t\tif verbose or hide_stderr:\n\t\t\t\tif not hide_stderr:\n\t\t\t\t\tprint(stderr.decode('utf-8', 'ignore'))\n\t\t\t\traise Exception(stderr.decode('utf-8', 'ignore'))\n\t\t\telse:\n\t\t\t\traise Exception(stderr.decode('utf-8', 'ignore'))\n\t\tif verbose:\n\t\t\tprint(stdout.decode('utf-8', 'ignore'))\n/README.md\n# Citation-Curation\n\n## Summary\n\nThis repository contains the scripts necessary to reproduce the results of the article [Citation-Curation: A Method to Improve the Quality of Citations in Software Engineering Literature](https://doi.org/10.1109", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tif not isinstance(commands, list):\n\t\t\tcommands = [commands]\n\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\n\t\tenv = env or os.environ\n\n\t\tp = None\n\t\ttry:\n\t\t\tp = Popen(commands, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n\t\t\tstdout, stderr = p.communicate(b' '.join(args))\n\t\t\tif p.returncode!= 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tif stderr:\n\t\t\t\t\t\tsys.stderr.write(stderr)\n\t\t\t\t\traise CalledProcessError(p.returncode,''.join(commands), output=stdout)\n\t\t\t\telse:\n\t\t\t\t\traise CalledProcessError(p.returncode,''.join(commands))\n\t\t\tif hide_stderr:\n\t\t\t\tstderr = None\n\t\t\treturn stdout, stderr\n\t\texcept OSError as e:\n\t\t\tif verbose:\n\t\t\t\traise CommandExecutionError(e, commands, args)\n\t\t\telse:\n\t\t\t\traise CommandExecutionError(e)\n\t\tfinally:\n\t\t\tif p is not None:\n\t\t\t\tp.stdin.close()\n\t\t\t\tp.stdout.close()\n\t\t\t\tp.stderr.close()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t\n\n\t# The VERSIONFILE_SOURCE attribute determines the source for the\n\t# version file. It defaults to VERSIONFILE_SOURCE_DEFAULT, which is\n\t# VERSIONFILE_SOURCE_GIT_DESCRIBE, but can be overridden by\n\t# setting the VERSIONFILE_SOURCE environment variable\n\tversionfile_source = os.environ.get('VERSIONFILE_SOURCE', None)\n\tif versionfile_source == 'git':\n\t\tversionfile_source = VERSIONFILE_SOURCE_GIT\n\telif versionfile_source =='setuptools':\n\t\tversionfile_source = VERSIONFILE_SOURCE_SETUPTOOLS\n\telif versionfile_source is None:\n\t\tversionfile_source = VERSIONFILE_SOURCE_GIT_DESCRIBE\n\telse:\n\t\tversionfile_source = None\n\n\t# The VERSIONFILE_OUTPUT attribute determines the path to the file\n\t# that will contain the version number. It defaults to\n\t# VERSIONFILE_OUTPUT_DEFAULT, which is _version.py in the\n\t# source tree.\n\tversionfile_output = os.environ.get('VERSIONFILE_OUTPUT', None)\n\tif versionfile_output is None:\n\t\tversionfile_output = VERSIONFILE_OUTPUT_DEFAULT\n\n\t# The VCS_COMMAND attribute determines the command used to determine\n\t# the version. It defaults to VCS_COMMAND_DEFAULT, which is\n\t# '", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"v\"\n\t\tconfig.versionfile_source = \"src/version.py\"\n\t\tconfig.verbose = False # print more verbosely\n\t\treturn config\n/src/version.py\n# This file is part of the Ginga project.\n#\n# Copyright (c) 2014, 2017 by the Ginga developers\n# Licensed under the GNU General Public License\n\n# Ginga version.\n#\n# This is the first part of the version string.\n#\n# It is a PEP440 compliant version.\n#\n# We have the following components:\n#\n# MAJOR: Major release, backwards-incompatible changes.\n# MINOR: Minor release, backwards-compatible new features.\n# MICRO: Patch release, backwards-compatible bug fixes.\n# VERSION: Version number.  This is a string like \"1.2.3\"\n#\n# The version number is a string of the form:\n#\n#    MAJOR.MINOR.MICRO\n#\n# The version number is converted into a numeric form:\n#\n#    MAJOR * 100", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t/README.md\n#'s Portfolio\n\nThis is a personal portfolio for, a data scientist at [Zillow](https://www.zillow.com/).\n\n## Built With\n\n* [Django](https://docs.djangoproject.com/en/2.2/) - The web framework used\n* [Django REST Framework](https://www.django-rest-framework.org/) - The REST framework used\n* [PostgreSQL](https://www.postgresql.org/) - The relational database used\n* [PostgreSQL Extensions](https://postgis.net/) - The spatial database extension used\n* [PostGIS](https://postgis.net/) - The spatial database used\n* [Bootstrap](https://getbootstrap.com/) - The CSS framework used\n* [Bootstrap Datepicker](https://bootstrap-datepicker.readthedocs.io/en/latest/) - The datepicker widget used\n* [Leaflet](https://leafletjs.com/) - The JavaScript library used for the map\n* [Leaflet Draw](https://github.com/Leaflet/Leaflet.draw) - The plugin for drawing shapes on the map\n* [Leaflet Geocoder](https://github.com/perliedman/leaflet-control-geocoder) - The plugin for searching places\n* [Leaflet Markercluster](https://github.com/Leaflet/Leaflet.", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t \n\tv = VersioneerConfig()\n\tv.VCS = 'git'\n\tv.style = 'pep440'\n\tv.tag_prefix = 'v'\n\tv.parentdir_prefix = 'python-'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_build = 'python_versioneer/_version.py'\n\tv.tag_prefix = 'v'\n\tv.parentdir_prefix = 'python-'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_build = 'python_versioneer/_version.py'\n\tv.tag_prefix = 'v'\n\tv.parentdir_prefix = 'python-'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_build = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t\n\n\t# Read in the configuration from the file\n\tconfig = VersioneerConfig()\n\tconfig.read()\n\n\t# Set the git author\n\tconfig.get_git_author()\n\n\t# Set the git version\n\tconfig.get_git_version()\n\n\t# Set the version\n\tconfig.get_version()\n\n\t# Set the version tag\n\tconfig.get_version_tag()\n\n\t# Set the version suffix\n\tconfig.get_version_suffix()\n\n\t# Set the version full\n\tconfig.get_version_full()\n\n\treturn config/versioneer/config.py\nfrom versioneer.utils import get_git_author, get_git_version, get_version, get_version_tag, get_version_suffix, get_version_full\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t/pypi_install_script/django-rest-framework-filters-0.1.0.tar/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='django-rest-framework-filters',\n    version='0.1.0',\n    packages=find_packages(),\n    url='https://github.com/michael-lazar/django-rest-framework-filters',\n    license='MIT',\n    author='',\n    author_email='',\n    description='Django REST Framework Filters',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n    install_requires=[\n        'django>=1.6',\n        'djangorestframework>=3',\n        'coreapi>=2.0.0',\n        'inflection>=0.3.1',\n    ],\n)\n/pypi_install_script/py_eve_api-1.0.0.tar/setup.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time   "], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t\n\n\t# The VERSIONFILE_SOURCE attribute determines the source for the\n\t# version file. It defaults to VERSIONFILE_SOURCE_DEFAULT, which is\n\t# VERSIONFILE_SOURCE_GIT_DESCRIBE, but can be overridden by\n\t# setting the VERSIONFILE_SOURCE environment variable\n\tversionfile_source = os.environ.get('VERSIONFILE_SOURCE', None)\n\tif versionfile_source == 'git':\n\t\tversionfile_source = VERSIONFILE_SOURCE_GIT\n\telif versionfile_source =='setuptools':\n\t\tversionfile_source = VERSIONFILE_SOURCE_SETUPTOOLS\n\telif versionfile_source is None:\n\t\tversionfile_source = VERSIONFILE_SOURCE_GIT_DESCRIBE\n\telse:\n\t\tversionfile_source = None\n\n\t# The VERSIONFILE_OUTPUT attribute determines the path to the file\n\t# that will contain the version number. It defaults to\n\t# VERSIONFILE_OUTPUT_DEFAULT, which is _version.py in the\n\t# source tree.\n\tversionfile_output = os.environ.get('VERSIONFILE_OUTPUT', None)\n\tif versionfile_output is None:\n\t\tversionfile_output = VERSIONFILE_OUTPUT_DEFAULT\n\n\t# The VCS_COMMAND attribute determines the command used to determine\n\t# the version. It defaults to VCS_COMMAND_DEFAULT, which is\n\t# '", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"v\"\n\t\tconfig.versionfile_source = \"src/version.py\"\n\t\tconfig.verbose = False # print more verbosely\n\t\treturn config\n/src/version.py\n# This file is part of the Ginga project.\n#\n# Copyright (c) 2014, 2017 by the Ginga developers\n# Licensed under the GNU General Public License\n\n# Ginga version.\n#\n# This is the first part of the version string.\n#\n# It is a PEP440 compliant version.\n#\n# We have the following components:\n#\n# MAJOR: Major release, backwards-incompatible changes.\n# MINOR: Minor release, backwards-compatible new features.\n# MICRO: Patch release, backwards-compatible bug fixes.\n# VERSION: Version number.  This is a string like \"1.2.3\"\n#\n# The version number is a string of the form:\n#\n#    MAJOR.MINOR.MICRO\n#\n# The version number is converted into a numeric form:\n#\n#    MAJOR * 100", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t/README.md\n#'s Portfolio\n\nThis is a personal portfolio for, a data scientist at [Zillow](https://www.zillow.com/).\n\n## Built With\n\n* [Django](https://docs.djangoproject.com/en/2.2/) - The web framework used\n* [Django REST Framework](https://www.django-rest-framework.org/) - The REST framework used\n* [PostgreSQL](https://www.postgresql.org/) - The relational database used\n* [PostgreSQL Extensions](https://postgis.net/) - The spatial database extension used\n* [PostGIS](https://postgis.net/) - The spatial database used\n* [Bootstrap](https://getbootstrap.com/) - The CSS framework used\n* [Bootstrap Datepicker](https://bootstrap-datepicker.readthedocs.io/en/latest/) - The datepicker widget used\n* [Leaflet](https://leafletjs.com/) - The JavaScript library used for the map\n* [Leaflet Draw](https://github.com/Leaflet/Leaflet.draw) - The plugin for drawing shapes on the map\n* [Leaflet Geocoder](https://github.com/perliedman/leaflet-control-geocoder) - The plugin for searching places\n* [Leaflet Markercluster](https://github.com/Leaflet/Leaflet.", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t \n\tv = VersioneerConfig()\n\tv.VCS = 'git'\n\tv.style = 'pep440'\n\tv.tag_prefix = 'v'\n\tv.parentdir_prefix = 'python-'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_build = 'python_versioneer/_version.py'\n\tv.tag_prefix = 'v'\n\tv.parentdir_prefix = 'python-'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_build = 'python_versioneer/_version.py'\n\tv.tag_prefix = 'v'\n\tv.parentdir_prefix = 'python-'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_build = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python_versioneer/_version.py'\n\tv.versionfile_source = 'python", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t\n\n\t# Read in the configuration from the file\n\tconfig = VersioneerConfig()\n\tconfig.read()\n\n\t# Set the git author\n\tconfig.get_git_author()\n\n\t# Set the git version\n\tconfig.get_git_version()\n\n\t# Set the version\n\tconfig.get_version()\n\n\t# Set the version tag\n\tconfig.get_version_tag()\n\n\t# Set the version suffix\n\tconfig.get_version_suffix()\n\n\t# Set the version full\n\tconfig.get_version_full()\n\n\treturn config/versioneer/config.py\nfrom versioneer.utils import get_git_author, get_git_version, get_version, get_version_tag, get_version_suffix, get_version_full\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\t/pypi_install_script/django-rest-framework-filters-0.1.0.tar/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='django-rest-framework-filters',\n    version='0.1.0',\n    packages=find_packages(),\n    url='https://github.com/michael-lazar/django-rest-framework-filters',\n    license='MIT',\n    author='',\n    author_email='',\n    description='Django REST Framework Filters',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n    install_requires=[\n        'django>=1.6',\n        'djangorestframework>=3',\n        'coreapi>=2.0.0',\n        'inflection>=0.3.1',\n    ],\n)\n/pypi_install_script/py_eve_api-1.0.0.tar/setup.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time   ", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tfunc.__vcs_handler__ = vcs\n\t\treturn func\n\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t/setup.py\nimport setuptools\n\nsetuptools.setup(\n\tname='vcscore',\n\tversion='0.0.1',\n\tdescription='A common core for version control systems',\n\turl='https://github.com/rabidgremlin/vcscore',\n\tclassifiers=[\n\t\t'Programming Language :: Python :: 3',\n\t\t'License :: OSI Approved :: MIT License',\n\t\t'Operating System :: OS Independent',\n\t],\n\tpackages=setuptools.find_packages(),\n\tpython_requires='>=3.5',\n\tinstall_requires=[\n\t\t'pytoml',\n\t\t'gitpython',\n\t\t'pathlib',\n\t],\n\textras_require={\n\t\t'git': ['gitpython'],\n\t},\n\tsetup_requires=[\n\t\t'pytest-runner',\n\t],\n\ttests_require=[\n\t\t'pytest',\n\t],\n\ttest_suite='test',\n\tauthor='',\n\tauthor_email='',\n\tlicense='MIT',\n)\n\n# TODO:\n# Add support for other VCSs\n# Add support for more VCS's than git\n# Add support for multiple types of git repositories (bare, normal, etc)\n# Add support for more than one working directory in a repository\n# Add support for multiple working directories in a repository\n# Add better error", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tfunc.is_handler = True\n\t\tfunc.vcs = vcs\n\t\tfunc.method = method\n\t\treturn func\n\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef wrapper(func):\n\t\t\"\"\"\n\t\tThe wrapper\n\t\t\"\"\"\n\t\tvcs.add_handler(method, func)\n\t\treturn func\n\treturn wrapper\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t \n\tdef decorator(method):\n\t\tget_vcs_handler(vcs).add(method)\n\t\treturn method\n\treturn decorator\n\t", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator for a method to register it as a handler of a object\n\t\t\"\"\"\n\t\tsetattr(func, 'handler_for', vcs)\n\t\tsetattr(func, 'handler_method', method)\n\n\t\treturn func\n\n\treturn decorator\n/test.py\nfrom vcs_handler import register_vcs_handler\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef register(func):\n\t\tvcs.register_handler(method, func)\n\t\treturn func\n\treturn register\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator to register a handler method\n\t\t\"\"\"\n\t\tfunc.vcs = vcs\n\t\tmethod(func)\n\t\treturn func\n\n\treturn decorator\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tfunc.__vcs_handler__ = vcs\n\t\treturn func\n\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t/setup.py\nimport setuptools\n\nsetuptools.setup(\n\tname='vcscore',\n\tversion='0.0.1',\n\tdescription='A common core for version control systems',\n\turl='https://github.com/rabidgremlin/vcscore',\n\tclassifiers=[\n\t\t'Programming Language :: Python :: 3',\n\t\t'License :: OSI Approved :: MIT License',\n\t\t'Operating System :: OS Independent',\n\t],\n\tpackages=setuptools.find_packages(),\n\tpython_requires='>=3.5',\n\tinstall_requires=[\n\t\t'pytoml',\n\t\t'gitpython',\n\t\t'pathlib',\n\t],\n\textras_require={\n\t\t'git': ['gitpython'],\n\t},\n\tsetup_requires=[\n\t\t'pytest-runner',\n\t],\n\ttests_require=[\n\t\t'pytest',\n\t],\n\ttest_suite='test',\n\tauthor='',\n\tauthor_email='',\n\tlicense='MIT',\n)\n\n# TODO:\n# Add support for other VCSs\n# Add support for more VCS's than git\n# Add support for multiple types of git repositories (bare, normal, etc)\n# Add support for more than one working directory in a repository\n# Add support for multiple working directories in a repository\n# Add better error", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\tfunc.is_handler = True\n\t\tfunc.vcs = vcs\n\t\tfunc.method = method\n\t\treturn func\n\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef wrapper(func):\n\t\t\"\"\"\n\t\tThe wrapper\n\t\t\"\"\"\n\t\tvcs.add_handler(method, func)\n\t\treturn func\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t \n\tdef decorator(method):\n\t\tget_vcs_handler(vcs).add(method)\n\t\treturn method\n\treturn decorator\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator for a method to register it as a handler of a object\n\t\t\"\"\"\n\t\tsetattr(func, 'handler_for', vcs)\n\t\tsetattr(func, 'handler_method', method)\n\n\t\treturn func\n\n\treturn decorator\n/test.py\nfrom vcs_handler import register_vcs_handler\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t\n\n\tdef register(func):\n\t\tvcs.register_handler(method, func)\n\t\treturn func\n\treturn register\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\t \n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator to register a handler method\n\t\t\"\"\"\n\t\tfunc.vcs = vcs\n\t\tmethod(func)\n\t\treturn func\n\n\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n        num_objects = 0\n        good_objects = 0\n        errors = 0\n        warnings = 0\n        num_errors = 0\n        num_warnings = 0\n        total_errors = 0\n        total_warnings = 0\n        \n        objects = []\n        if not validate_objects:\n            objects = self.get_all_objects()\n        else:\n            objects = self.get_all_objects(self.root)\n        \n        for obj in objects:\n            if not obj.is_object():\n                continue\n            num_objects += 1\n            if obj.is_valid(check_digests=check_digests):\n                good_objects += 1\n            else:\n                errors += 1\n                total_errors += 1\n            if obj.is_bad(check_digests=check_digests):\n                warnings += 1\n                total_warnings += 1\n            if show_warnings:\n                if obj.is_bad(check_digests=check_digests):\n                    print('WARNING: %s is bad' % (obj.name,))\n                else:\n                    print('ERROR: %s is bad' % (obj.name,))\n        return num_objects, good_objects, errors, warnings, total_errors, total_warnings\n        \n\n    def validate_storage_root(self, validate_objects=True, check_digests=True):\n        \"\"\"\n        Validate storage root.\n    Returns:", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\tgood_objects = 0\n\tfor root_dir in self.root_dirs:\n\t\tif validate_objects:\n\t\t\tfor root, dirs, files in os.walk(root_dir):\n\t\t\t\tfor name in files:\n\t\t\t\t\tobj_path = os.path.join(root, name)\n\t\t\t\t\tif check_digests:\n\t\t\t\t\t\tif StorageObject.is_object(obj_path):\n\t\t\t\t\t\t\tobj = StorageObject(obj_path)\n\t\t\t\t\t\t\tif obj.is_valid():\n\t\t\t\t\t\t\t\tgood_objects += 1\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tgood_objects += 1\n\t\t\t\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\t\t\t\tprint \"WARNING: %s does not have a valid digest.\" % obj_path\n\t\t\t\t\t\t\tnum_objects += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tnum_objects += 1\n\t\t\t\t\t\tgood_objects += 1\n\t\telse:\n\t\t\tfor root, dirs, files in os.walk(root_dir):\n\t\t\t\tfor name in files:\n\t\t\t\t\tnum_objects += 1\n\t\t\t\t\tgood_objects += 1\n\t\treturn num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tif validate_objects:\n\t\t\tprint('Validating storage root hierarchy...')\n\t\t\tprint()\n\t\t\tfor root_dir in self.root_dirs:\n\t\t\t\tnum_objects += 1\n\t\t\t\ttry:\n\t\t\t\t\tif self.root_dirs[root_dir].validate(check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\t\t\tgood_objects += 1\n\t\t\t\texcept:\n\t\t\t\t\tprint('Unable to validate storage root hierarchy')\n\t\t\t\t\traise\n\t\telse:\n\t\t\tprint('Storage root hierarchy validated')\n\t\treturn num_objects, good_objects\n\t\t", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tif validate_objects:\n\t\tif check_digests:\n\t\t\tif show_warnings:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\telse:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\telse:\n\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\tdef validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tif validate_objects:\n\t\tif check_digests:\n\t\t\tif show_warnings:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\telse:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\telse:\n\t\t", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n/storage/storage_node/storage/data_storage/data_storage_test.py\nimport unittest\nimport os\nimport time\nimport filecmp\nfrom storage.data_storage.data_storage import DataStorage\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n#\tprint \"Validating hierarchy...\"\n\tobjects_checked = 0\n\tgood_objects = 0\n\tbad_objects = 0\n\tbad_digests = 0\n\tif validate_objects:\n\t\tfor digest, obj in self.root_objects.items():\n\t\t\tobjects_checked = objects_checked + 1\n\t\t\tif not obj.check_integrity(show_warnings=show_warnings):\n\t\t\t\tbad_objects = bad_objects + 1\n\t\t\telse:\n\t\t\t\tgood_objects = good_objects + 1\n\tif check_digests:\n\t\tfor digest, obj in self.root_objects.items():\n\t\t\tif not obj.valid_digest:\n\t\t\t\tbad_digests = bad_digests + 1\n\treturn objects_checked, good_objects, bad_objects, bad_digests\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tif validate_objects and self.root_digest is not None:\n\t\t\tnum_objects, good_objects = validate_hierarchy(self.root_digest, self.root_object, self.root_name, self.root_prefix, check_digests, show_warnings)\n\t\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\t# TODO:\n\t\t# make sure that all objects are present\n\t\t# make sure that all objects have a parent\n\t\t# make sure that all objects have a child\n\t\t# make sure that all objects have a digest\n\t\t# make sure that all objects have a size\n\t\t# make sure that all objects have a blocklist\n\t\t# make sure that all objects have a blocklist length\n\t\t# make sure that all objects have a blocklist hash\n\t\t# make sure that all objects have a blocklist digest\n\t\t# make sure that all objects have a blocklist digest length\n\t\t# make sure that all objects have a blocklist digest hash\n\t\t# make sure that all objects have a blocklist digest hash length\n\t\t# make sure that all objects have a blocklist digest hash digest\n\t\t# make sure that all objects have a blocklist digest hash digest length\n\t\t# make sure that all objects have a blocklist digest hash digest hash\n\t\t# make sure that all objects have a blocklist digest hash digest hash length\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest length\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest hash\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest hash length\n\t\t# make sure that all objects have a blocklist", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n# \tprint \"validating hierarchy\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor path in self.path_map:\n\t\tif not path.startswith(\"content/\"):\n\t\t\tcontinue\n\t\tnum_objects += 1\n\t\tif path in self.object_map:\n\t\t\tgood_objects += 1\n\t\t\tif check_digests:\n\t\t\t\t#check_digest(self.object_map[path], self.digests[path])\n\t\t\t\tobj = self.object_map[path]\n\t\t\t\tif not obj.is_valid():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object is invalid: %s\" % path\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\"object is invalid: %s\" % path)\n\t\t\t\tif not obj.has_valid_digest():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object has invalid digest: %s\" % path\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\"object has invalid digest: %s\" % path)\n\t\t\t\tif obj.is_deleted():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object is deleted: %s\" % path\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\"object is deleted: %s\" % path)\n\t\t\t\tif obj.is_bad():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object is bad: %s\" % path\n\t\t\t\t", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\t#\n\t# Validate hierarchy\n\t#\n\tnum_objects = 0\n\tgood_objects = 0\n\tif validate_objects:\n\t\tfor (path, storage_root) in self.storage_roots.itervalues():\n\t\t\tif path == \"\":\n\t\t\t\tpath = \"/\"\n\t\t\tif show_warnings:\n\t\t\t\tprint \"Validating hierarchy:\", path\n\t\t\t(num_objects, good_objects) = storage_root.validate_hierarchy(path, num_objects, good_objects, check_digests, show_warnings)\n\tif check_digests:\n\t\tfor (path, storage_root) in self.storage_roots.iteritems():\n\t\t\tif path == \"\":\n\t\t\t\tpath = \"/\"\n\t\t\tif show_warnings:\n\t\t\t\tprint \"Validating digests:\", path\n\t\t\tstorage_root.validate_digests(path, show_warnings)\n\treturn (num_objects, good_objects)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n        num_objects = 0\n        good_objects = 0\n        errors = 0\n        warnings = 0\n        num_errors = 0\n        num_warnings = 0\n        total_errors = 0\n        total_warnings = 0\n        \n        objects = []\n        if not validate_objects:\n            objects = self.get_all_objects()\n        else:\n            objects = self.get_all_objects(self.root)\n        \n        for obj in objects:\n            if not obj.is_object():\n                continue\n            num_objects += 1\n            if obj.is_valid(check_digests=check_digests):\n                good_objects += 1\n            else:\n                errors += 1\n                total_errors += 1\n            if obj.is_bad(check_digests=check_digests):\n                warnings += 1\n                total_warnings += 1\n            if show_warnings:\n                if obj.is_bad(check_digests=check_digests):\n                    print('WARNING: %s is bad' % (obj.name,))\n                else:\n                    print('ERROR: %s is bad' % (obj.name,))\n        return num_objects, good_objects, errors, warnings, total_errors, total_warnings\n        \n\n    def validate_storage_root(self, validate_objects=True, check_digests=True):\n        \"\"\"\n        Validate storage root.\n    Returns:", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\tgood_objects = 0\n\tfor root_dir in self.root_dirs:\n\t\tif validate_objects:\n\t\t\tfor root, dirs, files in os.walk(root_dir):\n\t\t\t\tfor name in files:\n\t\t\t\t\tobj_path = os.path.join(root, name)\n\t\t\t\t\tif check_digests:\n\t\t\t\t\t\tif StorageObject.is_object(obj_path):\n\t\t\t\t\t\t\tobj = StorageObject(obj_path)\n\t\t\t\t\t\t\tif obj.is_valid():\n\t\t\t\t\t\t\t\tgood_objects += 1\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tgood_objects += 1\n\t\t\t\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\t\t\t\tprint \"WARNING: %s does not have a valid digest.\" % obj_path\n\t\t\t\t\t\t\tnum_objects += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tnum_objects += 1\n\t\t\t\t\t\tgood_objects += 1\n\t\telse:\n\t\t\tfor root, dirs, files in os.walk(root_dir):\n\t\t\t\tfor name in files:\n\t\t\t\t\tnum_objects += 1\n\t\t\t\t\tgood_objects += 1\n\t\treturn num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tif validate_objects:\n\t\t\tprint('Validating storage root hierarchy...')\n\t\t\tprint()\n\t\t\tfor root_dir in self.root_dirs:\n\t\t\t\tnum_objects += 1\n\t\t\t\ttry:\n\t\t\t\t\tif self.root_dirs[root_dir].validate(check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\t\t\tgood_objects += 1\n\t\t\t\texcept:\n\t\t\t\t\tprint('Unable to validate storage root hierarchy')\n\t\t\t\t\traise\n\t\telse:\n\t\t\tprint('Storage root hierarchy validated')\n\t\treturn num_objects, good_objects\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tif validate_objects:\n\t\tif check_digests:\n\t\t\tif show_warnings:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\telse:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\telse:\n\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\tdef validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tif validate_objects:\n\t\tif check_digests:\n\t\t\tif show_warnings:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\telse:\n\t\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\t\tself.validate_hierarchy(validate_objects, check_digests, False)\n\t\telse:\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n/storage/storage_node/storage/data_storage/data_storage_test.py\nimport unittest\nimport os\nimport time\nimport filecmp\nfrom storage.data_storage.data_storage import DataStorage\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n#\tprint \"Validating hierarchy...\"\n\tobjects_checked = 0\n\tgood_objects = 0\n\tbad_objects = 0\n\tbad_digests = 0\n\tif validate_objects:\n\t\tfor digest, obj in self.root_objects.items():\n\t\t\tobjects_checked = objects_checked + 1\n\t\t\tif not obj.check_integrity(show_warnings=show_warnings):\n\t\t\t\tbad_objects = bad_objects + 1\n\t\t\telse:\n\t\t\t\tgood_objects = good_objects + 1\n\tif check_digests:\n\t\tfor digest, obj in self.root_objects.items():\n\t\t\tif not obj.valid_digest:\n\t\t\t\tbad_digests = bad_digests + 1\n\treturn objects_checked, good_objects, bad_objects, bad_digests\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tif validate_objects and self.root_digest is not None:\n\t\t\tnum_objects, good_objects = validate_hierarchy(self.root_digest, self.root_object, self.root_name, self.root_prefix, check_digests, show_warnings)\n\t\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\t# TODO:\n\t\t# make sure that all objects are present\n\t\t# make sure that all objects have a parent\n\t\t# make sure that all objects have a child\n\t\t# make sure that all objects have a digest\n\t\t# make sure that all objects have a size\n\t\t# make sure that all objects have a blocklist\n\t\t# make sure that all objects have a blocklist length\n\t\t# make sure that all objects have a blocklist hash\n\t\t# make sure that all objects have a blocklist digest\n\t\t# make sure that all objects have a blocklist digest length\n\t\t# make sure that all objects have a blocklist digest hash\n\t\t# make sure that all objects have a blocklist digest hash length\n\t\t# make sure that all objects have a blocklist digest hash digest\n\t\t# make sure that all objects have a blocklist digest hash digest length\n\t\t# make sure that all objects have a blocklist digest hash digest hash\n\t\t# make sure that all objects have a blocklist digest hash digest hash length\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest length\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest hash\n\t\t# make sure that all objects have a blocklist digest hash digest hash digest hash length\n\t\t# make sure that all objects have a blocklist", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n# \tprint \"validating hierarchy\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor path in self.path_map:\n\t\tif not path.startswith(\"content/\"):\n\t\t\tcontinue\n\t\tnum_objects += 1\n\t\tif path in self.object_map:\n\t\t\tgood_objects += 1\n\t\t\tif check_digests:\n\t\t\t\t#check_digest(self.object_map[path], self.digests[path])\n\t\t\t\tobj = self.object_map[path]\n\t\t\t\tif not obj.is_valid():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object is invalid: %s\" % path\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\"object is invalid: %s\" % path)\n\t\t\t\tif not obj.has_valid_digest():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object has invalid digest: %s\" % path\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\"object has invalid digest: %s\" % path)\n\t\t\t\tif obj.is_deleted():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object is deleted: %s\" % path\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\"object is deleted: %s\" % path)\n\t\t\t\tif obj.is_bad():\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint \"object is bad: %s\" % path\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\t#\n\t# Validate hierarchy\n\t#\n\tnum_objects = 0\n\tgood_objects = 0\n\tif validate_objects:\n\t\tfor (path, storage_root) in self.storage_roots.itervalues():\n\t\t\tif path == \"\":\n\t\t\t\tpath = \"/\"\n\t\t\tif show_warnings:\n\t\t\t\tprint \"Validating hierarchy:\", path\n\t\t\t(num_objects, good_objects) = storage_root.validate_hierarchy(path, num_objects, good_objects, check_digests, show_warnings)\n\tif check_digests:\n\t\tfor (path, storage_root) in self.storage_roots.iteritems():\n\t\t\tif path == \"\":\n\t\t\t\tpath = \"/\"\n\t\t\tif show_warnings:\n\t\t\t\tprint \"Validating digests:\", path\n\t\t\tstorage_root.validate_digests(path, show_warnings)\n\treturn (num_objects, good_objects)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tpass\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n        # Create the root directory\n        os.mkdir(self.root_dir)\n        \n        # Create the OCFL inventory\n        inventory = OcflInventory(self.root_dir)\n        inventory.initialize()\n        \n        # Create the OCFL storage root\n        storage_root = OcflStorageRoot(self.root_dir, inventory)\n        storage_root.initialize()\n        \n        # Create the OCFL storage root version\n        version = OcflVersion(storage_root)\n        version.initialize()\n        \n        # Create the OCFL storage root version manifest\n        manifest = OcflVersionManifest(version, storage_root)\n        manifest.initialize()\n        \n        # Create the OCFL storage root version manifest digest\n        digest = OcflVersionManifestDigest(manifest, version)\n        digest.initialize()\n        \n        # Create the OCFL storage root version inventory\n        inventory = OcflVersionInventory(version)\n        inventory.initialize()\n        \n        # Create the OCFL storage root version inventory digest\n        digest = OcflVersionInventoryDigest(inventory, version)\n        digest.initialize()\n        \n        # Create the OCFL storage root version objects directory\n        objects = OcflVersionObjects(version)\n        objects.initialize()\n\n\t# Create the OCFL storage root version objects directory digest\n\tdigest = OcflVersionObjectsDigest(objects, version)\n\tdigest.initialize()\n        \n        # Create the OCFL storage root version objects directory object map\n        map = OcflVersion", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\t# create the OCFL object and the OCFL storage root\n\tself.object = OCFLObject()\n\tself.object.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\tself.object.set_location(self.root_dir)\n\t# create the OCFL storage root\n\tself.root = OCFLStorageRoot()\n\tself.root.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root manifest\n\tself.root_manifest = OCFLStorageRootManifest()\n\tself.root_manifest.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root version manifest\n\tself.root_version_manifest = OCFLStorageRootVersionManifest()\n\tself.root_version_manifest.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root inventory\n\tself.root_inventory = OCFLStorageRootInventory()\n\tself.root_inventory.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root version inventory\n\tself.root_", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tpass\n\n\tdef create_object(self, object_id, content, **kwargs):\n\t\t\"\"\"\n\t\tCreate an object in the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef delete_object(self, object_id):\n\t\t\"\"\"\n\t\tDelete an object from the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_object(self, object_id, version=None):\n\t\t\"\"\"\n\t\tFetch an object from the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef describe_object(self, object_id, version=None):\n\t\t\"\"\"\n\t\tGet information about an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef list_objects(self, version=None):\n\t\t\"\"\"\n\t\tList all objects in the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef create_inventory(self, object_id, content, **kwargs):\n\t\t\"\"\"\n\t\tCreate an inventory for an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef delete_inventory(self, object_id, inventory_id):\n\t\t\"\"\"\n\t\tDelete an inventory for an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_inventory(self, object_id, inventory_id, version=None):\n\t\t\"\"\"\n\t\tFetch an inventory for an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef describe_inventory(self, object_id, inventory_id,", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n        # Create the root directory\n        os.mkdir(self.root_dir)\n        \n        # Create the OCFL inventory\n        inventory = OcflInventory(self.root_dir)\n        inventory.initialize()\n        \n        # Create the OCFL storage root\n        storage_root = OcflStorageRoot(self.root_dir, inventory)\n        storage_root.initialize()\n        \n        # Create the OCFL storage root version\n        version = OcflVersion(storage_root)\n        version.initialize()\n        \n        # Create the OCFL storage root version manifest\n        manifest = OcflVersionManifest(version, storage_root)\n        manifest.initialize()\n        \n        # Create the OCFL storage root version manifest digest\n        digest = OcflVersionManifestDigest(manifest, version)\n        digest.initialize()\n        \n        # Create the OCFL storage root version inventory\n        inventory = OcflVersionInventory(version)\n        inventory.initialize()\n        \n        # Create the OCFL storage root version inventory digest\n        digest = OcflVersionInventoryDigest(inventory, version)\n        digest.initialize()\n        \n        # Create the OCFL storage root version objects directory\n        objects = OcflVersionObjects(version)\n        objects.initialize()\n\n\t# Create the OCFL storage root version objects directory digest\n\tdigest = OcflVersionObjectsDigest(objects, version)\n\tdigest.initialize()\n        \n        # Create the OCFL storage root version objects directory object map\n        map = OcflVersion", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\t# create the OCFL object and the OCFL storage root\n\tself.object = OCFLObject()\n\tself.object.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\tself.object.set_location(self.root_dir)\n\t# create the OCFL storage root\n\tself.root = OCFLStorageRoot()\n\tself.root.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root manifest\n\tself.root_manifest = OCFLStorageRootManifest()\n\tself.root_manifest.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root version manifest\n\tself.root_version_manifest = OCFLStorageRootVersionManifest()\n\tself.root_version_manifest.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root inventory\n\tself.root_inventory = OCFLStorageRootInventory()\n\tself.root_inventory.create(self.root_dir, self.OCFL_VERSION, self.OCFL_VERSION)\n\t# create the OCFL storage root version inventory\n\tself.root_", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tpass\n\n\tdef create_object(self, object_id, content, **kwargs):\n\t\t\"\"\"\n\t\tCreate an object in the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef delete_object(self, object_id):\n\t\t\"\"\"\n\t\tDelete an object from the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_object(self, object_id, version=None):\n\t\t\"\"\"\n\t\tFetch an object from the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef describe_object(self, object_id, version=None):\n\t\t\"\"\"\n\t\tGet information about an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef list_objects(self, version=None):\n\t\t\"\"\"\n\t\tList all objects in the storage root.\n\t\t\"\"\"\n\t\tpass\n\n\tdef create_inventory(self, object_id, content, **kwargs):\n\t\t\"\"\"\n\t\tCreate an inventory for an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef delete_inventory(self, object_id, inventory_id):\n\t\t\"\"\"\n\t\tDelete an inventory for an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_inventory(self, object_id, inventory_id, version=None):\n\t\t\"\"\"\n\t\tFetch an inventory for an object.\n\t\t\"\"\"\n\t\tpass\n\n\tdef describe_inventory(self, object_id, inventory_id,", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\t# get major, minor and patch\n\tm = re.match('(\\d+).(\\d+).(\\d+)', version)\n\tmajor = int(m.group(1))\n\tminor = int(m.group(2))\n\tpatch = int(m.group(3))\n\t\n\t# bump patch\n\tpatch += 1\n\t\n\t# return next version\n\treturn '{}.{}.{}'.format(major, minor, patch)\n\n\n#\n# main\n#\n\nif __name__ == '__main__':\n\t\n\t# get version\n\tversion = get_version()\n\t\n\t# get next version\n\tnext_version = next_version(version)\n\t\n\t# get changelog file\n\tchangelog_file = os.path.join(here, 'CHANGELOG')\n\t\n\t# get changelog\n\twith open(changelog_file, 'r') as f:\n\t\tchangelog = f.read()\n\t\n\t# get changelog version\n\tm = re.match('(\\d+.\\d+.\\d+)', changelog)\n\tchangelog_version = m.group(1)\n\t\n\t# print version\n\tprint('Current version: {}'.format(version))\n\tprint('Next version: {}'.format(next_version))\n\t\n\t# check versions\n\tif version == next_version:\n\t\tprint('Version {} is the same as next version {}. Exiting.'.format", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version + 1\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\tsplit = version.split(\".\")\n\tmajor = int(split[0])\n\tminor = int(split[1])\n\tpatch = int(split[2])\n\n\tpatch += 1\n\n\tversion = \"{major}.{minor}.{patch}\".format(major=major, minor=minor, patch=patch)\n\n\treturn version\n/README.md\n# py-version\n\nPython module for versioning your project.\n\n[![Build Status](https://travis-ci.org/joseph-briseno/py-version.svg?branch=master)](https://travis-ci.org/joseph-briseno/py-version)\n[![Coverage Status](https://coveralls.io/repos/github/joseph-briseno/py-version/badge.svg?branch=master)](https://coveralls.io/github/joseph-briseno/py-version?branch=master)\n\n## Install\n\n```\npip install py-version\n```\n\n## Usage\n\n```\nfrom version import Version\n\nv = Version()\n\n# Get version\nv.get()\n\n# Set version\nv.set(\"1.0.0\")\n\n# Get next\nv.get_next()\n```\n\n## CLI\n\n```\nusage: version [-h] {get,set,next}...\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\tif version == '0.0.0':\n\t\t\treturn '0.1.0'\n\t\telse:\n\t\t\treturn '.'.join(map(str, map(int, version.split('.')))) + '.1'\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version\n\t", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n/README.md\n# nextVersion\n\nAutomates the versioning of software projects\n\n## Usage\n\n## Development\n\n## Tests\n\n## Author\n\nWritten by \n\n## License\n\nMIT\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\t# get major, minor and patch\n\tm = re.match('(\\d+).(\\d+).(\\d+)', version)\n\tmajor = int(m.group(1))\n\tminor = int(m.group(2))\n\tpatch = int(m.group(3))\n\t\n\t# bump patch\n\tpatch += 1\n\t\n\t# return next version\n\treturn '{}.{}.{}'.format(major, minor, patch)\n\n\n#\n# main\n#\n\nif __name__ == '__main__':\n\t\n\t# get version\n\tversion = get_version()\n\t\n\t# get next version\n\tnext_version = next_version(version)\n\t\n\t# get changelog file\n\tchangelog_file = os.path.join(here, 'CHANGELOG')\n\t\n\t# get changelog\n\twith open(changelog_file, 'r') as f:\n\t\tchangelog = f.read()\n\t\n\t# get changelog version\n\tm = re.match('(\\d+.\\d+.\\d+)', changelog)\n\tchangelog_version = m.group(1)\n\t\n\t# print version\n\tprint('Current version: {}'.format(version))\n\tprint('Next version: {}'.format(next_version))\n\t\n\t# check versions\n\tif version == next_version:\n\t\tprint('Version {} is the same as next version {}. Exiting.'.format", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\tsplit = version.split(\".\")\n\tmajor = int(split[0])\n\tminor = int(split[1])\n\tpatch = int(split[2])\n\n\tpatch += 1\n\n\tversion = \"{major}.{minor}.{patch}\".format(major=major, minor=minor, patch=patch)\n\n\treturn version\n/README.md\n# py-version\n\nPython module for versioning your project.\n\n[![Build Status](https://travis-ci.org/joseph-briseno/py-version.svg?branch=master)](https://travis-ci.org/joseph-briseno/py-version)\n[![Coverage Status](https://coveralls.io/repos/github/joseph-briseno/py-version/badge.svg?branch=master)](https://coveralls.io/github/joseph-briseno/py-version?branch=master)\n\n## Install\n\n```\npip install py-version\n```\n\n## Usage\n\n```\nfrom version import Version\n\nv = Version()\n\n# Get version\nv.get()\n\n# Set version\nv.set(\"1.0.0\")\n\n# Get next\nv.get_next()\n```\n\n## CLI\n\n```\nusage: version [-h] {get,set,next}...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\tif version == '0.0.0':\n\t\t\treturn '0.1.0'\n\t\telse:\n\t\t\treturn '.'.join(map(str, map(int, version.split('.')))) + '.1'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n/README.md\n# nextVersion\n\nAutomates the versioning of software projects\n\n## Usage\n\n## Development\n\n## Tests\n\n## Author\n\nWritten by \n\n## License\n\nMIT\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\t\n\n\treturn version\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tself.version_content_digests = {}\n\t\tself.version_content_digests[0] = self.root_inventory.content_digest\n\t\tfor i in range(1, len(version_dirs)):\n\t\t\tif i not in self.version_content_digests:\n\t\t\t\traise Exception(\"No inventory for version %d\" % i)\n\t\t\t# Check that we have inventory for this version\n\t\t\tversion_inventory_path = os.path.join(self.base_path, version_dirs[i], \"inventory.json\")\n\t\t\ttry:\n\t\t\t\tself.version_content_digests[i] = self.validate_inventory(version_inventory_path)\n\t\t\texcept Exception as e:\n\t\t\t\t# This is not a fatal error, we just need to find out which version is the bad one.\n\t\t\t\tself.logger.warning(\"Error validating inventory for version %d: %s\" % (i, e))\n\t\t\t\tbreak\n\t\t# Verify that all the content digests are the same\n\t\tfor version in self.version_content_digests:\n\t\t\tif self.version_content_digests[0]!= self.version_content_digests[version]:\n\t\t\t\traise Exception(\"Content digest for version %d is not the same as for version 0\" % version)\n\n    def validate_inventories(self):\n        \"\"\"\n        Validate all", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tif not version_dirs:\n\t\t\treturn\n\t\tversion_dirs_rev = list(reversed(version_dirs))\n\t\tfor i in range(len(version_dirs_rev)):\n\t\t\tversion_dir = version_dirs_rev[i]\n\t\t\tif i == 0:\n\t\t\t\tinv_fn = \"root-inventory.json\"\n\t\t\telse:\n\t\t\t\tinv_fn = \"%s-inventory.json\" % version_dir\n\t\t\tfn = os.path.join(self.root_dir, version_dir, inv_fn)\n\t\t\twith open(fn, \"r\") as f:\n\t\t\t\tinv = json.load(f)\n\t\t\tif i == 0:\n\t\t\t\tself.root_inventory = inv\n\t\t\telse:\n\t\t\t\tself.validate_inventory(self.root_inventory, inv, version_dir)\n\n\tdef validate_inventory(self, root_inv, inv, version_dir):\n\t\t\"\"\"\nCheck that inventory inv conforms to root_inv.\n\nroot_inv is the root inventory from which inv is derived.\ninv is the inventory to validate.\nversion_dir is the version directory name which is used to generate the paths\nto the inventory and content files.\n\t\"\"\"\n\t\tif not inv:\n\t\t\treturn\n\t\tif not root_inv:\n\t\t\tprint(\"empty root inventory\", file=sys.stderr)", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n#\tinventory = {}\n#\t\n#\tfor version_dir in version_dirs:\n#\t\tinventory[version_dir] = get_inventory(version_dir)\n#\t\tif inventory[version_dir].get('content_digest')!= inventory[version_dir].get('content_digest'):\n#\t\t\traise Exception('Content digest is different from root inventory!')\n#\t\t\t\n#\troot_inventory['version_inventories'] = inventory\n\t\n\tself.validate_inventory_tree(version_dirs)\n\t\n\t\n\treturn root_inventory\n\t\n\t# TODO: check version inventories against each other.\n\t\n\t# TODO: check for duplicate content digests\n\t\n\t# TODO: check for content digests in different versions that don't match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tself.content_digests = {}\n\t\tfor v in version_dirs:\n\t\t\tif not (os.path.exists(os.path.join(self.path, v, \"inventory.txt\")) and os.path.getsize(os.path.join(self.path, v, \"inventory.txt\")) > 0):\n\t\t\t\traise ValueError(\"Missing inventory for version \" + v)\n\t\t\twith open(os.path.join(self.path, v, \"inventory.txt\"), \"r\") as f:\n\t\t\t\tfor line in f.readlines():\n\t\t\t\t\tline = line.strip()\n\t\t\t\t\tif not line:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif line.startswith(\"#\"):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif line.startswith(\"Content-Digest\"):\n\t\t\t\t\t\tself.content_digests[v] = line.split(\":\", 1)[1].strip()\n\t\t# Check that the digests are the same for all versions of content\n\t\tif len(self.content_digests) > 1:\n\t\t\tdigest = self.content_digests[version_dirs[0]]\n\t\t\tfor v in self.content_digests.keys():\n\t\t\t\tif self.content_digests[v]!= digest:\n\t\t\t\t\traise ValueError(\"Inventory digests are not consistent\")\n\n\tdef validate_content(self, version_dirs):\n\t\t\"\"\"\nEach version SHOULD have", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n        # Inventories are named \"inventories.json\" and have the same\n        # name as the corresponding version directory.\n        # The inventory for a version is a subset of the\n        # inventory for the previous version.\n        # Each version contains one or more files.\n        # Each file is named \"<name>.<format>\" where\n        # <name> is the name of the file as it occurs\n        # in the version directory and <format> is the\n        # format of the file.\n        # Each file has an \"asset\" object that contains\n        # the name, format, and digest of the file.\n        # Each file has a \"content\" object that contains the\n        # content of the file in the format it is stored.\n        #\n        # The inventory for the root version is a complete\n        # inventory for the entire repository.\n        #\n        # The root inventory is named \"inventory.json\"\n        # and it is the same as the inventory for version 1.\n\t#\n\t# We don't care about the \"content\" objects in the root inventory.\n\n        # We don't care about the \"content\" objects in the version inventories.\n\t#\n\t# We don't care about the \"asset\" objects in any version inventory.\n\t#\n\t# We keep a record of the \"digest\" objects and the \"name\" and \"format\"\n\t# of each file.\n\t#\n\t#", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tcontent_digests = {}\n\t\tfor version_dir in version_dirs:\n\t\t\tversion_inventory_path = os.path.join(version_dir, 'inventory.json')\n\t\t\twith open(version_inventory_path) as version_inventory_file:\n\t\t\t\tversion_inventory = json.load(version_inventory_file)\n\t\t\t# Get the root inventory\n\t\t\tif not hasattr(self, 'root_inventory'):\n\t\t\t\tself.root_inventory = version_inventory\n\t\t\t# Check for content digests that are different from those in the root inventory\n\t\t\tcontent_digests[version_dir] = set(version_inventory['content_digests'].keys())\n\t\t\tcontent_digests[version_dir].difference_update(self.root_inventory['content_digests'].keys())\n\t\t\tif len(content_digests[version_dir]) > 0:\n\t\t\t\t# Only bother comparing if there are any differences\n\t\t\t\tself.log.info('Content digests for %s are different from those in the root inventory' % version_dir)\n\t\t\t\tfor content_digest in content_digests[version_dir]:\n\t\t\t\t\tself.log.info('Root: %s, %s: %s' % (self.root_inventory['content_digests'][content_digest], version_dir, version_inventory['content_digests'][content_", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n        for version in version_dirs:\n                current_inventory = os.path.join(version, 'inventory')\n                if not os.path.exists(current_inventory):\n                        return False\n                # Compare the content digests in the root inventory and the version inventory\n                with open(current_inventory) as inventory_file:\n                        root_content = json.load(inventory_file)\n                with open(self.inventory) as inventory_file:\n                        version_content = json.load(inventory_file)\n                # Check the content digests\n                for entry in version_content:\n                        if entry['content_digest']!= root_content[entry['name']]:\n                                self.content_digest_differences[entry['name']] = [entry['content_digest'], root_content[entry['name']]]\n                # Check the content digests of the files in the version directory\n                for filename in os.listdir(version):\n                        if not filename.endswith('.json'):\n                                continue\n                        file_path = os.path.join(version, filename)\n                        with open(file_path) as file:\n                                file_content = json.load(file)\n                        if not self.content_digest_differences.has_key(filename):\n                                if file_content['content_digest']!= root_content[filename]:\n                                        self.content_digest_differences[filename] = [file_content['content_digest'], root", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\t# Find all inventories in the version directories.\n\t# If we find an inventory, it should be the same for all versions.\n\t# If we find an inventories.json file, it should be the same for all versions.\n\t# If we find a content.json file, it should be the same for all versions.\n\t# If we find a content directory, it should be the same for all versions.\n\t# If we find a content.json file, it should be the same for all versions.\n\n\t# For each version directory, if we find an inventory, it should be the same.\n\t# For each version directory, if we find an inventory.json file, it should be the same.\n\t# For each version directory, if we find a content.json file, it should be the same.\n\t# For each version directory, if we find a content directory, it should be the same.\n\t# For each version directory, if we find a content.json file, it should be the same.\n\n\t# If we find an inventory, it should be the same for all versions.\n\t# If we find an inventories.json file, it should be the same for all versions.\n\t# If we find a content.json file, it should be the same for all versions.\n\t# If we find a content directory, it should be the same for all", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tself.log.debug(\"validate_version_inventories: version_dirs=\" + str(version_dirs))\n\t\tself.root_inventory = self.get_inventory(os.path.join(self.root_dir, 'inventory.csv'))\n\t\tself.root_inventory_digests = self.get_inventory_digests(self.root_inventory)\n\t\tself.versions_with_inventories = []\n\t\tself.versions_with_digests = []\n\t\tself.versions_without_digests = []\n\t\tself.versions_with_different_digests = []\n\t\tfor version_dir in version_dirs:\n\t\t\tif not os.path.exists(os.path.join(self.root_dir, version_dir)):\n\t\t\t\tself.log.error(\"validate_version_inventories: version_dir=\" + version_dir + \" does not exist\")\n\t\t\t\tcontinue\n\t\t\tversion_inventory = self.get_inventory(os.path.join(self.root_dir, version_dir, 'inventory.csv'))\n\t\t\tversion_inventory_digests = self.get_inventory_digests(version_inventory)\n\t\t\tif version_inventory_digests!= self.root_inventory_digests:\n\t\t\t\tself.versions_with_different_digests.append(version_dir)\n\t\t\telse:\n\t\t\t\t", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\n\n\t# Check for missing inventories\n\t# Check for missing content digests\n\t# Check for differing content digests\n\n\t# Check for differing inventories\n\n\t# Check for differing inventory digests\n\n\t# Check for differing inventory content digests\n\n\t# Check for differing content digests from root inventory\n\n\t# Check for differing content digests from version inventories\n\n\tpass\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tself.version_content_digests = {}\n\t\tself.version_content_digests[0] = self.root_inventory.content_digest\n\t\tfor i in range(1, len(version_dirs)):\n\t\t\tif i not in self.version_content_digests:\n\t\t\t\traise Exception(\"No inventory for version %d\" % i)\n\t\t\t# Check that we have inventory for this version\n\t\t\tversion_inventory_path = os.path.join(self.base_path, version_dirs[i], \"inventory.json\")\n\t\t\ttry:\n\t\t\t\tself.version_content_digests[i] = self.validate_inventory(version_inventory_path)\n\t\t\texcept Exception as e:\n\t\t\t\t# This is not a fatal error, we just need to find out which version is the bad one.\n\t\t\t\tself.logger.warning(\"Error validating inventory for version %d: %s\" % (i, e))\n\t\t\t\tbreak\n\t\t# Verify that all the content digests are the same\n\t\tfor version in self.version_content_digests:\n\t\t\tif self.version_content_digests[0]!= self.version_content_digests[version]:\n\t\t\t\traise Exception(\"Content digest for version %d is not the same as for version 0\" % version)\n\n    def validate_inventories(self):\n        \"\"\"\n        Validate all", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tif not version_dirs:\n\t\t\treturn\n\t\tversion_dirs_rev = list(reversed(version_dirs))\n\t\tfor i in range(len(version_dirs_rev)):\n\t\t\tversion_dir = version_dirs_rev[i]\n\t\t\tif i == 0:\n\t\t\t\tinv_fn = \"root-inventory.json\"\n\t\t\telse:\n\t\t\t\tinv_fn = \"%s-inventory.json\" % version_dir\n\t\t\tfn = os.path.join(self.root_dir, version_dir, inv_fn)\n\t\t\twith open(fn, \"r\") as f:\n\t\t\t\tinv = json.load(f)\n\t\t\tif i == 0:\n\t\t\t\tself.root_inventory = inv\n\t\t\telse:\n\t\t\t\tself.validate_inventory(self.root_inventory, inv, version_dir)\n\n\tdef validate_inventory(self, root_inv, inv, version_dir):\n\t\t\"\"\"\nCheck that inventory inv conforms to root_inv.\n\nroot_inv is the root inventory from which inv is derived.\ninv is the inventory to validate.\nversion_dir is the version directory name which is used to generate the paths\nto the inventory and content files.\n\t\"\"\"\n\t\tif not inv:\n\t\t\treturn\n\t\tif not root_inv:\n\t\t\tprint(\"empty root inventory\", file=sys.stderr)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n#\tinventory = {}\n#\t\n#\tfor version_dir in version_dirs:\n#\t\tinventory[version_dir] = get_inventory(version_dir)\n#\t\tif inventory[version_dir].get('content_digest')!= inventory[version_dir].get('content_digest'):\n#\t\t\traise Exception('Content digest is different from root inventory!')\n#\t\t\t\n#\troot_inventory['version_inventories'] = inventory\n\t\n\tself.validate_inventory_tree(version_dirs)\n\t\n\t\n\treturn root_inventory\n\t\n\t# TODO: check version inventories against each other.\n\t\n\t# TODO: check for duplicate content digests\n\t\n\t# TODO: check for content digests in different versions that don't match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t# TODO: check for content digests in different versions that match\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tself.content_digests = {}\n\t\tfor v in version_dirs:\n\t\t\tif not (os.path.exists(os.path.join(self.path, v, \"inventory.txt\")) and os.path.getsize(os.path.join(self.path, v, \"inventory.txt\")) > 0):\n\t\t\t\traise ValueError(\"Missing inventory for version \" + v)\n\t\t\twith open(os.path.join(self.path, v, \"inventory.txt\"), \"r\") as f:\n\t\t\t\tfor line in f.readlines():\n\t\t\t\t\tline = line.strip()\n\t\t\t\t\tif not line:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif line.startswith(\"#\"):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif line.startswith(\"Content-Digest\"):\n\t\t\t\t\t\tself.content_digests[v] = line.split(\":\", 1)[1].strip()\n\t\t# Check that the digests are the same for all versions of content\n\t\tif len(self.content_digests) > 1:\n\t\t\tdigest = self.content_digests[version_dirs[0]]\n\t\t\tfor v in self.content_digests.keys():\n\t\t\t\tif self.content_digests[v]!= digest:\n\t\t\t\t\traise ValueError(\"Inventory digests are not consistent\")\n\n\tdef validate_content(self, version_dirs):\n\t\t\"\"\"\nEach version SHOULD have", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n        # Inventories are named \"inventories.json\" and have the same\n        # name as the corresponding version directory.\n        # The inventory for a version is a subset of the\n        # inventory for the previous version.\n        # Each version contains one or more files.\n        # Each file is named \"<name>.<format>\" where\n        # <name> is the name of the file as it occurs\n        # in the version directory and <format> is the\n        # format of the file.\n        # Each file has an \"asset\" object that contains\n        # the name, format, and digest of the file.\n        # Each file has a \"content\" object that contains the\n        # content of the file in the format it is stored.\n        #\n        # The inventory for the root version is a complete\n        # inventory for the entire repository.\n        #\n        # The root inventory is named \"inventory.json\"\n        # and it is the same as the inventory for version 1.\n\t#\n\t# We don't care about the \"content\" objects in the root inventory.\n\n        # We don't care about the \"content\" objects in the version inventories.\n\t#\n\t# We don't care about the \"asset\" objects in any version inventory.\n\t#\n\t# We keep a record of the \"digest\" objects and the \"name\" and \"format\"\n\t# of each file.\n\t#\n\t#", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tcontent_digests = {}\n\t\tfor version_dir in version_dirs:\n\t\t\tversion_inventory_path = os.path.join(version_dir, 'inventory.json')\n\t\t\twith open(version_inventory_path) as version_inventory_file:\n\t\t\t\tversion_inventory = json.load(version_inventory_file)\n\t\t\t# Get the root inventory\n\t\t\tif not hasattr(self, 'root_inventory'):\n\t\t\t\tself.root_inventory = version_inventory\n\t\t\t# Check for content digests that are different from those in the root inventory\n\t\t\tcontent_digests[version_dir] = set(version_inventory['content_digests'].keys())\n\t\t\tcontent_digests[version_dir].difference_update(self.root_inventory['content_digests'].keys())\n\t\t\tif len(content_digests[version_dir]) > 0:\n\t\t\t\t# Only bother comparing if there are any differences\n\t\t\t\tself.log.info('Content digests for %s are different from those in the root inventory' % version_dir)\n\t\t\t\tfor content_digest in content_digests[version_dir]:\n\t\t\t\t\tself.log.info('Root: %s, %s: %s' % (self.root_inventory['content_digests'][content_digest], version_dir, version_inventory['content_digests'][content_", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n        for version in version_dirs:\n                current_inventory = os.path.join(version, 'inventory')\n                if not os.path.exists(current_inventory):\n                        return False\n                # Compare the content digests in the root inventory and the version inventory\n                with open(current_inventory) as inventory_file:\n                        root_content = json.load(inventory_file)\n                with open(self.inventory) as inventory_file:\n                        version_content = json.load(inventory_file)\n                # Check the content digests\n                for entry in version_content:\n                        if entry['content_digest']!= root_content[entry['name']]:\n                                self.content_digest_differences[entry['name']] = [entry['content_digest'], root_content[entry['name']]]\n                # Check the content digests of the files in the version directory\n                for filename in os.listdir(version):\n                        if not filename.endswith('.json'):\n                                continue\n                        file_path = os.path.join(version, filename)\n                        with open(file_path) as file:\n                                file_content = json.load(file)\n                        if not self.content_digest_differences.has_key(filename):\n                                if file_content['content_digest']!= root_content[filename]:\n                                        self.content_digest_differences[filename] = [file_content['content_digest'], root", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\t# Find all inventories in the version directories.\n\t# If we find an inventory, it should be the same for all versions.\n\t# If we find an inventories.json file, it should be the same for all versions.\n\t# If we find a content.json file, it should be the same for all versions.\n\t# If we find a content directory, it should be the same for all versions.\n\t# If we find a content.json file, it should be the same for all versions.\n\n\t# For each version directory, if we find an inventory, it should be the same.\n\t# For each version directory, if we find an inventory.json file, it should be the same.\n\t# For each version directory, if we find a content.json file, it should be the same.\n\t# For each version directory, if we find a content directory, it should be the same.\n\t# For each version directory, if we find a content.json file, it should be the same.\n\n\t# If we find an inventory, it should be the same for all versions.\n\t# If we find an inventories.json file, it should be the same for all versions.\n\t# If we find a content.json file, it should be the same for all versions.\n\t# If we find a content directory, it should be the same for all", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tself.log.debug(\"validate_version_inventories: version_dirs=\" + str(version_dirs))\n\t\tself.root_inventory = self.get_inventory(os.path.join(self.root_dir, 'inventory.csv'))\n\t\tself.root_inventory_digests = self.get_inventory_digests(self.root_inventory)\n\t\tself.versions_with_inventories = []\n\t\tself.versions_with_digests = []\n\t\tself.versions_without_digests = []\n\t\tself.versions_with_different_digests = []\n\t\tfor version_dir in version_dirs:\n\t\t\tif not os.path.exists(os.path.join(self.root_dir, version_dir)):\n\t\t\t\tself.log.error(\"validate_version_inventories: version_dir=\" + version_dir + \" does not exist\")\n\t\t\t\tcontinue\n\t\t\tversion_inventory = self.get_inventory(os.path.join(self.root_dir, version_dir, 'inventory.csv'))\n\t\t\tversion_inventory_digests = self.get_inventory_digests(version_inventory)\n\t\t\tif version_inventory_digests!= self.root_inventory_digests:\n\t\t\t\tself.versions_with_different_digests.append(version_dir)\n\t\t\telse:\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\n\n\t# Check for missing inventories\n\t# Check for missing content digests\n\t# Check for differing content digests\n\n\t# Check for differing inventories\n\n\t# Check for differing inventory digests\n\n\t# Check for differing inventory content digests\n\n\t# Check for differing content digests from root inventory\n\n\t# Check for differing content digests from version inventories\n\n\tpass\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\t# get the item at the path\n\titem = xbmcgui.ListItem(path=path)\n\txbmc.Player().play(item)\n\txbmc.sleep(500)\n\t# get the current player object\n\tplayer = xbmc.Player()\n\t# get the current player properties\n\tproperties = player.getPlayingMediaInfoTag()\n\t# get the type of the current player\n\ttype = properties[\"type\"]\n\t# return the type of the current player\n\treturn type\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n\n\tif os.path.islink(path):\n\t\treturn 'link'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telse:\n\t\treturn 'unknown'\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n\n\tif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.islink(path):\n\t\treturn'symlink'\n\telse:\n\t\treturn 'unknown'\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n#\tprint \"path is: \", path\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"symlink\"\n\telse:\n\t\treturn \"unknown\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t/README.md\n# py_file_finder\n\n## A Python script to recursively search for files\n\n### Usage\n```\npython py_file_finder.py -p /some/path -f \\*\\.txt\n```\n\n### To Do\n* More options for filtering (e.g. file size)\n* More options for output (e.g. file size, modification time, etc.)\n* More options for output format (e.g. HTML, JSON, XML, etc.)\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\nfind_path_type(os.path.join(os.environ['HOME'], 'foo'))\n# => 'file'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'bar'))\n# => 'directory'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'bar.txt'))\n# => 'file'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'baz'))\n# => 'directory'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'baz', 'qux.txt'))\n# => 'file'\n```\n\n\n## `find_file_type`\n\n### Description\n\nGiven a path, determine the file type\n\n### Usage\n\n```python\nfind_file_type(path)\n```\n\n### Parameters\n\n#### `path`\n\n##### Description\n\nThe path to the file\n\n##### Type\n\n`string`\n\n### Return Value\n\n##### Description\n\nA string indicating the type of the file (see below)\n\n##### Types\n\n`string`\n\n### Examples\n\n```python\nfrom utilities import find_file_type\n\nfind_file_type('/usr/bin/python')\n#", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\t# get the item at the path\n\titem = xbmcgui.ListItem(path=path)\n\txbmc.Player().play(item)\n\txbmc.sleep(500)\n\t# get the current player object\n\tplayer = xbmc.Player()\n\t# get the current player properties\n\tproperties = player.getPlayingMediaInfoTag()\n\t# get the type of the current player\n\ttype = properties[\"type\"]\n\t# return the type of the current player\n\treturn type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n\n\tif os.path.islink(path):\n\t\treturn 'link'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telse:\n\t\treturn 'unknown'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n\n\tif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.islink(path):\n\t\treturn'symlink'\n\telse:\n\t\treturn 'unknown'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n#\tprint \"path is: \", path\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"symlink\"\n\telse:\n\t\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t/README.md\n# py_file_finder\n\n## A Python script to recursively search for files\n\n### Usage\n```\npython py_file_finder.py -p /some/path -f \\*\\.txt\n```\n\n### To Do\n* More options for filtering (e.g. file size)\n* More options for output (e.g. file size, modification time, etc.)\n* More options for output format (e.g. HTML, JSON, XML, etc.)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\nfind_path_type(os.path.join(os.environ['HOME'], 'foo'))\n# => 'file'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'bar'))\n# => 'directory'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'bar.txt'))\n# => 'file'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'baz'))\n# => 'directory'\nfind_path_type(os.path.join(os.environ['HOME'], 'foo', 'baz', 'qux.txt'))\n# => 'file'\n```\n\n\n## `find_file_type`\n\n### Description\n\nGiven a path, determine the file type\n\n### Usage\n\n```python\nfind_file_type(path)\n```\n\n### Parameters\n\n#### `path`\n\n##### Description\n\nThe path to the file\n\n##### Type\n\n`string`\n\n### Return Value\n\n##### Description\n\nA string indicating the type of the file (see below)\n\n##### Types\n\n`string`\n\n### Examples\n\n```python\nfrom utilities import find_file_type\n\nfind_file_type('/usr/bin/python')\n#", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\t# Use the Bugzilla bug ID to get additional info from Bugzilla.\n\t\tbz_bugs = self.get_bz_bugs(bug_ids)\n\t\t# Add the Bugzilla fields to the params dict.\n\t\tfor bug in bz_bugs:\n\t\t\tparams[self.bz_fields[bug['id']]['name']] = bug[self.bz_fields[bug['id']]['field']]\n\n\tdef get_bz_bugs(self, bug_ids):\n\t\t\"\"\"\n\t\tGet a list of bug info from Bugzilla\n\t\t\"\"\"\n\t\t# Get the list of bugs from Bugzilla.\n\t\tbz_bugs = self.bz.getbugs(bug_ids)\n\t\t# Return the list of bugs.\n\t\treturn bz_bugs\n\n\tdef get_bz_fields(self):\n\t\t\"\"\"\n\t\tGet a list of fields from Bugzilla\n\t\t\"\"\"\n\t\t# Get the list of fields from Bugzilla.\n\t\tbz_fields = self.bz.get_fields()\n\t\t# Return the list of fields.\n\t\treturn bz_fields\n\n\tdef get_bz_id(self):\n\t\t\"\"\"\n\t\tGet the Bugzilla id\n\t\t\"\"\"\n\t\t# Get the Bugzilla id.\n\t\tbz_id = self.bz.user\n\t\t# Return the Bugzilla id.\n\t\treturn bz_id\n\n# Create", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['ids'] = bug_ids\n\t\tparams['include_fields'] = ['summary', 'target_milestone', 'cc']\n\t\treturn params\n\tdef amend_bzparams_milestone(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.milestone:\n\t\t\tparams['milestone'] = self.args.milestone\n\t\treturn params\n\tdef amend_bzparams_product(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.product:\n\t\t\tparams['product'] = self.args.product\n\t\tif self.args.component:\n\t\t\tparams['component'] = self.args.component\n\t\treturn params\n\tdef amend_bzparams_bug_type(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.bug_type:\n\t\t\tparams['bug_type'] = self.args.bug_type\n\t\treturn params\n\tdef amend_bzparams_status(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.status:\n\t\t\tparams['status'] = self.args.status\n\t\treturn params\n# Bugzilla\n\n## Bugzilla\n\nBugzilla is a web-based bug tracking system", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n#\tbz = Bugzilla(self.user, self.password, self.bugzilla_url)\n\tbz = Bugzilla(self.user, self.password, self.bugzilla_url)\n\tbz.login()\n\t#print \"Amending bugzilla params\"\n\tfor bug_id in bug_ids:\n\t\t#print \"Amending bugzilla params for bug #{}\".format(bug_id)\n\t\tbz.update_bug(bug_id, params)\n\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['ids'] = bug_ids\n\t\tparams['include_fields'] = ['alias','status','assigned_to',\n\t\t\t'alias','assigned_to','cf_qa_target_milestone',\n\t\t\t'cf_qa_whiteboard','cf_qa_product_version','cf_qa_component',\n\t\t\t'cf_qa_milestone','cf_qa_target_milestone']\n\t\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['bug_id'] = bug_ids\n\t\tparams['short_desc'] = 'Testing'\n\t\tparams['comment'] = 'Testing'\n\t\tparams['cf_tracking_milestone'] = '6.1'\n\t\tparams['cf_tracking_status'] = 'Triaged'\n\t\tparams['cf_tracking_status_detail'] = 'Test'\n\t\tparams['cf_tracking_severity'] = 'Normal'\n\t\tparams['cf_tracking_priority'] = 'Normal'\n\t\tparams['cf_tracking_qa_contact'] = ''\n\t\tparams['cf_tracking_qa_contact_detail'] = ''\n\t\tparams['cf_tracking_qa_contact_email'] = ''\n\t\tparams['cf_tracking_qa_contact_email_detail'] = ''\n\t\tparams['cf_tracking_qa_contact_phone'] = '+66 9 131 4700'\n\t\tparams['cf_tracking_qa_owner'] = ''\n\t\tparams['cf_tracking_qa_owner_detail'] = ''\n\t\tparams['cf_tracking_qa_owner_email'] = ''\n\t\tparams['cf_tracking_qa_owner_email_detail'] = ''\n\t\tparams['cf_tracking_qa_owner_phone'] = '+66", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n#\tfor param in params:\n#\t\tif param['name'] in ['bz_bug_color', 'bz_bug_priority', 'bz_bug_status', 'bz_bug_severity', 'bz_bug_type']:\n#\t\t\tparam['value'] = params[param['name']]\n\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['bug_id'] = bug_ids\n\t\tparams['bug_ids'] = bug_ids\n\t\tparams['bug_id[]'] = bug_ids\n\t\tparams['bug_ids[]'] = bug_ids\n\t\tparams['bug_id%2C%2C'] = bug_ids\n\t\tparams['bug_ids%2C%2C'] = bug_ids\n\t\tparams['bug_id%2C%2C%2C'] = bug_ids\n\t\tparams['bug_ids%2C%2C%2C'] = bug_ids\n\t\tparams['bug_id%2C%2C%2C%2C'] = bug_ids\n\t\tparams['bug_ids%2C%2C%2C%2C'] = bug_ids\n\t\t\n\t\treturn params\n```\n\n### Usage\n\nTo use the module just import it and call the `amend_bzparams` method.\n\n```python\nfrom bzparams import bzparams\n\nparams = {}\nparams['bug_id'] = 12345\n\n# Amend to all possible params\nparams = bzparams.amend_bzparams(params, params['bug_id'])\n\n```\n\n## Tests\n\nTo run the tests just type `python test_bzparams.py` in the root folder.\n\n##", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['Bugzilla_bug_ids'] = bug_ids\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\t# Get the params from the bug\n\t\tbz_params = self.get_bz_params(bug_ids)\n\t\t# Get the params from the bug\n\t\tfor bug_id in bug_ids:\n\t\t\t# Get the params from the bug\n\t\t\tbz_params = self.get_bz_params(bug_id)\n\t\t\t# Amend the params\n\t\t\tfor param in bz_params:\n\t\t\t\tvalue = bz_params[param]\n\t\t\t\tparams[param] = value\n\t\n\tdef get_bz_params(self, bug_id):\n\t\t\"\"\"\n\t\tGet the Bugzilla params\n\t\t\"\"\"\n\t\tbz_params = {}\n\t\t# Get the bug\n\t\tbug = self.get_bug(bug_id)\n\t\t# Get the params\n\t\tfor param in self.get_params():\n\t\t\t# Get the param\n\t\t\tparam = self.get_param(param)\n\t\t\t# Get the value\n\t\t\tvalue = self.get_param_value(bug, param)\n\t\t\t# Add the value to the params\n\t\t\tbz_params[param] = value\n\t\treturn bz_params\n\t\n\tdef get_param_value(self, bug, param):\n\t\t\"\"\"\n\t\tGet the param value\n\t\t\"\"\"\n\t\tparam = self.get_param(param)\n\t\t# Get the value\n\t\tvalue = bug[param]", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams[\"bug_id\"] = str(bug_ids)\n\t\tparams[\"short_desc\"] = params[\"summary\"]\n\t\tparams[\"description\"] = params[\"details\"]\n\t\tparams[\"alias\"] = params[\"bugzilla_alias\"]\n\t\tparams[\"op_sys\"] = params[\"bugzilla_os\"]\n\t\tparams[\"rep_platform\"] = params[\"bugzilla_platform\"]\n\t\tparams[\"version\"] = params[\"bugzilla_version\"]\n\t\tparams[\"product\"] = params[\"bugzilla_product\"]\n\t\tparams[\"component\"] = params[\"bugzilla_component\"]\n\t\tparams[\"bug_severity\"] = params[\"bugzilla_severity\"]\n\t\tparams[\"bug_severity\"] = params[\"bugzilla_severity\"]\n\t\tparams[\"target_milestone\"] = params[\"bugzilla_target_milestone\"]\n\t\tparams[\"status_whiteboard\"] = params[\"bugzilla_status_whiteboard\"]\n\t\tparams[\"resolution\"] = params[\"bugzilla_resolution\"]\n\t\tparams[\"assigned_to\"] = params[\"bugzilla_assigned_to\"]\n\t\tparams[\"qa_contact\"] = params[\"bugzilla_qa_contact\"]\n\t\tparams[\"priority\"] = params[\"bugzilla_priority\"]\n\t\tparams[\"product\"] = params[\"bugzilla_product\"]\n\t\tparams[\"component\"] = params[\"bugzilla_component\"]\n\treturn params\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\t# Use the Bugzilla bug ID to get additional info from Bugzilla.\n\t\tbz_bugs = self.get_bz_bugs(bug_ids)\n\t\t# Add the Bugzilla fields to the params dict.\n\t\tfor bug in bz_bugs:\n\t\t\tparams[self.bz_fields[bug['id']]['name']] = bug[self.bz_fields[bug['id']]['field']]\n\n\tdef get_bz_bugs(self, bug_ids):\n\t\t\"\"\"\n\t\tGet a list of bug info from Bugzilla\n\t\t\"\"\"\n\t\t# Get the list of bugs from Bugzilla.\n\t\tbz_bugs = self.bz.getbugs(bug_ids)\n\t\t# Return the list of bugs.\n\t\treturn bz_bugs\n\n\tdef get_bz_fields(self):\n\t\t\"\"\"\n\t\tGet a list of fields from Bugzilla\n\t\t\"\"\"\n\t\t# Get the list of fields from Bugzilla.\n\t\tbz_fields = self.bz.get_fields()\n\t\t# Return the list of fields.\n\t\treturn bz_fields\n\n\tdef get_bz_id(self):\n\t\t\"\"\"\n\t\tGet the Bugzilla id\n\t\t\"\"\"\n\t\t# Get the Bugzilla id.\n\t\tbz_id = self.bz.user\n\t\t# Return the Bugzilla id.\n\t\treturn bz_id\n\n# Create", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['ids'] = bug_ids\n\t\tparams['include_fields'] = ['summary', 'target_milestone', 'cc']\n\t\treturn params\n\tdef amend_bzparams_milestone(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.milestone:\n\t\t\tparams['milestone'] = self.args.milestone\n\t\treturn params\n\tdef amend_bzparams_product(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.product:\n\t\t\tparams['product'] = self.args.product\n\t\tif self.args.component:\n\t\t\tparams['component'] = self.args.component\n\t\treturn params\n\tdef amend_bzparams_bug_type(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.bug_type:\n\t\t\tparams['bug_type'] = self.args.bug_type\n\t\treturn params\n\tdef amend_bzparams_status(self, params):\n\t\t\"\"\"\n\t\tAmend the Bugzilla params\n\t\t\"\"\"\n\t\tif self.args.status:\n\t\t\tparams['status'] = self.args.status\n\t\treturn params\n# Bugzilla\n\n## Bugzilla\n\nBugzilla is a web-based bug tracking system", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n#\tbz = Bugzilla(self.user, self.password, self.bugzilla_url)\n\tbz = Bugzilla(self.user, self.password, self.bugzilla_url)\n\tbz.login()\n\t#print \"Amending bugzilla params\"\n\tfor bug_id in bug_ids:\n\t\t#print \"Amending bugzilla params for bug #{}\".format(bug_id)\n\t\tbz.update_bug(bug_id, params)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['ids'] = bug_ids\n\t\tparams['include_fields'] = ['alias','status','assigned_to',\n\t\t\t'alias','assigned_to','cf_qa_target_milestone',\n\t\t\t'cf_qa_whiteboard','cf_qa_product_version','cf_qa_component',\n\t\t\t'cf_qa_milestone','cf_qa_target_milestone']\n\t\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['bug_id'] = bug_ids\n\t\tparams['short_desc'] = 'Testing'\n\t\tparams['comment'] = 'Testing'\n\t\tparams['cf_tracking_milestone'] = '6.1'\n\t\tparams['cf_tracking_status'] = 'Triaged'\n\t\tparams['cf_tracking_status_detail'] = 'Test'\n\t\tparams['cf_tracking_severity'] = 'Normal'\n\t\tparams['cf_tracking_priority'] = 'Normal'\n\t\tparams['cf_tracking_qa_contact'] = ''\n\t\tparams['cf_tracking_qa_contact_detail'] = ''\n\t\tparams['cf_tracking_qa_contact_email'] = ''\n\t\tparams['cf_tracking_qa_contact_email_detail'] = ''\n\t\tparams['cf_tracking_qa_contact_phone'] = '+66 9 131 4700'\n\t\tparams['cf_tracking_qa_owner'] = ''\n\t\tparams['cf_tracking_qa_owner_detail'] = ''\n\t\tparams['cf_tracking_qa_owner_email'] = ''\n\t\tparams['cf_tracking_qa_owner_email_detail'] = ''\n\t\tparams['cf_tracking_qa_owner_phone'] = '+66", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n#\tfor param in params:\n#\t\tif param['name'] in ['bz_bug_color', 'bz_bug_priority', 'bz_bug_status', 'bz_bug_severity', 'bz_bug_type']:\n#\t\t\tparam['value'] = params[param['name']]\n\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['bug_id'] = bug_ids\n\t\tparams['bug_ids'] = bug_ids\n\t\tparams['bug_id[]'] = bug_ids\n\t\tparams['bug_ids[]'] = bug_ids\n\t\tparams['bug_id%2C%2C'] = bug_ids\n\t\tparams['bug_ids%2C%2C'] = bug_ids\n\t\tparams['bug_id%2C%2C%2C'] = bug_ids\n\t\tparams['bug_ids%2C%2C%2C'] = bug_ids\n\t\tparams['bug_id%2C%2C%2C%2C'] = bug_ids\n\t\tparams['bug_ids%2C%2C%2C%2C'] = bug_ids\n\t\t\n\t\treturn params\n```\n\n### Usage\n\nTo use the module just import it and call the `amend_bzparams` method.\n\n```python\nfrom bzparams import bzparams\n\nparams = {}\nparams['bug_id'] = 12345\n\n# Amend to all possible params\nparams = bzparams.amend_bzparams(params, params['bug_id'])\n\n```\n\n## Tests\n\nTo run the tests just type `python test_bzparams.py` in the root folder.\n\n##", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams['Bugzilla_bug_ids'] = bug_ids\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\t# Get the params from the bug\n\t\tbz_params = self.get_bz_params(bug_ids)\n\t\t# Get the params from the bug\n\t\tfor bug_id in bug_ids:\n\t\t\t# Get the params from the bug\n\t\t\tbz_params = self.get_bz_params(bug_id)\n\t\t\t# Amend the params\n\t\t\tfor param in bz_params:\n\t\t\t\tvalue = bz_params[param]\n\t\t\t\tparams[param] = value\n\t\n\tdef get_bz_params(self, bug_id):\n\t\t\"\"\"\n\t\tGet the Bugzilla params\n\t\t\"\"\"\n\t\tbz_params = {}\n\t\t# Get the bug\n\t\tbug = self.get_bug(bug_id)\n\t\t# Get the params\n\t\tfor param in self.get_params():\n\t\t\t# Get the param\n\t\t\tparam = self.get_param(param)\n\t\t\t# Get the value\n\t\t\tvalue = self.get_param_value(bug, param)\n\t\t\t# Add the value to the params\n\t\t\tbz_params[param] = value\n\t\treturn bz_params\n\t\n\tdef get_param_value(self, bug, param):\n\t\t\"\"\"\n\t\tGet the param value\n\t\t\"\"\"\n\t\tparam = self.get_param(param)\n\t\t# Get the value\n\t\tvalue = bug[param]", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams[\"bug_id\"] = str(bug_ids)\n\t\tparams[\"short_desc\"] = params[\"summary\"]\n\t\tparams[\"description\"] = params[\"details\"]\n\t\tparams[\"alias\"] = params[\"bugzilla_alias\"]\n\t\tparams[\"op_sys\"] = params[\"bugzilla_os\"]\n\t\tparams[\"rep_platform\"] = params[\"bugzilla_platform\"]\n\t\tparams[\"version\"] = params[\"bugzilla_version\"]\n\t\tparams[\"product\"] = params[\"bugzilla_product\"]\n\t\tparams[\"component\"] = params[\"bugzilla_component\"]\n\t\tparams[\"bug_severity\"] = params[\"bugzilla_severity\"]\n\t\tparams[\"bug_severity\"] = params[\"bugzilla_severity\"]\n\t\tparams[\"target_milestone\"] = params[\"bugzilla_target_milestone\"]\n\t\tparams[\"status_whiteboard\"] = params[\"bugzilla_status_whiteboard\"]\n\t\tparams[\"resolution\"] = params[\"bugzilla_resolution\"]\n\t\tparams[\"assigned_to\"] = params[\"bugzilla_assigned_to\"]\n\t\tparams[\"qa_contact\"] = params[\"bugzilla_qa_contact\"]\n\t\tparams[\"priority\"] = params[\"bugzilla_priority\"]\n\t\tparams[\"product\"] = params[\"bugzilla_product\"]\n\t\tparams[\"component\"] = params[\"bugzilla_component\"]\n\treturn params\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\t# First, create a dictionary of nodes keyed on node.name\n\tnode_dict = {}\n\tfor node in nodes:\n\t\tif node.name not in node_dict:\n\t\t\tnode_dict[node.name] = node\n\t\telse:\n\t\t\tnode_dict[node.name].value = node.value\n\t\t\tnode_dict[node.name].attrs = node.attrs\n\t\t\tnode_dict[node.name].children = node.children\n\t\t\tnode_dict[node.name].parent = node.parent\n\n\treturn node_dict.values()\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = MappingNode()\n\t\tfor node in nodes:\n\t\t\tfor key in node.keys():\n\t\t\t\tif key in result:\n\t\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(node[key], MappingNode):\n\t\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], node[key]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[key] = node[key]\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = node[key]\n\t\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged_nodes = []\n\t\tfor node in nodes:\n\t\t\tfor merged_node in merged_nodes:\n\t\t\t\tif node.key == merged_node.key:\n\t\t\t\t\tif isinstance(node, MappingNode) and isinstance(merged_node, MappingNode):\n\t\t\t\t\t\tmerged_node.update(node)\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged_nodes.append(node)\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tmerged_nodes.append(node)\n\t\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged_nodes = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor key, value in node.data.iteritems():\n\t\t\t\t\tif key in merged_nodes:\n\t\t\t\t\t\tif isinstance(merged_nodes[key], MappingNode):\n\t\t\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], node])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tmerged_nodes[key] = MappingNode(data = {key: merged_nodes[key], key: value})\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tmerged_nodes[node.key] = node\n\t\treturn merged_nodes\n\treturn MappingNode(data = merged_nodes)\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\t# Remove duplicate keys and keep the last one\n\tnodes = OrderedDict((k, v) for k, v in nodes if isinstance(v, MappingNode))\n\t\t# Merge duplicate keys\n\treturn OrderedDict(\n\t\t(k, deep_merge_nodes(g))\n\t\tfor k, g in itertools.groupby(nodes.values(), lambda n: n.key)\n\t)\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tfor key in set(nodes[0].keys()).intersection(*nodes[1:]):\n\t\t\tnew_node = nodes[0][key]\n\t\t\tfor node in nodes[1:]:\n\t\t\t\tif key in node:\n\t\t\t\t\tnew_value = node[key]\n\t\t\t\t\tif isinstance(new_value, MappingNode):\n\t\t\t\t\t\tnew_value = deep_merge_nodes([new_node, new_value])\n\t\t\t\t\tnew_node[key] = new_value\n\t\t\tdel nodes[0][key]\n\t\treturn nodes[0]\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n#\tif not isinstance(nodes, dict):\n#\t\traise TypeError(\"nodes must be a dict\")\n\tmerged = {}\n\tfor node in nodes:\n\t\tif node.key in merged:\n\t\t\tif isinstance(merged[node.key], MappingNode):\n\t\t\t\tmerged[node.key].merge(node)\n\t\t\telse:\n\t\t\t\t# the last node has priority\n\t\t\t\tmerged[node.key] = node\n\t\telse:\n\t\t\tmerged[node.key] = node\n\treturn merged\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tresult.update(deep_merge_nodes(node.children))\n\t\tfor key, value in result.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\treturn result\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tresult.update(deep_merge_nodes(node.children))\n\tfor key, value in result.items():\n\t\tif key in result:\n\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n#\tnode_keys = set(map(lambda node: node.key, nodes))\n#\tnode_values = map(lambda node: node.value, nodes)\n#\tresult = [node for node in node_values if node.key in node_keys]\n#\treturn result\n\tnode_keys = set(map(lambda node: node.key, nodes))\n\tnode_values = map(lambda node: node.value, nodes)\n\tresult = {}\n\tfor node in node_values:\n\t\tif node.key in node_keys:\n\t\t\tresult[node.key] = node\n\t\telse:\n\t\t\tresult[node.key] = node\n\treturn result.values()\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\n\tmerged = nodes[0]\n\n\tfor node in nodes[1:]:\n\t\tif isinstance(node, MappingNode):\n\t\t\tmerged = deep_merge_nodes([merged, node])\n\t\telse:\n\t\t\tfor key, value in node.items():\n\t\t\t\tif key in merged:\n\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged[key] = value\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\n\treturn merged\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\t# First, create a dictionary of nodes keyed on node.name\n\tnode_dict = {}\n\tfor node in nodes:\n\t\tif node.name not in node_dict:\n\t\t\tnode_dict[node.name] = node\n\t\telse:\n\t\t\tnode_dict[node.name].value = node.value\n\t\t\tnode_dict[node.name].attrs = node.attrs\n\t\t\tnode_dict[node.name].children = node.children\n\t\t\tnode_dict[node.name].parent = node.parent\n\n\treturn node_dict.values()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = MappingNode()\n\t\tfor node in nodes:\n\t\t\tfor key in node.keys():\n\t\t\t\tif key in result:\n\t\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(node[key], MappingNode):\n\t\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], node[key]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[key] = node[key]\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = node[key]\n\t\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged_nodes = []\n\t\tfor node in nodes:\n\t\t\tfor merged_node in merged_nodes:\n\t\t\t\tif node.key == merged_node.key:\n\t\t\t\t\tif isinstance(node, MappingNode) and isinstance(merged_node, MappingNode):\n\t\t\t\t\t\tmerged_node.update(node)\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged_nodes.append(node)\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tmerged_nodes.append(node)\n\t\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged_nodes = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor key, value in node.data.iteritems():\n\t\t\t\t\tif key in merged_nodes:\n\t\t\t\t\t\tif isinstance(merged_nodes[key], MappingNode):\n\t\t\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], node])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tmerged_nodes[key] = MappingNode(data = {key: merged_nodes[key], key: value})\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tmerged_nodes[node.key] = node\n\t\treturn merged_nodes\n\treturn MappingNode(data = merged_nodes)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\t# Remove duplicate keys and keep the last one\n\tnodes = OrderedDict((k, v) for k, v in nodes if isinstance(v, MappingNode))\n\t\t# Merge duplicate keys\n\treturn OrderedDict(\n\t\t(k, deep_merge_nodes(g))\n\t\tfor k, g in itertools.groupby(nodes.values(), lambda n: n.key)\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tfor key in set(nodes[0].keys()).intersection(*nodes[1:]):\n\t\t\tnew_node = nodes[0][key]\n\t\t\tfor node in nodes[1:]:\n\t\t\t\tif key in node:\n\t\t\t\t\tnew_value = node[key]\n\t\t\t\t\tif isinstance(new_value, MappingNode):\n\t\t\t\t\t\tnew_value = deep_merge_nodes([new_node, new_value])\n\t\t\t\t\tnew_node[key] = new_value\n\t\t\tdel nodes[0][key]\n\t\treturn nodes[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n#\tif not isinstance(nodes, dict):\n#\t\traise TypeError(\"nodes must be a dict\")\n\tmerged = {}\n\tfor node in nodes:\n\t\tif node.key in merged:\n\t\t\tif isinstance(merged[node.key], MappingNode):\n\t\t\t\tmerged[node.key].merge(node)\n\t\t\telse:\n\t\t\t\t# the last node has priority\n\t\t\t\tmerged[node.key] = node\n\t\telse:\n\t\t\tmerged[node.key] = node\n\treturn merged\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tresult.update(deep_merge_nodes(node.children))\n\t\tfor key, value in result.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\treturn result\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tresult.update(deep_merge_nodes(node.children))\n\tfor key, value in result.items():\n\t\tif key in result:\n\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n#\tnode_keys = set(map(lambda node: node.key, nodes))\n#\tnode_values = map(lambda node: node.value, nodes)\n#\tresult = [node for node in node_values if node.key in node_keys]\n#\treturn result\n\tnode_keys = set(map(lambda node: node.key, nodes))\n\tnode_values = map(lambda node: node.value, nodes)\n\tresult = {}\n\tfor node in node_values:\n\t\tif node.key in node_keys:\n\t\t\tresult[node.key] = node\n\t\telse:\n\t\t\tresult[node.key] = node\n\treturn result.values()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\n\tmerged = nodes[0]\n\n\tfor node in nodes[1:]:\n\t\tif isinstance(node, MappingNode):\n\t\t\tmerged = deep_merge_nodes([merged, node])\n\t\telse:\n\t\t\tfor key, value in node.items():\n\t\t\t\tif key in merged:\n\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged[key] = value\n\t\t\t\telse:\n\t\t\t\t\tmerged[key] = value\n\n\treturn merged\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('--version', action='version', version='%(prog)s 1.0')\n\tparser.add_argument('--infile', help='Input file', required=True)\n\tparser.add_argument('--outfile', help='Output file')\n\tparser.add_argument('--delimiter', help='Input file delimiter', default='\\t')\n\tparser.add_argument('--skip', help='Number of lines to skip from the beginning of the input file', type=int, default=1)\n\tparser.add_argument('--header', help='Number of lines to skip from the beginning of the input file', default=False)\n\tparser.add_argument('--columns', help='Columns to select', nargs='+', type=int, default=[])\n\tparser.add_argument('--columns_header', help='Header for the columns to select', default=False)\n\tparser.add_argument('--columns_delimiter', help='Delimiter for the columns to select', default='\\t')\n\tparser.add_argument('--exclude_columns', help='Columns to exclude', nargs='+', type=int, default=[])\n\tparser.add_argument('--exclude_columns_header', help='Header for the columns to exclude', default=False)\n\tparser.add_argument('--exclude_columns_delimiter', help", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = ArgumentParser(description=\"Generate a report of the current state of the system\")\n\tparser.add_argument(\"-t\", \"--target\", dest=\"target\", default=None, help=\"The hostname of the target to report on\")\n\tparser.add_argument(\"-u\", \"--username\", dest=\"username\", default=None, help=\"The username to use when connecting to the target\")\n\tparser.add_argument(\"-p\", \"--password\", dest=\"password\", default=None, help=\"The password to use when connecting to the target\")\n\tparser.add_argument(\"-c\", \"--command\", dest=\"command\", default=None, help=\"The command to run on the target\")\n\tparser.add_argument(\"-d\", \"--debug\", dest=\"debug\", action=\"store_true\", default=False, help=\"Enable debugging\")\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", default=None, help=\"The name of the file containing the command to run\")\n\tparser.add_argument(\"-m\", \"--method\", dest=\"method\", default=None, help=\"The method to use to connect to the target\")\n\tparser.add_argument(\"-s\", \"--server\", dest=\"server\", default=None, help=\"The address of the CIRT server to connect to\")\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = ArgumentParser(description=\"Process a FASTQ file and output a FASTA file with the corresponding sequence(s) from the original file(s)\")\n\n\tparser.add_argument(\"input_file\", type=str, help=\"A FASTQ file with sequences to be processed\")\n\tparser.add_argument(\"-s\", \"--sequences\", type=str, nargs=\"+\", help=\"A list of sequences to be searched for in the FASTQ file. If not provided, all sequences in the input file will be searched for.\")\n\tparser.add_argument(\"-o\", \"--output_file\", type=str, help=\"The name of the output file. If not provided, the output file will be written to STDOUT\")\n\tparser.add_argument(\"-t\", \"--threads\", type=int, default=1, help=\"The number of threads to be used for the search. Default is 1 thread\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 1.0\")\n\tparser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", help=\"Suppress verbose output\")\n\tparser.add_argument(\"-i\", \"--input_format\", type=str, default=\"fastq\", help=\"The format of the input file. Default is FASTQ\")\n\tparser.add_argument(\"-n\", \"--name_format\",", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=\"Retrieve and parse the latest version of a given GitHub repository\")\n\tparser.add_argument(\"repository\", help=\"The GitHub repository to parse\", type=str)\n\tparser.add_argument(\"--name\", help=\"The name of the file to retrieve\", type=str, default=\"README.md\")\n\tparser.add_argument(\"--path\", help=\"The path of the file to retrieve\", type=str, default=\"/\")\n\tparser.add_argument(\"--token\", help=\"A GitHub API token\", type=str)\n\tparser.add_argument(\"--output\", help=\"The output file\", type=str, default=\"output.md\")\n\t\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Convert LIMS JSON to CSV format')\n\tparser.add_argument('input_json_file', type=str, help='Input JSON file')\n\tparser.add_argument('output_csv_file', type=str, help='Output CSV file')\n\tparser.add_argument('--columns', type=str, help='Columns to extract from the JSON file. The default is to extract all columns')\n\tparser.add_argument('--debug', action='store_true', help='Print debug messages')\n\tparser.add_argument('--verbose', action='store_true', help='Print verbose messages')\n\tparser.add_argument('--quiet', action='store_true', help='Do not print any messages')\n\t\n\t# Parse the arguments and return them as an ArgumentParser instance\n\treturn parser.parse_args(*arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\t# Create the parser\n\tparser = argparse.ArgumentParser(description='Compute the number of cells in a cropped image')\n\t \n\t# Add the arguments\n\tparser.add_argument(\n\t\t'-c', '--crop',\n\t\taction='store_true',\n\t\thelp='Crop the image to remove the borders'\n\t)\n\t \n\tparser.add_argument(\n\t\t'-i', '--image',\n\t\ttype=str,\n\t\thelp='Name of the image to process'\n\t)\n\t \n\tparser.add_argument(\n\t\t'-o', '--output',\n\t\ttype=str,\n\t\thelp='Name of the output file'\n\t)\n\t \n\t# Execute the parse_args() method\n\targs = parser.parse_args(*arguments)\n\t \n\t# Return the parsed arguments as an ArgumentParser instance\n\treturn args\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(prog='qsub_wrapper', description='qsub wrapper for multiple jobs')\n\n\tparser.add_argument('-i', '--infile', required=True, help='File containing the list of jobs to run')\n\tparser.add_argument('-t', '--template', required=True, help='Template for the job')\n\tparser.add_argument('-o', '--output', required=True, help='Output directory')\n\tparser.add_argument('-b', '--binary', required=True, help='Path to binary')\n\tparser.add_argument('-m', '--mapper', required=True, help='Path to mapper')\n\tparser.add_argument('-r', '--reducer', required=True, help='Path to reducer')\n\tparser.add_argument('-s', '--submitter', required=True, help='Path to submitter')\n\tparser.add_argument('-c', '--compiler', required=True, help='Path to compiler')\n\tparser.add_argument('-q', '--queue', required=True, help='Queue to submit jobs to')\n\tparser.add_argument('-j', '--jobs', type=int, default=1, help='Number of jobs to run in parallel')\n\tparser.add_argument('-H', '--hold', default=None, help='Hold jobs until a certain job is finished')\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='This script compares two directories, looking for files that are present in one directory but not the other, and vice versa.')\n\tparser.add_argument('directory1', metavar='DIRECTORY1', type=str, help='Name of the first directory to compare')\n\tparser.add_argument('directory2', metavar='DIRECTORY2', type=str, help='Name of the second directory to compare')\n\tparser.add_argument('-v', '--verbose', action='store_true', help='Print extra information')\n\tparser.add_argument('-o', '--output', type=str, help='Filename to which output should be written')\n\tparser.add_argument('-c', '--check', action='store_true', help='Only check for files that exist in both directories')\n\tparser.add_argument('-s', '--silent', action='store_true', help='Suppress printing of output')\n\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Script which computes 95% confidence interval for a list of measurements and stores the results in a csv file\")\n\tparser.add_argument(\"--input_file\", dest=\"input_file\", required=True, help=\"Name of the csv file from which the data will be read\")\n\tparser.add_argument(\"--output_file\", dest=\"output_file\", required=False, default=\"output.csv\", help=\"Name of the csv file in which the results will be written\")\n\tparser.add_argument(\"--header\", dest=\"header\", required=False, default=False, help=\"Flag which indicates if the input file has a header\")\n\tparser.add_argument(\"--measure_column\", dest=\"measure_column\", required=False, default=\"measure\", help=\"Name of the column which contains the measurements\")\n\tparser.add_argument(\"--confidence\", dest=\"confidence\", required=False, default=95, help=\"Confidence level of the confidence interval (in %)\")\n\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Calculate the average and standard deviation of a set of numbers on the command line\")\n\tparser.add_argument('numbers', help=\"A sequence of numbers\", nargs=\"+\", type=float)\n\tparser.add_argument('-s', '--standard-deviation', help=\"Calculate the standard deviation\", action=\"store_true\")\n\tparser.add_argument('-c', '--correlation', help=\"Calculate the correlation\", action=\"store_true\")\n\tparser.add_argument('-v', '--variance', help=\"Calculate the variance\", action=\"store_true\")\n\tparser.add_argument('-a', '--average', help=\"Calculate the average\", action=\"store_true\")\n\treturn parser.parse_args(*arguments)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('--version', action='version', version='%(prog)s 1.0')\n\tparser.add_argument('--infile', help='Input file', required=True)\n\tparser.add_argument('--outfile', help='Output file')\n\tparser.add_argument('--delimiter', help='Input file delimiter', default='\\t')\n\tparser.add_argument('--skip', help='Number of lines to skip from the beginning of the input file', type=int, default=1)\n\tparser.add_argument('--header', help='Number of lines to skip from the beginning of the input file', default=False)\n\tparser.add_argument('--columns', help='Columns to select', nargs='+', type=int, default=[])\n\tparser.add_argument('--columns_header', help='Header for the columns to select', default=False)\n\tparser.add_argument('--columns_delimiter', help='Delimiter for the columns to select', default='\\t')\n\tparser.add_argument('--exclude_columns', help='Columns to exclude', nargs='+', type=int, default=[])\n\tparser.add_argument('--exclude_columns_header', help='Header for the columns to exclude', default=False)\n\tparser.add_argument('--exclude_columns_delimiter', help", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = ArgumentParser(description=\"Generate a report of the current state of the system\")\n\tparser.add_argument(\"-t\", \"--target\", dest=\"target\", default=None, help=\"The hostname of the target to report on\")\n\tparser.add_argument(\"-u\", \"--username\", dest=\"username\", default=None, help=\"The username to use when connecting to the target\")\n\tparser.add_argument(\"-p\", \"--password\", dest=\"password\", default=None, help=\"The password to use when connecting to the target\")\n\tparser.add_argument(\"-c\", \"--command\", dest=\"command\", default=None, help=\"The command to run on the target\")\n\tparser.add_argument(\"-d\", \"--debug\", dest=\"debug\", action=\"store_true\", default=False, help=\"Enable debugging\")\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", default=None, help=\"The name of the file containing the command to run\")\n\tparser.add_argument(\"-m\", \"--method\", dest=\"method\", default=None, help=\"The method to use to connect to the target\")\n\tparser.add_argument(\"-s\", \"--server\", dest=\"server\", default=None, help=\"The address of the CIRT server to connect to\")\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = ArgumentParser(description=\"Process a FASTQ file and output a FASTA file with the corresponding sequence(s) from the original file(s)\")\n\n\tparser.add_argument(\"input_file\", type=str, help=\"A FASTQ file with sequences to be processed\")\n\tparser.add_argument(\"-s\", \"--sequences\", type=str, nargs=\"+\", help=\"A list of sequences to be searched for in the FASTQ file. If not provided, all sequences in the input file will be searched for.\")\n\tparser.add_argument(\"-o\", \"--output_file\", type=str, help=\"The name of the output file. If not provided, the output file will be written to STDOUT\")\n\tparser.add_argument(\"-t\", \"--threads\", type=int, default=1, help=\"The number of threads to be used for the search. Default is 1 thread\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 1.0\")\n\tparser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", help=\"Suppress verbose output\")\n\tparser.add_argument(\"-i\", \"--input_format\", type=str, default=\"fastq\", help=\"The format of the input file. Default is FASTQ\")\n\tparser.add_argument(\"-n\", \"--name_format\",", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=\"Retrieve and parse the latest version of a given GitHub repository\")\n\tparser.add_argument(\"repository\", help=\"The GitHub repository to parse\", type=str)\n\tparser.add_argument(\"--name\", help=\"The name of the file to retrieve\", type=str, default=\"README.md\")\n\tparser.add_argument(\"--path\", help=\"The path of the file to retrieve\", type=str, default=\"/\")\n\tparser.add_argument(\"--token\", help=\"A GitHub API token\", type=str)\n\tparser.add_argument(\"--output\", help=\"The output file\", type=str, default=\"output.md\")\n\t\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Convert LIMS JSON to CSV format')\n\tparser.add_argument('input_json_file', type=str, help='Input JSON file')\n\tparser.add_argument('output_csv_file', type=str, help='Output CSV file')\n\tparser.add_argument('--columns', type=str, help='Columns to extract from the JSON file. The default is to extract all columns')\n\tparser.add_argument('--debug', action='store_true', help='Print debug messages')\n\tparser.add_argument('--verbose', action='store_true', help='Print verbose messages')\n\tparser.add_argument('--quiet', action='store_true', help='Do not print any messages')\n\t\n\t# Parse the arguments and return them as an ArgumentParser instance\n\treturn parser.parse_args(*arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\t# Create the parser\n\tparser = argparse.ArgumentParser(description='Compute the number of cells in a cropped image')\n\t \n\t# Add the arguments\n\tparser.add_argument(\n\t\t'-c', '--crop',\n\t\taction='store_true',\n\t\thelp='Crop the image to remove the borders'\n\t)\n\t \n\tparser.add_argument(\n\t\t'-i', '--image',\n\t\ttype=str,\n\t\thelp='Name of the image to process'\n\t)\n\t \n\tparser.add_argument(\n\t\t'-o', '--output',\n\t\ttype=str,\n\t\thelp='Name of the output file'\n\t)\n\t \n\t# Execute the parse_args() method\n\targs = parser.parse_args(*arguments)\n\t \n\t# Return the parsed arguments as an ArgumentParser instance\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(prog='qsub_wrapper', description='qsub wrapper for multiple jobs')\n\n\tparser.add_argument('-i', '--infile', required=True, help='File containing the list of jobs to run')\n\tparser.add_argument('-t', '--template', required=True, help='Template for the job')\n\tparser.add_argument('-o', '--output', required=True, help='Output directory')\n\tparser.add_argument('-b', '--binary', required=True, help='Path to binary')\n\tparser.add_argument('-m', '--mapper', required=True, help='Path to mapper')\n\tparser.add_argument('-r', '--reducer', required=True, help='Path to reducer')\n\tparser.add_argument('-s', '--submitter', required=True, help='Path to submitter')\n\tparser.add_argument('-c', '--compiler', required=True, help='Path to compiler')\n\tparser.add_argument('-q', '--queue', required=True, help='Queue to submit jobs to')\n\tparser.add_argument('-j', '--jobs', type=int, default=1, help='Number of jobs to run in parallel')\n\tparser.add_argument('-H', '--hold', default=None, help='Hold jobs until a certain job is finished')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='This script compares two directories, looking for files that are present in one directory but not the other, and vice versa.')\n\tparser.add_argument('directory1', metavar='DIRECTORY1', type=str, help='Name of the first directory to compare')\n\tparser.add_argument('directory2', metavar='DIRECTORY2', type=str, help='Name of the second directory to compare')\n\tparser.add_argument('-v', '--verbose', action='store_true', help='Print extra information')\n\tparser.add_argument('-o', '--output', type=str, help='Filename to which output should be written')\n\tparser.add_argument('-c', '--check', action='store_true', help='Only check for files that exist in both directories')\n\tparser.add_argument('-s', '--silent', action='store_true', help='Suppress printing of output')\n\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Script which computes 95% confidence interval for a list of measurements and stores the results in a csv file\")\n\tparser.add_argument(\"--input_file\", dest=\"input_file\", required=True, help=\"Name of the csv file from which the data will be read\")\n\tparser.add_argument(\"--output_file\", dest=\"output_file\", required=False, default=\"output.csv\", help=\"Name of the csv file in which the results will be written\")\n\tparser.add_argument(\"--header\", dest=\"header\", required=False, default=False, help=\"Flag which indicates if the input file has a header\")\n\tparser.add_argument(\"--measure_column\", dest=\"measure_column\", required=False, default=\"measure\", help=\"Name of the column which contains the measurements\")\n\tparser.add_argument(\"--confidence\", dest=\"confidence\", required=False, default=95, help=\"Confidence level of the confidence interval (in %)\")\n\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Calculate the average and standard deviation of a set of numbers on the command line\")\n\tparser.add_argument('numbers', help=\"A sequence of numbers\", nargs=\"+\", type=float)\n\tparser.add_argument('-s', '--standard-deviation', help=\"Calculate the standard deviation\", action=\"store_true\")\n\tparser.add_argument('-c', '--correlation', help=\"Calculate the correlation\", action=\"store_true\")\n\tparser.add_argument('-v', '--variance', help=\"Calculate the variance\", action=\"store_true\")\n\tparser.add_argument('-a', '--average', help=\"Calculate the average\", action=\"store_true\")\n\treturn parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(\n\t\t\tflag.lstrip('-') for flag in parser.parse_args()._get_kwargs()[0][1]\n\t\t)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn \" \".join([f\"--{flag}\" for flag in parser._option_string_actions])\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflag_strings = []\n\t\tfor action in parser._actions:\n\t\t\tif isinstance(action, argparse._StoreAction):\n\t\t\t\tif action.dest.startswith('flag_'):\n\t\t\t\t\tflag_strings.append(action.dest)\n\t\treturn''.join(flag_strings)\n\t\t\n\t\t@contextlib.contextmanager\n\t\tdef set_log_level(level):\n\t\t\t\"\"\"\n\t\t\tGiven a logging level, set the log level for the root logger and return\n\t\t\tthe previous level.\n\t\t\t\"\"\"\n\t\t\t\tprevious_level = logger.getEffectiveLevel()\n\t\t\t\tlogger.setLevel(level)\n\t\t\t\ttry:\n\t\t\t\t\tyield\n\t\t\t\tfinally:\n\t\t\t\t\tlogger.setLevel(previous_level)\n\t\t\t\t\t\n\t\t\t\t\t\nif __name__ == '__main__':\n\timport argparse\n\timport os\n\timport sys\n\tfrom. import __version__\n\tfrom.utils import set_log_level\n\t\n\t# Set up the argument parser.\n\tparser = argparse.ArgumentParser(description='PyTorch GANs')\n\tparser.add_argument('--version', action='version', version='%(prog)s'+ __version__)\n\tparser.add_argument('--config', default='', help='Config file path')\n\tparser.add_argument('--gpu', default='', help='Comma separated list of GPU(s) to use.')\n\tparser.add_argument('--mode', default='train',", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = {}\n\t\tfor action in parser._actions:\n\t\t\tif isinstance(action, argparse._StoreAction):\n\t\t\t\tflags[action.dest] = None\n\t\t\telif isinstance(action, argparse._StoreTrueAction):\n\t\t\t\tflags[action.dest] = True\n\t\t\telse:\n\t\t\t\tflags[action.dest] = False\n\t\treturn \" \".join([\"--{}\".format(flag) for flag, value in flags.items() if value])\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn (parser.format_help() + '\\n' + '\\n'.join(parser.format_usage())).split()\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(a for a in parser._actions if isinstance(a, argparse._StoreAction))\n    print \"I am %s\" % self.name\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn \" \".join(f.lstrip(\"-\") for f in parser.parse_known_args()[0].__dict__.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(map(lambda arg: arg.name if arg.name!= 'output' else '--output', parser._actions))\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join([f for f in parser._actions if isinstance(f, argparse._StoreAction)])\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = parser.parse_args()._get_kwargs()\n\treturn''.join(['--{} {}'.format(f, v) for f, v in flags])\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(\n\t\t\tflag.lstrip('-') for flag in parser.parse_args()._get_kwargs()[0][1]\n\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn \" \".join([f\"--{flag}\" for flag in parser._option_string_actions])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflag_strings = []\n\t\tfor action in parser._actions:\n\t\t\tif isinstance(action, argparse._StoreAction):\n\t\t\t\tif action.dest.startswith('flag_'):\n\t\t\t\t\tflag_strings.append(action.dest)\n\t\treturn''.join(flag_strings)\n\t\t\n\t\t@contextlib.contextmanager\n\t\tdef set_log_level(level):\n\t\t\t\"\"\"\n\t\t\tGiven a logging level, set the log level for the root logger and return\n\t\t\tthe previous level.\n\t\t\t\"\"\"\n\t\t\t\tprevious_level = logger.getEffectiveLevel()\n\t\t\t\tlogger.setLevel(level)\n\t\t\t\ttry:\n\t\t\t\t\tyield\n\t\t\t\tfinally:\n\t\t\t\t\tlogger.setLevel(previous_level)\n\t\t\t\t\t\n\t\t\t\t\t\nif __name__ == '__main__':\n\timport argparse\n\timport os\n\timport sys\n\tfrom. import __version__\n\tfrom.utils import set_log_level\n\t\n\t# Set up the argument parser.\n\tparser = argparse.ArgumentParser(description='PyTorch GANs')\n\tparser.add_argument('--version', action='version', version='%(prog)s'+ __version__)\n\tparser.add_argument('--config', default='', help='Config file path')\n\tparser.add_argument('--gpu', default='', help='Comma separated list of GPU(s) to use.')\n\tparser.add_argument('--mode', default='train',", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = {}\n\t\tfor action in parser._actions:\n\t\t\tif isinstance(action, argparse._StoreAction):\n\t\t\t\tflags[action.dest] = None\n\t\t\telif isinstance(action, argparse._StoreTrueAction):\n\t\t\t\tflags[action.dest] = True\n\t\t\telse:\n\t\t\t\tflags[action.dest] = False\n\t\treturn \" \".join([\"--{}\".format(flag) for flag, value in flags.items() if value])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn (parser.format_help() + '\\n' + '\\n'.join(parser.format_usage())).split()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(a for a in parser._actions if isinstance(a, argparse._StoreAction))\n    print \"I am %s\" % self.name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn \" \".join(f.lstrip(\"-\") for f in parser.parse_known_args()[0].__dict__.keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(map(lambda arg: arg.name if arg.name!= 'output' else '--output', parser._actions))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join([f for f in parser._actions if isinstance(f, argparse._StoreAction)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = parser.parse_args()._get_kwargs()\n\treturn''.join(['--{} {}'.format(f, v) for f, v in flags])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"A simple tool to compare the results of two executions\")\n\t\tparser.add_argument(\"file1\", help=\"The first file to compare\")\n\t\tparser.add_argument(\"file2\", help=\"The second file to compare\")\n\t\tparser.add_argument(\"file3\", help=\"The third file to compare\")\n\t\tparser.add_argument(\"-i\", \"--ignore_case\", action=\"store_true\", help=\"Ignore case of string comparisons\")\n\t\tparser.add_argument(\"-n\", \"--number\", help=\"The number of lines to compare\")\n\t\tparser.add_argument(\"-o\", \"--output\", help=\"The output file to write the result to\")\n\t\tparser.add_argument(\"-r\", \"--reverse\", action=\"store_true\", help=\"Reverse the output of the comparison\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Show the detailed comparison\")\n\t\tparser.add_argument(\"-w\", \"--write_output_only\", action=\"store_true\", help=\"Write the output to the file only\")\n\t\targs = parser.parse_args(unparsed_arguments)\n\t\treturn vars(args)\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Script for running the web app')\n\t\tparser.add_argument('--port',\n\t\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\t\tdefault=5000,\n\t\t\t\t\t\t\thelp='Port for the web app')\n\t\treturn parser.parse_args(*unparsed_arguments)\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\t# Create the parser\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis program will do stuff\n\t\"\"\")\n\tparser.add_argument(\"-i\", \"--input\", type=str, required=True, help=\"\"\"\n\tThe input file to be used\n\t\"\"\")\n\tparser.add_argument(\"-o\", \"--output\", type=str, required=True, help=\"\"\"\n\tThe output file to be used\n\t\"\"\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"\"\"\n\tVerbose mode\n\t\"\"\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", help=\"\"\"\n\tDebug mode\n\t\"\"\")\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\targuments = parser.parse_args(unparsed_arguments)\n\treturn arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\t# parser = argparse.ArgumentParser()\n\t\t# parser.add_argument('--config', '-c', required=True, help='config file')\n\t\t# parser.add_argument('--model', '-m', required=True, help='model file')\n\t\t# parser.add_argument('--weights', '-w', required=True, help='weights file')\n\t\t# parser.add_argument('--checkpoint', '-p', required=True, help='checkpoint file')\n\t\t# args, unparsed_arguments = parser.parse_known_args(unparsed_arguments)\n\t\t# return vars(args)\n\n\tdef load_config(config_path):\n\t\t\"\"\"\n\t\tLoads config from a yaml file\n\t\t\"\"\"\n\t\twith open(config_path, 'r') as stream:\n\t\t\tconfig = yaml.load(stream)\n\t\treturn config\n\n\tdef load_model(model_path):\n\t\t\"\"\"\n\t\tLoads the model from a h5 file\n\t\t\"\"\"\n\t\tmodel = load_model(model_path)\n\t\treturn model\n\n\tdef load_weights(weights_path, model):\n\t\t\"\"\"\n\t\tLoads the weights from a h5 file\n\t\t\"\"\"\n\t\tmodel.load_weights(weights_path)\n\t\treturn model\n\n\tdef load_checkpoint(checkpoint_path):\n\t\t\"\"\"\n\t\tLoads the model from a h5", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\t# create the top-level parser\n\tparser = argparse.ArgumentParser(description='Script that parses the given data')\n\n\t# create the parser for the \"parse\" command\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\t# create the parser for the \"parse\" command\n\tparser_parse = subparsers.add_parser('parse', help='parse the given data')\n\tparser_parse.add_argument('input', help='file to parse')\n\tparser_parse.add_argument('output', help='output file')\n\n\t# create the parser for the \"parse\" command\n\tparser_extract = subparsers.add_parser('extract', help='extract the given data')\n\tparser_extract.add_argument('input', help='file to parse')\n\tparser_extract.add_argument('output', help='output file')\n\n\t# create the parser for the \"parse\" command\n\tparser_extract_filter = subparsers.add_parser('extract_filter', help='extract the given data')\n\tparser_extract_filter.add_argument('input', help='file to parse')\n\tparser_extract_filter.add_argument('output', help='output file')\n\tparser_extract_filter.add_argument('filter', help='filter to apply')\n\n\t#", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-C\", \"--configuration\", dest=\"configuration\", help=\"Configuration file to use\", required=False)\n\tparser.add_argument(\"-c\", \"--count\", dest=\"count\", help=\"Number of items to return\", required=False)\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", help=\"File name\", required=False)\n\tparser.add_argument(\"-d\", \"--debug\", dest=\"debug\", action=\"store_true\", help=\"Enables debug output\", required=False)\n\tparser.add_argument(\"-i\", \"--id\", dest=\"id\", help=\"Item id\", required=False)\n\tparser.add_argument(\"-k\", \"--key\", dest=\"key\", help=\"Item key\", required=False)\n\tparser.add_argument(\"-l\", \"--list\", dest=\"list\", help=\"List name\", required=False)\n\tparser.add_argument(\"-m\", \"--method\", dest=\"method\", help=\"Method name\", required=False)\n\tparser.add_argument(\"-n\", \"--name\", dest=\"name\", help=\"Item name\", required=False)\n\tparser.add_argument(\"-p\", \"--password\", dest=\"password\", help=\"Password\", required=False)\n\tparser.add_argument(\"-s\", \"--server\", dest=\"server\", help=\"Server to", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(description='Generate a report for a given application or system')\n\n\t# Add the arguments to the parser\n\tparser.add_argument('--report_type', type=str, default=\"json\", help='Set the report type, json or xml', dest=\"report_type\")\n\tparser.add_argument('--file_name', type=str, default=\"test\", help='Set the file name for the report', dest=\"file_name\")\n\tparser.add_argument('--log_level', type=str, default=\"DEBUG\", help='Set the log level', dest=\"log_level\")\n\tparser.add_argument('--config_file', type=str, default=\"config.json\", help='Set the configuration file', dest=\"config_file\")\n\tparser.add_argument('--json_file', type=str, default=\"json_file\", help='Set the json file', dest=\"json_file\")\n\tparser.add_argument('--xml_file', type=str, default=\"xml_file\", help='Set the xml file', dest=\"xml_file\")\n\tparser.add_argument('--output_file', type=str, default=\"output_file\", help='Set the output file', dest=\"output_file\")\n\tparser.add_argument('--database_file', type=str,", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(description='Reversing tools')\n\n\t# Create the parser for the sub command\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# Create the parser for the'reverse' command\n\treverse_command_parser = subparsers.add_parser('reverse', help='reverse help')\n\treverse_command_parser.add_argument('reverse', type=str, help='reverse help')\n\treverse_command_parser.add_argument('key', type=str, help='key help')\n\n\t# Create the parser for the 'hex' command\n\thex_command_parser = subparsers.add_parser('hex', help='hex help')\n\thex_command_parser.add_argument('hex', type=str, help='hex help')\n\n\t# Create the parser for the 'base64' command\n\tbase64_command_parser = subparsers.add_parser('base64', help='base64 help')\n\tbase64_command_parser.add_argument('base64', type=str, help='base64 help')\n\n\t# Create the parser for the 'rot13' command\n\trot13_command_parser = subparsers.add_parser('rot13', help='rot1", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog='py-argparse',\n\t\t\t\t\t\t\t\t\t\tdescription='py-argparse - a simple and lightweight argparse-like python module',\n\t\t\t\t\t\t\t\t\t\tepilog='For more information please visit http://github.com/k4m4/py-argparse')\n\n\t\tparser.add_argument('-s', '--string',\n\t\t\t\t\t\t\thelp='simple string argument')\n\t\tparser.add_argument('-d', '--directory',\n\t\t\t\t\t\t\thelp='simple directory argument')\n\t\tparser.add_argument('-n', '--number',\n\t\t\t\t\t\t\thelp='simple number argument')\n\t\tparser.add_argument('-b', '--boolean',\n\t\t\t\t\t\t\thelp='simple boolean argument')\n\n\t\targs = vars(parser.parse_args(unparsed_arguments))\n\n\t\treturn args\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"A simple tool to compare the results of two executions\")\n\t\tparser.add_argument(\"file1\", help=\"The first file to compare\")\n\t\tparser.add_argument(\"file2\", help=\"The second file to compare\")\n\t\tparser.add_argument(\"file3\", help=\"The third file to compare\")\n\t\tparser.add_argument(\"-i\", \"--ignore_case\", action=\"store_true\", help=\"Ignore case of string comparisons\")\n\t\tparser.add_argument(\"-n\", \"--number\", help=\"The number of lines to compare\")\n\t\tparser.add_argument(\"-o\", \"--output\", help=\"The output file to write the result to\")\n\t\tparser.add_argument(\"-r\", \"--reverse\", action=\"store_true\", help=\"Reverse the output of the comparison\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Show the detailed comparison\")\n\t\tparser.add_argument(\"-w\", \"--write_output_only\", action=\"store_true\", help=\"Write the output to the file only\")\n\t\targs = parser.parse_args(unparsed_arguments)\n\t\treturn vars(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Script for running the web app')\n\t\tparser.add_argument('--port',\n\t\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\t\tdefault=5000,\n\t\t\t\t\t\t\thelp='Port for the web app')\n\t\treturn parser.parse_args(*unparsed_arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\t# Create the parser\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis program will do stuff\n\t\"\"\")\n\tparser.add_argument(\"-i\", \"--input\", type=str, required=True, help=\"\"\"\n\tThe input file to be used\n\t\"\"\")\n\tparser.add_argument(\"-o\", \"--output\", type=str, required=True, help=\"\"\"\n\tThe output file to be used\n\t\"\"\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"\"\"\n\tVerbose mode\n\t\"\"\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", help=\"\"\"\n\tDebug mode\n\t\"\"\")\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\targuments = parser.parse_args(unparsed_arguments)\n\treturn arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\t# parser = argparse.ArgumentParser()\n\t\t# parser.add_argument('--config', '-c', required=True, help='config file')\n\t\t# parser.add_argument('--model', '-m', required=True, help='model file')\n\t\t# parser.add_argument('--weights', '-w', required=True, help='weights file')\n\t\t# parser.add_argument('--checkpoint', '-p', required=True, help='checkpoint file')\n\t\t# args, unparsed_arguments = parser.parse_known_args(unparsed_arguments)\n\t\t# return vars(args)\n\n\tdef load_config(config_path):\n\t\t\"\"\"\n\t\tLoads config from a yaml file\n\t\t\"\"\"\n\t\twith open(config_path, 'r') as stream:\n\t\t\tconfig = yaml.load(stream)\n\t\treturn config\n\n\tdef load_model(model_path):\n\t\t\"\"\"\n\t\tLoads the model from a h5 file\n\t\t\"\"\"\n\t\tmodel = load_model(model_path)\n\t\treturn model\n\n\tdef load_weights(weights_path, model):\n\t\t\"\"\"\n\t\tLoads the weights from a h5 file\n\t\t\"\"\"\n\t\tmodel.load_weights(weights_path)\n\t\treturn model\n\n\tdef load_checkpoint(checkpoint_path):\n\t\t\"\"\"\n\t\tLoads the model from a h5", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\t# create the top-level parser\n\tparser = argparse.ArgumentParser(description='Script that parses the given data')\n\n\t# create the parser for the \"parse\" command\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\t# create the parser for the \"parse\" command\n\tparser_parse = subparsers.add_parser('parse', help='parse the given data')\n\tparser_parse.add_argument('input', help='file to parse')\n\tparser_parse.add_argument('output', help='output file')\n\n\t# create the parser for the \"parse\" command\n\tparser_extract = subparsers.add_parser('extract', help='extract the given data')\n\tparser_extract.add_argument('input', help='file to parse')\n\tparser_extract.add_argument('output', help='output file')\n\n\t# create the parser for the \"parse\" command\n\tparser_extract_filter = subparsers.add_parser('extract_filter', help='extract the given data')\n\tparser_extract_filter.add_argument('input', help='file to parse')\n\tparser_extract_filter.add_argument('output', help='output file')\n\tparser_extract_filter.add_argument('filter', help='filter to apply')\n\n\t#", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-C\", \"--configuration\", dest=\"configuration\", help=\"Configuration file to use\", required=False)\n\tparser.add_argument(\"-c\", \"--count\", dest=\"count\", help=\"Number of items to return\", required=False)\n\tparser.add_argument(\"-f\", \"--file\", dest=\"file\", help=\"File name\", required=False)\n\tparser.add_argument(\"-d\", \"--debug\", dest=\"debug\", action=\"store_true\", help=\"Enables debug output\", required=False)\n\tparser.add_argument(\"-i\", \"--id\", dest=\"id\", help=\"Item id\", required=False)\n\tparser.add_argument(\"-k\", \"--key\", dest=\"key\", help=\"Item key\", required=False)\n\tparser.add_argument(\"-l\", \"--list\", dest=\"list\", help=\"List name\", required=False)\n\tparser.add_argument(\"-m\", \"--method\", dest=\"method\", help=\"Method name\", required=False)\n\tparser.add_argument(\"-n\", \"--name\", dest=\"name\", help=\"Item name\", required=False)\n\tparser.add_argument(\"-p\", \"--password\", dest=\"password\", help=\"Password\", required=False)\n\tparser.add_argument(\"-s\", \"--server\", dest=\"server\", help=\"Server to", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(description='Generate a report for a given application or system')\n\n\t# Add the arguments to the parser\n\tparser.add_argument('--report_type', type=str, default=\"json\", help='Set the report type, json or xml', dest=\"report_type\")\n\tparser.add_argument('--file_name', type=str, default=\"test\", help='Set the file name for the report', dest=\"file_name\")\n\tparser.add_argument('--log_level', type=str, default=\"DEBUG\", help='Set the log level', dest=\"log_level\")\n\tparser.add_argument('--config_file', type=str, default=\"config.json\", help='Set the configuration file', dest=\"config_file\")\n\tparser.add_argument('--json_file', type=str, default=\"json_file\", help='Set the json file', dest=\"json_file\")\n\tparser.add_argument('--xml_file', type=str, default=\"xml_file\", help='Set the xml file', dest=\"xml_file\")\n\tparser.add_argument('--output_file', type=str, default=\"output_file\", help='Set the output file', dest=\"output_file\")\n\tparser.add_argument('--database_file', type=str,", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\n\n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(description='Reversing tools')\n\n\t# Create the parser for the sub command\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# Create the parser for the'reverse' command\n\treverse_command_parser = subparsers.add_parser('reverse', help='reverse help')\n\treverse_command_parser.add_argument('reverse', type=str, help='reverse help')\n\treverse_command_parser.add_argument('key', type=str, help='key help')\n\n\t# Create the parser for the 'hex' command\n\thex_command_parser = subparsers.add_parser('hex', help='hex help')\n\thex_command_parser.add_argument('hex', type=str, help='hex help')\n\n\t# Create the parser for the 'base64' command\n\tbase64_command_parser = subparsers.add_parser('base64', help='base64 help')\n\tbase64_command_parser.add_argument('base64', type=str, help='base64 help')\n\n\t# Create the parser for the 'rot13' command\n\trot13_command_parser = subparsers.add_parser('rot13', help='rot1", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog='py-argparse',\n\t\t\t\t\t\t\t\t\t\tdescription='py-argparse - a simple and lightweight argparse-like python module',\n\t\t\t\t\t\t\t\t\t\tepilog='For more information please visit http://github.com/k4m4/py-argparse')\n\n\t\tparser.add_argument('-s', '--string',\n\t\t\t\t\t\t\thelp='simple string argument')\n\t\tparser.add_argument('-d', '--directory',\n\t\t\t\t\t\t\thelp='simple directory argument')\n\t\tparser.add_argument('-n', '--number',\n\t\t\t\t\t\t\thelp='simple number argument')\n\t\tparser.add_argument('-b', '--boolean',\n\t\t\t\t\t\t\thelp='simple boolean argument')\n\n\t\targs = vars(parser.parse_args(unparsed_arguments))\n\n\t\treturn args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targuments = vars(unparsed_arguments)\n\t\tclaimed_arguments = {}\n\t\tunclaimed_arguments = []\n\t\tfor subparser_name, subparser_arguments in subparsers.items():\n\t\t\tif subparser_name in arguments:\n\t\t\t\tclaimed_arguments[subparser_name] = subparser_arguments.parse_args(arguments[subparser_name])\n\t\t\t\tdel arguments[subparser_name]\n\t\t\telse:\n\t\t\t\tunclaimed_arguments.append(arguments[subparser_name])\n\t\treturn claimed_arguments, unclaimed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# TODO: I don't know what the best way to handle this is. It seems like argparse is\n\t\t# supposed to handle this, but it does not.\n\t\targ_dict = {}\n\t\tremaining_args = []\n\t\tfor arg in unparsed_arguments:\n\t\t\tparser = None\n\t\t\tfor name, parser in subparsers.items():\n\t\t\t\tif parser.parse_known_args([arg]):\n\t\t\t\t\tbreak\n\t\t\tif parser is None:\n\t\t\t\tremaining_args.append(arg)\n\t\t\telse:\n\t\t\t\targ_dict[name] = parser.parse_args([arg])\n\t\treturn (arg_dict, remaining_args)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(add_help=False)\n\t\tparser.add_argument(\"--repository\", nargs=1, required=True, metavar=\"REPO\",\n\t\t\thelp=\"the repository to operate on\")\n\t\tsubparsers = parser.add_subparsers(dest=\"subparser\")\n\t\tsubparsers.add_parser(\"init\")\n\t\tsubparsers.add_parser(\"fetch\")\n\t\tsubparsers.add_parser(\"push\")\n\t\tsubparsers.add_parser(\"pull\")\n\t\tsubparsers.add_parser(\"commit\")\n\t\tsubparsers.add_parser(\"branch\")\n\t\tsubparsers.add_parser(\"tag\")\n\t\tsubparsers.add_parser(\"log\")\n\t\tsubparsers.add_parser(\"merge\")\n\t\tsubparsers.add_parser(\"checkout\")\n\t\tsubparsers.add_parser(\"show\")\n\t\tsubparsers.add_parser(\"status\")\n\t\tsubparsers.add_parser(\"config\")\n\t\tsubparsers.add_parser(\"remote\")\n\t\tsubparsers.add_parser(\"describe\")\n\t\tsubparsers.add_parser(\"rebase\")\n\t\tsubparsers.add_parser(\"reset\")\n\t\tsubparsers.add_parser(\"cherry-pick\")\n\t\tsubparsers.add_parser(\"revert\")\n\t\tsubparsers.add_parser(\"bisect\")\n\t\tsubparsers.add_parser(\"merge-base\")", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Parse the common arguments first\n\t\tcommon_parser = argparse.ArgumentParser()\n\t\tcommon_parser.add_argument(\"--repository\", help=\"The path to the repository to use.\")\n\t\tcommon_args, remaining_args = common_parser.parse_known_args(unparsed_arguments)\n\n\t\t# Parse each subparser in turn\n\t\tsubparsers_arguments = dict()\n\t\tfor subparser_name, subparser in subparsers.iteritems():\n\t\t\tsubparser_arguments, remaining_args = subparser.parse_known_args(remaining_args)\n\t\t\tsubparsers_arguments[subparser_name] = subparser_arguments\n\n\t\treturn subparsers_arguments, remaining_args\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targuments = {}\n\t\tremaining_arguments = []\n\n\t\tfor subparser_name, subparser_argument_parser in subparsers.items():\n\t\t\tsubparser_arguments = []\n\t\t\tfor argument in unparsed_arguments:\n\t\t\t\tif argument.startswith(subparser_name):\n\t\t\t\t\tsubparser_arguments.append(argument)\n\t\t\t\telse:\n\t\t\t\t\tremaining_arguments.append(argument)\n\t\t\tif len(subparser_arguments) > 0:\n\t\t\t\targuments[subparser_name] = subparser_argument_parser.parse_args(subparser_arguments)\n\t\treturn arguments, remaining_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparsed_subparsers = {}\n\t\tremaining_arguments = []\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tsubparser_arguments = [arg for arg in unparsed_arguments if arg.startswith(subparser_name)]\n\t\t\tif subparser_arguments:\n\t\t\t\tparsed_args = subparser.parse_args(subparser_arguments)\n\t\t\t\tparsed_subparsers[subparser_name] = parsed_args\n\t\t\t\tremaining_arguments.extend(unparsed_arguments[len(subparser_arguments):])\n\t\treturn parsed_subparsers, remaining_arguments\n\n@main.command()\n@click.option(\n\"--repository\",\nhelp=\"Specify the repository to use. Default is the current directory.\",\ndefault=os.getcwd(),\n)\n@click.option(\n\"--version\",\nhelp=\"Specify the version of the repository.\",\ndefault=\"master\",\n)\n@click.option(\n\"--target\",\nhelp=\"Specify the target to build.\",\ndefault=\"all\",\n)\n@click.option(\n\"--clean\",\nhelp=\"Clean the repository before building.\",\nis_flag=True,\n)\n@click.option(\n\"--no-clean\",\nhelp=\"Don't clean the repository before building.\",\nis_flag=True,\n)\n@click.option(\n\"--build-", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tremaining_arguments = unparsed_arguments\n\t\tparsed_arguments = {}\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser.add_argument(\"--repository\", default=None, help=\"The repository to use.\")\n\t\t\t\tsubparser.add_argument(\"--path\", default=None, help=\"The path within the repository.\")\n\t\t\t\tsubparser.add_argument(\"--file\", default=None, help=\"The file within the repository.\")\n\t\t\t\tsubparser.add_argument(\"--line\", default=None, help=\"The line within the file.\")\n\t\t\t\tsubparser.add_argument(\"--column\", default=None, help=\"The column within the line.\")\n\t\t\t\tsubparser.add_argument(\"--revision\", default=None, help=\"The revision within the repository.\")\n\t\t\t\tsubparser.add_argument(\"--include\", default=None, help=\"A regular expression to match files to include.\")\n\t\t\t\tsubparser.add_argument(\"--exclude\", default=None, help=\"A regular expression to match files to exclude.\")\n\t\t\t\tsubparser.add_argument(\"--text\", default=None, help=\"A regular expression to match text.\")\n\t\t\t\tsubparser.add_argument(\"--ignore-whitespace\", action=\"store_true\", help=\"Ignore whitespace changes.\")\n\t\t\t\tsubparser.add_argument(\"--ignore-whitespace-only\", action=\"store_true\", help=\"Ignore whitespace-", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tsubparsers_by_name = dict((p.name, p) for p in subparsers)\n\t\tunclaimed_arguments = []\n\t\tparsed_subparsers = []\n\t\tfor argument_set in unparsed_arguments:\n\t\t\tif not argument_set:\n\t\t\t\tcontinue\n\t\t\tsubparser = argument_set[0]\n\t\t\tif subparser not in subparsers_by_name:\n\t\t\t\tunclaimed_arguments.append(argument_set)\n\t\t\t\tcontinue\n\t\t\t# We found a subparser. Let's see if it has any common arguments that we should\n\t\t\t# parse.\n\t\t\tsubparser_args = subparsers_by_name[subparser]\n\t\t\tcommon_arguments = [a for a in argument_set if a not in subparser_args.choices]\n\t\t\tcommon_arguments.sort(key=lambda a: a.dest)\n\t\t\tparsed_subparser = subparser_args.parse_known_args(common_arguments)[0]\n\t\t\t# We need to assign the common arguments to the subparser, since argparse does not\n\t\t\t# do that for us.\n\t\t\tfor argument in common_arguments:\n\t\t\t\tsetattr(parsed_subparser, argument.dest, getattr(parsed_subparser, argument.dest))\n\t\t\tparsed_subparsers.append(parsed_subparser)\n\t\treturn parsed_subparsers, unclaimed_arguments\n\treturn", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n#\tsubparsers = {\"subparser1\": subparser1, \"subparser2\": subparser2,...}\n\tparser_result = {}\n\tremaining = []\n\tfor arg in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.iteritems():\n\t\t\ttry:\n\t\t\t\tparser_result[subparser_name] = subparser.parse_args([arg])\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tremaining.append(arg)\n\treturn parser_result, remaining\n\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tsubparser_arguments = {}\n\t\tremaining_arguments = unparsed_arguments\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\t\tparser_result = subparser.parse_args(remaining_arguments)\n\t\t\t\tif parser_result.action:\n\t\t\t\t\tsubparser_arguments[subparser_name] = parser_result\n\t\t\t\tremaining_arguments = parser_result.remaining_args\n\t\treturn (subparser_arguments, remaining_arguments)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targuments = vars(unparsed_arguments)\n\t\tclaimed_arguments = {}\n\t\tunclaimed_arguments = []\n\t\tfor subparser_name, subparser_arguments in subparsers.items():\n\t\t\tif subparser_name in arguments:\n\t\t\t\tclaimed_arguments[subparser_name] = subparser_arguments.parse_args(arguments[subparser_name])\n\t\t\t\tdel arguments[subparser_name]\n\t\t\telse:\n\t\t\t\tunclaimed_arguments.append(arguments[subparser_name])\n\t\treturn claimed_arguments, unclaimed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# TODO: I don't know what the best way to handle this is. It seems like argparse is\n\t\t# supposed to handle this, but it does not.\n\t\targ_dict = {}\n\t\tremaining_args = []\n\t\tfor arg in unparsed_arguments:\n\t\t\tparser = None\n\t\t\tfor name, parser in subparsers.items():\n\t\t\t\tif parser.parse_known_args([arg]):\n\t\t\t\t\tbreak\n\t\t\tif parser is None:\n\t\t\t\tremaining_args.append(arg)\n\t\t\telse:\n\t\t\t\targ_dict[name] = parser.parse_args([arg])\n\t\treturn (arg_dict, remaining_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(add_help=False)\n\t\tparser.add_argument(\"--repository\", nargs=1, required=True, metavar=\"REPO\",\n\t\t\thelp=\"the repository to operate on\")\n\t\tsubparsers = parser.add_subparsers(dest=\"subparser\")\n\t\tsubparsers.add_parser(\"init\")\n\t\tsubparsers.add_parser(\"fetch\")\n\t\tsubparsers.add_parser(\"push\")\n\t\tsubparsers.add_parser(\"pull\")\n\t\tsubparsers.add_parser(\"commit\")\n\t\tsubparsers.add_parser(\"branch\")\n\t\tsubparsers.add_parser(\"tag\")\n\t\tsubparsers.add_parser(\"log\")\n\t\tsubparsers.add_parser(\"merge\")\n\t\tsubparsers.add_parser(\"checkout\")\n\t\tsubparsers.add_parser(\"show\")\n\t\tsubparsers.add_parser(\"status\")\n\t\tsubparsers.add_parser(\"config\")\n\t\tsubparsers.add_parser(\"remote\")\n\t\tsubparsers.add_parser(\"describe\")\n\t\tsubparsers.add_parser(\"rebase\")\n\t\tsubparsers.add_parser(\"reset\")\n\t\tsubparsers.add_parser(\"cherry-pick\")\n\t\tsubparsers.add_parser(\"revert\")\n\t\tsubparsers.add_parser(\"bisect\")\n\t\tsubparsers.add_parser(\"merge-base\")", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Parse the common arguments first\n\t\tcommon_parser = argparse.ArgumentParser()\n\t\tcommon_parser.add_argument(\"--repository\", help=\"The path to the repository to use.\")\n\t\tcommon_args, remaining_args = common_parser.parse_known_args(unparsed_arguments)\n\n\t\t# Parse each subparser in turn\n\t\tsubparsers_arguments = dict()\n\t\tfor subparser_name, subparser in subparsers.iteritems():\n\t\t\tsubparser_arguments, remaining_args = subparser.parse_known_args(remaining_args)\n\t\t\tsubparsers_arguments[subparser_name] = subparser_arguments\n\n\t\treturn subparsers_arguments, remaining_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targuments = {}\n\t\tremaining_arguments = []\n\n\t\tfor subparser_name, subparser_argument_parser in subparsers.items():\n\t\t\tsubparser_arguments = []\n\t\t\tfor argument in unparsed_arguments:\n\t\t\t\tif argument.startswith(subparser_name):\n\t\t\t\t\tsubparser_arguments.append(argument)\n\t\t\t\telse:\n\t\t\t\t\tremaining_arguments.append(argument)\n\t\t\tif len(subparser_arguments) > 0:\n\t\t\t\targuments[subparser_name] = subparser_argument_parser.parse_args(subparser_arguments)\n\t\treturn arguments, remaining_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparsed_subparsers = {}\n\t\tremaining_arguments = []\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tsubparser_arguments = [arg for arg in unparsed_arguments if arg.startswith(subparser_name)]\n\t\t\tif subparser_arguments:\n\t\t\t\tparsed_args = subparser.parse_args(subparser_arguments)\n\t\t\t\tparsed_subparsers[subparser_name] = parsed_args\n\t\t\t\tremaining_arguments.extend(unparsed_arguments[len(subparser_arguments):])\n\t\treturn parsed_subparsers, remaining_arguments\n\n@main.command()\n@click.option(\n\"--repository\",\nhelp=\"Specify the repository to use. Default is the current directory.\",\ndefault=os.getcwd(),\n)\n@click.option(\n\"--version\",\nhelp=\"Specify the version of the repository.\",\ndefault=\"master\",\n)\n@click.option(\n\"--target\",\nhelp=\"Specify the target to build.\",\ndefault=\"all\",\n)\n@click.option(\n\"--clean\",\nhelp=\"Clean the repository before building.\",\nis_flag=True,\n)\n@click.option(\n\"--no-clean\",\nhelp=\"Don't clean the repository before building.\",\nis_flag=True,\n)\n@click.option(\n\"--build-", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tremaining_arguments = unparsed_arguments\n\t\tparsed_arguments = {}\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser.add_argument(\"--repository\", default=None, help=\"The repository to use.\")\n\t\t\t\tsubparser.add_argument(\"--path\", default=None, help=\"The path within the repository.\")\n\t\t\t\tsubparser.add_argument(\"--file\", default=None, help=\"The file within the repository.\")\n\t\t\t\tsubparser.add_argument(\"--line\", default=None, help=\"The line within the file.\")\n\t\t\t\tsubparser.add_argument(\"--column\", default=None, help=\"The column within the line.\")\n\t\t\t\tsubparser.add_argument(\"--revision\", default=None, help=\"The revision within the repository.\")\n\t\t\t\tsubparser.add_argument(\"--include\", default=None, help=\"A regular expression to match files to include.\")\n\t\t\t\tsubparser.add_argument(\"--exclude\", default=None, help=\"A regular expression to match files to exclude.\")\n\t\t\t\tsubparser.add_argument(\"--text\", default=None, help=\"A regular expression to match text.\")\n\t\t\t\tsubparser.add_argument(\"--ignore-whitespace\", action=\"store_true\", help=\"Ignore whitespace changes.\")\n\t\t\t\tsubparser.add_argument(\"--ignore-whitespace-only\", action=\"store_true\", help=\"Ignore whitespace-", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tsubparsers_by_name = dict((p.name, p) for p in subparsers)\n\t\tunclaimed_arguments = []\n\t\tparsed_subparsers = []\n\t\tfor argument_set in unparsed_arguments:\n\t\t\tif not argument_set:\n\t\t\t\tcontinue\n\t\t\tsubparser = argument_set[0]\n\t\t\tif subparser not in subparsers_by_name:\n\t\t\t\tunclaimed_arguments.append(argument_set)\n\t\t\t\tcontinue\n\t\t\t# We found a subparser. Let's see if it has any common arguments that we should\n\t\t\t# parse.\n\t\t\tsubparser_args = subparsers_by_name[subparser]\n\t\t\tcommon_arguments = [a for a in argument_set if a not in subparser_args.choices]\n\t\t\tcommon_arguments.sort(key=lambda a: a.dest)\n\t\t\tparsed_subparser = subparser_args.parse_known_args(common_arguments)[0]\n\t\t\t# We need to assign the common arguments to the subparser, since argparse does not\n\t\t\t# do that for us.\n\t\t\tfor argument in common_arguments:\n\t\t\t\tsetattr(parsed_subparser, argument.dest, getattr(parsed_subparser, argument.dest))\n\t\t\tparsed_subparsers.append(parsed_subparser)\n\t\treturn parsed_subparsers, unclaimed_arguments\n\treturn", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n#\tsubparsers = {\"subparser1\": subparser1, \"subparser2\": subparser2,...}\n\tparser_result = {}\n\tremaining = []\n\tfor arg in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.iteritems():\n\t\t\ttry:\n\t\t\t\tparser_result[subparser_name] = subparser.parse_args([arg])\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tremaining.append(arg)\n\treturn parser_result, remaining\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tsubparser_arguments = {}\n\t\tremaining_arguments = unparsed_arguments\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\t\tparser_result = subparser.parse_args(remaining_arguments)\n\t\t\t\tif parser_result.action:\n\t\t\t\t\tsubparser_arguments[subparser_name] = parser_result\n\t\t\t\tremaining_arguments = parser_result.remaining_args\n\t\treturn (subparser_arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool for creating SRT subtitles\")\n\tparser.add_argument(\n\t\t\"input_file\", \n\t\thelp=\"The file to parse\")\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subcommand\")\n\tsubparsers.required = True\n\t \n\t# Parser for the'split' subcommand\n\tsplit_parser = subparsers.add_parser(\n\t\t\"split\", \n\t\thelp=\"Split a file into one subtitle per file\")\n\tsplit_parser.add_argument(\n\t\t\"output_directory\",\n\t\thelp=\"The directory to store the subtitles\")\n\tsplit_parser.add_argument(\n\t\t\"--time_delta\", \n\t\thelp=\"The time delta between two subtitles\",\n\t\ttype=int,\n\t\tdefault=5)\n\tsplit_parser.add_argument(\n\t\t\"--language\", \n\t\thelp=\"The language code of the subtitle\",\n\t\tdefault=\"eng\")\n\n\treturn parser, subparsers.choices\n/subtitles/subtitle.py\nimport re\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tsubparsers = parser.add_subparsers()\n\t\treturn (parser, subparsers)\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\t# Build parser and subparsers\n\tparser = Parser(prog='s3-bucket-inspector', description='''\n\tAn interactive bucket inspector for Amazon S3.\n\t''')\n\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s 0.1')\n\tparser.add_argument('-d', '--debug', action='store_true')\n\tparser.add_argument('-s', '--s3-endpoint', default='s3.amazonaws.com', help='''\n\tThe S3 endpoint.\n\t''')\n\tparser.add_argument('-a', '--access-key-id', help='''\n\tThe access key ID.\n\t''')\n\tparser.add_argument('-k', '--secret-access-key', help='''\n\tThe secret access key.\n\t''')\n\n\tparser.add_subparsers(dest='subparser_name')\n\n\t# Build the subparsers\n\n\tparser_list = parser.add_parser('list', help='''\n\tList the buckets in an S3 account.\n\t''')\n\tparser_list.add_argument('-a', '--access-key-id', help='''\n\tThe access key ID.\n\t''')\n\tparser_list.add_argument('-k', '--secret-access-key', help='''\n\t", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\t# build main parser\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\t\n\n\t# build subparsers\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\t# build parser for the \"echo\" command\n\tparser_echo = subparsers.add_parser('echo', help='echo the string you use here')\n\tparser_echo.add_argument('string', help='The string to echo')\n\n\tparser_echo.add_argument('--upper', '-u', action='store_true', help='Make the string upper case')\n\n\t# build parser for the \"init\" command\n\tparser_init = subparsers.add_parser('init', help='echo the string you use here')\n\tparser_init.add_argument('name', help='The name of the project')\n\tparser_init.add_argument('package', help='The package name')\n\n\t# build parser for the \"test\" command\n\tparser_test = subparsers.add_parser('test', help='Test a given module')\n\tparser_test.add_argument('module', help='The module to test')\n\n\t# build parser for the \"run\" command\n\tparser_run = subparsers.add_parser('run', help='Test a given module')\n\tparser_run.add_argument('module', help", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tA script to parse a large data file into a set of files.\n\t\t\"\"\")\n\n\tsubparsers = parser.add_subparsers(dest='subparser_name')\n\n\t# Create a parser for the \"split\" command.\n\tparser_split = subparsers.add_parser('split', help='Split a file into smaller files')\n\tparser_split.add_argument('-f', '--file', help='Path to file to split', required=True)\n\tparser_split.add_argument('-n', '--num', help='Number of files to split into', type=int, required=True)\n\n\t# Create a parser for the \"merge\" command.\n\tparser_merge = subparsers.add_parser('merge', help='Merge a set of files into a single file')\n\tparser_merge.add_argument('-d', '--dir', help='Directory containing the files to merge', required=True)\n\tparser_merge.add_argument('-o', '--output', help='Output file name', required=True)\n\n\t# Create a parser for the \"sort\" command.\n\tparser_sort = subparsers.add_parser('sort', help='Sort a file')\n\tparser_sort.add_argument('-f', '--file', help='Path to file to sort', required=True", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparser_train = subparsers.add_parser('train', help='train model')\n\tparser_train.add_argument('--model_name', type=str, default='model')\n\tparser_train.add_argument('--train_file', type=str, default='datasets/train.csv')\n\tparser_train.add_argument('--test_file', type=str, default='datasets/test.csv')\n\tparser_train.add_argument('--valid_file', type=str, default='datasets/val.csv')\n\tparser_train.add_argument('--num_epochs', type=int, default=100)\n\tparser_train.add_argument('--batch_size', type=int, default=1)\n\tparser_train.add_argument('--learning_rate', type=float, default=0.001)\n\tparser_train.add_argument('--seed', type=int, default=1111)\n\tparser_train.add_argument('--num_workers', type=int, default=1)\n\tparser_train.add_argument('--save_path', type=str, default='./data/model.pt')\n\tparser", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\t# create the top-level parser\n\tparser = argparse.ArgumentParser(prog='main.py')\n\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\t# create the parser for the \"train\" command\n\tparser_train = subparsers.add_parser('train')\n\tparser_train.add_argument('data_dir')\n\tparser_train.add_argument('--save_dir', default='checkpoints')\n\tparser_train.add_argument('--log_dir', default='logs')\n\tparser_train.add_argument('--num_workers', default=4, type=int)\n\tparser_train.add_argument('--batch_size', default=16, type=int)\n\tparser_train.add_argument('--num_epochs', default=200, type=int)\n\tparser_train.add_argument('--lr', default=1e-3, type=float)\n\tparser_train.add_argument('--lr_decay_rate', default=0.1, type=float)\n\tparser_train.add_argument('--lr_decay_epoch', default=100, type=int)\n\tparser_train.add_argument('--weight_decay', default=1e-4, type=float)\n\tparser_train.add_argument('--momentum', default=0", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\t# Create a parser for the main program\n\tmain_parser = argparse.ArgumentParser(description='A test script to check the arguments')\n\n\t# Create the parser for the sub-commands\n\tsubparsers = main_parser.add_subparsers(title='subcommands', dest='subcommand')\n\t\n\t# Create the parser for the sub-command 'check'\n\tcheck_parser = subparsers.add_parser('check')\n\tcheck_parser.add_argument('--check-type', type=str, dest='check_type', required=True, help='The type of check to perform. This can be one of \"file_exists\", \"file_is_not_empty\", \"file_is_empty\", \"file_has_content\", \"file_does_not_have_content\", \"directory_exists\", \"file_is_readable\", \"file_is_writable\", \"file_is_executable\", \"file_is_not_executable\", \"file_is_readable_writable_executable\"')\n\tcheck_parser.add_argument('--file', type=str, dest='file', required=True, help='The file to check')\n\t\n\t# Create the parser for the sub-command 'create'\n\tcreate_parser = subparsers.add_parser('create')\n\tcreate_parser.add_argument('--create-type', type=str,", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\tparser = argparse.ArgumentParser(description='Manage a test suite.')\n\tparser = argparse.ArgumentParser(description='Manage a test suite.')\n\tparser.add_argument('--version', action='version', version='%(prog)s 1.0')\n\tsubparsers = parser.add_subparsers(title='subcommands', dest='subcommand')\n\t\n\t### Add subparsers for each test suite ###\n\n\t# Add the parser for the 'test_suite' subcommand\n\tparser_test_suite = subparsers.add_parser('test_suite', help='Run a test suite')\n\tparser_test_suite.add_argument('-d', '--debug', action='store_true', help='Run in debug mode')\n\tparser_test_suite.add_argument('-u', '--user', metavar='USER', help='User to use')\n\tparser_test_suite.add_argument('-p', '--password', metavar='PASSWD', help='Password to use')\n\tparser_test_suite.add_argument('--config', metavar='CONFIG', help='Specify configuration file')\n\t\n\t# Add the parser for the 'test_suite' subcommand\n\tparser_test_suite = subparsers.add_parser('test_suite', help='Run a test suite')\n\tparser_test_suite.add_argument('-d', '--debug', action='store", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='A script for testing the '\n\t\t\t\t\t\t\t\t\t\t'functionality of the Python wrapper for '\n\t\t\t\t\t\t\t\t\t\t'the libGDX library.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\tparsers = []\n\tfor name in subparsers_dict:\n\t\tparsers.append(subparsers.add_parser(name, help=subparsers_dict[name]))\n\t\n\treturn parser, parsers\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool for creating SRT subtitles\")\n\tparser.add_argument(\n\t\t\"input_file\", \n\t\thelp=\"The file to parse\")\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subcommand\")\n\tsubparsers.required = True\n\t \n\t# Parser for the'split' subcommand\n\tsplit_parser = subparsers.add_parser(\n\t\t\"split\", \n\t\thelp=\"Split a file into one subtitle per file\")\n\tsplit_parser.add_argument(\n\t\t\"output_directory\",\n\t\thelp=\"The directory to store the subtitles\")\n\tsplit_parser.add_argument(\n\t\t\"--time_delta\", \n\t\thelp=\"The time delta between two subtitles\",\n\t\ttype=int,\n\t\tdefault=5)\n\tsplit_parser.add_argument(\n\t\t\"--language\", \n\t\thelp=\"The language code of the subtitle\",\n\t\tdefault=\"eng\")\n\n\treturn parser, subparsers.choices\n/subtitles/subtitle.py\nimport re\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tsubparsers = parser.add_subparsers()\n\t\treturn (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\t# Build parser and subparsers\n\tparser = Parser(prog='s3-bucket-inspector', description='''\n\tAn interactive bucket inspector for Amazon S3.\n\t''')\n\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s 0.1')\n\tparser.add_argument('-d', '--debug', action='store_true')\n\tparser.add_argument('-s', '--s3-endpoint', default='s3.amazonaws.com', help='''\n\tThe S3 endpoint.\n\t''')\n\tparser.add_argument('-a', '--access-key-id', help='''\n\tThe access key ID.\n\t''')\n\tparser.add_argument('-k', '--secret-access-key', help='''\n\tThe secret access key.\n\t''')\n\n\tparser.add_subparsers(dest='subparser_name')\n\n\t# Build the subparsers\n\n\tparser_list = parser.add_parser('list', help='''\n\tList the buckets in an S3 account.\n\t''')\n\tparser_list.add_argument('-a', '--access-key-id', help='''\n\tThe access key ID.\n\t''')\n\tparser_list.add_argument('-k', '--secret-access-key', help='''\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\t# build main parser\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\t\n\n\t# build subparsers\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\t# build parser for the \"echo\" command\n\tparser_echo = subparsers.add_parser('echo', help='echo the string you use here')\n\tparser_echo.add_argument('string', help='The string to echo')\n\n\tparser_echo.add_argument('--upper', '-u', action='store_true', help='Make the string upper case')\n\n\t# build parser for the \"init\" command\n\tparser_init = subparsers.add_parser('init', help='echo the string you use here')\n\tparser_init.add_argument('name', help='The name of the project')\n\tparser_init.add_argument('package', help='The package name')\n\n\t# build parser for the \"test\" command\n\tparser_test = subparsers.add_parser('test', help='Test a given module')\n\tparser_test.add_argument('module', help='The module to test')\n\n\t# build parser for the \"run\" command\n\tparser_run = subparsers.add_parser('run', help='Test a given module')\n\tparser_run.add_argument('module', help", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tA script to parse a large data file into a set of files.\n\t\t\"\"\")\n\n\tsubparsers = parser.add_subparsers(dest='subparser_name')\n\n\t# Create a parser for the \"split\" command.\n\tparser_split = subparsers.add_parser('split', help='Split a file into smaller files')\n\tparser_split.add_argument('-f', '--file', help='Path to file to split', required=True)\n\tparser_split.add_argument('-n', '--num', help='Number of files to split into', type=int, required=True)\n\n\t# Create a parser for the \"merge\" command.\n\tparser_merge = subparsers.add_parser('merge', help='Merge a set of files into a single file')\n\tparser_merge.add_argument('-d', '--dir', help='Directory containing the files to merge', required=True)\n\tparser_merge.add_argument('-o', '--output', help='Output file name', required=True)\n\n\t# Create a parser for the \"sort\" command.\n\tparser_sort = subparsers.add_parser('sort', help='Sort a file')\n\tparser_sort.add_argument('-f', '--file', help='Path to file to sort', required=True", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparser_train = subparsers.add_parser('train', help='train model')\n\tparser_train.add_argument('--model_name', type=str, default='model')\n\tparser_train.add_argument('--train_file', type=str, default='datasets/train.csv')\n\tparser_train.add_argument('--test_file', type=str, default='datasets/test.csv')\n\tparser_train.add_argument('--valid_file', type=str, default='datasets/val.csv')\n\tparser_train.add_argument('--num_epochs', type=int, default=100)\n\tparser_train.add_argument('--batch_size', type=int, default=1)\n\tparser_train.add_argument('--learning_rate', type=float, default=0.001)\n\tparser_train.add_argument('--seed', type=int, default=1111)\n\tparser_train.add_argument('--num_workers', type=int, default=1)\n\tparser_train.add_argument('--save_path', type=str, default='./data/model.pt')\n\tparser", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\t# create the top-level parser\n\tparser = argparse.ArgumentParser(prog='main.py')\n\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\t# create the parser for the \"train\" command\n\tparser_train = subparsers.add_parser('train')\n\tparser_train.add_argument('data_dir')\n\tparser_train.add_argument('--save_dir', default='checkpoints')\n\tparser_train.add_argument('--log_dir', default='logs')\n\tparser_train.add_argument('--num_workers', default=4, type=int)\n\tparser_train.add_argument('--batch_size', default=16, type=int)\n\tparser_train.add_argument('--num_epochs', default=200, type=int)\n\tparser_train.add_argument('--lr', default=1e-3, type=float)\n\tparser_train.add_argument('--lr_decay_rate', default=0.1, type=float)\n\tparser_train.add_argument('--lr_decay_epoch', default=100, type=int)\n\tparser_train.add_argument('--weight_decay', default=1e-4, type=float)\n\tparser_train.add_argument('--momentum', default=0", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\t# Create a parser for the main program\n\tmain_parser = argparse.ArgumentParser(description='A test script to check the arguments')\n\n\t# Create the parser for the sub-commands\n\tsubparsers = main_parser.add_subparsers(title='subcommands', dest='subcommand')\n\t\n\t# Create the parser for the sub-command 'check'\n\tcheck_parser = subparsers.add_parser('check')\n\tcheck_parser.add_argument('--check-type', type=str, dest='check_type', required=True, help='The type of check to perform. This can be one of \"file_exists\", \"file_is_not_empty\", \"file_is_empty\", \"file_has_content\", \"file_does_not_have_content\", \"directory_exists\", \"file_is_readable\", \"file_is_writable\", \"file_is_executable\", \"file_is_not_executable\", \"file_is_readable_writable_executable\"')\n\tcheck_parser.add_argument('--file', type=str, dest='file', required=True, help='The file to check')\n\t\n\t# Create the parser for the sub-command 'create'\n\tcreate_parser = subparsers.add_parser('create')\n\tcreate_parser.add_argument('--create-type', type=str,", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\tparser = argparse.ArgumentParser(description='Manage a test suite.')\n\tparser = argparse.ArgumentParser(description='Manage a test suite.')\n\tparser.add_argument('--version', action='version', version='%(prog)s 1.0')\n\tsubparsers = parser.add_subparsers(title='subcommands', dest='subcommand')\n\t\n\t### Add subparsers for each test suite ###\n\n\t# Add the parser for the 'test_suite' subcommand\n\tparser_test_suite = subparsers.add_parser('test_suite', help='Run a test suite')\n\tparser_test_suite.add_argument('-d', '--debug', action='store_true', help='Run in debug mode')\n\tparser_test_suite.add_argument('-u', '--user', metavar='USER', help='User to use')\n\tparser_test_suite.add_argument('-p', '--password', metavar='PASSWD', help='Password to use')\n\tparser_test_suite.add_argument('--config', metavar='CONFIG', help='Specify configuration file')\n\t\n\t# Add the parser for the 'test_suite' subcommand\n\tparser_test_suite = subparsers.add_parser('test_suite', help='Run a test suite')\n\tparser_test_suite.add_argument('-d', '--debug', action='store", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='A script for testing the '\n\t\t\t\t\t\t\t\t\t\t'functionality of the Python wrapper for '\n\t\t\t\t\t\t\t\t\t\t'the libGDX library.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\tparsers = []\n\tfor name in subparsers_dict:\n\t\tparsers.append(subparsers.add_parser(name, help=subparsers_dict[name]))\n\t\n\treturn parser, parsers\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\tdef _merge(d, u):\n\t\tif u is None:\n\t\t\treturn d\n\t\tif d is None:\n\t\t\treturn u\n\n\t\tif isinstance(d, MappingNode):\n\t\t\tif isinstance(u, MappingNode):\n\t\t\t\td.update(u)\n\t\t\telif isinstance(u, list):\n\t\t\t\tif isinstance(d, list):\n\t\t\t\t\td.extend(u)\n\t\t\t\telse:\n\t\t\t\t\td = u\n\t\t\telif isinstance(u, MappingNode):\n\t\t\t\td = u\n\t\t\telse:\n\t\t\t\td = u\n\t\telif isinstance(d, list):\n\t\t\tif isinstance(u, list):\n\t\t\t\td.extend(u)\n\t\t\telse:\n\t\t\t\td = [d, u]\n\t\telif isinstance(d, MappingNode):\n\t\t\td = [d, u]\n\t\telse:\n\t\t\td = [d, u]\n\t\treturn d\n\n\t#if isinstance(nodes, list):\n\t#\treturn [_merge(nodes[0], _merge(nodes[1], nodes[2]))]\n\t#return [nodes[0], nodes[1], nodes[2]]\n\treturn reduce(_merge, nodes)\n\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\t# make sure we can process the node type\n\tif not isinstance(nodes, list):\n\t\traise TypeError(\"Expected list of nodes, got %s\" % type(nodes))\n\t# if there are no nodes, then return nothing\n\tif len(nodes) == 0:\n\t\treturn nodes\n\t# if there is only one node, then return it\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\t# if there is more than one node, then merge them\n\t# first, group the nodes by their path\n\tnode_groups = {}\n\tfor node in nodes:\n\t\tnode_path = node.path\n\t\tif node_path in node_groups:\n\t\t\tnode_groups[node_path].append(node)\n\t\telse:\n\t\t\tnode_groups[node_path] = [node]\n\t# now, create a new list of nodes\n\tnew_nodes = []\n\t# for each group of nodes, merge them\n\tfor node_path, nodes in node_groups.items():\n\t\t# if there is only one node, then add it to the new list\n\t\tif len(nodes) == 1:\n\t\t\tnew_nodes.append(nodes[0])\n\t\t\tcontinue\n\t\t# if there is more than one node, then merge the nodes\n\t\tnode = nodes[0]\n\t\t", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tresult.update(node.deep_merge())\n\t\treturn result\n\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\t**kwargs will be merged with any node values found in the same position of any of the nodes.\n\t\te.g. MappingNode(a=1, b=2), MappingNode(b=3), MappingNode(a=4) --> MappingNode(a=4, b=3)\n\t\t\"\"\"\n\t\tself.kwargs = kwargs\n\n\tdef __str__(self):\n\t\treturn str(self.deep_merge())\n\n\tdef __repr__(self):\n\t\treturn repr(self.deep_merge())\n\n\tdef __bool__(self):\n\t\treturn bool(self.deep_merge())\n\n\tdef __len__(self):\n\t\treturn len(self.deep_merge())\n\n\tdef __getitem__(self, key):\n\t\treturn self.deep_merge()[key]\n\n\tdef __setitem__(self, key, value):\n\t\tself.kwargs[key] = value\n\n\tdef __delitem__(self, key):\n\t\tdel self.kwargs[key]\n\n\tdef __iter__(self):\n\t\treturn iter(self.deep_merge())\n\n\tdef __call__(self, **kwargs):\n\t\t\"\"\"\n\t\tmerge with the current node and return the result", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged = {}\n\t\tfor node in nodes:\n\t\t\tfor key, value in node.iteritems():\n\t\t\t\tif key not in merged:\n\t\t\t\t\tmerged[key] = value\n\t\t\t\telif isinstance(merged[key], MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes((merged[key], value))\n\t\treturn merged\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged = MappingNode()\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor key, value in node.items():\n\t\t\t\t\tif key in merged:\n\t\t\t\t\t\tif isinstance(merged[key], MappingNode):\n\t\t\t\t\t\t\tmerged[key].update(value)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tmerged[key] = value\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\traise TypeError(\"expected MappingNode, got %s\" % type(node))\n\treturn merged\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor k, v in node.items():\n\t\t\t\t\tif k not in result:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\t\t\telif isinstance(v, MappingNode):\n\t\t\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tresult[k] = v\n\t\t\telse:\n\t\t\t\tresult[node.key] = node.value\n\t\treturn result\n\n\tdef merge_nodes(nodes):\n\t\t\"\"\"\n\t\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor k, v in node.items():\n\t\t\t\t\tif k not in result:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\t\t\tresult[k] = merge_nodes([result[k], v])\n\t\t\t\t\t\telif isinstance(v, MappingNode):\n\t\t\t\t\t\t\tresult[k] = merge_nodes([result[k", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\t#if type(nodes) is not list:\n\t#\tnodes = [nodes]\n\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\n\t# find keys that are common to all nodes\n\tcommon_keys = set()\n\tfor node in nodes:\n\t\tcommon_keys.update(node.keys())\n\n\t# merge the common keys\n\tmerged_node_keys = set()\n\tfor key in common_keys:\n\t\tmerged_node_keys.update(deep_merge_nodes([node[key] for node in nodes if key in node]))\n\n\tif len(nodes) > 1:\n\t\tmerged_node_keys = merged_node_keys.union(common_keys)\n\t\n\treturn MappingNode({key: deep_merge_nodes([node[key] for node in nodes if key in node]) for key in merged_node_keys})\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tfor key, value in node.items():\n\t\t\t\tresult[key] = deep_merge_nodes([result.get(key, MappingNode()), value])\n\t\treturn result\n```\n\n## 2.\n\n### 2.1\n\n```python", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tresult.update(node.as_dict())\n\t\treturn result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\tdef _merge(d, u):\n\t\tif u is None:\n\t\t\treturn d\n\t\tif d is None:\n\t\t\treturn u\n\n\t\tif isinstance(d, MappingNode):\n\t\t\tif isinstance(u, MappingNode):\n\t\t\t\td.update(u)\n\t\t\telif isinstance(u, list):\n\t\t\t\tif isinstance(d, list):\n\t\t\t\t\td.extend(u)\n\t\t\t\telse:\n\t\t\t\t\td = u\n\t\t\telif isinstance(u, MappingNode):\n\t\t\t\td = u\n\t\t\telse:\n\t\t\t\td = u\n\t\telif isinstance(d, list):\n\t\t\tif isinstance(u, list):\n\t\t\t\td.extend(u)\n\t\t\telse:\n\t\t\t\td = [d, u]\n\t\telif isinstance(d, MappingNode):\n\t\t\td = [d, u]\n\t\telse:\n\t\t\td = [d, u]\n\t\treturn d\n\n\t#if isinstance(nodes, list):\n\t#\treturn [_merge(nodes[0], _merge(nodes[1], nodes[2]))]\n\t#return [nodes[0], nodes[1], nodes[2]]\n\treturn reduce(_merge, nodes)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\t# make sure we can process the node type\n\tif not isinstance(nodes, list):\n\t\traise TypeError(\"Expected list of nodes, got %s\" % type(nodes))\n\t# if there are no nodes, then return nothing\n\tif len(nodes) == 0:\n\t\treturn nodes\n\t# if there is only one node, then return it\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\t# if there is more than one node, then merge them\n\t# first, group the nodes by their path\n\tnode_groups = {}\n\tfor node in nodes:\n\t\tnode_path = node.path\n\t\tif node_path in node_groups:\n\t\t\tnode_groups[node_path].append(node)\n\t\telse:\n\t\t\tnode_groups[node_path] = [node]\n\t# now, create a new list of nodes\n\tnew_nodes = []\n\t# for each group of nodes, merge them\n\tfor node_path, nodes in node_groups.items():\n\t\t# if there is only one node, then add it to the new list\n\t\tif len(nodes) == 1:\n\t\t\tnew_nodes.append(nodes[0])\n\t\t\tcontinue\n\t\t# if there is more than one node, then merge the nodes\n\t\tnode = nodes[0]\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tresult.update(node.deep_merge())\n\t\treturn result\n\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\t**kwargs will be merged with any node values found in the same position of any of the nodes.\n\t\te.g. MappingNode(a=1, b=2), MappingNode(b=3), MappingNode(a=4) --> MappingNode(a=4, b=3)\n\t\t\"\"\"\n\t\tself.kwargs = kwargs\n\n\tdef __str__(self):\n\t\treturn str(self.deep_merge())\n\n\tdef __repr__(self):\n\t\treturn repr(self.deep_merge())\n\n\tdef __bool__(self):\n\t\treturn bool(self.deep_merge())\n\n\tdef __len__(self):\n\t\treturn len(self.deep_merge())\n\n\tdef __getitem__(self, key):\n\t\treturn self.deep_merge()[key]\n\n\tdef __setitem__(self, key, value):\n\t\tself.kwargs[key] = value\n\n\tdef __delitem__(self, key):\n\t\tdel self.kwargs[key]\n\n\tdef __iter__(self):\n\t\treturn iter(self.deep_merge())\n\n\tdef __call__(self, **kwargs):\n\t\t\"\"\"\n\t\tmerge with the current node and return the result", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged = {}\n\t\tfor node in nodes:\n\t\t\tfor key, value in node.iteritems():\n\t\t\t\tif key not in merged:\n\t\t\t\t\tmerged[key] = value\n\t\t\t\telif isinstance(merged[key], MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes((merged[key], value))\n\t\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tmerged = MappingNode()\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor key, value in node.items():\n\t\t\t\t\tif key in merged:\n\t\t\t\t\t\tif isinstance(merged[key], MappingNode):\n\t\t\t\t\t\t\tmerged[key].update(value)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tmerged[key] = value\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\traise TypeError(\"expected MappingNode, got %s\" % type(node))\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor k, v in node.items():\n\t\t\t\t\tif k not in result:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\t\t\telif isinstance(v, MappingNode):\n\t\t\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tresult[k] = v\n\t\t\telse:\n\t\t\t\tresult[node.key] = node.value\n\t\treturn result\n\n\tdef merge_nodes(nodes):\n\t\t\"\"\"\n\t\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tif isinstance(node, MappingNode):\n\t\t\t\tfor k, v in node.items():\n\t\t\t\t\tif k not in result:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\t\t\tresult[k] = merge_nodes([result[k], v])\n\t\t\t\t\t\telif isinstance(v, MappingNode):\n\t\t\t\t\t\t\tresult[k] = merge_nodes([result[k", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\n\t#if type(nodes) is not list:\n\t#\tnodes = [nodes]\n\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\n\t# find keys that are common to all nodes\n\tcommon_keys = set()\n\tfor node in nodes:\n\t\tcommon_keys.update(node.keys())\n\n\t# merge the common keys\n\tmerged_node_keys = set()\n\tfor key in common_keys:\n\t\tmerged_node_keys.update(deep_merge_nodes([node[key] for node in nodes if key in node]))\n\n\tif len(nodes) > 1:\n\t\tmerged_node_keys = merged_node_keys.union(common_keys)\n\t\n\treturn MappingNode({key: deep_merge_nodes([node[key] for node in nodes if key in node]) for key in merged_node_keys})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tfor key, value in node.items():\n\t\t\t\tresult[key] = deep_merge_nodes([result.get(key, MappingNode()), value])\n\t\treturn result\n```\n\n## 2.\n\n### 2.1\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\tresult = {}\n\t\tfor node in nodes:\n\t\t\tresult.update(node.as_dict())\n\t\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"-i\", \"--input\", help=\"Input file\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file\", required=True)\n\tparser.add_argument(\"-c\", \"--cutoff\", help=\"Cutoff for SNPs\", type=int, default=10)\n\tparser.add_argument(\"-s\", \"--snp_type\", help=\"SNP Type\", choices=[\"SNP\", \"INDEL\"], default=\"SNP\")\n\tparser.add_argument(\"-m\", \"--max_snps\", help=\"Maximum number of SNPs to keep\", type=int, default=10000000)\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"Verbose\", action=\"store_true\")\n\treturn parser.parse_args()\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = ArgumentParser(add_help=False)\n\t\n\t# add help argument\n\tparser.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\t\n\tfor argument in arguments:\n\t\tparser.add_argument(*argument)\n\t\n\treturn parser\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Process command line arguments for this script')\n\tparser.add_argument('input_directory', help='Input directory containing the raw data')\n\tparser.add_argument('output_directory', help='Output directory to write the processed data')\n\tparser.add_argument('sample_name', help='Name of the sample')\n\tparser.add_argument('library_name', help='Name of the library')\n\tparser.add_argument('run_name', help='Name of the run')\n\tparser.add_argument('run_number', help='Run number of the run')\n\tparser.add_argument('instrument_type', help='Instrument type of the run')\n\tparser.add_argument('instrument_model', help='Instrument model of the run')\n\tparser.add_argument('instrument_serial', help='Instrument serial number of the run')\n\tparser.add_argument('instrument_firmware', help='Instrument firmware version of the run')\n\tparser.add_argument('instrument_control', help='Instrument control version of the run')\n\tparser.add_argument('instrument_data', help='Instrument data version of the run')\n\tparser.add_argument('instrument_analysis', help='Instrument analysis version of the run')\n\tparser.add_argument('instrument_analysis_control', help='Instrument", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Find the most similar files in a directory')\n\tparser.add_argument('-d', '--directory', help='The directory to scan for similar files', required=True)\n\tparser.add_argument('-s', '--similarity', help='The minimum similarity to consider (between 0 and 1)')\n\tparser.add_argument('-t', '--threshold', help='The minimum size of files to consider, in bytes')\n\tparser.add_argument('-c', '--chunksize', help='The number of bytes to read at most from a file when computing the hash')\n\tparser.add_argument('-n', '--num-results', help='The number of most similar files to return')\n\tparser.add_argument('-v', '--verbose', help='Print debug information', action='store_true')\n\tparser.add_argument('-q', '--quiet', help=\"Don't print anything\", action='store_true')\n\tparser.add_argument('-f', '--format', help='The output format, one of: csv or json', default='csv')\n\n\treturn parser.parse_args(args=arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\t# Create an ArgumentParser instance\n\tparser = argparse.ArgumentParser(description='Calculate the average of a list of numbers.')\n\t# add the input file argument\n\tparser.add_argument('input_file', help='The input file', type=argparse.FileType('r'))\n\t# parse the arguments and return the ArgumentParser instance\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='This script allows you to calculate the MSE and RMSE between two datasets')\n\n\trequired_group = parser.add_argument_group('required arguments')\n\t\n\trequired_group.add_argument('--data1', help='First dataset', required=True)\n\trequired_group.add_argument('--data2', help='Second dataset', required=True)\n\trequired_group.add_argument('--output', help='Output file', required=True)\n\t\n\targs = parser.parse_args()\n\t\n\treturn args\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n#   parse arguments\n\tparser = argparse.ArgumentParser(description='This script generates a random password. If no arguments are given, a random password with 12 characters is generated. If one or more arguments are given, the program will generate a password with the number of characters specified, using the character set specified. The number of characters must be a positive integer, and the character set must be one of the following: lowercase, uppercase, numbers or symbols. The character set can be specified only once. The user can specify the character set in any order.')\n\tparser.add_argument('-l', '--lowercase', action='store_true', default=False, help='Include lowercase letters in generated password')\n\tparser.add_argument('-u', '--uppercase', action='store_true', default=False, help='Include uppercase letters in generated password')\n\tparser.add_argument('-n', '--numbers', action='store_true', default=False, help='Include numbers in generated password')\n\tparser.add_argument('-s', '--symbols', action='store_true', default=False, help='Include symbols in generated password')\n\tparser.add_argument('-c', '--characters', type=int, default=12, help='Number of characters to generate in password (default: %(default)s)')\n\tparser.add_argument('character_set', nargs='?', default='ALL', help", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\t# create the argument parser\n\tparser = argparse.ArgumentParser(description=\"This script converts a file into a format suitable for use with the GenomeSpace platform\")\n\n\t\t# parse the arguments\n\targs = parser.parse_args(arguments)\n\n\treturn args\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument('-i', '--input', type=str, help='Input file in the form <path>[<tab>][<column>][/]<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>]>]', default=None)\n\tparser.add_argument('-o', '--output', type=str, help='Output file', default=None)\n\tparser.add_argument('-r', '--rows', type=int, help='Number of rows to process', default=None)\n\tparser.add_argument('-c', '--columns', type=str, help='Column separator', default=None)\n\tparser.add_argument('-t', '--tabs', type=str, help='Tab separator', default=None)\n\tparser.add_argument('-a', '--append', action='store_true', help='Append to output', default=None)\n\tparser.add_argument('-v', '--verbose', action='store_true', help='Verbose', default=None)\n\tparser.add_argument", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"-i\", \"--input\", help=\"Input file\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file\", required=True)\n\tparser.add_argument(\"-c\", \"--cutoff\", help=\"Cutoff for SNPs\", type=int, default=10)\n\tparser.add_argument(\"-s\", \"--snp_type\", help=\"SNP Type\", choices=[\"SNP\", \"INDEL\"], default=\"SNP\")\n\tparser.add_argument(\"-m\", \"--max_snps\", help=\"Maximum number of SNPs to keep\", type=int, default=10000000)\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"Verbose\", action=\"store_true\")\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = ArgumentParser(add_help=False)\n\t\n\t# add help argument\n\tparser.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\t\n\tfor argument in arguments:\n\t\tparser.add_argument(*argument)\n\t\n\treturn parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Process command line arguments for this script')\n\tparser.add_argument('input_directory', help='Input directory containing the raw data')\n\tparser.add_argument('output_directory', help='Output directory to write the processed data')\n\tparser.add_argument('sample_name', help='Name of the sample')\n\tparser.add_argument('library_name', help='Name of the library')\n\tparser.add_argument('run_name', help='Name of the run')\n\tparser.add_argument('run_number', help='Run number of the run')\n\tparser.add_argument('instrument_type', help='Instrument type of the run')\n\tparser.add_argument('instrument_model', help='Instrument model of the run')\n\tparser.add_argument('instrument_serial', help='Instrument serial number of the run')\n\tparser.add_argument('instrument_firmware', help='Instrument firmware version of the run')\n\tparser.add_argument('instrument_control', help='Instrument control version of the run')\n\tparser.add_argument('instrument_data', help='Instrument data version of the run')\n\tparser.add_argument('instrument_analysis', help='Instrument analysis version of the run')\n\tparser.add_argument('instrument_analysis_control', help='Instrument", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Find the most similar files in a directory')\n\tparser.add_argument('-d', '--directory', help='The directory to scan for similar files', required=True)\n\tparser.add_argument('-s', '--similarity', help='The minimum similarity to consider (between 0 and 1)')\n\tparser.add_argument('-t', '--threshold', help='The minimum size of files to consider, in bytes')\n\tparser.add_argument('-c', '--chunksize', help='The number of bytes to read at most from a file when computing the hash')\n\tparser.add_argument('-n', '--num-results', help='The number of most similar files to return')\n\tparser.add_argument('-v', '--verbose', help='Print debug information', action='store_true')\n\tparser.add_argument('-q', '--quiet', help=\"Don't print anything\", action='store_true')\n\tparser.add_argument('-f', '--format', help='The output format, one of: csv or json', default='csv')\n\n\treturn parser.parse_args(args=arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\t# Create an ArgumentParser instance\n\tparser = argparse.ArgumentParser(description='Calculate the average of a list of numbers.')\n\t# add the input file argument\n\tparser.add_argument('input_file', help='The input file', type=argparse.FileType('r'))\n\t# parse the arguments and return the ArgumentParser instance\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='This script allows you to calculate the MSE and RMSE between two datasets')\n\n\trequired_group = parser.add_argument_group('required arguments')\n\t\n\trequired_group.add_argument('--data1', help='First dataset', required=True)\n\trequired_group.add_argument('--data2', help='Second dataset', required=True)\n\trequired_group.add_argument('--output', help='Output file', required=True)\n\t\n\targs = parser.parse_args()\n\t\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n#   parse arguments\n\tparser = argparse.ArgumentParser(description='This script generates a random password. If no arguments are given, a random password with 12 characters is generated. If one or more arguments are given, the program will generate a password with the number of characters specified, using the character set specified. The number of characters must be a positive integer, and the character set must be one of the following: lowercase, uppercase, numbers or symbols. The character set can be specified only once. The user can specify the character set in any order.')\n\tparser.add_argument('-l', '--lowercase', action='store_true', default=False, help='Include lowercase letters in generated password')\n\tparser.add_argument('-u', '--uppercase', action='store_true', default=False, help='Include uppercase letters in generated password')\n\tparser.add_argument('-n', '--numbers', action='store_true', default=False, help='Include numbers in generated password')\n\tparser.add_argument('-s', '--symbols', action='store_true', default=False, help='Include symbols in generated password')\n\tparser.add_argument('-c', '--characters', type=int, default=12, help='Number of characters to generate in password (default: %(default)s)')\n\tparser.add_argument('character_set', nargs='?', default='ALL', help", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\t# create the argument parser\n\tparser = argparse.ArgumentParser(description=\"This script converts a file into a format suitable for use with the GenomeSpace platform\")\n\n\t\t# parse the arguments\n\targs = parser.parse_args(arguments)\n\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument('-i', '--input', type=str, help='Input file in the form <path>[<tab>][<column>][/]<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>][/][<path>[<tab>][<column>]>]', default=None)\n\tparser.add_argument('-o', '--output', type=str, help='Output file', default=None)\n\tparser.add_argument('-r', '--rows', type=int, help='Number of rows to process', default=None)\n\tparser.add_argument('-c', '--columns', type=str, help='Column separator', default=None)\n\tparser.add_argument('-t', '--tabs', type=str, help='Tab separator', default=None)\n\tparser.add_argument('-a', '--append', action='store_true', help='Append to output', default=None)\n\tparser.add_argument('-v', '--verbose', action='store_true', help='Verbose', default=None)\n\tparser.add_argument", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\t# Remove the default help flag.\n\tflags = [a for a in parser._actions if a.option_strings]\n\treturn \" \".join(f for action in flags for f in action.option_strings)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t \n\treturn \" \".join([\"%s=%s\" % (x.dest, x.default) for x in parser._actions])\n \nif __name__ == \"__main__\":\n\t# create the top-level parser\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument('cmd', help='sub-command name')\n\targs, unknown = parser.parse_known_args()\n\t\n\tif args.cmd == 'build':\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='Create a new project.',\n\t\t\tparents=[base_parser])\n\t\tparser.add_argument('project_name', help='the name of the project')\n\t\tparser.add_argument('--force', '-f', action='store_true', help='force creation of project')\n\t\targs = parser.parse_args(unknown)\n\n\t\tif os.path.exists(os.path.join(os.getcwd(), args.project_name)):\n\t\t\tprint(\"Project %s already exists\" % args.project_name)\n\t\t\tsys.exit(1)\n\n\t\tif not args.force:\n\t\t\tprint(\"Creating project %s.\" % args.project_name)\n\t\t\tif not os.path.exists(os.path.join(os.getcwd(), args.project_name)):\n\t\t\t\tos.mkdir(args.project_name)\n\t\t\t\t", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = []\n\t\tfor action in parser._actions:\n\t\t\tfor flag in action.option_strings:\n\t\t\t\tflags.append(flag)\n\t\treturn''.join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = {}\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tflags[action.dest] = action.option_strings[0]\n\treturn''.join(flags.values())\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\treturn''.join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\nparser = argparse.ArgumentParser(description=__doc__)\nparser_flags(parser)\nargs = parser.parse_args()\n\n# Read the data, convert it to a dictionary:\n\nfrom collections import defaultdict\n\ndata = defaultdict(dict)\n\nfor line in open(args.input):\n\tline = line.rstrip()\n\tif not line or line[0] == \"#\": continue\n\tfields = line.split(\"\\t\")\n\tdata[fields[0]][fields[1]] = fields[2]\n\n# Print the data:\n\nfor k, v in data.iteritems():\n\tprint k, \"\\t\".join(v.values())\n/README.md\n# cnv-analysis\n\nScripts for processing CNV calls.\n/scripts/cnv-overlap.py\n#!/usr/bin/env python2.7\n\n\"\"\"\nUsage:\n\tcnv-overlap.py <cnv-calls.txt> <genes-of-interest.txt>\n\"\"\"\n\nimport sys\nimport argparse\nimport os\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(f for f in parser._get_optional_actions() if f.dest!= 'help')\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\t# Get a list of all flags that have been set on the parser\n\tflags = parser.parse_args(None).to_dict()\n\n\t# Remove the \"flags\" and \"verbose\" keys from the dict\n\tflags.pop(\"flags\", None)\n\tflags.pop(\"verbose\", None)\n\n\t# Join the flag keys into a space-separated string\n\tflags = \" \".join([\"--\" + key for key in flags.keys()])\n\n\treturn flags\n# Copyright (c) 2016-2018, \n# See LICENSE for details\n\n\"\"\"\nCreate and manage a database connection pool for the application.\n\"\"\"\n\n# Import standard library modules\nimport sqlite3\n\n# Import local modules\nfrom. import config\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = []\n\t\tfor action in parser._actions:\n\t\t\tif action.option_strings:\n\t\t\t\tflags += action.option_strings\n\t\treturn''.join(flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\t# Remove the default help flag.\n\tflags = [a for a in parser._actions if a.option_strings]\n\treturn \" \".join(f for action in flags for f in action.option_strings)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t \n\treturn \" \".join([\"%s=%s\" % (x.dest, x.default) for x in parser._actions])\n \nif __name__ == \"__main__\":\n\t# create the top-level parser\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument('cmd', help='sub-command name')\n\targs, unknown = parser.parse_known_args()\n\t\n\tif args.cmd == 'build':\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='Create a new project.',\n\t\t\tparents=[base_parser])\n\t\tparser.add_argument('project_name', help='the name of the project')\n\t\tparser.add_argument('--force', '-f', action='store_true', help='force creation of project')\n\t\targs = parser.parse_args(unknown)\n\n\t\tif os.path.exists(os.path.join(os.getcwd(), args.project_name)):\n\t\t\tprint(\"Project %s already exists\" % args.project_name)\n\t\t\tsys.exit(1)\n\n\t\tif not args.force:\n\t\t\tprint(\"Creating project %s.\" % args.project_name)\n\t\t\tif not os.path.exists(os.path.join(os.getcwd(), args.project_name)):\n\t\t\t\tos.mkdir(args.project_name)\n\t\t\t\t", "is_pass": false, "return_code": 2}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = []\n\t\tfor action in parser._actions:\n\t\t\tfor flag in action.option_strings:\n\t\t\t\tflags.append(flag)\n\t\treturn''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = {}\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tflags[action.dest] = action.option_strings[0]\n\treturn''.join(flags.values())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\treturn''.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\nparser = argparse.ArgumentParser(description=__doc__)\nparser_flags(parser)\nargs = parser.parse_args()\n\n# Read the data, convert it to a dictionary:\n\nfrom collections import defaultdict\n\ndata = defaultdict(dict)\n\nfor line in open(args.input):\n\tline = line.rstrip()\n\tif not line or line[0] == \"#\": continue\n\tfields = line.split(\"\\t\")\n\tdata[fields[0]][fields[1]] = fields[2]\n\n# Print the data:\n\nfor k, v in data.iteritems():\n\tprint k, \"\\t\".join(v.values())\n/README.md\n# cnv-analysis\n\nScripts for processing CNV calls.\n/scripts/cnv-overlap.py\n#!/usr/bin/env python2.7\n\n\"\"\"\nUsage:\n\tcnv-overlap.py <cnv-calls.txt> <genes-of-interest.txt>\n\"\"\"\n\nimport sys\nimport argparse\nimport os\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\treturn''.join(f for f in parser._get_optional_actions() if f.dest!= 'help')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\t# Get a list of all flags that have been set on the parser\n\tflags = parser.parse_args(None).to_dict()\n\n\t# Remove the \"flags\" and \"verbose\" keys from the dict\n\tflags.pop(\"flags\", None)\n\tflags.pop(\"verbose\", None)\n\n\t# Join the flag keys into a space-separated string\n\tflags = \" \".join([\"--\" + key for key in flags.keys()])\n\n\treturn flags\n# Copyright (c) 2016-2018, \n# See LICENSE for details\n\n\"\"\"\nCreate and manage a database connection pool for the application.\n\"\"\"\n\n# Import standard library modules\nimport sqlite3\n\n# Import local modules\nfrom. import config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\tflags = []\n\t\tfor action in parser._actions:\n\t\t\tif action.option_strings:\n\t\t\t\tflags += action.option_strings\n\t\treturn''.join(flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tsubparsers = parser.add_subparsers(dest='command', help='command to execute')\n\t\tsubparsers.required = True\n\n\t\t# get all commands\n\t\tcommands = list(\n\t\t\tset(\n\t\t\t\tname.split('.')[1]\n\t\t\t\tfor name in COMMANDS\n\t\t\t\tif name.startswith('borgmatic.commands.')\n\t\t\t)\n\t\t)\n\n\t\t# add completion for each command\n\t\tfor command in commands:\n\t\t\tcmd = getattr(commands, command)\n\t\t\tcmd.add_arguments(subparsers.add_parser(command))\n\n\t\t# parse the arguments\n\t\targs = parser.parse_args()\n\n\t\t# print the shell completion script\n\t\tprint(args.command.format_help())\n\n\n\n@cli.command()\n@click.argument('repository', type=click.Path())\n@click.option('--config', '-c', type=click.Path(), default=None, help=\"Specify path to config file.\")\n@click.option('--debug', '-d', is_flag=True, help=\"Enable debug output.\")", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\nif __name__ == '__main__':\n\tmain()\n#!/bin/bash\n\nset -e\n\n# Setup some variables\n\n# The name of this script\nscript=$0\n\n# The directory this script is in\nscriptdir=\"$(dirname $0)\"\n\n# The directory this script is in\nscriptdir", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\tdef _format_help(help_string):\n\t\t\"\"\"\n\t\tFormats the help string to produce a bash completion snippet.\n\t\t\"\"\"\n\t\thelp_string = help_string.replace('{', '\\\\{').replace('}', '\\\\}')\n\t\thelp_string = help_string.replace('|', '\\\\|')\n\t\treturn help_string\n\n\tdef _get_parser_for_command(command):\n\t\t\"\"\"\n\t\tGet the parser for the given command.\n\t\t\"\"\"\n\t\tparser = None\n\t\tfor action_group in Borgmatic.parser.actions:\n\t\t\tfor action_name, action in action_group._group_actions.items():\n\t\t\t\tif action.option_strings[0] == command:\n\t\t\t\t\tparser = action\n\t\treturn parser\n\n\tdef _get_completions_for_parser(parser):\n\t\t\"\"\"\n\t\tGet the completion snippet for the given parser.\n\t\t\"\"\"\n\t\tcompletions = [\n\t\t\t'--' + option.lstrip('-')\n\t\t\tfor option in parser.option_strings\n\t\t]\n\t\tcompletions += [\n\t\t\t'--' + action.dest.replace('_', '-')\n\t\t\tfor action in parser.actions\n\t\t]\n\t\treturn completions\n\n\tdef _get_completions_for_command(command):\n\t\t\"\"\"\n\t\tGet the completion snippet for the given command.\n\t\t\"\"\"\n\t\tparser", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\t# The bash completion command is called `borgmatic`\n\t# We start with the basic bash completion logic from the `borg` manpage\n\t# We then go through the argument parser, and add the appropriate options\n\t# and arguments to the command.\n\tcommand = 'borgmatic'\n\t# The first line of the manpage states:\n\t#\n\t# > A shell function which is run by the BASH shell for command completion.\n\t# > Source this function in your.bashrc to enable command completion.\n\tprint('complete -F _borgmatic borgmatic')\n\n\t# The second line states:\n\t# > If you are using the bash-completion package, you can source this file\n\t# > in your.bashrc or.bash_profile.\n\t# >\n\t# > This will enable command completion for borg.\n\n\t# The third line states:\n\t# > You can run this command to get a list of all available commands:\n\t# >\n\t# >   borgmatic --list-commands\n\tprint(f'{command} --list-commands')\n\n\t# The fourth line states:\n\t# > You can use --help to get help for individual commands:\n\t# >\n\t# >   borgmatic --help init\n\tprint(f'{command} --help')\n\n\t# The f", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\t#print(\"Bash completion for borgmatic\")\n\t#print(\"  BORG_REPOSITORY: \" + str(BORG_REPOSITORY))\n\t#print(\"  BORG_PASSPHRASE: \" + str(BORG_PASSPHRASE))\n\t#print(\"  BORG_REPOSITORY_EXCLUDE: \" + str(BORG_REPOSITORY_EXCLUDE))\n\t#print(\"  BORG_REPOSITORY_INCLUDE: \" + str(BORG_REPOSITORY_INCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH: \" + str(BORG_REPOSITORY_PATH))\n\t#print(\"  BORG_REPOSITORY_PATH_EXCLUDE: \" + str(BORG_REPOSITORY_PATH_EXCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH_INCLUDE: \" + str(BORG_REPOSITORY_PATH_INCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH_EXCLUDE: \" + str(BORG_REPOSITORY_PATH_EXCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH_INCLUDE: \" + str(BORG_REPOSITORY_PATH_INCLUDE))\n\n\tborg_repository_path = BORG_REPOSITORY_PATH\n\tif not borg_repository_path:\n\t\tborg_repository_path = BORG_REPOSITORY_PATH_INCLUDE\n\tif not borg_repository", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\t# first, get the parser and subcommands of the borgmatic command\n\t\tparser = borgmatic.cli.get_parser(borgmatic.cli.BorgmaticCommand, args=[])\n\t\tcommands = set(parser._subparsers._name_parser_map.keys())\n\n\t\t# filter out commands that are not subcommands of the borgmatic command\n\t\tfor c in list(commands):\n\t\t\tif not c.startswith('borgmatic:'):\n\t\t\t\tcommands.remove(c)\n\n\t\t# now, get the argument specifications for each of the borgmatic subcommands\n\t\tspecs = {}\n\t\tfor c in commands:\n\t\t\tparser = borgmatic.cli.get_parser(borgmatic.cli.BorgmaticCommand, args=[c.split(':')[1]])\n\t\t\tfor a in parser._actions:\n\t\t\t\tif a.dest in specs.keys():\n\t\t\t\t\tspecs[a.dest].extend(['--' + a.option_strings[0]])\n\t\t\t\telse:\n\t\t\t\t\tspecs[a.dest] = ['--' + a.option_strings[0]]\n\n\t\t# now, get the argument specifications for each of the borg-backup subcommands\n\t\tparser = borgmatic.cli.get_parser(borgmatic.cli.BorgBackupCommand, args=[])\n\t\tfor a in parser._actions:\n\t\t\tif a", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tsubparsers = parser.add_subparsers(dest='command', help='command to execute')\n\t\tsubparsers.required = True\n\n\t\t# get all commands\n\t\tcommands = list(\n\t\t\tset(\n\t\t\t\tname.split('.')[1]\n\t\t\t\tfor name in COMMANDS\n\t\t\t\tif name.startswith('borgmatic.commands.')\n\t\t\t)\n\t\t)\n\n\t\t# add completion for each command\n\t\tfor command in commands:\n\t\t\tcmd = getattr(commands, command)\n\t\t\tcmd.add_arguments(subparsers.add_parser(command))\n\n\t\t# parse the arguments\n\t\targs = parser.parse_args()\n\n\t\t# print the shell completion script\n\t\tprint(args.command.format_help())\n\n\n\n@cli.command()\n@click.argument('repository', type=click.Path())\n@click.option('--config', '-c', type=click.Path(), default=None, help=\"Specify path to config file.\")\n@click.option('--debug', '-d', is_flag=True, help=\"Enable debug output.\")", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\t# Get the borgmatic command from the borgmatic module\n\ttry:\n\t\tfrom borgmatic.borgmatic import borgmatic\n\texcept ImportError:\n\t\tprint(\"Couldn't import borgmatic\")\n\t\tsys.exit(1)\n\n\nif __name__ == '__main__':\n\tmain()\n#!/bin/bash\n\nset -e\n\n# Setup some variables\n\n# The name of this script\nscript=$0\n\n# The directory this script is in\nscriptdir=\"$(dirname $0)\"\n\n# The directory this script is in\nscriptdir", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\tdef _format_help(help_string):\n\t\t\"\"\"\n\t\tFormats the help string to produce a bash completion snippet.\n\t\t\"\"\"\n\t\thelp_string = help_string.replace('{', '\\\\{').replace('}', '\\\\}')\n\t\thelp_string = help_string.replace('|', '\\\\|')\n\t\treturn help_string\n\n\tdef _get_parser_for_command(command):\n\t\t\"\"\"\n\t\tGet the parser for the given command.\n\t\t\"\"\"\n\t\tparser = None\n\t\tfor action_group in Borgmatic.parser.actions:\n\t\t\tfor action_name, action in action_group._group_actions.items():\n\t\t\t\tif action.option_strings[0] == command:\n\t\t\t\t\tparser = action\n\t\treturn parser\n\n\tdef _get_completions_for_parser(parser):\n\t\t\"\"\"\n\t\tGet the completion snippet for the given parser.\n\t\t\"\"\"\n\t\tcompletions = [\n\t\t\t'--' + option.lstrip('-')\n\t\t\tfor option in parser.option_strings\n\t\t]\n\t\tcompletions += [\n\t\t\t'--' + action.dest.replace('_', '-')\n\t\t\tfor action in parser.actions\n\t\t]\n\t\treturn completions\n\n\tdef _get_completions_for_command(command):\n\t\t\"\"\"\n\t\tGet the completion snippet for the given command.\n\t\t\"\"\"\n\t\tparser", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\t# The bash completion command is called `borgmatic`\n\t# We start with the basic bash completion logic from the `borg` manpage\n\t# We then go through the argument parser, and add the appropriate options\n\t# and arguments to the command.\n\tcommand = 'borgmatic'\n\t# The first line of the manpage states:\n\t#\n\t# > A shell function which is run by the BASH shell for command completion.\n\t# > Source this function in your.bashrc to enable command completion.\n\tprint('complete -F _borgmatic borgmatic')\n\n\t# The second line states:\n\t# > If you are using the bash-completion package, you can source this file\n\t# > in your.bashrc or.bash_profile.\n\t# >\n\t# > This will enable command completion for borg.\n\n\t# The third line states:\n\t# > You can run this command to get a list of all available commands:\n\t# >\n\t# >   borgmatic --list-commands\n\tprint(f'{command} --list-commands')\n\n\t# The fourth line states:\n\t# > You can use --help to get help for individual commands:\n\t# >\n\t# >   borgmatic --help init\n\tprint(f'{command} --help')\n\n\t# The f", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\t#print(\"Bash completion for borgmatic\")\n\t#print(\"  BORG_REPOSITORY: \" + str(BORG_REPOSITORY))\n\t#print(\"  BORG_PASSPHRASE: \" + str(BORG_PASSPHRASE))\n\t#print(\"  BORG_REPOSITORY_EXCLUDE: \" + str(BORG_REPOSITORY_EXCLUDE))\n\t#print(\"  BORG_REPOSITORY_INCLUDE: \" + str(BORG_REPOSITORY_INCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH: \" + str(BORG_REPOSITORY_PATH))\n\t#print(\"  BORG_REPOSITORY_PATH_EXCLUDE: \" + str(BORG_REPOSITORY_PATH_EXCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH_INCLUDE: \" + str(BORG_REPOSITORY_PATH_INCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH_EXCLUDE: \" + str(BORG_REPOSITORY_PATH_EXCLUDE))\n\t#print(\"  BORG_REPOSITORY_PATH_INCLUDE: \" + str(BORG_REPOSITORY_PATH_INCLUDE))\n\n\tborg_repository_path = BORG_REPOSITORY_PATH\n\tif not borg_repository_path:\n\t\tborg_repository_path = BORG_REPOSITORY_PATH_INCLUDE\n\tif not borg_repository", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t\t# first, get the parser and subcommands of the borgmatic command\n\t\tparser = borgmatic.cli.get_parser(borgmatic.cli.BorgmaticCommand, args=[])\n\t\tcommands = set(parser._subparsers._name_parser_map.keys())\n\n\t\t# filter out commands that are not subcommands of the borgmatic command\n\t\tfor c in list(commands):\n\t\t\tif not c.startswith('borgmatic:'):\n\t\t\t\tcommands.remove(c)\n\n\t\t# now, get the argument specifications for each of the borgmatic subcommands\n\t\tspecs = {}\n\t\tfor c in commands:\n\t\t\tparser = borgmatic.cli.get_parser(borgmatic.cli.BorgmaticCommand, args=[c.split(':')[1]])\n\t\t\tfor a in parser._actions:\n\t\t\t\tif a.dest in specs.keys():\n\t\t\t\t\tspecs[a.dest].extend(['--' + a.option_strings[0]])\n\t\t\t\telse:\n\t\t\t\t\tspecs[a.dest] = ['--' + a.option_strings[0]]\n\n\t\t# now, get the argument specifications for each of the borg-backup subcommands\n\t\tparser = borgmatic.cli.get_parser(borgmatic.cli.BorgBackupCommand, args=[])\n\t\tfor a in parser._actions:\n\t\t\tif a", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Run the specified subcommand\")\n\n\tsubparsers = parser.add_subparsers(dest=\"subcommand\")\n\tsubparsers.required = True\n\n\tsubparser_init = subparsers.add_parser(\"init\", help=\"Initialize the repository\")\n\tsubparser_init.add_argument(\"--force\", action=\"store_true\", help=\"Force reinitialization\")\n\tsubparser_init.add_argument(\"--url\", help=\"URL of the repository to be used\")\n\n\tsubparser_status = subparsers.add_parser(\"status\", help=\"Show the status of the repository\")\n\n\tsubparser_add = subparsers.add_parser(\"add\", help=\"Add a file to the repository\")\n\tsubparser_add.add_argument(\"files\", metavar=\"file\", nargs=\"+\", help=\"File to add to the repository\")\n\tsubparser_add.add_argument(\"--all\", action=\"store_true\", help=\"Add all files in the current directory\")\n\n\tsubparser_commit = subparsers.add_parser(\"commit\", help=\"Commit changes to the repository\")\n\tsubparser_commit.add_argument(\"message\", help=\"Commit message\")\n\n\tsubparser_log = subparsers.add_parser(\"log\", help=\"Show the commit log\")\n\n\tsubparser_checkout = subparsers.add_parser(\"checkout\",", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest=\"command\", title=\"commands\")\n\n\t# make the global parser\n\tglobal_parser = subparsers.add_parser(\"global\", help=\"Global configuration options.\")\n\tglobal_parser.add_argument(\"-c\", \"--config\", metavar=\"FILE\", help=\"Load configuration from FILE.\")\n\tglobal_parser.add_argument(\"-l\", \"--list-config\", action=\"store_true\", help=\"List available configuration options.\")\n\tglobal_parser.add_argument(\"--version\", action=\"version\", version=__version__)\n\n\t# make the 'check' parser\n\tcheck_parser = subparsers.add_parser(\"check\", help=\"Check the configuration of the system.\")\n\tcheck_parser.add_argument(\"-s\", \"--show-config\", action=\"store_true\", help=\"Show the configuration.\")\n\tcheck_parser.add_argument(\"-f\", \"--fix\", action=\"store_true\", help=\"Attempt to automatically fix any issues.\")\n\n\t# make the 'install' parser\n\tinstall_parser = subparsers.add_parser(\"install\", help=\"Install the system.\")\n\tinstall_parser.add_argument(\"-s\", \"--show-config\", action=\"store_true\", help=\"Show the configuration.\")\n\tinstall_parser.add_argument(\"-f\", \"--fix\", action", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=\"Run the training, validation, and testing loop for a specified model.\")\n\tparser.add_argument('--model_name', type=str, default=None, help=\"Name of a trained model to use.\")\n\tparser.add_argument('--model_type', type=str, default=None, help=\"Type of model to use. Options are: 'logistic', 'linear_svm','mlp'.\")\n\tparser.add_argument('--train', dest='train', action='store_true', help=\"Run training loop.\")\n\tparser.add_argument('--validate', dest='validate', action='store_true', help=\"Run validation loop.\")\n\tparser.add_argument('--test', dest='test', action='store_true', help=\"Run test loop.\")\n\tparser.add_argument('--folds', type=int, default=None, help=\"Number of folds to use for cross validation.\")\n\tparser.add_argument('--model_save_dir', type=str, default=None, help=\"Directory where trained model should be saved.\")\n\tparser.add_argument('--log_file', type=str, default=None, help=\"File to log output to.\")\n\tparser.add_argument('--train_dir', type=str, default=None, help=\"Directory containing training data.\")\n\tparser.add", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis is the main script for the MIST2 pipeline. It manages the execution of all the subcommands,\n\tincluding the main command \"run\". The \"run\" command is the only one that does not require any\n\targuments, but all the other subcommands (e.g. \"list\", \"run\") require additional arguments.\n\t\"\"\")\n\t\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=__version__)\n\t\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\t\t# Global arguments\n\t\tglobal_parser = argparse.ArgumentParser(add_help=False)\n\t\tglobal_parser.add_argument(\"-o\", \"--output\", default=\"mist2_output\", help=\"Output directory\")\n\t\tglobal_parser.add_argument(\"-p\", \"--project\", default=\"mist2_project\", help=\"Project directory\")\n\t\tglobal_parser.add_argument(\"-s\", \"--source\", default=\"mist2_source\", help=\"Source directory\")\n\t\tglobal_parser.add_argument(\"--config\", default=\"mist2.py\", help=\"Config file\")\n\n\t\t# List subcommand\n\t\tlist_parser = subparsers.add_parser(\"list\", help=\"List available commands\")\n\t\tlist_parser.add_argument(\"command\", help=\"Command\")\n\t\tlist_parser", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Perform automated QC on raw reads\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\t# set up a subparser for each command.\n\tsubparsers.add_parser(\"get_fastqs\", help=\"Get fastq files from sra\")\n\tsubparsers.add_parser(\"get_fastqs_by_sample\", help=\"Get fastq files from sra by sample\")\n\tsubparsers.add_parser(\"get_fastqs_by_sample_and_date\", help=\"Get fastq files from sra by sample and date\")\n\tsubparsers.add_parser(\"get_sra_run_info\", help=\"Get sra run information by sample\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_date\", help=\"Get sra run information by sample and date\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_sample\", help=\"Get sra run information by sample\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_sample_and_date\", help=\"Get sra run information by sample and date\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_sample_and_lane\", help=\"Get sra run", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__doc__)\n\t\tparser.add_argument(\"--config\", help=\"YAML configuration file\", default=DEFAULT_CONFIG_FILE)\n\t\tparser.add_argument(\"--log-level\", help=\"Logging level\", default=DEFAULT_LOG_LEVEL)\n\t\tsubparsers = parser.add_subparsers(title=\"subcommands\", description=\"valid subcommands\", dest=\"command\")\n\t\tsubparsers.required = True\n\t\tsubparsers.add_parser(\"create\", help=\"create a new project\")\n\t\tsubparsers.add_parser(\"publish\", help=\"publish a project to the web\")\n\t\tsubparsers.add_parser(\"rebuild\", help=\"rebuild a project\")\n\t\tsubparsers.add_parser(\"upgrade\", help=\"upgrade a project\")\n\t\tsubparsers.add_parser(\"run\", help=\"run a project\")\n\t\tsubparsers.add_parser(\"deploy\", help=\"deploy a project\")\n\t\tsubparsers.add_parser(\"init\", help=\"initialize a project\")\n\t\tsubparsers.add_parser(\"serve\", help=\"serve a project\")\n\t\tsubparsers.add_parser(\"build\", help=\"build a project\")\n\t\tsubparsers.add_parser(\"publish-build\", help=\"publish a project\")\n\t\tsubparsers.add_parser(\"test\", help=\"test a project\")\n\t\tsubparsers.add_parser(\"destroy\",", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tclass CustomFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n\t\tpass\n\n\tdefault_argument_parser = argparse.ArgumentParser(description=__doc__, formatter_class=CustomFormatter)\n\tdefault_argument_parser.add_argument(\"--debug\", action=\"store_true\", help=\"Print debug messages.\")\n\tdefault_argument_parser.add_argument(\"--version\", action=\"version\", version=\"1.0\")\n\tdefault_argument_parser.add_argument(\"-v\", \"--verbosity\", type=int, choices=[0, 1, 2, 3], default=1, help=\"Verbosity level. 0 = silent, 1 = only warning, 2 = warning and info, 3 = debug\")\n\n\tsubparsers = default_argument_parser.add_subparsers(title=\"Subcommands\", description=\"Valid subcommands\", help=\"Sub-command help\")\n\n\t#\n\t# subparser for the \"create\" subcommand\n\t#\n\tcreate_subparser = subparsers.add_parser(\"create\", help=\"Create a new file\")\n\tcreate_subparser.add_argument(\"--name\", help=\"Name of the file to be created.\", required=True)\n\tcreate_subparser.add_argument(\"--path\", help=\"Path of the file to be created.\", required=True)\n\tcreate_subparser", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"A script to generate random data for testing purposes.\")\n\t\tsubparsers = parser.add_subparsers(dest=\"command\", help=\"sub-command help\")\n\n\t\tglobal_parser = subparsers.add_parser(\"global\", help=\"Global options\")\n\t\tglobal_parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Turn on verbose output\")\n\t\tglobal_parser.add_argument(\"--debug\", action=\"store_true\", help=\"Turn on debug output\")\n\n\t\tfor subcommand in subcommands:\n\t\t\tsubcommand.add_subparser(subparsers)\n\n\t\treturn parser.parse_args(*unparsed_arguments)\n\n\tdef main():\n\t\targs = parse_arguments(*sys.argv[1:])\n\t\tif args.command == \"global\":\n\t\t\tglobal_options.update(vars(args))\n\t\telse:\n\t\t\tsubcommand_options.update(vars(args))\n\n\t\tif args.debug:\n\t\t\tglobal_options[\"verbose\"] = True\n\n\t\tif global_options[\"verbose\"]:\n\t\t\tprint(\"Global options: {}\".format(global_options))\n\t\t\tprint(\"Sub-command options: {}\".format(subcommand_options))\n\n\t\tsubcommands[args.command](subcommand_options)\n\n\tif __name__ == \"__main__\":\n\t\tmain()\n\n\"\"\"\nThe following is a list", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='A parser for the UW-Madison 2016-2017 FPGA course project')\n\tsubparsers = parser.add_subparsers(dest='command', help='sub-command help')\n\t \n\t# Subparser for creating a new project\n\tparser_create = subparsers.add_parser('create', help='Creates a new project')\n\tparser_create.add_argument('project_name', help='The name of the project')\n\tparser_create.add_argument('-f', '--force', action='store_true', help='Overwrite an existing project')\n\t \n\t# Subparser for adding a new module to a project\n\tparser_add = subparsers.add_parser('add', help='Adds a new module to a project')\n\tparser_add.add_argument('module_name', help='The name of the module to add')\n\tparser_add.add_argument('-p', '--project', help='The name of the project to add the module to')\n\tparser_add.add_argument('-f', '--force', action='store_true', help='Overwrite an existing module')\n\t \n\t# Subparser for adding a new module to a project\n\tparser_remove = subparsers.add_parser('remove', help='Removes a module from a project')\n\t", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)\n\t\tparser.add_argument('--version', action='version', version='%(prog)s 0.1')\n\t\tsubparsers = parser.add_subparsers(help='sub-command help', dest='subparser_name')\n\n\t\t# Add subparser for each of the subcommands.\n\t\tsubparsers.add_parser('subcommand1', help='subcommand1 help')\n\t\tsubparsers.add_parser('subcommand2', help='subcommand2 help')\n\n\t\t# Parse the command-line arguments.\n\t\treturn vars(parser.parse_args(args=unparsed_arguments))\n\n\n\nif __name__ == '__main__':\n\targuments = parse_arguments(*sys.argv[1:])\n\tif arguments['subparser_name'] =='subcommand1':\n\t\t# Do something for subcommand1.\n\t\tprint('subcommand1 was called.')\n\telif arguments['subparser_name'] =='subcommand2':\n\t\t# Do something for subcommand2.\n\t\tprint('subcommand2 was called.')\n\telse:\n\t\t# Print help.\n\t\tparser.print_help()\n/04_python_exceptions/01_exception_handling_basics.py\n# -*- coding: utf-8 -*-\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Run the specified subcommand\")\n\n\tsubparsers = parser.add_subparsers(dest=\"subcommand\")\n\tsubparsers.required = True\n\n\tsubparser_init = subparsers.add_parser(\"init\", help=\"Initialize the repository\")\n\tsubparser_init.add_argument(\"--force\", action=\"store_true\", help=\"Force reinitialization\")\n\tsubparser_init.add_argument(\"--url\", help=\"URL of the repository to be used\")\n\n\tsubparser_status = subparsers.add_parser(\"status\", help=\"Show the status of the repository\")\n\n\tsubparser_add = subparsers.add_parser(\"add\", help=\"Add a file to the repository\")\n\tsubparser_add.add_argument(\"files\", metavar=\"file\", nargs=\"+\", help=\"File to add to the repository\")\n\tsubparser_add.add_argument(\"--all\", action=\"store_true\", help=\"Add all files in the current directory\")\n\n\tsubparser_commit = subparsers.add_parser(\"commit\", help=\"Commit changes to the repository\")\n\tsubparser_commit.add_argument(\"message\", help=\"Commit message\")\n\n\tsubparser_log = subparsers.add_parser(\"log\", help=\"Show the commit log\")\n\n\tsubparser_checkout = subparsers.add_parser(\"checkout\",", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest=\"command\", title=\"commands\")\n\n\t# make the global parser\n\tglobal_parser = subparsers.add_parser(\"global\", help=\"Global configuration options.\")\n\tglobal_parser.add_argument(\"-c\", \"--config\", metavar=\"FILE\", help=\"Load configuration from FILE.\")\n\tglobal_parser.add_argument(\"-l\", \"--list-config\", action=\"store_true\", help=\"List available configuration options.\")\n\tglobal_parser.add_argument(\"--version\", action=\"version\", version=__version__)\n\n\t# make the 'check' parser\n\tcheck_parser = subparsers.add_parser(\"check\", help=\"Check the configuration of the system.\")\n\tcheck_parser.add_argument(\"-s\", \"--show-config\", action=\"store_true\", help=\"Show the configuration.\")\n\tcheck_parser.add_argument(\"-f\", \"--fix\", action=\"store_true\", help=\"Attempt to automatically fix any issues.\")\n\n\t# make the 'install' parser\n\tinstall_parser = subparsers.add_parser(\"install\", help=\"Install the system.\")\n\tinstall_parser.add_argument(\"-s\", \"--show-config\", action=\"store_true\", help=\"Show the configuration.\")\n\tinstall_parser.add_argument(\"-f\", \"--fix\", action", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description=\"Run the training, validation, and testing loop for a specified model.\")\n\tparser.add_argument('--model_name', type=str, default=None, help=\"Name of a trained model to use.\")\n\tparser.add_argument('--model_type', type=str, default=None, help=\"Type of model to use. Options are: 'logistic', 'linear_svm','mlp'.\")\n\tparser.add_argument('--train', dest='train', action='store_true', help=\"Run training loop.\")\n\tparser.add_argument('--validate', dest='validate', action='store_true', help=\"Run validation loop.\")\n\tparser.add_argument('--test', dest='test', action='store_true', help=\"Run test loop.\")\n\tparser.add_argument('--folds', type=int, default=None, help=\"Number of folds to use for cross validation.\")\n\tparser.add_argument('--model_save_dir', type=str, default=None, help=\"Directory where trained model should be saved.\")\n\tparser.add_argument('--log_file', type=str, default=None, help=\"File to log output to.\")\n\tparser.add_argument('--train_dir', type=str, default=None, help=\"Directory containing training data.\")\n\tparser.add", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis is the main script for the MIST2 pipeline. It manages the execution of all the subcommands,\n\tincluding the main command \"run\". The \"run\" command is the only one that does not require any\n\targuments, but all the other subcommands (e.g. \"list\", \"run\") require additional arguments.\n\t\"\"\")\n\t\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=__version__)\n\t\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\t\t# Global arguments\n\t\tglobal_parser = argparse.ArgumentParser(add_help=False)\n\t\tglobal_parser.add_argument(\"-o\", \"--output\", default=\"mist2_output\", help=\"Output directory\")\n\t\tglobal_parser.add_argument(\"-p\", \"--project\", default=\"mist2_project\", help=\"Project directory\")\n\t\tglobal_parser.add_argument(\"-s\", \"--source\", default=\"mist2_source\", help=\"Source directory\")\n\t\tglobal_parser.add_argument(\"--config\", default=\"mist2.py\", help=\"Config file\")\n\n\t\t# List subcommand\n\t\tlist_parser = subparsers.add_parser(\"list\", help=\"List available commands\")\n\t\tlist_parser.add_argument(\"command\", help=\"Command\")\n\t\tlist_parser", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Perform automated QC on raw reads\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\t# set up a subparser for each command.\n\tsubparsers.add_parser(\"get_fastqs\", help=\"Get fastq files from sra\")\n\tsubparsers.add_parser(\"get_fastqs_by_sample\", help=\"Get fastq files from sra by sample\")\n\tsubparsers.add_parser(\"get_fastqs_by_sample_and_date\", help=\"Get fastq files from sra by sample and date\")\n\tsubparsers.add_parser(\"get_sra_run_info\", help=\"Get sra run information by sample\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_date\", help=\"Get sra run information by sample and date\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_sample\", help=\"Get sra run information by sample\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_sample_and_date\", help=\"Get sra run information by sample and date\")\n\tsubparsers.add_parser(\"get_sra_run_info_by_sample_and_lane\", help=\"Get sra run", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__doc__)\n\t\tparser.add_argument(\"--config\", help=\"YAML configuration file\", default=DEFAULT_CONFIG_FILE)\n\t\tparser.add_argument(\"--log-level\", help=\"Logging level\", default=DEFAULT_LOG_LEVEL)\n\t\tsubparsers = parser.add_subparsers(title=\"subcommands\", description=\"valid subcommands\", dest=\"command\")\n\t\tsubparsers.required = True\n\t\tsubparsers.add_parser(\"create\", help=\"create a new project\")\n\t\tsubparsers.add_parser(\"publish\", help=\"publish a project to the web\")\n\t\tsubparsers.add_parser(\"rebuild\", help=\"rebuild a project\")\n\t\tsubparsers.add_parser(\"upgrade\", help=\"upgrade a project\")\n\t\tsubparsers.add_parser(\"run\", help=\"run a project\")\n\t\tsubparsers.add_parser(\"deploy\", help=\"deploy a project\")\n\t\tsubparsers.add_parser(\"init\", help=\"initialize a project\")\n\t\tsubparsers.add_parser(\"serve\", help=\"serve a project\")\n\t\tsubparsers.add_parser(\"build\", help=\"build a project\")\n\t\tsubparsers.add_parser(\"publish-build\", help=\"publish a project\")\n\t\tsubparsers.add_parser(\"test\", help=\"test a project\")\n\t\tsubparsers.add_parser(\"destroy\",", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tclass CustomFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n\t\tpass\n\n\tdefault_argument_parser = argparse.ArgumentParser(description=__doc__, formatter_class=CustomFormatter)\n\tdefault_argument_parser.add_argument(\"--debug\", action=\"store_true\", help=\"Print debug messages.\")\n\tdefault_argument_parser.add_argument(\"--version\", action=\"version\", version=\"1.0\")\n\tdefault_argument_parser.add_argument(\"-v\", \"--verbosity\", type=int, choices=[0, 1, 2, 3], default=1, help=\"Verbosity level. 0 = silent, 1 = only warning, 2 = warning and info, 3 = debug\")\n\n\tsubparsers = default_argument_parser.add_subparsers(title=\"Subcommands\", description=\"Valid subcommands\", help=\"Sub-command help\")\n\n\t#\n\t# subparser for the \"create\" subcommand\n\t#\n\tcreate_subparser = subparsers.add_parser(\"create\", help=\"Create a new file\")\n\tcreate_subparser.add_argument(\"--name\", help=\"Name of the file to be created.\", required=True)\n\tcreate_subparser.add_argument(\"--path\", help=\"Path of the file to be created.\", required=True)\n\tcreate_subparser", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"A script to generate random data for testing purposes.\")\n\t\tsubparsers = parser.add_subparsers(dest=\"command\", help=\"sub-command help\")\n\n\t\tglobal_parser = subparsers.add_parser(\"global\", help=\"Global options\")\n\t\tglobal_parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Turn on verbose output\")\n\t\tglobal_parser.add_argument(\"--debug\", action=\"store_true\", help=\"Turn on debug output\")\n\n\t\tfor subcommand in subcommands:\n\t\t\tsubcommand.add_subparser(subparsers)\n\n\t\treturn parser.parse_args(*unparsed_arguments)\n\n\tdef main():\n\t\targs = parse_arguments(*sys.argv[1:])\n\t\tif args.command == \"global\":\n\t\t\tglobal_options.update(vars(args))\n\t\telse:\n\t\t\tsubcommand_options.update(vars(args))\n\n\t\tif args.debug:\n\t\t\tglobal_options[\"verbose\"] = True\n\n\t\tif global_options[\"verbose\"]:\n\t\t\tprint(\"Global options: {}\".format(global_options))\n\t\t\tprint(\"Sub-command options: {}\".format(subcommand_options))\n\n\t\tsubcommands[args.command](subcommand_options)\n\n\tif __name__ == \"__main__\":\n\t\tmain()\n\n\"\"\"\nThe following is a list", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='A parser for the UW-Madison 2016-2017 FPGA course project')\n\tsubparsers = parser.add_subparsers(dest='command', help='sub-command help')\n\t \n\t# Subparser for creating a new project\n\tparser_create = subparsers.add_parser('create', help='Creates a new project')\n\tparser_create.add_argument('project_name', help='The name of the project')\n\tparser_create.add_argument('-f', '--force', action='store_true', help='Overwrite an existing project')\n\t \n\t# Subparser for adding a new module to a project\n\tparser_add = subparsers.add_parser('add', help='Adds a new module to a project')\n\tparser_add.add_argument('module_name', help='The name of the module to add')\n\tparser_add.add_argument('-p', '--project', help='The name of the project to add the module to')\n\tparser_add.add_argument('-f', '--force', action='store_true', help='Overwrite an existing module')\n\t \n\t# Subparser for adding a new module to a project\n\tparser_remove = subparsers.add_parser('remove', help='Removes a module from a project')\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)\n\t\tparser.add_argument('--version', action='version', version='%(prog)s 0.1')\n\t\tsubparsers = parser.add_subparsers(help='sub-command help', dest='subparser_name')\n\n\t\t# Add subparser for each of the subcommands.\n\t\tsubparsers.add_parser('subcommand1', help='subcommand1 help')\n\t\tsubparsers.add_parser('subcommand2', help='subcommand2 help')\n\n\t\t# Parse the command-line arguments.\n\t\treturn vars(parser.parse_args(args=unparsed_arguments))\n\n\n\nif __name__ == '__main__':\n\targuments = parse_arguments(*sys.argv[1:])\n\tif arguments['subparser_name'] =='subcommand1':\n\t\t# Do something for subcommand1.\n\t\tprint('subcommand1 was called.')\n\telif arguments['subparser_name'] =='subcommand2':\n\t\t# Do something for subcommand2.\n\t\tprint('subcommand2 was called.')\n\telse:\n\t\t# Print help.\n\t\tparser.print_help()\n/04_python_exceptions/01_exception_handling_basics.py\n# -*- coding: utf-8 -*-\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Add all the subparsers to the \"common\" parser so they can share common arguments.\n\t\tcommon_parser = argparse.ArgumentParser(add_help=False)\n\t\tcommon_parser.add_argument(\"--repository\",\n\t\t\t\t\t\t\t\t   help=\"The Git repository to operate on.\",\n\t\t\t\t\t\t\t\t   required=True)\n\t\tfor subparser in subparsers.values():\n\t\t\tsubparser.add_argument_group(common_parser.add_argument_group())\n\n\t\t# Parse the common arguments and the remaining arguments.\n\t\tparsed_common, unparsed_common = common_parser.parse_known_args(unparsed_arguments)\n\t\tparsed_subparsers = {}\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_subparser, unparsed_subparser = subparser.parse_known_args(unparsed_common)\n\t\t\t\tparsed_subparsers[subparser_name] = parsed_subparser\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\n\t\treturn parsed_subparsers, unparsed_subparser\n\n\tdef add_subparser(self, name, description):\n\t\t\"\"\"\n\t\tAdd a subparser with the given name and description.\n\t\t\"\"\"\n\t\tparser = self.add_parser(name, description=description)\n\t\tself.subparsers[name] = parser\n\t\treturn parser\n\n\tdef run(self):", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\t# Parse all arguments given to this script. The first argument is the name of the script that\n\t# invoked it, so we should ignore it.\n\tparsed_arguments = parser.parse_args(unparsed_arguments[1:])\n\n\t# Initialize a dict for storing arguments.\n\targuments = {}\n\n\t# Initialize a list for storing remaining arguments.\n\tremaining_arguments = []\n\n\t# If a subparser was given, it should have already been added to the subparsers list.\n\tif parsed_arguments.subparser_name:\n\t\tif parsed_arguments.subparser_name not in subparsers:\n\t\t\traise ValueError(\"Unknown subparser '%s'\" % parsed_arguments.subparser_name)\n\t\tsubparser = subparsers[parsed_arguments.subparser_name]\n\n\t\t# Try to parse the remaining arguments with the given subparser.\n\t\tsubparser_arguments = subparser.parse_args(unparsed_arguments[2:])\n\n\t\t# Add the parsed subparser arguments to the dict.\n\t\targuments[parsed_arguments.subparser_name] = subparser_arguments\n\n\t\t# Add the remaining arguments to the list.\n\t\tremaining_arguments = unparsed_arguments[len(subparser_arguments.args) + 2:]\n\n\telse:\n\t\t# No subparser was given, so add the remaining arguments to the list.\n\t\tremaining", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"--repository\", help=\"path to the repository\")\n\t\targs, remaining_args = parser.parse_known_args(unparsed_arguments)\n\t\tif not args.repository:\n\t\t\tprint(\"Please specify --repository\")\n\t\t\tsys.exit(1)\n\t\trepository_path = args.repository\n\t\tif not os.path.isdir(repository_path):\n\t\t\tprint(\"Repository path '%s' not found\" % repository_path)\n\t\t\tsys.exit(1)\n\t\tparsed_subparsers = {}\n\t\tfor name, subparser in subparsers.items():\n\t\t\tnamespace, remaining_args = subparser.parse_known_args(remaining_args)\n\t\t\tparsed_subparsers[name] = namespace\n\t\treturn parsed_subparsers, remaining_args\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# TODO: support multiple actions in a single command?\n\tfor subparser, subparser_arguments in subparsers.items():\n\t\t# First, try to parse the subparser's arguments\n\t\tsubparser_arguments.parse_args(unparsed_arguments[:], namespace=argparse.Namespace())\n\t\t# Then remove them from the list of unparsed arguments, so that the remaining arguments are\n\t\t# passed to the main parser\n\t\tunparsed_arguments = unparsed_arguments[subparser_arguments.parse_known_args()[1]:]\n\treturn unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targs = {}\n\t\tremaining = []\n\t\tfor arg in unparsed_arguments:\n\t\t\tif arg == \"--repository\":\n\t\t\t\tremaining.append(arg)\n\t\t\t\tcontinue\n\t\t\tparser = None\n\t\t\tfor name, subparser in subparsers.items():\n\t\t\t\tif arg == name:\n\t\t\t\t\tparser = subparser\n\t\t\t\t\tbreak\n\t\t\tif not parser:\n\t\t\t\tremaining.append(arg)\n\t\t\t\tcontinue\n\t\t\targs[parser.dest] = parser.parse_args(remaining)\n\t\t\tremaining = []\n\t\treturn (args, remaining)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tsubparser_args = {}\n\tremaining_args = []\n\tfor subparser, subparser_parser in subparsers.items():\n\t\tif subparser in unparsed_arguments:\n\t\t\tsubparser_args[subparser] = subparser_parser.parse_args(unparsed_arguments[subparser])\n\t\t\tremaining_args.extend(unparsed_arguments[subparser])\n\t\telse:\n\t\t\tsubparser_args[subparser] = None\n\n\treturn subparser_args, remaining_args\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Create a dict mapping from subparser name to a parsed namespace of arguments.\n\t\targuments = {}\n\t\t# Create a list of remaining arguments not claimed by any subparser.\n\t\tremaining_arguments = []\n\t\t# Iterate over all arguments.\n\t\tfor argument in unparsed_arguments:\n\t\t\t# Get the name of the subparser.\n\t\t\tsubparser_name = argument.split(\"-\")[0]\n\t\t\t# If the subparser name is valid, give it a shot at parsing the arguments.\n\t\t\tif subparser_name in subparsers:\n\t\t\t\t# Get the subparser.\n\t\t\t\tsubparser = subparsers[subparser_name]\n\t\t\t\t# Parse all arguments and store the result.\n\t\t\t\targuments[subparser_name] = subparser.parse_args(argument)\n\t\t\t# Otherwise, just add the argument to the remaining arguments.\n\t\t\telse:\n\t\t\t\tremaining_arguments.append(argument)\n\t\t# Return a tuple containing the arguments and remaining arguments.\n\t\treturn (arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targuments = dict()\n\t\tremaining_arguments = list()\n\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tsubparser_parser = subparsers[subparser_name]\n\t\t\ttry:\n\t\t\t\targuments[subparser_name] = subparser_parser.parse_args(unparsed_arguments)\n\t\t\texcept SystemExit:\n\t\t\t\tcontinue\n\t\t\tunparsed_arguments = subparser_parser.parse_known_args(unparsed_arguments)[1]\n\t\t\tremaining_arguments.extend(unparsed_arguments)\n\n\t\treturn arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Create a parser for each subparser.\n\tsubparser_parsers = {}\n\tfor subparser_name, subparser_parser in subparsers.items():\n\t\tsubparser_parsers[subparser_name] = subparser_parser.add_subparsers(dest=\"action\", required=True)\n\n\t\t# All subparsers share some arguments, including the repository.\n\t\tsubparser_parser.add_argument(\"--repository\", default=None, help=\"The repository to use.\", metavar=\"NAME\")\n\n\t\t# Add arguments specific to each subparser.\n\t\tfor subparser_subparser in subparser_parsers.values():\n\t\t\tsubparser_subparser.add_argument(\"--format\", default=None, help=\"Output format.\", metavar=\"FORMAT\")\n\t\t\tsubparser_subparser.add_argument(\"--no-color\", \"--no-colors\", action=\"store_true\", help=\"Disable colors.\")\n\n\t# Parse all arguments.\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--repository\", default=None, help=\"The repository to use.\", metavar=\"NAME\")\n\tparser.add_argument(\"--format\", default=None, help=\"Output format.\", metavar=\"FORMAT\")\n\tparser.add_argument(\"--no-color\", \"--no-colors\", action=\"store_true\", help=\"Disable colors.\")\n\targuments = parser.parse_known_args(unparsed", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Add all the subparsers to the \"common\" parser so they can share common arguments.\n\t\tcommon_parser = argparse.ArgumentParser(add_help=False)\n\t\tcommon_parser.add_argument(\"--repository\",\n\t\t\t\t\t\t\t\t   help=\"The Git repository to operate on.\",\n\t\t\t\t\t\t\t\t   required=True)\n\t\tfor subparser in subparsers.values():\n\t\t\tsubparser.add_argument_group(common_parser.add_argument_group())\n\n\t\t# Parse the common arguments and the remaining arguments.\n\t\tparsed_common, unparsed_common = common_parser.parse_known_args(unparsed_arguments)\n\t\tparsed_subparsers = {}\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_subparser, unparsed_subparser = subparser.parse_known_args(unparsed_common)\n\t\t\t\tparsed_subparsers[subparser_name] = parsed_subparser\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\n\t\treturn parsed_subparsers, unparsed_subparser\n\n\tdef add_subparser(self, name, description):\n\t\t\"\"\"\n\t\tAdd a subparser with the given name and description.\n\t\t\"\"\"\n\t\tparser = self.add_parser(name, description=description)\n\t\tself.subparsers[name] = parser\n\t\treturn parser\n\n\tdef run(self):", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\t# Parse all arguments given to this script. The first argument is the name of the script that\n\t# invoked it, so we should ignore it.\n\tparsed_arguments = parser.parse_args(unparsed_arguments[1:])\n\n\t# Initialize a dict for storing arguments.\n\targuments = {}\n\n\t# Initialize a list for storing remaining arguments.\n\tremaining_arguments = []\n\n\t# If a subparser was given, it should have already been added to the subparsers list.\n\tif parsed_arguments.subparser_name:\n\t\tif parsed_arguments.subparser_name not in subparsers:\n\t\t\traise ValueError(\"Unknown subparser '%s'\" % parsed_arguments.subparser_name)\n\t\tsubparser = subparsers[parsed_arguments.subparser_name]\n\n\t\t# Try to parse the remaining arguments with the given subparser.\n\t\tsubparser_arguments = subparser.parse_args(unparsed_arguments[2:])\n\n\t\t# Add the parsed subparser arguments to the dict.\n\t\targuments[parsed_arguments.subparser_name] = subparser_arguments\n\n\t\t# Add the remaining arguments to the list.\n\t\tremaining_arguments = unparsed_arguments[len(subparser_arguments.args) + 2:]\n\n\telse:\n\t\t# No subparser was given, so add the remaining arguments to the list.\n\t\tremaining", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"--repository\", help=\"path to the repository\")\n\t\targs, remaining_args = parser.parse_known_args(unparsed_arguments)\n\t\tif not args.repository:\n\t\t\tprint(\"Please specify --repository\")\n\t\t\tsys.exit(1)\n\t\trepository_path = args.repository\n\t\tif not os.path.isdir(repository_path):\n\t\t\tprint(\"Repository path '%s' not found\" % repository_path)\n\t\t\tsys.exit(1)\n\t\tparsed_subparsers = {}\n\t\tfor name, subparser in subparsers.items():\n\t\t\tnamespace, remaining_args = subparser.parse_known_args(remaining_args)\n\t\t\tparsed_subparsers[name] = namespace\n\t\treturn parsed_subparsers, remaining_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# TODO: support multiple actions in a single command?\n\tfor subparser, subparser_arguments in subparsers.items():\n\t\t# First, try to parse the subparser's arguments\n\t\tsubparser_arguments.parse_args(unparsed_arguments[:], namespace=argparse.Namespace())\n\t\t# Then remove them from the list of unparsed arguments, so that the remaining arguments are\n\t\t# passed to the main parser\n\t\tunparsed_arguments = unparsed_arguments[subparser_arguments.parse_known_args()[1]:]\n\treturn unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targs = {}\n\t\tremaining = []\n\t\tfor arg in unparsed_arguments:\n\t\t\tif arg == \"--repository\":\n\t\t\t\tremaining.append(arg)\n\t\t\t\tcontinue\n\t\t\tparser = None\n\t\t\tfor name, subparser in subparsers.items():\n\t\t\t\tif arg == name:\n\t\t\t\t\tparser = subparser\n\t\t\t\t\tbreak\n\t\t\tif not parser:\n\t\t\t\tremaining.append(arg)\n\t\t\t\tcontinue\n\t\t\targs[parser.dest] = parser.parse_args(remaining)\n\t\t\tremaining = []\n\t\treturn (args, remaining)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tsubparser_args = {}\n\tremaining_args = []\n\tfor subparser, subparser_parser in subparsers.items():\n\t\tif subparser in unparsed_arguments:\n\t\t\tsubparser_args[subparser] = subparser_parser.parse_args(unparsed_arguments[subparser])\n\t\t\tremaining_args.extend(unparsed_arguments[subparser])\n\t\telse:\n\t\t\tsubparser_args[subparser] = None\n\n\treturn subparser_args, remaining_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Create a dict mapping from subparser name to a parsed namespace of arguments.\n\t\targuments = {}\n\t\t# Create a list of remaining arguments not claimed by any subparser.\n\t\tremaining_arguments = []\n\t\t# Iterate over all arguments.\n\t\tfor argument in unparsed_arguments:\n\t\t\t# Get the name of the subparser.\n\t\t\tsubparser_name = argument.split(\"-\")[0]\n\t\t\t# If the subparser name is valid, give it a shot at parsing the arguments.\n\t\t\tif subparser_name in subparsers:\n\t\t\t\t# Get the subparser.\n\t\t\t\tsubparser = subparsers[subparser_name]\n\t\t\t\t# Parse all arguments and store the result.\n\t\t\t\targuments[subparser_name] = subparser.parse_args(argument)\n\t\t\t# Otherwise, just add the argument to the remaining arguments.\n\t\t\telse:\n\t\t\t\tremaining_arguments.append(argument)\n\t\t# Return a tuple containing the arguments and remaining arguments.\n\t\treturn (arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\targuments = dict()\n\t\tremaining_arguments = list()\n\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tsubparser_parser = subparsers[subparser_name]\n\t\t\ttry:\n\t\t\t\targuments[subparser_name] = subparser_parser.parse_args(unparsed_arguments)\n\t\t\texcept SystemExit:\n\t\t\t\tcontinue\n\t\t\tunparsed_arguments = subparser_parser.parse_known_args(unparsed_arguments)[1]\n\t\t\tremaining_arguments.extend(unparsed_arguments)\n\n\t\treturn arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t# Create a parser for each subparser.\n\tsubparser_parsers = {}\n\tfor subparser_name, subparser_parser in subparsers.items():\n\t\tsubparser_parsers[subparser_name] = subparser_parser.add_subparsers(dest=\"action\", required=True)\n\n\t\t# All subparsers share some arguments, including the repository.\n\t\tsubparser_parser.add_argument(\"--repository\", default=None, help=\"The repository to use.\", metavar=\"NAME\")\n\n\t\t# Add arguments specific to each subparser.\n\t\tfor subparser_subparser in subparser_parsers.values():\n\t\t\tsubparser_subparser.add_argument(\"--format\", default=None, help=\"Output format.\", metavar=\"FORMAT\")\n\t\t\tsubparser_subparser.add_argument(\"--no-color\", \"--no-colors\", action=\"store_true\", help=\"Disable colors.\")\n\n\t# Parse all arguments.\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--repository\", default=None, help=\"The repository to use.\", metavar=\"NAME\")\n\tparser.add_argument(\"--format\", default=None, help=\"Output format.\", metavar=\"FORMAT\")\n\tparser.add_argument(\"--no-color\", \"--no-colors\", action=\"store_true\", help=\"Disable colors.\")\n\targuments = parser.parse_known_args(unparsed", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='A program to generate and check solutions for the \"Challenging Code\" problems from the \"Python for Everybody\" book by.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# Make subparser for \"generate\" command\n\tparser_generate = subparsers.add_parser('generate', help='generate help')\n\tparser_generate.add_argument('problem', metavar='problem', type=int, help='generate a solution for the problem number')\n\tparser_generate.add_argument('file', metavar='file', type=str, help='output file name')\n\t\n\t# Make subparser for \"check\" command\n\tparser_check = subparsers.add_parser('check', help='check help')\n\tparser_check.add_argument('problem', metavar='problem', type=int, help='check a solution for the problem number')\n\tparser_check.add_argument('file', metavar='file', type=str, help='solution file name')\n\t\n\treturn (parser, subparsers)\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Get a list of the files in a directory, or get a list of files that contain a specified string.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# Create the parser for the \"list\" command\n\tlist_parser = subparsers.add_parser('list', help='list help')\n\tlist_parser.add_argument('-d', '--directory', help='directory to search', required=True)\n\tlist_parser.add_argument('-s', '--string', help='string to search for')\n\tlist_parser.add_argument('-r', '--recursive', help='search recursively', action='store_true')\n\tlist_parser.set_defaults(which='list')\n\n\t# Create the parser for the \"grep\" command\n\tgrep_parser = subparsers.add_parser('grep', help='grep help')\n\tgrep_parser.add_argument('-d', '--directory', help='directory to search', required=True)\n\tgrep_parser.add_argument('-s', '--string', help='string to search for', required=True)\n\tgrep_parser.add_argument('-r', '--recursive', help='search recursively', action='store_true')\n\tgrep_parser.set_defaults(which='grep')\n\n\treturn parser, subparsers", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Run all the things.',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparser_list = subparsers.add_parser('list', help='List all the things.')\n\tparser_list.set_defaults(func=list_things)\n\n\tparser_find = subparsers.add_parser('find', help='Find the things.')\n\tparser_find.add_argument('search', help='Search for things.')\n\tparser_find.set_defaults(func=find_things)\n\n\tparser_add = subparsers.add_parser('add', help='Add the things.')\n\tparser_add.add_argument('new_thing', help='The thing to add.')\n\tparser_add.set_defaults(func=add_thing)\n\n\tparser_remove = subparsers.add_parser('remove', help='Remove the things.')\n\tparser_remove.add_argument('thing', help='The thing to remove.')\n\tparser_remove.set_defaults(func=remove_thing)\n\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\tmain_parser = argparse.ArgumentParser()\n#\tsubparsers = main_parser.add_subparsers()\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Create subparsers for each command.\n\tfor cmd in COMMANDS.keys():\n\t\tsp = subparsers.add_parser(cmd)\n\t\tsp.add_argument(\"cmd\", help=\"The command to run\", default=cmd, type=str, nargs=\"*\")\n\t\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t # Create the top-level parser\n\ttop_level_parser = argparse.ArgumentParser(prog=\"vigra\")\n\tsubparsers = top_level_parser.add_subparsers(help=\"sub-command help\")\n\n\t# Add the subparsers\n\tfor parser_name, parser_func in COMMAND_PARSERS.items():\n\t\tparser = subparsers.add_parser(parser_name, help=parser_func.help)\n\t\tparser.set_defaults(func=parser_func)\n\t\tparser_func(parser)\n\n\treturn top_level_parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\ttop = argparse.ArgumentParser(prog=sys.argv[0], description=app.description)\n\ttop = argparse.ArgumentParser(prog=sys.argv[0], description=app.description, add_help=False)\n\ttop.add_argument(\"-h\", \"--help\", action=\"store_true\", help=\"Show help message and exit\")\n\n\tsubparsers = top.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\t# Create the top-level parser for the 'init' command.\n\tinit_parser = subparsers.add_parser(\"init\", help=commands.init.help)\n\tinit_parser.set_defaults(func=commands.init.run)\n\t# Add any additional arguments specific to the 'init' command.\n\tinit_parser.add_argument(\"project_name\", help=\"The name of the project to initialize\")\n\tinit_parser.add_argument(\"project_description\", help=\"Description of the project\")\n\tinit_parser.add_argument(\"-p\", \"--project-directory\", help=\"The directory where the project is created\")\n\n\t# Create the top-level parser for the 'create' command.\n\tcreate_parser = subparsers.add_parser(\"create\", help=commands.create.help)\n\tcreate_parser.set_defaults(func=commands.create", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a command with a given interval and/or number of times.\")\n\t\n\t# Create the subparsers\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subcommand\")\n\t\n\t# Create the subparser for the \"interval\" subcommand\n\tinterval_parser = subparsers.add_parser(\n\t\t\"interval\",\n\t\thelp=\"run a command with a given interval\")\n\t\n\t# Create the subparser for the \"times\" subcommand\n\ttimes_parser = subparsers.add_parser(\n\t\t\"times\",\n\t\thelp=\"run a command with a given number of times\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall_parser = subparsers.add_parser(\n\t\t\"all\",\n\t\thelp=\"run a command with a given interval and number of times\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall_parser.add_argument(\n\t\t\"command\",\n\t\thelp=\"command to run\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall_parser.add_argument(\n\t\t\"interval\",\n\t\ttype=int,\n\t\thelp=\"interval between executions\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis is a utility to edit and print out the configuration files for\n\tthe Python package, Pillow. This is the command-line interface to the\n\tconfig module.\n\t\"\"\")\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\",\n\t\tdescription=\"valid subcommands\",\n\t\tdest=\"subcommand\")\n\n\tsubparsers.required = True\n\n\tparser_print = subparsers.add_parser(\"print\",\n\t\thelp=\"print the configuration data\")\n\tparser_print.add_argument(\"section\",\n\t\tnargs=\"?\",\n\t\thelp=\"the configuration section to print\")\n\tparser_print.add_argument(\"-f\", \"--format\",\n\t\tchoices=(\"ini\", \"json\", \"py\"),\n\t\thelp=\"the output format to use\",\n\t\tdefault=\"ini\")\n\n\tparser_set = subparsers.add_parser(\"set\",\n\t\thelp=\"set a configuration value\")\n\tparser_set.add_argument(\"section\",\n\t\tnargs=\"+\",\n\t\thelp=\"the configuration section to set\")\n\tparser_set.add_argument(\"value\",\n\t\thelp=\"the value to set\")\n\n\tparser_edit = subparsers.add_parser(\"edit\",\n\t\thelp=\"edit the configuration file\")\n\tparser_edit.add_argument(\"--editor\",\n\t\thelp", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis script is used to create a new configuration file, which can be used\n\tto run the DCSimulations.py script.\n\t\"\"\")\n\n\tsubparsers = parser.add_subparsers(help='commands')\n\n\tparser_config = subparsers.add_parser('config', help='create a new configuration file')\n\tparser_config.add_argument('configname', help='name of the configuration file to be created')\n\n\tparser_config.set_defaults(func=make_config)\n\n\tparser_parse = subparsers.add_parser('parse', help='parse a configuration file')\n\tparser_parse.add_argument('configname', help='name of the configuration file to be parsed')\n\tparser_parse.add_argument('outfile', help='name of the output file')\n\n\tparser_parse.set_defaults(func=parse_config)\n\n\tparser_run = subparsers.add_parser('run', help='run a configuration file')\n\tparser_run.add_argument('configname', help='name of the configuration file to be run')\n\n\tparser_run.set_defaults(func=run_config)\n\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\ttop_parser = argparse.ArgumentParser(\n\t\tprog = 'paws',\n\t\tdescription = 'A web browser with a twist',\n\t\tepilog = 'Use -h or --help with a subcommand for more information.',\n\t\tadd_help = False)\n\ttop_parser.add_argument(\n\t\t'-h',\n\t\t'--help',\n\t\taction = 'help',\n\t\thelp = 'Show this help message and exit')\n\ttop_parser.add_argument(\n\t\t'command',\n\t\tnargs = '?',\n\t\thelp = 'The subcommand to run (or nothing to start the browser)')\n\t \n\tsubparsers = top_parser.add_subparsers(\n\t\tdest = 'command',\n\t\ttitle ='subcommands',\n\t\tdescription = 'Available subcommands')\n\t \n\tbrowser_parser = subparsers.add_parser(\n\t\t'browser',\n\t\thelp = 'Start the web browser')\n\tbrowser_parser.add_argument(\n\t\t'url',\n\t\thelp = 'The URL to open (if not specified, open a new tab)',\n\t\tnargs = '?',\n\t\tdefault = '')\n\t \n\tsettings_parser = subparsers.add_parser(\n\t\t'settings',\n\t\thelp = 'Set or view browser settings')\n\tsettings_parser.add_argument(\n\t\t'key',\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='A program to generate and check solutions for the \"Challenging Code\" problems from the \"Python for Everybody\" book by.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# Make subparser for \"generate\" command\n\tparser_generate = subparsers.add_parser('generate', help='generate help')\n\tparser_generate.add_argument('problem', metavar='problem', type=int, help='generate a solution for the problem number')\n\tparser_generate.add_argument('file', metavar='file', type=str, help='output file name')\n\t\n\t# Make subparser for \"check\" command\n\tparser_check = subparsers.add_parser('check', help='check help')\n\tparser_check.add_argument('problem', metavar='problem', type=int, help='check a solution for the problem number')\n\tparser_check.add_argument('file', metavar='file', type=str, help='solution file name')\n\t\n\treturn (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser(description='Get a list of the files in a directory, or get a list of files that contain a specified string.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\t# Create the parser for the \"list\" command\n\tlist_parser = subparsers.add_parser('list', help='list help')\n\tlist_parser.add_argument('-d', '--directory', help='directory to search', required=True)\n\tlist_parser.add_argument('-s', '--string', help='string to search for')\n\tlist_parser.add_argument('-r', '--recursive', help='search recursively', action='store_true')\n\tlist_parser.set_defaults(which='list')\n\n\t# Create the parser for the \"grep\" command\n\tgrep_parser = subparsers.add_parser('grep', help='grep help')\n\tgrep_parser.add_argument('-d', '--directory', help='directory to search', required=True)\n\tgrep_parser.add_argument('-s', '--string', help='string to search for', required=True)\n\tgrep_parser.add_argument('-r', '--recursive', help='search recursively', action='store_true')\n\tgrep_parser.set_defaults(which='grep')\n\n\treturn parser, subparsers", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Run all the things.',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparser_list = subparsers.add_parser('list', help='List all the things.')\n\tparser_list.set_defaults(func=list_things)\n\n\tparser_find = subparsers.add_parser('find', help='Find the things.')\n\tparser_find.add_argument('search', help='Search for things.')\n\tparser_find.set_defaults(func=find_things)\n\n\tparser_add = subparsers.add_parser('add', help='Add the things.')\n\tparser_add.add_argument('new_thing', help='The thing to add.')\n\tparser_add.set_defaults(func=add_thing)\n\n\tparser_remove = subparsers.add_parser('remove', help='Remove the things.')\n\tparser_remove.add_argument('thing', help='The thing to remove.')\n\tparser_remove.set_defaults(func=remove_thing)\n\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\tmain_parser = argparse.ArgumentParser()\n#\tsubparsers = main_parser.add_subparsers()\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Create subparsers for each command.\n\tfor cmd in COMMANDS.keys():\n\t\tsp = subparsers.add_parser(cmd)\n\t\tsp.add_argument(\"cmd\", help=\"The command to run\", default=cmd, type=str, nargs=\"*\")\n\t\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t # Create the top-level parser\n\ttop_level_parser = argparse.ArgumentParser(prog=\"vigra\")\n\tsubparsers = top_level_parser.add_subparsers(help=\"sub-command help\")\n\n\t# Add the subparsers\n\tfor parser_name, parser_func in COMMAND_PARSERS.items():\n\t\tparser = subparsers.add_parser(parser_name, help=parser_func.help)\n\t\tparser.set_defaults(func=parser_func)\n\t\tparser_func(parser)\n\n\treturn top_level_parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\ttop = argparse.ArgumentParser(prog=sys.argv[0], description=app.description)\n\ttop = argparse.ArgumentParser(prog=sys.argv[0], description=app.description, add_help=False)\n\ttop.add_argument(\"-h\", \"--help\", action=\"store_true\", help=\"Show help message and exit\")\n\n\tsubparsers = top.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\t# Create the top-level parser for the 'init' command.\n\tinit_parser = subparsers.add_parser(\"init\", help=commands.init.help)\n\tinit_parser.set_defaults(func=commands.init.run)\n\t# Add any additional arguments specific to the 'init' command.\n\tinit_parser.add_argument(\"project_name\", help=\"The name of the project to initialize\")\n\tinit_parser.add_argument(\"project_description\", help=\"Description of the project\")\n\tinit_parser.add_argument(\"-p\", \"--project-directory\", help=\"The directory where the project is created\")\n\n\t# Create the top-level parser for the 'create' command.\n\tcreate_parser = subparsers.add_parser(\"create\", help=commands.create.help)\n\tcreate_parser.set_defaults(func=commands.create", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a command with a given interval and/or number of times.\")\n\t\n\t# Create the subparsers\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subcommand\")\n\t\n\t# Create the subparser for the \"interval\" subcommand\n\tinterval_parser = subparsers.add_parser(\n\t\t\"interval\",\n\t\thelp=\"run a command with a given interval\")\n\t\n\t# Create the subparser for the \"times\" subcommand\n\ttimes_parser = subparsers.add_parser(\n\t\t\"times\",\n\t\thelp=\"run a command with a given number of times\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall_parser = subparsers.add_parser(\n\t\t\"all\",\n\t\thelp=\"run a command with a given interval and number of times\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall_parser.add_argument(\n\t\t\"command\",\n\t\thelp=\"command to run\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall_parser.add_argument(\n\t\t\"interval\",\n\t\ttype=int,\n\t\thelp=\"interval between executions\")\n\t\n\t# Create the subparser for the \"all\" subcommand\n\tall", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis is a utility to edit and print out the configuration files for\n\tthe Python package, Pillow. This is the command-line interface to the\n\tconfig module.\n\t\"\"\")\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\",\n\t\tdescription=\"valid subcommands\",\n\t\tdest=\"subcommand\")\n\n\tsubparsers.required = True\n\n\tparser_print = subparsers.add_parser(\"print\",\n\t\thelp=\"print the configuration data\")\n\tparser_print.add_argument(\"section\",\n\t\tnargs=\"?\",\n\t\thelp=\"the configuration section to print\")\n\tparser_print.add_argument(\"-f\", \"--format\",\n\t\tchoices=(\"ini\", \"json\", \"py\"),\n\t\thelp=\"the output format to use\",\n\t\tdefault=\"ini\")\n\n\tparser_set = subparsers.add_parser(\"set\",\n\t\thelp=\"set a configuration value\")\n\tparser_set.add_argument(\"section\",\n\t\tnargs=\"+\",\n\t\thelp=\"the configuration section to set\")\n\tparser_set.add_argument(\"value\",\n\t\thelp=\"the value to set\")\n\n\tparser_edit = subparsers.add_parser(\"edit\",\n\t\thelp=\"edit the configuration file\")\n\tparser_edit.add_argument(\"--editor\",\n\t\thelp", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis script is used to create a new configuration file, which can be used\n\tto run the DCSimulations.py script.\n\t\"\"\")\n\n\tsubparsers = parser.add_subparsers(help='commands')\n\n\tparser_config = subparsers.add_parser('config', help='create a new configuration file')\n\tparser_config.add_argument('configname', help='name of the configuration file to be created')\n\n\tparser_config.set_defaults(func=make_config)\n\n\tparser_parse = subparsers.add_parser('parse', help='parse a configuration file')\n\tparser_parse.add_argument('configname', help='name of the configuration file to be parsed')\n\tparser_parse.add_argument('outfile', help='name of the output file')\n\n\tparser_parse.set_defaults(func=parse_config)\n\n\tparser_run = subparsers.add_parser('run', help='run a configuration file')\n\tparser_run.add_argument('configname', help='name of the configuration file to be run')\n\n\tparser_run.set_defaults(func=run_config)\n\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t \n\ttop_parser = argparse.ArgumentParser(\n\t\tprog = 'paws',\n\t\tdescription = 'A web browser with a twist',\n\t\tepilog = 'Use -h or --help with a subcommand for more information.',\n\t\tadd_help = False)\n\ttop_parser.add_argument(\n\t\t'-h',\n\t\t'--help',\n\t\taction = 'help',\n\t\thelp = 'Show this help message and exit')\n\ttop_parser.add_argument(\n\t\t'command',\n\t\tnargs = '?',\n\t\thelp = 'The subcommand to run (or nothing to start the browser)')\n\t \n\tsubparsers = top_parser.add_subparsers(\n\t\tdest = 'command',\n\t\ttitle ='subcommands',\n\t\tdescription = 'Available subcommands')\n\t \n\tbrowser_parser = subparsers.add_parser(\n\t\t'browser',\n\t\thelp = 'Start the web browser')\n\tbrowser_parser.add_argument(\n\t\t'url',\n\t\thelp = 'The URL to open (if not specified, open a new tab)',\n\t\tnargs = '?',\n\t\tdefault = '')\n\t \n\tsettings_parser = subparsers.add_parser(\n\t\t'settings',\n\t\thelp = 'Set or view browser settings')\n\tsettings_parser.add_argument(\n\t\t'key',\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infoblox_client/utils.py", "project": "infobloxopen/infoblox-client", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\t# If the list is empty\n\tif len(response) == 0:\n\t\treturn []\n\t\t# If the length of the list is less than max_results\n\telif len(response) < max_results:\n\t\treturn response\n\t\t# If the length of the list is greater than max_results\n\telse:\n\t\t# Get the number of pages to be split\n\t\tnum_pages = len(response) // max_results\n\t\t# If the number of pages is 1\n\t\tif num_pages == 1:\n\t\t\t# Return the list\n\t\t\treturn response\n\t\telse:\n\t\t\t# Split the list into pages\n\t\t\tfor i in range(num_pages):\n\t\t\t\tyield response[i * max_results: (i + 1) * max_results]\n/utils/time_utils.py\nfrom datetime import datetime, timedelta\nfrom os import environ\n\nimport pytz\n\nfrom utils import logging_utils\n\nlogger = logging_utils.get_logger(__name__)\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\t# Check length of response\n\twhile len(response) > max_results:\n\t\t# If the length of response is higher than the maximum number of objects in one page,\n\t\t# return the first max_results objects from the response and call the function again with\n\t\t# the remaining objects as a response.\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\t# If the length of the response is lower than the maximum number of objects in one page,\n\t# return the response.\n\tif len(response) <= max_results:\n\t\tyield response\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\n\tif len(response) == max_results:\n\t\tyield response[:max_results]\n\t\tfor response_chunk in paging(response[max_results:], max_results):\n\t\t\tyield response_chunk\n\telse:\n\t\tyield response\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n        for i in range(0, len(response), max_results):\n            yield response[i:i+max_results]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t \n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i+max_results]\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\t# Make sure that the maximum number of results is not exceeded\n\tif max_results > len(response):\n\t\tmax_results = len(response)\n\t# Make sure that the maximum number of results is not less than 0\n\tif max_results <= 0:\n\t\tmax_results = 1\n\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\t# If the list is empty\n\tif len(response) == 0:\n\t\treturn []\n\t\t# If the length of the list is less than max_results\n\telif len(response) < max_results:\n\t\treturn response\n\t\t# If the length of the list is greater than max_results\n\telse:\n\t\t# Get the number of pages to be split\n\t\tnum_pages = len(response) // max_results\n\t\t# If the number of pages is 1\n\t\tif num_pages == 1:\n\t\t\t# Return the list\n\t\t\treturn response\n\t\telse:\n\t\t\t# Split the list into pages\n\t\t\tfor i in range(num_pages):\n\t\t\t\tyield response[i * max_results: (i + 1) * max_results]\n/utils/time_utils.py\nfrom datetime import datetime, timedelta\nfrom os import environ\n\nimport pytz\n\nfrom utils import logging_utils\n\nlogger = logging_utils.get_logger(__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\t# Check length of response\n\twhile len(response) > max_results:\n\t\t# If the length of response is higher than the maximum number of objects in one page,\n\t\t# return the first max_results objects from the response and call the function again with\n\t\t# the remaining objects as a response.\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\t# If the length of the response is lower than the maximum number of objects in one page,\n\t# return the response.\n\tif len(response) <= max_results:\n\t\tyield response\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\n\tif len(response) == max_results:\n\t\tyield response[:max_results]\n\t\tfor response_chunk in paging(response[max_results:], max_results):\n\t\t\tyield response_chunk\n\telse:\n\t\tyield response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n        for i in range(0, len(response), max_results):\n            yield response[i:i+max_results]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t \n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i+max_results]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\t# Make sure that the maximum number of results is not exceeded\n\tif max_results > len(response):\n\t\tmax_results = len(response)\n\t# Make sure that the maximum number of results is not less than 0\n\tif max_results <= 0:\n\t\tmax_results = 1\n\n\tfor i in range(0, len(response), max_results):\n\t\tyield response[i:i + max_results]\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\t# Convert to bytes\n\tsize_num = int(size[:-1])\n\tmultiplier = {\"K\": 1024, \"M\": 1024 * 1024, \"G\": 1024 * 1024 * 1024}\n\treturn size_num * multiplier[size[-1]]\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t \n\tunits = {'': 1, 'K': 1024, 'M': 1024**2, 'G': 1024**3}\n\tmatch = re.match(r\"(\\d+\\.?\\d*)([KM]?)\", size)\n\tif match:\n\t\treturn int(float(match.group(1)) * units[match.group(2).lower()])\n\telse:\n\t\treturn 0\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t \n\tif size[-1] == 'K':\n\t\treturn int(float(size[:-1]) * 1024)\n\telif size[-1] == 'M':\n\t\treturn int(float(size[:-1]) * 1024 * 1024)\n\telif size[-1] == 'G':\n\t\treturn int(float(size[:-1]) * 1024 * 1024 * 1024)\n\telse:\n\t\traise ValueError('Invalid file size')\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\tsize_dict = {\n\t    \"K\": 1024,\n\t    \"M\": 1024 * 1024,\n\t    \"G\": 1024 * 1024 * 1024,\n\t}\n\n\tsize = size.strip().upper()\n\tvalue = int(size[:-1])\n\tsuffix = size[-1]\n\tif suffix not in size_dict:\n\t    raise ValueError(f\"{size} is not a valid size\")\n\treturn value * size_dict[suffix]\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\t# Get rid of whitespace\n\tsize = size.strip()\n\n\t\t# Get rid of units\n\tsize, units = size[:-1], size[-1]\n\n\t\t# Do the conversion\n\tif units in 'KMGT':\n\t\tsize = float(size) * 1024 ** ('KMGT'.index(units) + 1)\n\telif units == 'k':\n\t\tsize *= 1024\n\telif units =='m':\n\t\tsize *= 1024 ** 2\n\telif units == 'g':\n\t\tsize *= 1024 ** 3\n\telif units == 't':\n\t\tsize *= 1024 ** 4\n\telse:\n\t\traise ValueError(f\"unknown unit: {units}\")\n\n\treturn int(size)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n        size = size.replace('G', '*10**9')\n        size = size.replace('M', '*10**6')\n        size = size.replace('K', '*10**3')\n        size = size.replace('T', '*10**12')\n        size = size.replace('B', '*10**0')\n        size = size.replace('KB', '*10**3')\n        size = size.replace('MB', '*10**6')\n        size = size.replace('GB', '*10**9')\n        size = size.replace('TB', '*10**12')\n\n        size = size.replace('*', '*float(')\n\n        return int(eval(size))\n/README.md\n# Pillow\n\nPillow is a simple python tool for converting images from one format to another.\n\n## Installation\n\n```bash\npip install pillow\n```\n\n## Usage\n\n```python\nfrom pillow import Pillow\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\t# Parse the unit\n\tunit = size[-1]\n\t# Convert to bytes\n\tif unit == 'K':\n\t\treturn int(size[:-1]) * 1024\n\telif unit == 'M':\n\t\treturn int(size[:-1]) * 1024 * 1024\n\telif unit == 'G':\n\t\treturn int(size[:-1]) * 1024 * 1024 * 1024\n\telse:\n\t\traise ValueError('Unknown unit: {}'.format(unit))\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\tif size == \"\":\n\t\treturn 0\n\n\tunits = {\"B\": 1, \"K\": 1024, \"M\": 1024 ** 2, \"G\": 1024 ** 3, \"T\": 1024 ** 4, \"P\": 1024 ** 5}\n\n\tunit = size[-1:]\n\tif unit not in units:\n\t\traise ValueError(f\"Invalid unit {unit}\")\n\n\treturn int(float(size[:-1]) * units[unit])\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\ttry:\n\t\tbytes = 0\n\t\tfor s in size:\n\t\t\tif s.isdigit():\n\t\t\t\tbytes = bytes * 10 + ord(s) - ord('0')\n\t\t\telif s == 'M':\n\t\t\t\tbytes *= 1024 * 1024\n\t\t\telif s == 'K':\n\t\t\t\tbytes *= 1024\n\t\t\telif s == 'B':\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\traise ValueError(f'Invalid file size: {size}')\n\t\treturn bytes\n\texcept Exception as e:\n\t\tprint(Fore.RED + f'Error: {e}' + Style.RESET_ALL)\n\t\texit(1)\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n        unit = size[-1]\n        if unit == 'K':\n                return int(float(size[:-1]) * 1024)\n        elif unit == 'M':\n                return int(float(size[:-1]) * 1024 * 1024)\n        elif unit == 'G':\n                return int(float(size[:-1]) * 1024 * 1024 * 1024)\n        elif unit == 'T':\n                return int(float(size[:-1]) * 1024 * 1024 * 1024 * 1024)\n        else:\n                return int(size)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\t# Convert to bytes\n\tsize_num = int(size[:-1])\n\tmultiplier = {\"K\": 1024, \"M\": 1024 * 1024, \"G\": 1024 * 1024 * 1024}\n\treturn size_num * multiplier[size[-1]]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t \n\tunits = {'': 1, 'K': 1024, 'M': 1024**2, 'G': 1024**3}\n\tmatch = re.match(r\"(\\d+\\.?\\d*)([KM]?)\", size)\n\tif match:\n\t\treturn int(float(match.group(1)) * units[match.group(2).lower()])\n\telse:\n\t\treturn 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t \n\tif size[-1] == 'K':\n\t\treturn int(float(size[:-1]) * 1024)\n\telif size[-1] == 'M':\n\t\treturn int(float(size[:-1]) * 1024 * 1024)\n\telif size[-1] == 'G':\n\t\treturn int(float(size[:-1]) * 1024 * 1024 * 1024)\n\telse:\n\t\traise ValueError('Invalid file size')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\tsize_dict = {\n\t    \"K\": 1024,\n\t    \"M\": 1024 * 1024,\n\t    \"G\": 1024 * 1024 * 1024,\n\t}\n\n\tsize = size.strip().upper()\n\tvalue = int(size[:-1])\n\tsuffix = size[-1]\n\tif suffix not in size_dict:\n\t    raise ValueError(f\"{size} is not a valid size\")\n\treturn value * size_dict[suffix]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\t# Get rid of whitespace\n\tsize = size.strip()\n\n\t\t# Get rid of units\n\tsize, units = size[:-1], size[-1]\n\n\t\t# Do the conversion\n\tif units in 'KMGT':\n\t\tsize = float(size) * 1024 ** ('KMGT'.index(units) + 1)\n\telif units == 'k':\n\t\tsize *= 1024\n\telif units =='m':\n\t\tsize *= 1024 ** 2\n\telif units == 'g':\n\t\tsize *= 1024 ** 3\n\telif units == 't':\n\t\tsize *= 1024 ** 4\n\telse:\n\t\traise ValueError(f\"unknown unit: {units}\")\n\n\treturn int(size)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n        size = size.replace('G', '*10**9')\n        size = size.replace('M', '*10**6')\n        size = size.replace('K', '*10**3')\n        size = size.replace('T', '*10**12')\n        size = size.replace('B', '*10**0')\n        size = size.replace('KB', '*10**3')\n        size = size.replace('MB', '*10**6')\n        size = size.replace('GB', '*10**9')\n        size = size.replace('TB', '*10**12')\n\n        size = size.replace('*', '*float(')\n\n        return int(eval(size))\n/README.md\n# Pillow\n\nPillow is a simple python tool for converting images from one format to another.\n\n## Installation\n\n```bash\npip install pillow\n```\n\n## Usage\n\n```python\nfrom pillow import Pillow\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\t# Parse the unit\n\tunit = size[-1]\n\t# Convert to bytes\n\tif unit == 'K':\n\t\treturn int(size[:-1]) * 1024\n\telif unit == 'M':\n\t\treturn int(size[:-1]) * 1024 * 1024\n\telif unit == 'G':\n\t\treturn int(size[:-1]) * 1024 * 1024 * 1024\n\telse:\n\t\traise ValueError('Unknown unit: {}'.format(unit))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\tif size == \"\":\n\t\treturn 0\n\n\tunits = {\"B\": 1, \"K\": 1024, \"M\": 1024 ** 2, \"G\": 1024 ** 3, \"T\": 1024 ** 4, \"P\": 1024 ** 5}\n\n\tunit = size[-1:]\n\tif unit not in units:\n\t\traise ValueError(f\"Invalid unit {unit}\")\n\n\treturn int(float(size[:-1]) * units[unit])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n\n\ttry:\n\t\tbytes = 0\n\t\tfor s in size:\n\t\t\tif s.isdigit():\n\t\t\t\tbytes = bytes * 10 + ord(s) - ord('0')\n\t\t\telif s == 'M':\n\t\t\t\tbytes *= 1024 * 1024\n\t\t\telif s == 'K':\n\t\t\t\tbytes *= 1024\n\t\t\telif s == 'B':\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\traise ValueError(f'Invalid file size: {size}')\n\t\treturn bytes\n\texcept Exception as e:\n\t\tprint(Fore.RED + f'Error: {e}' + Style.RESET_ALL)\n\t\texit(1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\t\n        unit = size[-1]\n        if unit == 'K':\n                return int(float(size[:-1]) * 1024)\n        elif unit == 'M':\n                return int(float(size[:-1]) * 1024 * 1024)\n        elif unit == 'G':\n                return int(float(size[:-1]) * 1024 * 1024 * 1024)\n        elif unit == 'T':\n                return int(float(size[:-1]) * 1024 * 1024 * 1024 * 1024)\n        else:\n                return int(size)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "contrib/planb-swiftsync.py", "project": "ossobv/planb", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\tif len(dicts) == 0:\n\t\t\treturn dict()\n\t\tif len(dicts) == 1:\n\t\t\treturn dicts[0]\n\t\tif len(dicts) == 2:\n\t\t\treturn dict(dicts[0], **dicts[1])\n\t\treturn _dictsum([dicts[0], _dictsum(dicts[1:])])\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\treturn dict(chain(*[d.items() for d in dicts]))\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\ttotal = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k not in total:\n\t\t\t\ttotal[k] = d[k]\n\t\t\telse:\n\t\t\t\ttotal[k] += d[k]\n\treturn total\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tresult = dict()\n\tfor dic in dicts:\n\t\tfor key in dic:\n\t\t\tif key in result:\n\t\t\t\tresult[key] += dic[key]\n\t\t\telse:\n\t\t\t\tresult[key] = dic[key]\n\treturn result\n\t", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tresult = {}\n\tfor d in dicts:\n\t\tfor key in d:\n\t\t\tif key in result:\n\t\t\t\tresult[key] += d[key]\n\t\t\telse:\n\t\t\t\tresult[key] = d[key]\n\treturn result\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\n\n\tdictsum={}\n\tfor dict in dicts:\n\t\tfor key in dict:\n\t\t\tif key in dictsum:\n\t\t\t\tdictsum[key] += dict[key] \n\t\t\telse:\n\t\t\t\tdictsum[key] = dict[key]\n\treturn dictsum\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\t# dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n\t#return a dictionary: dic:{'a': 6, 'b': 2}\n\tresult={}\n\tfor dict in dicts:\n\t\tfor key, value in dict.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key]=result[key]+value\n\t\t\telse:\n\t\t\t\tresult[key]=value\n\treturn result", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in result:\n\t\t\t\tresult[k] += v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\treturn result\n\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tdicts = [d for d in dicts if d is not None]\n\tif len(dicts) == 0:\n\t\treturn None\n\telif len(dicts) == 1:\n\t\treturn dicts[0]\n\telse:\n\t\td1 = dicts[0]\n\t\tfor d2 in dicts[1:]:\n\t\t\td1 = _dictsum(d1,d2)\n\t\treturn d1\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\tdictsum = {}\n\t\tfor d in dicts:\n\t\t\tfor key, value in d.items():\n\t\t\t\tdictsum[key] = dictsum.get(key, 0) + value\n\t\treturn dictsum\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\tif len(dicts) == 0:\n\t\t\treturn dict()\n\t\tif len(dicts) == 1:\n\t\t\treturn dicts[0]\n\t\tif len(dicts) == 2:\n\t\t\treturn dict(dicts[0], **dicts[1])\n\t\treturn _dictsum([dicts[0], _dictsum(dicts[1:])])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\treturn dict(chain(*[d.items() for d in dicts]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\ttotal = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k not in total:\n\t\t\t\ttotal[k] = d[k]\n\t\t\telse:\n\t\t\t\ttotal[k] += d[k]\n\treturn total\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tresult = dict()\n\tfor dic in dicts:\n\t\tfor key in dic:\n\t\t\tif key in result:\n\t\t\t\tresult[key] += dic[key]\n\t\t\telse:\n\t\t\t\tresult[key] = dic[key]\n\treturn result\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tresult = {}\n\tfor d in dicts:\n\t\tfor key in d:\n\t\t\tif key in result:\n\t\t\t\tresult[key] += d[key]\n\t\t\telse:\n\t\t\t\tresult[key] = d[key]\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\n\n\tdictsum={}\n\tfor dict in dicts:\n\t\tfor key in dict:\n\t\t\tif key in dictsum:\n\t\t\t\tdictsum[key] += dict[key] \n\t\t\telse:\n\t\t\t\tdictsum[key] = dict[key]\n\treturn dictsum\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\t# dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n\t#return a dictionary: dic:{'a': 6, 'b': 2}\n\tresult={}\n\tfor dict in dicts:\n\t\tfor key, value in dict.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key]=result[key]+value\n\t\t\telse:\n\t\t\t\tresult[key]=value\n\treturn result", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in result:\n\t\t\t\tresult[k] += v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t \n\tdicts = [d for d in dicts if d is not None]\n\tif len(dicts) == 0:\n\t\treturn None\n\telif len(dicts) == 1:\n\t\treturn dicts[0]\n\telse:\n\t\td1 = dicts[0]\n\t\tfor d2 in dicts[1:]:\n\t\t\td1 = _dictsum(d1,d2)\n\t\treturn d1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\t\tdictsum = {}\n\t\tfor d in dicts:\n\t\t\tfor key, value in d.items():\n\t\t\t\tdictsum[key] = dictsum.get(key, 0) + value\n\t\treturn dictsum\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pyseed/apibase.py", "project": "SEED-platform/py-seed", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor url_key, url_value in url_args.items():\n\t\t# replace the placeholder with the value from the dictionary\n\t\turl = url.replace('{%s}' % url_key, str(url_value))\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor k in url_args:\n\t\turl = url.replace(k, url_args[k])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\n\t# Replace each argument in the url with the argument in url_args\n\tif url_args:\n\t\tfor url_arg_key, url_arg_value in url_args.items():\n\t\t\turl = url.replace(f\"{{{url_arg_key}}}\", url_arg_value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\n\t# TODO: write code here.\n\t# url_args is an object, which can be iterated over.\n\t# Iterate over the keys and values of url_args, and replace the keys of\n\t# the first parameter with values.\n\t\n\turl = url.split('?')[0]\n\n\t# iterate over url_args\n\tfor key, value in url_args.iteritems():\n\t\t# split url into list of parts\n\t\tparts = url.split('/')\n\n\t\t# get index of part that contains key\n\t\tindex = 0\n\t\tif key in parts[0]:\n\t\t\tindex = 0\n\t\telif key in parts[1]:\n\t\t\tindex = 1\n\t\telif key in parts[2]:\n\t\t\tindex = 2\n\t\telif key in parts[3]:\n\t\t\tindex = 3\n\t\telse:\n\t\t\tindex = None\n\n\t\t# if the key was found, replace its value\n\t\tif index is not None:\n\t\t\tparts[index] = value\n\n\t\t# join parts back into url\n\t\turl = '/'.join(parts)\n\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor key, val in url_args.items():\n\t\turl = url.replace(key, val)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor key in url_args:\n\t\turl = url.replace(\"{%s}\" % key, url_args[key])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tif url_args is None:\n\t\treturn url\n\tres = url\n\tfor key, value in url_args.items():\n\t\tres = res.replace('{' + key + '}', value)\n\treturn res\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor key, value in url_args.items():\n\t\turl = url.replace(key, value)\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\n\tfor key in url_args.keys():\n\t\tif key in url:\n\t\t\turl = url.replace(key, url_args[key])\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\t# url_args = {\n\t\t# \t\"key1\": \"value1\",\n\t\t# \t\"key2\": \"value2\"\n\t\t# }\n\tfor key in url_args.keys():\n\t\ttry:\n\t\t\turl = url.replace(\"{\"+key+\"}\", url_args[key])\n\t\texcept:\n\t\t\tpass\n\treturn url\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor url_key, url_value in url_args.items():\n\t\t# replace the placeholder with the value from the dictionary\n\t\turl = url.replace('{%s}' % url_key, str(url_value))\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor k in url_args:\n\t\turl = url.replace(k, url_args[k])\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\n\t# Replace each argument in the url with the argument in url_args\n\tif url_args:\n\t\tfor url_arg_key, url_arg_value in url_args.items():\n\t\t\turl = url.replace(f\"{{{url_arg_key}}}\", url_arg_value)\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\n\t# TODO: write code here.\n\t# url_args is an object, which can be iterated over.\n\t# Iterate over the keys and values of url_args, and replace the keys of\n\t# the first parameter with values.\n\t\n\turl = url.split('?')[0]\n\n\t# iterate over url_args\n\tfor key, value in url_args.iteritems():\n\t\t# split url into list of parts\n\t\tparts = url.split('/')\n\n\t\t# get index of part that contains key\n\t\tindex = 0\n\t\tif key in parts[0]:\n\t\t\tindex = 0\n\t\telif key in parts[1]:\n\t\t\tindex = 1\n\t\telif key in parts[2]:\n\t\t\tindex = 2\n\t\telif key in parts[3]:\n\t\t\tindex = 3\n\t\telse:\n\t\t\tindex = None\n\n\t\t# if the key was found, replace its value\n\t\tif index is not None:\n\t\t\tparts[index] = value\n\n\t\t# join parts back into url\n\t\turl = '/'.join(parts)\n\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor key, val in url_args.items():\n\t\turl = url.replace(key, val)\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor key in url_args:\n\t\turl = url.replace(\"{%s}\" % key, url_args[key])\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tif url_args is None:\n\t\treturn url\n\tres = url\n\tfor key, value in url_args.items():\n\t\tres = res.replace('{' + key + '}', value)\n\treturn res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t \n\tfor key, value in url_args.items():\n\t\turl = url.replace(key, value)\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\n\tfor key in url_args.keys():\n\t\tif key in url:\n\t\t\turl = url.replace(key, url_args[key])\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\t# url_args = {\n\t\t# \t\"key1\": \"value1\",\n\t\t# \t\"key2\": \"value2\"\n\t\t# }\n\tfor key in url_args.keys():\n\t\ttry:\n\t\t\turl = url.replace(\"{\"+key+\"}\", url_args[key])\n\t\texcept:\n\t\t\tpass\n\treturn url\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cinder/api/api_utils.py", "project": "openstack/cinder", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\treturn type(val) == str and val.lower() == 'none'\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\n\n\treturn (type(val) == str and val.lower() == 'none')\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n    \treturn isinstance(val, str) and val.lower() == 'none'\n\t", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\tif isinstance(val, str):\n\t\treturn val.lower() == 'none'\n\telse:\n\t\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\tif isinstance(val, str):\n\t\treturn val.lower() == 'none'\n\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\n\treturn isinstance(val, str) and val.lower() == 'none'\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\treturn val == 'none' if isinstance(val, str) else False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\treturn isinstance(val, str) and val.lower() == 'none'\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\n\tif isinstance(val, str):\n\t\treturn val.lower().strip() == 'none'\n\telse:\n\t\treturn False\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\treturn type(val) == str and val.lower() == 'none'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\n\n\treturn (type(val) == str and val.lower() == 'none')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n    \treturn isinstance(val, str) and val.lower() == 'none'\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\tif isinstance(val, str):\n\t\treturn val.lower() == 'none'\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\tif isinstance(val, str):\n\t\treturn val.lower() == 'none'\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\n\treturn isinstance(val, str) and val.lower() == 'none'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\treturn val == 'none' if isinstance(val, str) else False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t \n\treturn isinstance(val, str) and val.lower() == 'none'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\n\tif isinstance(val, str):\n\t\treturn val.lower().strip() == 'none'\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n\t# remove any trailing slashes\n\tpath = path.rstrip('/')\n\troot = root.rstrip('/')\n\t\n\t# remove root from path\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\n\t# this should never happen\n\traise Exception('Path does not start with root:'+ path +'' + root)\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n    \tif not path.startswith(root):\n\t\traise Exception(\"Path not in root\")\n\treturn path[len(root):]\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\tif not path.startswith(root):\n\t\t\traise Exception('path does not start with root')\n\t\treturn path[len(root):]\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception('Path does not start with root')\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t \n\tif root in path:\n\t\treturn path.split(root)[1]\n\telse:\n\t\traise IOError('Cannot strip root from nonexistent path: '+path+'/'+root)\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n    \tif not path.startswith(root):\n\t\traise Exception(\"Path '{0}' does not contain root '{1}'\".format(path, root))\n\n\treturn path[len(root):]\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Path doesn't start with root\")\n/src/main.py\n#!/usr/bin/python\n\nimport sys\nfrom optparse import OptionParser\nimport os\nimport shutil\nimport time\nimport re\nfrom datetime import datetime, timedelta\n\nfrom utils import *\nfrom file_utils import *\n\n# TODO: \n# - fix date parsing\n# - add delete option\n\n# TODO: use a logging module\n\n# TODO: consider using argparse \n# TODO: try to use the python logging module\n# TODO: fix date handling\n# TODO: add more logging\n\n# TODO: add option to log all files into a log file\n# TODO: add option to log all files into a log file\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t \n\ttry:\n\t\tpath = path.removeprefix(root)\n\t\treturn path\n\texcept ValueError:\n\t\traise ValueError(\"Path does not begin with root\")\n\t\t", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t \n\ttry:\n\t\treturn path.replace(root, '')\n\texcept:\n\t\traise Exception(\"Could not strip root from path\")\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n\t# remove any trailing slashes\n\tpath = path.rstrip('/')\n\troot = root.rstrip('/')\n\t\n\t# remove root from path\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\n\t# this should never happen\n\traise Exception('Path does not start with root:'+ path +'' + root)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n    \tif not path.startswith(root):\n\t\traise Exception(\"Path not in root\")\n\treturn path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\tif not path.startswith(root):\n\t\t\traise Exception('path does not start with root')\n\t\treturn path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception('Path does not start with root')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t \n\tif root in path:\n\t\treturn path.split(root)[1]\n\telse:\n\t\traise IOError('Cannot strip root from nonexistent path: '+path+'/'+root)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n    \tif not path.startswith(root):\n\t\traise Exception(\"Path '{0}' does not contain root '{1}'\".format(path, root))\n\n\treturn path[len(root):]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t\n\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Path doesn't start with root\")\n/src/main.py\n#!/usr/bin/python\n\nimport sys\nfrom optparse import OptionParser\nimport os\nimport shutil\nimport time\nimport re\nfrom datetime import datetime, timedelta\n\nfrom utils import *\nfrom file_utils import *\n\n# TODO: \n# - fix date parsing\n# - add delete option\n\n# TODO: use a logging module\n\n# TODO: consider using argparse \n# TODO: try to use the python logging module\n# TODO: fix date handling\n# TODO: add more logging\n\n# TODO: add option to log all files into a log file\n# TODO: add option to log all files into a log file\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t \n\ttry:\n\t\tpath = path.removeprefix(root)\n\t\treturn path\n\texcept ValueError:\n\t\traise ValueError(\"Path does not begin with root\")\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\t \n\ttry:\n\t\treturn path.replace(root, '')\n\texcept:\n\t\traise Exception(\"Could not strip root from path\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\tflags = ''\n\tfor action in parser._actions:\n\t\tif action.help:\n\t\t\tflags += '-{} '.format(action.option_strings[0])\n\treturn flags.strip()\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\t# get the list of flags and remove the first two elements, which are the script name and the help flag\n\tflags = parser._get_optional_actions()\n\tflags = [flag.dest for flag in flags]\n\tflags = [flag for flag in flags if flag!= \"help\"]\n\tflags = \" \".join(flags)\n\n\treturn flags\n\n\nif __name__ == \"__main__\":\n\n\t# set up the argument parser\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbosity\", action=\"store_true\", help=\"increase output verbosity\")\n\tparser.add_argument(\"-m\", \"--mode\", dest=\"mode\", choices=[\"train\", \"eval\", \"predict\"], help=\"run the model in train, eval, or predict mode\")\n\tparser.add_argument(\"-e\", \"--eval_config\", dest=\"eval_config\", default=\"./configs/eval_config.json\", help=\"path to eval config json file\")\n\tparser.add_argument(\"-p\", \"--predict_config\", dest=\"predict_config\", default=\"./configs/predict_config.json\", help=\"path to predict config json file\")\n\tparser.add_argument(\"-c\", \"--checkpoint\", dest=\"checkpoint\", help=\"path to checkpoint file (required for eval and predict)\")\n\tparser.add_argument(\"-o\", \"--output_dir\",", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\treturn a\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t \n\tflags = []\n\tfor action in parser._actions:\n\t\tfor flag in action.option_strings:\n\t\t\tflags.append(flag)\n\treturn \" \".join(flags)(self):\n        pass\n\n    def get_name(self):\n        \"\"\"\n        Returns the name of the module.\n        \"\"\"\n        return self.__class__.__name__\n\n    def get_path(self):\n        \"\"\"\n        Returns the path to the module.\n        \"\"\"\n        return self.get_name()\n\n    def get_description(self):\n        \"\"\"\n        Returns the description of the module.\n        \"\"\"\n        return \"Base module for all modules.\"\n\n    def get_author(self):\n        \"\"\"\n        Returns the author of the module.\n        \"\"\"\n        return \"Author\"\n\n    def get_version(self):\n        \"\"\"\n        Returns the version of the module.\n        \"\"\"\n        return \"1.0.0\"\n\n    def get_command(self):\n        \"\"\"\n        Returns the command used for invoking the module.\n        \"\"\"\n        return self.get_name()\n\n    def get_parser(self):\n        \"\"\"\n        Returns the parser for the module.\n        \"\"\"\n        parser = argparse.ArgumentParser(prog=self.get_command(), description=self.get_description())\n        self.add_parser_arguments(parser)\n        return parser\n\n    def add_parser_arguments(self, parser):\n        \"\"\"\n        Adds arguments to the parser.\n\n        Args:", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\t# parser.print_help()\n\t# print(parser.parse_args())\n\tflags = \" \".join(parser.parse_args(args=None, namespace=None))\n\t# print(flags)\n\treturn flags\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\t# Add all flags\n\tflag_list = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflag_list.append(action.option_strings[0])\n\treturn''.join(flag_list)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\t# return \" \".join(script.parse_args()._get_args_line()[0].split(' ')[1:])\n\treturn \" \".join(parser.parse_args()._get_args_line()[0].split(' ')[1:])\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t \n\tflags = set()\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tif action.dest == 'help':\n\t\t\tcontinue\n\t\tflags.update(action.option_strings)\n\treturn''.join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n        return \" \".join(\n            [\n                \"--{}\".format(f.dest)\n                for f in parser._actions\n                if f.dest not in (\"help\", \"version\")\n            ]\n        )\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\tflags = ''\n\tfor action in parser._actions:\n\t\tif action.help:\n\t\t\tflags += '-{} '.format(action.option_strings[0])\n\treturn flags.strip()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\t# get the list of flags and remove the first two elements, which are the script name and the help flag\n\tflags = parser._get_optional_actions()\n\tflags = [flag.dest for flag in flags]\n\tflags = [flag for flag in flags if flag!= \"help\"]\n\tflags = \" \".join(flags)\n\n\treturn flags\n\n\nif __name__ == \"__main__\":\n\n\t# set up the argument parser\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbosity\", action=\"store_true\", help=\"increase output verbosity\")\n\tparser.add_argument(\"-m\", \"--mode\", dest=\"mode\", choices=[\"train\", \"eval\", \"predict\"], help=\"run the model in train, eval, or predict mode\")\n\tparser.add_argument(\"-e\", \"--eval_config\", dest=\"eval_config\", default=\"./configs/eval_config.json\", help=\"path to eval config json file\")\n\tparser.add_argument(\"-p\", \"--predict_config\", dest=\"predict_config\", default=\"./configs/predict_config.json\", help=\"path to predict config json file\")\n\tparser.add_argument(\"-c\", \"--checkpoint\", dest=\"checkpoint\", help=\"path to checkpoint file (required for eval and predict)\")\n\tparser.add_argument(\"-o\", \"--output_dir\",", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\treturn a\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t \n\tflags = []\n\tfor action in parser._actions:\n\t\tfor flag in action.option_strings:\n\t\t\tflags.append(flag)\n\treturn \" \".join(flags)(self):\n        pass\n\n    def get_name(self):\n        \"\"\"\n        Returns the name of the module.\n        \"\"\"\n        return self.__class__.__name__\n\n    def get_path(self):\n        \"\"\"\n        Returns the path to the module.\n        \"\"\"\n        return self.get_name()\n\n    def get_description(self):\n        \"\"\"\n        Returns the description of the module.\n        \"\"\"\n        return \"Base module for all modules.\"\n\n    def get_author(self):\n        \"\"\"\n        Returns the author of the module.\n        \"\"\"\n        return \"Author\"\n\n    def get_version(self):\n        \"\"\"\n        Returns the version of the module.\n        \"\"\"\n        return \"1.0.0\"\n\n    def get_command(self):\n        \"\"\"\n        Returns the command used for invoking the module.\n        \"\"\"\n        return self.get_name()\n\n    def get_parser(self):\n        \"\"\"\n        Returns the parser for the module.\n        \"\"\"\n        parser = argparse.ArgumentParser(prog=self.get_command(), description=self.get_description())\n        self.add_parser_arguments(parser)\n        return parser\n\n    def add_parser_arguments(self, parser):\n        \"\"\"\n        Adds arguments to the parser.\n\n        Args:", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\t# parser.print_help()\n\t# print(parser.parse_args())\n\tflags = \" \".join(parser.parse_args(args=None, namespace=None))\n\t# print(flags)\n\treturn flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n\n\t# Add all flags\n\tflag_list = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflag_list.append(action.option_strings[0])\n\treturn''.join(flag_list)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\t# return \" \".join(script.parse_args()._get_args_line()[0].split(' ')[1:])\n\treturn \" \".join(parser.parse_args()._get_args_line()[0].split(' ')[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t \n\tflags = set()\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tif action.dest == 'help':\n\t\t\tcontinue\n\t\tflags.update(action.option_strings)\n\treturn''.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\t\n        return \" \".join(\n            [\n                \"--{}\".format(f.dest)\n                for f in parser._actions\n                if f.dest not in (\"help\", \"version\")\n            ]\n        )\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        # Expand the path name to an absolute path\n\tpath_name = os.path.abspath(path_name)\n\n\tif verbose:\n\t\tprint(\"Checking if {} is processed\".format(path_name))\n\n\tif path_name in processed:\n\t\tprint(\"ERROR: {} is already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t \n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tlogging.info(\"Already processed: {}\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        # Expand path to absolute path.\n        path = os.path.abspath(path_name)\n\n        # Check if it's already in the set (already processed).\n        if path in processed:\n            if verbose:\n                print \"Already processed %s\" % path_name\n            return True\n\n        # Add it to the set.\n        processed.add(path)\n        return False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        # Expand the path name so it will work on both Unix and Windows\n        path_name = os.path.expanduser(path_name)\n        if os.path.isdir(path_name):\n            # It's a directory. Look inside it for files to process\n            for entry in os.listdir(path_name):\n                entry_name = os.path.join(path_name, entry)\n                if was_processed(processed, entry_name, verbose):\n                    return True\n        else:\n            # It's a file.\n            if path_name in processed:\n                if verbose:\n                    print(\"Skipping {}, it was already processed\".format(path_name))\n                return True\n            else:\n                processed.add(path_name)\n                return False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\n\t# Get the absolute path name.\n\tabs_path = os.path.abspath(path_name)\n\n\t# Check if it's already in the set.\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed: \" + path_name)\n\t\treturn True\n\n\t# It's not in the set, so add it and return False.\n\tprocessed.add(abs_path)\n\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        if os.path.exists(path_name):\n            if os.path.isdir(path_name):\n                p_name = os.path.abspath(path_name)\n                #print(\"p_name is %s\" % p_name)\n                if p_name in processed:\n                    if verbose:\n                        print(\"Directory %s is already processed.\" % p_name)\n                    return True\n                else:\n                    processed.add(p_name)\n                    return False\n            else:\n                p_name = os.path.abspath(path_name)\n                if p_name in processed:\n                    if verbose:\n                        print(\"File %s is already processed.\" % p_name)\n                    return True\n                else:\n                    processed.add(p_name)\n                    return False\n        else:\n            if verbose:\n                print(\"Path %s does not exist.\" % path_name)\n            return True\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\n\t# Expand the path name to an absolution path\n\tpath_name = os.path.abspath(path_name)\n\n\t# Check if the pathname is already in the set\n\tif path_name in processed:\n\t\tprint \"\\n{0} is already processed. Aborting.\".format(path_name)\n\t\tif verbose:\n\t\t\tprint \"Processed files:\"\n\t\t\tfor entry in sorted(processed):\n\t\t\t\tprint entry\n\t\treturn True\n\n\t# Add the pathname to the set and return False\n\tprocessed.add(path_name)\n\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\n\tif os.path.isabs(path_name):\n\t\tabs_path = path_name\n\telse:\n\t\tabs_path = os.path.abspath(path_name)\n\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Skipping already processed file {0}\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t \n\t# Get the absolute path\n\tpath_name = os.path.abspath(path_name)\n\n\t# If it's already in the set, report the problem and return True\n\tif (path_name in processed):\n\t\tif (verbose):\n\t\t\tprint(\"Already processed: \" + path_name)\n\t\treturn True\n\telse:\n\t\t# Otherwise add it to the set and return False\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        if not os.path.isabs(path_name):\n            path_name = os.path.abspath(path_name)\n\t\n        if path_name in processed:\n            if verbose:\n                print(\"Skipping processed path {}\".format(path_name))\n            return True\n\n        return False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        # Expand the path name to an absolute path\n\tpath_name = os.path.abspath(path_name)\n\n\tif verbose:\n\t\tprint(\"Checking if {} is processed\".format(path_name))\n\n\tif path_name in processed:\n\t\tprint(\"ERROR: {} is already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t \n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tlogging.info(\"Already processed: {}\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        # Expand path to absolute path.\n        path = os.path.abspath(path_name)\n\n        # Check if it's already in the set (already processed).\n        if path in processed:\n            if verbose:\n                print \"Already processed %s\" % path_name\n            return True\n\n        # Add it to the set.\n        processed.add(path)\n        return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        # Expand the path name so it will work on both Unix and Windows\n        path_name = os.path.expanduser(path_name)\n        if os.path.isdir(path_name):\n            # It's a directory. Look inside it for files to process\n            for entry in os.listdir(path_name):\n                entry_name = os.path.join(path_name, entry)\n                if was_processed(processed, entry_name, verbose):\n                    return True\n        else:\n            # It's a file.\n            if path_name in processed:\n                if verbose:\n                    print(\"Skipping {}, it was already processed\".format(path_name))\n                return True\n            else:\n                processed.add(path_name)\n                return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\n\t# Get the absolute path name.\n\tabs_path = os.path.abspath(path_name)\n\n\t# Check if it's already in the set.\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed: \" + path_name)\n\t\treturn True\n\n\t# It's not in the set, so add it and return False.\n\tprocessed.add(abs_path)\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        if os.path.exists(path_name):\n            if os.path.isdir(path_name):\n                p_name = os.path.abspath(path_name)\n                #print(\"p_name is %s\" % p_name)\n                if p_name in processed:\n                    if verbose:\n                        print(\"Directory %s is already processed.\" % p_name)\n                    return True\n                else:\n                    processed.add(p_name)\n                    return False\n            else:\n                p_name = os.path.abspath(path_name)\n                if p_name in processed:\n                    if verbose:\n                        print(\"File %s is already processed.\" % p_name)\n                    return True\n                else:\n                    processed.add(p_name)\n                    return False\n        else:\n            if verbose:\n                print(\"Path %s does not exist.\" % path_name)\n            return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\n\t# Expand the path name to an absolution path\n\tpath_name = os.path.abspath(path_name)\n\n\t# Check if the pathname is already in the set\n\tif path_name in processed:\n\t\tprint \"\\n{0} is already processed. Aborting.\".format(path_name)\n\t\tif verbose:\n\t\t\tprint \"Processed files:\"\n\t\t\tfor entry in sorted(processed):\n\t\t\t\tprint entry\n\t\treturn True\n\n\t# Add the pathname to the set and return False\n\tprocessed.add(path_name)\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\n\tif os.path.isabs(path_name):\n\t\tabs_path = path_name\n\telse:\n\t\tabs_path = os.path.abspath(path_name)\n\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Skipping already processed file {0}\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t \n\t# Get the absolute path\n\tpath_name = os.path.abspath(path_name)\n\n\t# If it's already in the set, report the problem and return True\n\tif (path_name in processed):\n\t\tif (verbose):\n\t\t\tprint(\"Already processed: \" + path_name)\n\t\treturn True\n\telse:\n\t\t# Otherwise add it to the set and return False\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\t\n        if not os.path.isabs(path_name):\n            path_name = os.path.abspath(path_name)\n\t\n        if path_name in processed:\n            if verbose:\n                print(\"Skipping processed path {}\".format(path_name))\n            return True\n\n        return False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "eppy/geometry/surface.py", "project": "santoshphilip/eppy", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n    \tlist_vertices = []\n\tfor i in range(0,len(vertices)):\n\t\tv1 = vertices[i]\n\t\tv2 = vertices[(i-1) % len(vertices)]\n\t\tv3 = vertices[(i+1) % len(vertices)]\n\t\t\n\t\tlist_vertices.append((v1,v2,v3))\n\treturn list_vertices\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n#\tif vertices[0] == 1:\n#\t\tprint vertices[0], vertices[1], vertices[2]\n\tresult = []\n\tsize = len(vertices)\n\tfor i in xrange(1, size):\n\t\tif vertices[i] == 1:\n\t\t\tresult.append((vertices[i-1], vertices[i], vertices[(i+1)%size]))\n\t\t\t\n\t\telse:\n\t\t\tresult.append((vertices[i], vertices[(i+1)%size], vertices[i-1]))\n\treturn result\n\t", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        return [(vertices[i], vertices[(i+1)%len(vertices)], vertices[(i+2)%len(vertices)]) for i in range(len(vertices))]\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        vertices_tuple = []\n\n        # Loop through the vertices\n        for i in range(len(vertices)):\n            # Get the point before the vertex\n            before_v = vertices[i-1]\n            # Get the point after the vertex\n            after_v = vertices[i%len(vertices)]\n            # Get the vertex\n            v = vertices[i]\n\n            vertices_tuple.append((v, before_v, after_v))\n\n        return vertices_tuple\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\n\tfor i in range(len(vertices)):\n\t\tif i == 0:\n\t\t\ttemp_tuple = (vertices[i],vertices[i+1],vertices[i+2])\n\t\telif i == len(vertices) - 1:\n\t\t\ttemp_tuple = (vertices[i],vertices[i-1],vertices[0])\n\t\telse:\n\t\t\ttemp_tuple = (vertices[i],vertices[i-1],vertices[i+1])\n\t\tvertices_tuple.append(temp_tuple)\n\treturn vertices_tuple\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\n\t# vertices = [list(vertices)]\n\tvertices = [list(vertices)]\n\tvertices = np.array(vertices)\n\tprint(\"vertices\", vertices)\n\t# print(type(vertices))\n\tpoints = []\n\tfor vertex in vertices:\n\t\t# print(vertex)\n\t\t# print(vertex[0])\n\t\t# print(vertex[1])\n\t\t# print(vertex[2])\n\t\tif type(vertex[0]) is not list:\n\t\t\tpoints.append(tuple(vertex))\n\t\telse:\n\t\t\tpoints.append(tuple((vertex[0][0], vertex[0][1], vertex[0][2])))\n\t\t\tpoints.append(tuple((vertex[1][0], vertex[1][1], vertex[1][2])))\n\t\t\tpoints.append(tuple((vertex[2][0], vertex[2][1], vertex[2][2])))\n\treturn points\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\t#vertices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\t#print(vertices)\n\t#print(len(vertices))\n\tif len(vertices) % 2!= 0:\n\t\traise Exception(\"The length of the list must be even\")\n\tindex = 0\n\tlength = len(vertices)\n\tresult = []\n\n\tfor i in range(0, len(vertices) - 2, 2):\n\t\t#print(i)\n\t\tif i < 0:\n\t\t\ti = 0\n\t\tresult.append((vertices[i], vertices[i + 1], vertices[i + 2]))\n\t\tindex += 1\n\n\t#print(result)\n\treturn result\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        n = len(vertices)\n        vertice3 = [0 for i in range(n)]\n        for i in range(n):\n            vertice = vertices[i]\n            if i == 0:\n                vertice3[i] = (vertices[n-1], vertice, vertices[i+1])\n            elif i == n-1:\n                vertice3[i] = (vertices[i-1], vertice, vertices[0])\n            else:\n                vertice3[i] = (vertices[i-1], vertice, vertices[i+1])\n        return vertice3\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        if len(vertices) < 3:\n            return -1\n        else:\n            return [vertices[i] for i in range(len(vertices))] + [vertices[0]] + [vertices[2]]\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n    \tvertices_tuple = []\n    \tfor v in vertices:\n        \tif v - 1 >= 0 and v + 1 < len(vertices):\n            \t\tvertices_tuple.append((vertices[v-1], vertices[v], vertices[v+1]))\n        \telif v + 1 >= len(vertices):\n            \t\tvertices_tuple.append((vertices[v-1], vertices[v], vertices[0]))\n        \telse:\n            \t\tvertices_tuple.append((vertices[len(vertices)-1], vertices[v], vertices[0]))\n\n    \treturn vertices_tuple\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n    \tlist_vertices = []\n\tfor i in range(0,len(vertices)):\n\t\tv1 = vertices[i]\n\t\tv2 = vertices[(i-1) % len(vertices)]\n\t\tv3 = vertices[(i+1) % len(vertices)]\n\t\t\n\t\tlist_vertices.append((v1,v2,v3))\n\treturn list_vertices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n#\tif vertices[0] == 1:\n#\t\tprint vertices[0], vertices[1], vertices[2]\n\tresult = []\n\tsize = len(vertices)\n\tfor i in xrange(1, size):\n\t\tif vertices[i] == 1:\n\t\t\tresult.append((vertices[i-1], vertices[i], vertices[(i+1)%size]))\n\t\t\t\n\t\telse:\n\t\t\tresult.append((vertices[i], vertices[(i+1)%size], vertices[i-1]))\n\treturn result\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        return [(vertices[i], vertices[(i+1)%len(vertices)], vertices[(i+2)%len(vertices)]) for i in range(len(vertices))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        vertices_tuple = []\n\n        # Loop through the vertices\n        for i in range(len(vertices)):\n            # Get the point before the vertex\n            before_v = vertices[i-1]\n            # Get the point after the vertex\n            after_v = vertices[i%len(vertices)]\n            # Get the vertex\n            v = vertices[i]\n\n            vertices_tuple.append((v, before_v, after_v))\n\n        return vertices_tuple\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\n\tfor i in range(len(vertices)):\n\t\tif i == 0:\n\t\t\ttemp_tuple = (vertices[i],vertices[i+1],vertices[i+2])\n\t\telif i == len(vertices) - 1:\n\t\t\ttemp_tuple = (vertices[i],vertices[i-1],vertices[0])\n\t\telse:\n\t\t\ttemp_tuple = (vertices[i],vertices[i-1],vertices[i+1])\n\t\tvertices_tuple.append(temp_tuple)\n\treturn vertices_tuple\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\n\t# vertices = [list(vertices)]\n\tvertices = [list(vertices)]\n\tvertices = np.array(vertices)\n\tprint(\"vertices\", vertices)\n\t# print(type(vertices))\n\tpoints = []\n\tfor vertex in vertices:\n\t\t# print(vertex)\n\t\t# print(vertex[0])\n\t\t# print(vertex[1])\n\t\t# print(vertex[2])\n\t\tif type(vertex[0]) is not list:\n\t\t\tpoints.append(tuple(vertex))\n\t\telse:\n\t\t\tpoints.append(tuple((vertex[0][0], vertex[0][1], vertex[0][2])))\n\t\t\tpoints.append(tuple((vertex[1][0], vertex[1][1], vertex[1][2])))\n\t\t\tpoints.append(tuple((vertex[2][0], vertex[2][1], vertex[2][2])))\n\treturn points\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\t#vertices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\t#print(vertices)\n\t#print(len(vertices))\n\tif len(vertices) % 2!= 0:\n\t\traise Exception(\"The length of the list must be even\")\n\tindex = 0\n\tlength = len(vertices)\n\tresult = []\n\n\tfor i in range(0, len(vertices) - 2, 2):\n\t\t#print(i)\n\t\tif i < 0:\n\t\t\ti = 0\n\t\tresult.append((vertices[i], vertices[i + 1], vertices[i + 2]))\n\t\tindex += 1\n\n\t#print(result)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        n = len(vertices)\n        vertice3 = [0 for i in range(n)]\n        for i in range(n):\n            vertice = vertices[i]\n            if i == 0:\n                vertice3[i] = (vertices[n-1], vertice, vertices[i+1])\n            elif i == n-1:\n                vertice3[i] = (vertices[i-1], vertice, vertices[0])\n            else:\n                vertice3[i] = (vertices[i-1], vertice, vertices[i+1])\n        return vertice3\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n        if len(vertices) < 3:\n            return -1\n        else:\n            return [vertices[i] for i in range(len(vertices))] + [vertices[0]] + [vertices[2]]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n    \tvertices_tuple = []\n    \tfor v in vertices:\n        \tif v - 1 >= 0 and v + 1 < len(vertices):\n            \t\tvertices_tuple.append((vertices[v-1], vertices[v], vertices[v+1]))\n        \telif v + 1 >= len(vertices):\n            \t\tvertices_tuple.append((vertices[v-1], vertices[v], vertices[0]))\n        \telse:\n            \t\tvertices_tuple.append((vertices[len(vertices)-1], vertices[v], vertices[0]))\n\n    \treturn vertices_tuple\n", "is_pass": false, "return_code": 1}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tresult = []\n\twhile number >= padding:\n\t\tnumber, i = divmod(number, padding)\n\t\tresult.append(alphabet[i])\n\tresult.append(alphabet[number])\n\treturn ''.join(reversed(result))\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t \n\tif padding is None:\n\t\tpadding = len(alphabet)\n\tresult = []\n\twhile number >= padding:\n\t\tresult.append(alphabet[number % padding])\n\t\tnumber = int(number / padding)\n\tresult.append(alphabet[number])\n\treturn ''.join(reversed(result))\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is not None:\n\t\tnumber = str(number).zfill(padding)\n\n\tresult = \"\"\n\twhile number >= len(alphabet):\n\t\tresult = alphabet[number % len(alphabet)] + result\n\t\tnumber = number // len(alphabet)\n\n\tresult = alphabet[number % len(alphabet)] + result\n\treturn result\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif number == 0:\n\t\treturn alphabet[number]\n\telse:\n\t\t# create a list of numbers from 9 down to 0\n\t\tnumbers = range(9, -1, -1)\n\t\t# pad the list with padding, if it's set\n\t\tif padding is not None:\n\t\t\tnumbers = [padding] * padding + numbers\n\t\t# convert the int to a string using the numbers as indices\n\t\treturn ''.join(alphabet[i] for i in numbers if i <= number)\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tstring = \"\"\n\twhile number!= 0:\n\t\tstring = alphabet[number % padding] + string\n\t\tnumber = number // padding\n\n\treturn string\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tdef pad_left(s, length):\n\t\treturn s.zfill(length)\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tif number < 0:\n\t\traise ValueError(\"Cannot have negative number\")\n\n\tresult = \"\"\n\n\twhile number > 0:\n\t\tresult = alphabet[number % padding] + result\n\t\tnumber = number // padding\n\n\tif padding > len(alphabet):\n\t\tresult = pad_left(result, padding)\n\n\treturn result\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tresult = []\n\twhile number > 0:\n\t\tresult.append(alphabet[number % len(alphabet)])\n\t\tnumber = number // len(alphabet)\n\n\t# reverse result\n\tresult.reverse()\n\t# add padding\n\tif padding is not None:\n\t\tresult = result + [alphabet[0]] * (padding - len(result))\n\n\t# convert list to string\n\treturn ''.join(result)\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is None:\n\t\tpadding = 0\n\n\tdef gen_string(x: int) -> str:\n\t\treturn int_to_string(x, alphabet, padding)\n\n\treturn \"\".join(gen_string(number // len(alphabet) ** i) + alphabet[number % len(alphabet) ** i] for i in range(padding, -1, -1))\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is not None:\n\t\tnumber = number + padding\n\n\tif number < 0:\n\t\tnumber = number * -1\n\n\tresult = \"\"\n\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber = number // len(alphabet)\n\n\treturn result[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif not number:\n\t\treturn ''\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tnumber, remainder = divmod(number, padding)\n\treturn int_to_string(number, alphabet, padding) + alphabet[remainder]\n/README.md\n# uuid-converter\nConvert int to string, using the given alphabet.\nThe number represents a short uuid.\nThe output has the most significant digit first.\n\n[![Build Status](https://travis-ci.org/fran-k/uuid-converter.svg?branch=master)](https://travis-ci.org/fran-k/uuid-converter)\n\n## Usage\n\n```python\nfrom uuid_converter import int_to_string\n\n\nalphabet = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\nnumber = 12345678901234567890\n\nprint(int_to_string(number, alphabet))\n# 1100000000000000000000000000000000000000000000000000000000000000"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tresult = []\n\twhile number >= padding:\n\t\tnumber, i = divmod(number, padding)\n\t\tresult.append(alphabet[i])\n\tresult.append(alphabet[number])\n\treturn ''.join(reversed(result))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t \n\tif padding is None:\n\t\tpadding = len(alphabet)\n\tresult = []\n\twhile number >= padding:\n\t\tresult.append(alphabet[number % padding])\n\t\tnumber = int(number / padding)\n\tresult.append(alphabet[number])\n\treturn ''.join(reversed(result))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is not None:\n\t\tnumber = str(number).zfill(padding)\n\n\tresult = \"\"\n\twhile number >= len(alphabet):\n\t\tresult = alphabet[number % len(alphabet)] + result\n\t\tnumber = number // len(alphabet)\n\n\tresult = alphabet[number % len(alphabet)] + result\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif number == 0:\n\t\treturn alphabet[number]\n\telse:\n\t\t# create a list of numbers from 9 down to 0\n\t\tnumbers = range(9, -1, -1)\n\t\t# pad the list with padding, if it's set\n\t\tif padding is not None:\n\t\t\tnumbers = [padding] * padding + numbers\n\t\t# convert the int to a string using the numbers as indices\n\t\treturn ''.join(alphabet[i] for i in numbers if i <= number)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\n\tstring = \"\"\n\twhile number!= 0:\n\t\tstring = alphabet[number % padding] + string\n\t\tnumber = number // padding\n\n\treturn string\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tdef pad_left(s, length):\n\t\treturn s.zfill(length)\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tif number < 0:\n\t\traise ValueError(\"Cannot have negative number\")\n\n\tresult = \"\"\n\n\twhile number > 0:\n\t\tresult = alphabet[number % padding] + result\n\t\tnumber = number // padding\n\n\tif padding > len(alphabet):\n\t\tresult = pad_left(result, padding)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tresult = []\n\twhile number > 0:\n\t\tresult.append(alphabet[number % len(alphabet)])\n\t\tnumber = number // len(alphabet)\n\n\t# reverse result\n\tresult.reverse()\n\t# add padding\n\tif padding is not None:\n\t\tresult = result + [alphabet[0]] * (padding - len(result))\n\n\t# convert list to string\n\treturn ''.join(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is None:\n\t\tpadding = 0\n\n\tdef gen_string(x: int) -> str:\n\t\treturn int_to_string(x, alphabet, padding)\n\n\treturn \"\".join(gen_string(number // len(alphabet) ** i) + alphabet[number % len(alphabet) ** i] for i in range(padding, -1, -1))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif padding is not None:\n\t\tnumber = number + padding\n\n\tif number < 0:\n\t\tnumber = number * -1\n\n\tresult = \"\"\n\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber = number // len(alphabet)\n\n\treturn result[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\n\tif not number:\n\t\treturn ''\n\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\n\tnumber, remainder = divmod(number, padding)\n\treturn int_to_string(number, alphabet, padding) + alphabet[remainder]\n/README.md\n# uuid-converter\nConvert int to string, using the given alphabet.\nThe number represents a short uuid.\nThe output has the most significant digit first.\n\n[![Build Status](https://travis-ci.org/fran-k/uuid-converter.svg?branch=master)](https://travis-ci.org/fran-k/uuid-converter)\n\n## Usage\n\n```python\nfrom uuid_converter import int_to_string\n\n\nalphabet = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\nnumber = 12345678901234567890\n\nprint(int_to_string(number, alphabet))\n# 1100000000000000000000000000000000000000000000000000000000000000", "is_pass": false, "return_code": 1}]}
{"file_path": "neutron_lib/agent/common/utils.py", "project": "openstack/neutron-lib", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tif register_number in flow_params.keys() and register_value in flow_params.keys():\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t \n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n#\tflow_params[register_number] = flow_params[register_value]\n#\tdel flow_params[register_value]\n/README.md\n# Misty_Project\n\nThis is the main repository for my Misty Project. The goal of this project is to create a program that will take a flowchart and generate a program that will execute the flowchart.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\n\n### Prerequisites\n\nWhat things you need to install the software and how to install them\n\n```\npip install opencv-python\n```\n\n### Installing\n\nA step by step series of examples that tell you how to get a development env running\n\nSay what the step will be\n\n```\nGive the example\n```\n\nAnd repeat\n\n```\nuntil finished\n```\n\nEnd with an example of getting some data out of the system or using it for a little demo\n\n## Running the tests\n\nExplain how to run the automated tests for this system\n\n### Break down into end to end tests\n\nExplain what these tests test and why\n\n```\nGive an example\n```\n\n### And coding style tests\n\nExplain what these tests test and why\n\n```", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tif register_number in flow_params:\n\t\tflow_params[register_value] = flow_params[register_number]\n\t\tdel flow_params[register_number]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n\treturn flow_params\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tif register_value in flow_params:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\t# Check that the register_number and register_value are both integers, and that the flow_params dictionary contains both registers\n\tif not isinstance(register_number, int) or not isinstance(register_value, int) or not register_number in flow_params or not register_value in flow_params:\n\t\traise InvalidRegisterError(\"Invalid register number or value\")\n\n\t# Replace the register value with the actual value\n\tflow_params[register_number] = flow_params[register_value]\n\n\t# Delete the register value\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n/README.md\n# Ransomware\n\n## Goal\n\nThe goal of this project is to create a ransomware that will encrypt a victim's computer and require a password to decrypt the files.\n\n## How to run\n\n```\npython ransomware.py\n```\n\n## What it does\n\nThe ransomware will encrypt all files in the directory it is run in. In the beginning, it will ask the user for a password and then encrypt the files. It will then keep the password in memory and the password will be used to decrypt the files. After the files are decrypted, it will ask the user for another password that will be used to encrypt the files again.\n\n## What it does under the hood\n\nThe ransomware will create a new file called `decrypted.txt` and copy the contents of the `encrypted.txt` into the `decrypted.txt` file. Then, the ransomware will encrypt the files in the directory and add the encrypted files to a list. This list is then serialized and saved to a file called `ransomware_state.txt`. This file is then encrypted and saved to a file called `ransomware_state_encrypted.txt`. The password is then saved to a file called `ransomware_state_encrypted.txt`. The password is then sent to the server for storage. The"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tif register_number in flow_params.keys() and register_value in flow_params.keys():\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t \n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n#\tflow_params[register_number] = flow_params[register_value]\n#\tdel flow_params[register_value]\n/README.md\n# Misty_Project\n\nThis is the main repository for my Misty Project. The goal of this project is to create a program that will take a flowchart and generate a program that will execute the flowchart.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\n\n### Prerequisites\n\nWhat things you need to install the software and how to install them\n\n```\npip install opencv-python\n```\n\n### Installing\n\nA step by step series of examples that tell you how to get a development env running\n\nSay what the step will be\n\n```\nGive the example\n```\n\nAnd repeat\n\n```\nuntil finished\n```\n\nEnd with an example of getting some data out of the system or using it for a little demo\n\n## Running the tests\n\nExplain how to run the automated tests for this system\n\n### Break down into end to end tests\n\nExplain what these tests test and why\n\n```\nGive an example\n```\n\n### And coding style tests\n\nExplain what these tests test and why\n\n```", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tif register_number in flow_params:\n\t\tflow_params[register_value] = flow_params[register_number]\n\t\tdel flow_params[register_number]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n\treturn flow_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\tif register_value in flow_params:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n\t# Check that the register_number and register_value are both integers, and that the flow_params dictionary contains both registers\n\tif not isinstance(register_number, int) or not isinstance(register_value, int) or not register_number in flow_params or not register_value in flow_params:\n\t\traise InvalidRegisterError(\"Invalid register number or value\")\n\n\t# Replace the register value with the actual value\n\tflow_params[register_number] = flow_params[register_value]\n\n\t# Delete the register value\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\t\n\n/README.md\n# Ransomware\n\n## Goal\n\nThe goal of this project is to create a ransomware that will encrypt a victim's computer and require a password to decrypt the files.\n\n## How to run\n\n```\npython ransomware.py\n```\n\n## What it does\n\nThe ransomware will encrypt all files in the directory it is run in. In the beginning, it will ask the user for a password and then encrypt the files. It will then keep the password in memory and the password will be used to decrypt the files. After the files are decrypted, it will ask the user for another password that will be used to encrypt the files again.\n\n## What it does under the hood\n\nThe ransomware will create a new file called `decrypted.txt` and copy the contents of the `encrypted.txt` into the `decrypted.txt` file. Then, the ransomware will encrypt the files in the directory and add the encrypted files to a list. This list is then serialized and saved to a file called `ransomware_state.txt`. This file is then encrypted and saved to a file called `ransomware_state_encrypted.txt`. The password is then saved to a file called `ransomware_state_encrypted.txt`. The password is then sent to the server for storage. The", "is_pass": false, "return_code": 1}]}
{"file_path": "release_dashboard/templatetags/rd_extras.py", "project": "sipwise/repoapi", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace(\".\", arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n        return value.replace(\".\", arg)\n\t\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\treturn value.replace('.',arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\tif value!= \"\":\n\t\tvalue = value.replace('.', arg)\n\treturn value\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n\n\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n        return value.replace(\".\", arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\treturn value.replace('.', arg)\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace('.', arg)\n/README.md\n# PyJester - Jest testing framework for Python\n\n[![Build Status](https://travis-ci.org/d-i-t-a/pyjester.svg?branch=master)](https://travis-ci.org/d-i-t-a/pyjester)\n[![Coverage Status](https://coveralls.io/repos/github/d-i-t-a/pyjester/badge.svg)](https://coveralls.io/github/d-i-t-a/pyjester)\n[![PyPI version](https://badge.fury.io/py/pyjester.svg)](https://badge.fury.io/py/pyjester)\n\n![PyJester Logo](https://raw.githubusercontent.com/d-i-t-a/pyjester/master/pyjester.png)\n\n## About\n\nPyJester is a testing library for Python, which aims to be a modern alternative to unittest and nose.\n\n## Installation\n\nPyJester is available on PyPI and can be installed with pip:\n\n```bash\npip install pyjester\n```\n\n## Usage\n\n### Writing tests\n\nTests are implemented as classes and methods. Every test method should start with `test`, and be decorated with `@test`.\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace('.', arg)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace(\".\", arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n        return value.replace(\".\", arg)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\treturn value.replace('.',arg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\tif value!= \"\":\n\t\tvalue = value.replace('.', arg)\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n\n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n        return value.replace(\".\", arg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\treturn value.replace('.', arg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace('.', arg)\n/README.md\n# PyJester - Jest testing framework for Python\n\n[![Build Status](https://travis-ci.org/d-i-t-a/pyjester.svg?branch=master)](https://travis-ci.org/d-i-t-a/pyjester)\n[![Coverage Status](https://coveralls.io/repos/github/d-i-t-a/pyjester/badge.svg)](https://coveralls.io/github/d-i-t-a/pyjester)\n[![PyPI version](https://badge.fury.io/py/pyjester.svg)](https://badge.fury.io/py/pyjester)\n\n![PyJester Logo](https://raw.githubusercontent.com/d-i-t-a/pyjester/master/pyjester.png)\n\n## About\n\nPyJester is a testing library for Python, which aims to be a modern alternative to unittest and nose.\n\n## Installation\n\nPyJester is available on PyPI and can be installed with pip:\n\n```bash\npip install pyjester\n```\n\n## Usage\n\n### Writing tests\n\nTests are implemented as classes and methods. Every test method should start with `test`, and be decorated with `@test`.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t \n\treturn value.replace('.', arg)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "rows/utils/__init__.py", "project": "turicas/rows", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n        subcls = set()\n        for sc in cls.__subclasses__():\n            subcls.add(sc)\n            subcls |= subclasses(sc)\n        return subcls\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t/README.md\n# Python3-Utils\nA repository of useful Python3 utilities that I have written and found useful\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn type(cls).subclasses()\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn [c for c in cls.__subclasses__()] + [s for c in cls.__subclasses__() for s in subclasses(c)]\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t \n\tsubs = []\n\tfor sub in cls.__subclasses__():\n\t\tsubs.append(sub)\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn type(cls.__name__, cls.__bases__, dict(cls.__dict__))\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t \n\treturn set(cls.__subclasses__()).union(\n\t\t[s for c in cls.__subclasses__() for s in subclasses(c)])\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\n\treturn [subcls for subcls in cls.__subclasses__() if subcls.__name__!= cls.__name__]\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n        subcls = set()\n        for sc in cls.__subclasses__():\n            subcls.add(sc)\n            subcls |= subclasses(sc)\n        return subcls\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t/README.md\n# Python3-Utils\nA repository of useful Python3 utilities that I have written and found useful\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn type(cls).subclasses()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn [c for c in cls.__subclasses__()] + [s for c in cls.__subclasses__() for s in subclasses(c)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t \n\tsubs = []\n\tfor sub in cls.__subclasses__():\n\t\tsubs.append(sub)\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn type(cls.__name__, cls.__bases__, dict(cls.__dict__))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t \n\treturn set(cls.__subclasses__()).union(\n\t\t[s for c in cls.__subclasses__() for s in subclasses(c)])\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\n\treturn [subcls for subcls in cls.__subclasses__() if subcls.__name__!= cls.__name__]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif string == '':\n\t\treturn 0\n\tif string[0] not in alphabet:\n\t\treturn 0\n\n\treturn int(string[0])*pow(len(alphabet), len(string)-1) + string_to_int(string[1:], alphabet)\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n```\n\n## Test\n\nYou can run the unit tests by typing:\n\n```\npython3 -m unittest tests\n```\n\n## License\n\nThis package is licensed with the MIT License. Check the [LICENSE](LICENSE) file for more information.\n/tests/test_alphabet_helper.py\nimport unittest\n\nfrom src.alphabet_helper import AlphabetHelper\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif string == '':\n\t\treturn 0\n\n\tif not isinstance(string, str):\n\t\traise ValueError('input should be string')\n\n\tif not isinstance(alphabet, list):\n\t\traise ValueError('alphabet should be list')\n\n\tif not all(isinstance(i, str) for i in alphabet):\n\t\traise ValueError('alphabet should be string')\n\n\tif len(alphabet) < 2:\n\t\traise ValueError('alphabet should contain at least 2 letters')\n\n\tif not all(len(i) == 1 for i in alphabet):\n\t\traise ValueError('alphabet should contain letters')\n\n\tif not all(i in string for i in alphabet):\n\t\traise ValueError('string should contain only letters in alphabet')\n\n\treturn alphabet.index(string[0]) * 10 ** (len(string) - 1) + string_to_int(string[1:], alphabet)\n\n\n# def int_to_string(intg: int, alphabet: List[str]) -> str:\n# \t\"\"\"\n# \tConvert a number to a string, using the given alphabet.\n#     :param intg: positive integer\n#     :param alphabet: list of letters\n#     :return: str, the corresponding string of the given number using the given transition rule.\n# \t\"\"\"\n\n\n# \tif not isinstance(intg, int", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\t# convert the string to list of ints using the given alphabet\n\tint_list = [alphabet.index(i) for i in string]\n\n\t# convert the list of ints to int using binary\n\tdecimal = int(''.join(str(i) for i in int_list), 2)\n\treturn decimal\n\n\nif __name__ == '__main__':\n\tmain()\n#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <vector>\n#include <map>\n#include <set>\n#include <iterator>\n#include <algorithm>\n#include <climits>\n#include <cstdlib>\n\nusing namespace std;\n\n\nint main(int argc, char* argv[]){\n  // input file\n  ifstream input;\n  input.open(argv[1]);\n\n  // output file\n  ofstream output;\n  output.open(argv[2]);\n\n  // read the input file\n  int n;\n  input>>n;\n  vector<int> v(n);\n  for(int i=0; i<n; i++){\n    input>>v[i];\n  }\n\n  // first element is the min value\n  int min = v[0];\n  // second element is the max value\n  int max = v[0];\n  // third element is the sum of all the values\n  int sum = v[0", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\treturn 0\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif string == \"\":\n\t\treturn 0\n\n\tif string in alphabet:\n\t\treturn alphabet.index(string)\n\n\treturn (string_to_int(string[:-1], alphabet) * len(alphabet)) + string_to_int(string[-1], alphabet)\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif not isinstance(string, str):\n\t\traise TypeError('string must be a string')\n\tif not isinstance(alphabet, list):\n\t\traise TypeError('alphabet must be a list')\n\tif len(string) == 0:\n\t\traise ValueError('string must be greater than 0')\n\tif len(alphabet) == 0:\n\t\traise ValueError('alphabet must be greater than 0')\n\tfor letter in alphabet:\n\t\tif not isinstance(letter, str):\n\t\t\traise TypeError('alphabet must be a list of strings')\n\tfor letter in string:\n\t\tif letter not in alphabet:\n\t\t\traise ValueError('string must be composed of alphabet letters')\n\n\treturn int(''.join([str(alphabet.index(letter)) for letter in string]))\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t...\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\n\n\n\treturn 0\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif string == '':\n\t\treturn 0\n\tif string[0] not in alphabet:\n\t\treturn 0\n\n\treturn int(string[0])*pow(len(alphabet), len(string)-1) + string_to_int(string[1:], alphabet)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n```\n\n## Test\n\nYou can run the unit tests by typing:\n\n```\npython3 -m unittest tests\n```\n\n## License\n\nThis package is licensed with the MIT License. Check the [LICENSE](LICENSE) file for more information.\n/tests/test_alphabet_helper.py\nimport unittest\n\nfrom src.alphabet_helper import AlphabetHelper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif string == '':\n\t\treturn 0\n\n\tif not isinstance(string, str):\n\t\traise ValueError('input should be string')\n\n\tif not isinstance(alphabet, list):\n\t\traise ValueError('alphabet should be list')\n\n\tif not all(isinstance(i, str) for i in alphabet):\n\t\traise ValueError('alphabet should be string')\n\n\tif len(alphabet) < 2:\n\t\traise ValueError('alphabet should contain at least 2 letters')\n\n\tif not all(len(i) == 1 for i in alphabet):\n\t\traise ValueError('alphabet should contain letters')\n\n\tif not all(i in string for i in alphabet):\n\t\traise ValueError('string should contain only letters in alphabet')\n\n\treturn alphabet.index(string[0]) * 10 ** (len(string) - 1) + string_to_int(string[1:], alphabet)\n\n\n# def int_to_string(intg: int, alphabet: List[str]) -> str:\n# \t\"\"\"\n# \tConvert a number to a string, using the given alphabet.\n#     :param intg: positive integer\n#     :param alphabet: list of letters\n#     :return: str, the corresponding string of the given number using the given transition rule.\n# \t\"\"\"\n\n\n# \tif not isinstance(intg, int", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\t# convert the string to list of ints using the given alphabet\n\tint_list = [alphabet.index(i) for i in string]\n\n\t# convert the list of ints to int using binary\n\tdecimal = int(''.join(str(i) for i in int_list), 2)\n\treturn decimal\n\n\nif __name__ == '__main__':\n\tmain()\n#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <vector>\n#include <map>\n#include <set>\n#include <iterator>\n#include <algorithm>\n#include <climits>\n#include <cstdlib>\n\nusing namespace std;\n\n\nint main(int argc, char* argv[]){\n  // input file\n  ifstream input;\n  input.open(argv[1]);\n\n  // output file\n  ofstream output;\n  output.open(argv[2]);\n\n  // read the input file\n  int n;\n  input>>n;\n  vector<int> v(n);\n  for(int i=0; i<n; i++){\n    input>>v[i];\n  }\n\n  // first element is the min value\n  int min = v[0];\n  // second element is the max value\n  int max = v[0];\n  // third element is the sum of all the values\n  int sum = v[0", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\treturn 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif string == \"\":\n\t\treturn 0\n\n\tif string in alphabet:\n\t\treturn alphabet.index(string)\n\n\treturn (string_to_int(string[:-1], alphabet) * len(alphabet)) + string_to_int(string[-1], alphabet)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\tif not isinstance(string, str):\n\t\traise TypeError('string must be a string')\n\tif not isinstance(alphabet, list):\n\t\traise TypeError('alphabet must be a list')\n\tif len(string) == 0:\n\t\traise ValueError('string must be greater than 0')\n\tif len(alphabet) == 0:\n\t\traise ValueError('alphabet must be greater than 0')\n\tfor letter in alphabet:\n\t\tif not isinstance(letter, str):\n\t\t\traise TypeError('alphabet must be a list of strings')\n\tfor letter in string:\n\t\tif letter not in alphabet:\n\t\t\traise ValueError('string must be composed of alphabet letters')\n\n\treturn int(''.join([str(alphabet.index(letter)) for letter in string]))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\t\n\n\n\n\n\treturn 0\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        print(f\"Downloading repo archive from {url} to {destination_path}...\")\n\n        response = requests.get(url)\n        if response.status_code!= 200:\n            raise ValueError(\"Non 200 response code received from server\")\n\n        with open(destination_path, 'wb') as f:\n            f.write(response.content)\n\n        print(\"Archive downloaded and saved\")\n        print(\"Extracting archive...\")\n\n        with tarfile.open(destination_path, 'r:gz') as tar:\n            tar.extractall(destination_path)\n\n        return destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # download archive\n        print(\"downloading archive\")\n        subprocess.run([\"wget\", url, \"-P\", str(destination_path)])\n        # extract archive\n        print(\"extracting archive\")\n        subprocess.run([\"tar\", \"-xzf\", str(destination_path), \"-C\", str(destination_path)])\n        return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        filename = Path(url).name\n        archive_path = destination_path / filename\n        if not archive_path.exists():\n                print(f\"Downloading {filename}\")\n                response = requests.get(url, stream=True)\n                with open(archive_path, 'wb') as f:\n                        shutil.copyfileobj(response.raw, f)\n\t\n        print(f\"Extracting {filename} to {destination_path}\")\n        with tarfile.open(archive_path) as tar:\n                tar.extractall(path=destination_path)\n        return destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # Download archive\n        archive_file = destination_path.joinpath(url.split('/')[-1])\n        if not archive_file.exists():\n            print(\"Downloading {}...\".format(archive_file))\n            download_file(url, archive_file)\n\n        # Unpack archive\n        if not destination_path.exists():\n            destination_path.mkdir()\n        print(\"Extracting {}...\".format(archive_file))\n        with tarfile.open(archive_file) as tar:\n            tar.extractall(path=destination_path)\n        archive_file.unlink()\n        return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        if not destination_path.exists():\n                destination_path.mkdir(parents=True, exist_ok=True)\n\n        tar_path = destination_path.joinpath(Path(url).name)\n        if not tar_path.exists():\n                print(\"Downloading %s\" % url)\n                with urllib.request.urlopen(url) as response, open(tar_path, 'wb') as out_file:\n                        shutil.copyfileobj(response, out_file)\n\n        print(\"Extracting %s\" % tar_path)\n        with tarfile.open(tar_path, 'r:gz') as tar:\n                tar.extractall(path=destination_path)\n\n        return tar_path.parent.joinpath(tar_path.stem)\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # Download the tar.gz archive\n\tdownload_file_to_path(url, destination_path)\n\t\n        # Get the path of the.tar.gz archive\n\ttar_gz_path = destination_path / url.split(\"/\")[-1]\n\t\n        # Extract the archive to the destination path\n\textract_archive(tar_gz_path, destination_path)\n\t\n        # Return the path where the archive was extracted\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n\n\t# download the archive\n\tdownload_archive(url, destination_path)\n\t\n\t# extract the archive\n\textract_archive(destination_path)\n\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        logger.info('Downloading %s', url)\n        downloaded_file_path = destination_path.joinpath(Path(url).name)\n        downloaded_file_path.parent.mkdir(parents=True, exist_ok=True)\n        download_file(url, downloaded_file_path)\n        logger.info('Downloaded %s', downloaded_file_path)\n\n        logger.info('Extracting %s', downloaded_file_path)\n        extracted_path = destination_path.joinpath(Path(url).stem)\n        extracted_path.mkdir(parents=True, exist_ok=True)\n        extract_archive(downloaded_file_path, extracted_path)\n        logger.info('Extracted %s to %s', downloaded_file_path, extracted_path)\n\n        return extracted_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # download the tar.gz archive to the destination path\n        filename = url.split(\"/\")[-1]\n        with open(destination_path / filename, \"wb\") as f:\n            r = requests.get(url, stream=True)\n            total_length = r.headers.get('content-length')\n\n            if total_length is None: # no content length header\n                f.write(r.content)\n            else:\n                dl = 0\n                total_length = int(total_length)\n                for data in r.iter_content(chunk_size=4096):\n                    dl += len(data)\n                    f.write(data)\n\n                    done = int(50 * dl / total_length)\n                    sys.stdout.write(\"\\r[%s%s]\" % ('=' * done,'' * (50-done)))\n                    sys.stdout.flush()\n\n        # extract the archive\n        print(\"\\nextracting archive\")\n        tar = tarfile.open(destination_path / filename, \"r:gz\")\n        tar.extractall(path=destination_path)\n\n        # delete the tar.gz archive\n        print(\"deleting archive\")\n        (destination_path / filename).unlink()\n\n        # make sure that the destination path exists\n        if not destination_path.exists():\n            destination_path.mkdir()\n\n        return destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        print(f\"Downloading {url}\")\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as archive_file:\n            with closing(urlopen(url)) as archive_file_http:\n                shutil.copyfileobj(archive_file_http, archive_file)\n                archive_file.seek(0)\n                tar = tarfile.open(fileobj=archive_file, mode=\"r:gz\")\n                tar.extractall(path=destination_path)\n                return destination_path\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        print(f\"Downloading repo archive from {url} to {destination_path}...\")\n\n        response = requests.get(url)\n        if response.status_code!= 200:\n            raise ValueError(\"Non 200 response code received from server\")\n\n        with open(destination_path, 'wb') as f:\n            f.write(response.content)\n\n        print(\"Archive downloaded and saved\")\n        print(\"Extracting archive...\")\n\n        with tarfile.open(destination_path, 'r:gz') as tar:\n            tar.extractall(destination_path)\n\n        return destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # download archive\n        print(\"downloading archive\")\n        subprocess.run([\"wget\", url, \"-P\", str(destination_path)])\n        # extract archive\n        print(\"extracting archive\")\n        subprocess.run([\"tar\", \"-xzf\", str(destination_path), \"-C\", str(destination_path)])\n        return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        filename = Path(url).name\n        archive_path = destination_path / filename\n        if not archive_path.exists():\n                print(f\"Downloading {filename}\")\n                response = requests.get(url, stream=True)\n                with open(archive_path, 'wb') as f:\n                        shutil.copyfileobj(response.raw, f)\n\t\n        print(f\"Extracting {filename} to {destination_path}\")\n        with tarfile.open(archive_path) as tar:\n                tar.extractall(path=destination_path)\n        return destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # Download archive\n        archive_file = destination_path.joinpath(url.split('/')[-1])\n        if not archive_file.exists():\n            print(\"Downloading {}...\".format(archive_file))\n            download_file(url, archive_file)\n\n        # Unpack archive\n        if not destination_path.exists():\n            destination_path.mkdir()\n        print(\"Extracting {}...\".format(archive_file))\n        with tarfile.open(archive_file) as tar:\n            tar.extractall(path=destination_path)\n        archive_file.unlink()\n        return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        if not destination_path.exists():\n                destination_path.mkdir(parents=True, exist_ok=True)\n\n        tar_path = destination_path.joinpath(Path(url).name)\n        if not tar_path.exists():\n                print(\"Downloading %s\" % url)\n                with urllib.request.urlopen(url) as response, open(tar_path, 'wb') as out_file:\n                        shutil.copyfileobj(response, out_file)\n\n        print(\"Extracting %s\" % tar_path)\n        with tarfile.open(tar_path, 'r:gz') as tar:\n                tar.extractall(path=destination_path)\n\n        return tar_path.parent.joinpath(tar_path.stem)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # Download the tar.gz archive\n\tdownload_file_to_path(url, destination_path)\n\t\n        # Get the path of the.tar.gz archive\n\ttar_gz_path = destination_path / url.split(\"/\")[-1]\n\t\n        # Extract the archive to the destination path\n\textract_archive(tar_gz_path, destination_path)\n\t\n        # Return the path where the archive was extracted\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n\n\t# download the archive\n\tdownload_archive(url, destination_path)\n\t\n\t# extract the archive\n\textract_archive(destination_path)\n\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        logger.info('Downloading %s', url)\n        downloaded_file_path = destination_path.joinpath(Path(url).name)\n        downloaded_file_path.parent.mkdir(parents=True, exist_ok=True)\n        download_file(url, downloaded_file_path)\n        logger.info('Downloaded %s', downloaded_file_path)\n\n        logger.info('Extracting %s', downloaded_file_path)\n        extracted_path = destination_path.joinpath(Path(url).stem)\n        extracted_path.mkdir(parents=True, exist_ok=True)\n        extract_archive(downloaded_file_path, extracted_path)\n        logger.info('Extracted %s to %s', downloaded_file_path, extracted_path)\n\n        return extracted_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        # download the tar.gz archive to the destination path\n        filename = url.split(\"/\")[-1]\n        with open(destination_path / filename, \"wb\") as f:\n            r = requests.get(url, stream=True)\n            total_length = r.headers.get('content-length')\n\n            if total_length is None: # no content length header\n                f.write(r.content)\n            else:\n                dl = 0\n                total_length = int(total_length)\n                for data in r.iter_content(chunk_size=4096):\n                    dl += len(data)\n                    f.write(data)\n\n                    done = int(50 * dl / total_length)\n                    sys.stdout.write(\"\\r[%s%s]\" % ('=' * done,'' * (50-done)))\n                    sys.stdout.flush()\n\n        # extract the archive\n        print(\"\\nextracting archive\")\n        tar = tarfile.open(destination_path / filename, \"r:gz\")\n        tar.extractall(path=destination_path)\n\n        # delete the tar.gz archive\n        print(\"deleting archive\")\n        (destination_path / filename).unlink()\n\n        # make sure that the destination path exists\n        if not destination_path.exists():\n            destination_path.mkdir()\n\n        return destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the.tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\t\n        print(f\"Downloading {url}\")\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as archive_file:\n            with closing(urlopen(url)) as archive_file_http:\n                shutil.copyfileobj(archive_file_http, archive_file)\n                archive_file.seek(0)\n                tar = tarfile.open(fileobj=archive_file, mode=\"r:gz\")\n                tar.extractall(path=destination_path)\n                return destination_path\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/systeminfo.py", "project": "cloudmesh/cloudmesh-common", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn True if sys.platform == 'darwin' else False\n/src/test/test_utils.py\nimport os\nimport tempfile\nimport unittest\n\nfrom src.utils import os_is_mac, os_is_windows, os_is_linux, get_temp_dir, set_temp_dir\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn sys.platform == 'darwin'\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t\n#\treturn 'darwin' in sys.platform\n\treturn platform.system() == 'Darwin'\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t\n#\tif sys.platform == \"darwin\":\n#\t\treturn True\n#\telse:\n#\t\treturn False\n\t\t"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn True if sys.platform == 'darwin' else False\n/src/test/test_utils.py\nimport os\nimport tempfile\nimport unittest\n\nfrom src.utils import os_is_mac, os_is_windows, os_is_linux, get_temp_dir, set_temp_dir\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn sys.platform == 'darwin'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t\n#\treturn 'darwin' in sys.platform\n\treturn platform.system() == 'Darwin'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t \n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t\n#\tif sys.platform == \"darwin\":\n#\t\treturn True\n#\telse:\n#\t\treturn False\n\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\t#print(\"item: \",item)\n\tfor k in item.keys():\n\t\tif k.find(\"*\") > -1:\n\t\t\titem[re.compile(fnmatch.translate(k))] = item[k]\n\t\t\tdel item[k]\n\treturn(item)\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\treturn {fnmatch.translate(k): v for k, v in item.items()}\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\n\n\tregex_dict = {}\n\tfor k,v in item.items():\n\t\tregex_dict[re.compile(k)] = v\n\treturn regex_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\t# if not isinstance(item, dict):\n\t#     raise TypeError('Not a dict')\n\tregex_keys = {}\n\tfor key in item:\n\t\tregex_key = re.compile(key)\n\t\tregex_keys[regex_key] = item[key]\n\treturn regex_keys\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\tregex_dict = {}\n\t\tfor key in item:\n\t\t\tregex_dict[re.compile(r\"^\" + key.replace(\"*\", \".*\") + \"$\")] = item[key]\n\t\treturn regex_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\n#\tprint('regex_dict')\n#\tprint(item)\n\tnew_dict = {}\n\tfor key in item.keys():\n\t\tnew_dict[re.compile(key)] = item[key]\n#\tprint(new_dict)\n\treturn new_dict\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\treturn {re.compile(regexify(key)): value for key, value in item.items()}\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\t# Convert the keys to regexes\n\tregex_keys = {}\n\tfor key in item.keys():\n\t\tregex_keys[re.compile(key)] = item[key]\n\treturn regex_keys\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\n\n\tkeys = item.keys()\n\tregex_dict = {}\n\n\tfor key in keys:\n\t\tregex_dict[re.compile(key)] = item[key]\n\n\treturn regex_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t \n\treturn {fnmatch.translate(k):v for k,v in item.items()}\n\n\n# def find_files(filename, directory):\n# \t\"\"\"\n# \tFind files\n# \tGiven a filename and a directory, return the list of files that match the filename.\n# \n# \tArgs:\n# \t    filename: filename to search for\n# \t    directory: directory to search in\n# \tReturns:\n# \t    list of filenames in directory matching filename\n# \t\"\"\"\n# \t \n# \tif not isinstance(filename, str):\n# \t\traise TypeError('filename must be a str')\n# \tif not isinstance(directory, str):\n# \t\traise TypeError('directory must be a str')\n# \t \n# \tif not os.path.isdir(directory):\n# \t\traise TypeError('directory is not a valid directory')\n# \n# \tresult = []\n# \tfor root, dirs, files in os.walk(directory):\n# \t\tfor name in files:\n# \t\t\tif fnmatch.fnmatch(name, filename):\n# \t\t\t\tresult.append(os.path.join(root, name))\n# \n# \treturn result\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\t#print(\"item: \",item)\n\tfor k in item.keys():\n\t\tif k.find(\"*\") > -1:\n\t\t\titem[re.compile(fnmatch.translate(k))] = item[k]\n\t\t\tdel item[k]\n\treturn(item)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\treturn {fnmatch.translate(k): v for k, v in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\n\n\tregex_dict = {}\n\tfor k,v in item.items():\n\t\tregex_dict[re.compile(k)] = v\n\treturn regex_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\t# if not isinstance(item, dict):\n\t#     raise TypeError('Not a dict')\n\tregex_keys = {}\n\tfor key in item:\n\t\tregex_key = re.compile(key)\n\t\tregex_keys[regex_key] = item[key]\n\treturn regex_keys\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\tregex_dict = {}\n\t\tfor key in item:\n\t\t\tregex_dict[re.compile(r\"^\" + key.replace(\"*\", \".*\") + \"$\")] = item[key]\n\t\treturn regex_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\n#\tprint('regex_dict')\n#\tprint(item)\n\tnew_dict = {}\n\tfor key in item.keys():\n\t\tnew_dict[re.compile(key)] = item[key]\n#\tprint(new_dict)\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\treturn {re.compile(regexify(key)): value for key, value in item.items()}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\t# Convert the keys to regexes\n\tregex_keys = {}\n\tfor key in item.keys():\n\t\tregex_keys[re.compile(key)] = item[key]\n\treturn regex_keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t\n\n\tkeys = item.keys()\n\tregex_dict = {}\n\n\tfor key in keys:\n\t\tregex_dict[re.compile(key)] = item[key]\n\n\treturn regex_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\t \n\treturn {fnmatch.translate(k):v for k,v in item.items()}\n\n\n# def find_files(filename, directory):\n# \t\"\"\"\n# \tFind files\n# \tGiven a filename and a directory, return the list of files that match the filename.\n# \n# \tArgs:\n# \t    filename: filename to search for\n# \t    directory: directory to search in\n# \tReturns:\n# \t    list of filenames in directory matching filename\n# \t\"\"\"\n# \t \n# \tif not isinstance(filename, str):\n# \t\traise TypeError('filename must be a str')\n# \tif not isinstance(directory, str):\n# \t\traise TypeError('directory must be a str')\n# \t \n# \tif not os.path.isdir(directory):\n# \t\traise TypeError('directory is not a valid directory')\n# \n# \tresult = []\n# \tfor root, dirs, files in os.walk(directory):\n# \t\tfor name in files:\n# \t\t\tif fnmatch.fnmatch(name, filename):\n# \t\t\t\tresult.append(os.path.join(root, name))\n# \n# \treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rdiffweb/core/librdiff.py", "project": "ikus060/rdiffweb", "name": "unquote", "docstring": "Remove quote from the given name.", "_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\t#remove quote\n\tname = re.sub(r'\\\"', '', name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\n\tpattern = r'^\"'\n\tmatch = re.search(pattern, name)\n\tif match:\n\t\tname = name[1:]\n\n\tpattern = r'\"$'\n\tmatch = re.search(pattern, name)\n\tif match:\n\t\tname = name[:-1]\n\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\t#print(name)\n\treturn re.sub(r\"^'(.*)'$\", r\"\\1\", name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\t#pattern = re.compile(r\"^\\\"(.*)\\\"$\")\n\t\t#match = pattern.search(name)\n\t\t#if match:\n\t\t#\tname = match.group(1)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\n\treturn name.replace('\"', '')\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\n\tname = re.sub(r\"^u?\", \"\", name)\n\tname = re.sub(r\"^'\", \"\", name)\n\tname = re.sub(r'\"$', \"\", name)\n\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t \n\treturn re.sub(\"[\\\"']\", \"\", name)\n\n#\n#\tGet the file name from the given file path\n#", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\treturn re.sub(r'\\\"',\"\",name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t \n\t# compile the regular expression\n\tregex = re.compile('^\\\"(.*)\\\"$')\n\t\n\t# unquote name\n\treturn regex.sub(r'\\1', name)import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabaz_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\t#remove quote\n\tname = re.sub(r'\\\"', '', name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\n\tpattern = r'^\"'\n\tmatch = re.search(pattern, name)\n\tif match:\n\t\tname = name[1:]\n\n\tpattern = r'\"$'\n\tmatch = re.search(pattern, name)\n\tif match:\n\t\tname = name[:-1]\n\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\t#print(name)\n\treturn re.sub(r\"^'(.*)'$\", r\"\\1\", name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\t#pattern = re.compile(r\"^\\\"(.*)\\\"$\")\n\t\t#match = pattern.search(name)\n\t\t#if match:\n\t\t#\tname = match.group(1)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\n\treturn name.replace('\"', '')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\n\n\tname = re.sub(r\"^u?\", \"\", name)\n\tname = re.sub(r\"^'\", \"\", name)\n\tname = re.sub(r'\"$', \"\", name)\n\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t \n\treturn re.sub(\"[\\\"']\", \"\", name)\n\n#\n#\tGet the file name from the given file path\n#", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t\treturn re.sub(r'\\\"',\"\",name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\t \n\t# compile the regular expression\n\tregex = re.compile('^\\\"(.*)\\\"$')\n\t\n\t# unquote name\n\treturn regex.sub(r'\\1', name)import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabaz_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/shlex.py", "project": "cloudmesh/cloudmesh-common", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\n\tif platform == 'this':\n\t\t# detect platform\n\t\timport sys\n\t\tif sys.platform.startswith('win'):\n\t\t\treturn split(s, 0)\n\t\telse:\n\t\t\treturn split(s, 1)\n\telif platform == 1:\n\t\t# POSIX style\n\t\treturn s.split(os.path.pathsep)\n\telif platform == 0:\n\t\t# Windows/CMD style\n\t\treturn s.split(os.pathsep)\n/bin/git/git-sub-rebase.sh\n#!/bin/sh\n#\n# Rebase all submodules under current git directory\n#\n# Author:  <>\n#\n# Version: 1.0.1\n#\n# License: GPLv3\n#\n# Reference:\n#     https://stackoverflow.com/questions/5732939/git-rebase-all-submodules-in-a-repository\n\ngit submodule foreach 'git fetch'  # fetch all submodules\ngit submodule foreach 'git rebase'  # rebase all submodules\n/lib/python/pytools/pytools/path.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2017  <>\n#\n# Licensed under the GNU General", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n    \n\tif platform == 'this':\n\t\tif os.name == 'nt':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\t\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception('split(): Invalid platform: %s' % platform)\n\t\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tif os.name == 'nt':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 0:\n\t\treturn re.split(r'[\\\\\\/]', s)\n\telif platform == 1:\n\t\treturn re.split(r'\\/', s)\n\telse:\n\t\traise ValueError('Unsupported platform: %s' % platform)\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tif sys.platform[:3] == 'win':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception('Invalid platform number.')\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tplatform = sys.platform\n\telif platform == 1:\n\t\tplatform = 'posix'\n\telif platform == 0:\n\t\tplatform = 'nt'\n\telif platform == 'win':\n\t\tplatform = 'nt'\n\telif platform == 'linux':\n\t\tplatform = 'posix'\n\telif platform == 'linux2':\n\t\tplatform = 'posix'\n\telif platform == 'darwin':\n\t\tplatform = 'posix'\n\telif platform == 'cygwin':\n\t\tplatform = 'posix'\n\telif platform == 'os2':\n\t\tplatform = 'posix'\n\telif platform == 'os2emx':\n\t\tplatform = 'posix'\n\telif platform == 'riscos':\n\t\tplatform = 'posix'\n\telse:\n\t\traise Exception(\"Unknown platform: \" + platform)\n\n\tif platform == 'nt':\n\t\treturn s.split('\\\\')\n\telse:\n\t\treturn s.split('/')\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\n\tif platform == 'this':\n\t\tif sys.platform.startswith('win'):\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\treturn 'Invalid platform %s' % platform\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t/README.md\n# py-utils\nA utils library for python.\n/test/test_utils_py.py\n#!/usr/bin/env python\n# encoding: utf-8\n\nimport sys\nimport os\nimport unittest\n\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\nimport utils.py as py_utils\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\n\tif platform == 'this':\n\t\tplatform = sys.platform\n\telif platform == '1':\n\t\tplatform = 1\n\telif platform == '0':\n\t\tplatform = 0\n\telse:\n\t\traise Exception('Unknown platform: %s' % platform)\n\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception('Unknown platform: %s' % platform)\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tplatform = 1 if os.name == 'posix' else 0\n\n\tif platform == 1: # POSIX\n\t\treturn s.split('/')\n\telse: # Windows\n\t\treturn s.split('\\\\')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\n\tif platform == 'this':\n\t\t# detect platform\n\t\timport sys\n\t\tif sys.platform.startswith('win'):\n\t\t\treturn split(s, 0)\n\t\telse:\n\t\t\treturn split(s, 1)\n\telif platform == 1:\n\t\t# POSIX style\n\t\treturn s.split(os.path.pathsep)\n\telif platform == 0:\n\t\t# Windows/CMD style\n\t\treturn s.split(os.pathsep)\n/bin/git/git-sub-rebase.sh\n#!/bin/sh\n#\n# Rebase all submodules under current git directory\n#\n# Author:  <>\n#\n# Version: 1.0.1\n#\n# License: GPLv3\n#\n# Reference:\n#     https://stackoverflow.com/questions/5732939/git-rebase-all-submodules-in-a-repository\n\ngit submodule foreach 'git fetch'  # fetch all submodules\ngit submodule foreach 'git rebase'  # rebase all submodules\n/lib/python/pytools/pytools/path.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2017  <>\n#\n# Licensed under the GNU General", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n    \n\tif platform == 'this':\n\t\tif os.name == 'nt':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\t\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception('split(): Invalid platform: %s' % platform)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tif os.name == 'nt':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 0:\n\t\treturn re.split(r'[\\\\\\/]', s)\n\telif platform == 1:\n\t\treturn re.split(r'\\/', s)\n\telse:\n\t\traise ValueError('Unsupported platform: %s' % platform)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tif sys.platform[:3] == 'win':\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception('Invalid platform number.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tplatform = sys.platform\n\telif platform == 1:\n\t\tplatform = 'posix'\n\telif platform == 0:\n\t\tplatform = 'nt'\n\telif platform == 'win':\n\t\tplatform = 'nt'\n\telif platform == 'linux':\n\t\tplatform = 'posix'\n\telif platform == 'linux2':\n\t\tplatform = 'posix'\n\telif platform == 'darwin':\n\t\tplatform = 'posix'\n\telif platform == 'cygwin':\n\t\tplatform = 'posix'\n\telif platform == 'os2':\n\t\tplatform = 'posix'\n\telif platform == 'os2emx':\n\t\tplatform = 'posix'\n\telif platform == 'riscos':\n\t\tplatform = 'posix'\n\telse:\n\t\traise Exception(\"Unknown platform: \" + platform)\n\n\tif platform == 'nt':\n\t\treturn s.split('\\\\')\n\telse:\n\t\treturn s.split('/')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\n\tif platform == 'this':\n\t\tif sys.platform.startswith('win'):\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\treturn 'Invalid platform %s' % platform\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t/README.md\n# py-utils\nA utils library for python.\n/test/test_utils_py.py\n#!/usr/bin/env python\n# encoding: utf-8\n\nimport sys\nimport os\nimport unittest\n\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\nimport utils.py as py_utils\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t\n\n\tif platform == 'this':\n\t\tplatform = sys.platform\n\telif platform == '1':\n\t\tplatform = 1\n\telif platform == '0':\n\t\tplatform = 0\n\telse:\n\t\traise Exception('Unknown platform: %s' % platform)\n\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception('Unknown platform: %s' % platform)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\t \n\tif platform == 'this':\n\t\tplatform = 1 if os.name == 'posix' else 0\n\n\tif platform == 1: # POSIX\n\t\treturn s.split('/')\n\telse: # Windows\n\t\treturn s.split('\\\\')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/tests/__init__.py", "project": "SoftwareHeritage/swh-lister", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t \n\ttry:\n\t\tif filename is None:\n\t\t\tfilename = os.path.basename(archive_path)\n\t\tif not os.path.exists(tmp_path):\n\t\t\tos.makedirs(tmp_path)\n\texcept Exception as e:\n\t\traise e\n\n\tif not archive_path.endswith(\".zip\"):\n\t\traise Exception(\"Archive file must be a zip file\")\n\n\tfilename = os.path.join(tmp_path, filename)\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(tmp_path)\n\n\treturn filename\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\t# TODO: this assumes that the archive is a zip file.\n\t\t# Should check for the file type and raise an exception if it is not a zip file.\n\t# Get the file name from the path if not specified\n\tif not filename:\n\t\tfilename = archive_path.name\n\n\t# Uncompress the file\n\ttmp_file_path = PosixPath(tmp_path) / filename.replace(\".zip\", \"\")\n\tif tmp_file_path.exists():\n\t\tshutil.rmtree(str(tmp_file_path))\n\ttmp_file_path.mkdir(parents=True, exist_ok=True)\n\n\t# Unzip the file\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(str(tmp_file_path))\n\t\t# Get the name of the extracted folder\n\t\textracted_file = os.listdir(str(tmp_file_path))[0]\n\t\t# Return the repo url\n\t\treturn f\"{tmp_file_path}/{extracted_file}\"\n/tests/test_setup.py\nimport os\nimport subprocess\nimport tempfile\nfrom pathlib import Path\n\nimport pytest\n\nimport github_actions_toolkit as toolkit\n\n# Test config\nTEST_REPO_URL = \"https://github.com/actions/toolkit", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif not os.path.exists(archive_path):\n\t\traise Exception(f\"File {archive_path} does not exist.\")\n\n\t# Check if filename is provided.\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\t# Extract the file path.\n\textract_path = os.path.join(tmp_path, filename)\n\textract_path = PosixPath(extract_path)\n\n\t# Create the directory if it does not exist.\n\tif not extract_path.parent.exists():\n\t\textract_path.parent.mkdir(parents=True, exist_ok=True)\n\n\t# Extract the file.\n\twith tarfile.open(archive_path, \"r\") as tar:\n\t\ttar.extractall(path=extract_path)\n\n\t# Return the repo url.\n\treturn extract_path.as_uri()\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\t# check if the archive exists\n\tif not os.path.exists(archive_path):\n\t\traise Exception(f\"Archive {archive_path} does not exist\")\n\n\t# check if filename exists\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\t# Check if the file already exists\n\tif os.path.exists(tmp_path / filename):\n\t\traise Exception(f\"File {tmp_path / filename} already exists\")\n\n\t# Uncompress the archive\n\twith open(archive_path, \"rb\") as f_in:\n\t\twith open(tmp_path / filename, \"wb\") as f_out:\n\t\t\tshutil.copyfileobj(f_in, f_out)\n\n\trepo_url = f\"file:///{tmp_path / filename}\"\n\treturn repo_url\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\t# Create a temporary directory to uncompress the archive into.\n\ttmp_dir = mkdtemp(dir=tmp_path)\n\n\t# Uncompress the archive into the temporary directory.\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(tmp_dir)\n\n\t# Return the repo url of the uncompressed repository.\n\treturn git.Repo.init(tmp_dir).working_tree_dir\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\tassert filename\n\n\ttarget_path = os.path.join(tmp_path, filename)\n\n\tif not os.path.exists(target_path):\n\t\twith open(archive_path, \"rb\") as f:\n\t\t\twith open(target_path, \"wb\") as f_w:\n\t\t\t\tf_w.write(f.read())\n\n\tif not os.path.exists(target_path):\n\t\traise Exception(\"Archive not found: {}\".format(archive_path))\n\n\tif os.path.isdir(target_path):\n\t\traise Exception(\"Target path is a directory: {}\".format(target_path))\n\n\tif not os.path.isfile(target_path):\n\t\traise Exception(\"Target path is not a file: {}\".format(target_path))\n\n\tif not os.path.exists(target_path):\n\t\traise Exception(\"Target path does not exist: {}\".format(target_path))\n\n\t# Uncompress archive\n\tif archive_path.endswith(\".zip\"):\n\t\twith zipfile.ZipFile(target_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(tmp_path)\n\telif archive_path.endswith(\".gz\"):\n\t\twith gzip.open(target_path, \"", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\t# TODO: add exception handling\n\t# TODO: remove the tmp_path hardcoding\n\t# TODO: remove filename\n\t# TODO: remove the hardcoding\n\t# TODO: add type hints\n\t# TODO: add comment\n\t# TODO: remove the hardcoding\n\t# TODO: add exception handling\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\trepo_path = os.path.join(tmp_path, filename)\n\tif not os.path.exists(repo_path):\n\t\trepo_path = os.path.join(tmp_path, \"temp_repo\")\n\n\t# TODO: Implement me\n\n\treturn repo_path\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\tfilename_without_ext = os.path.splitext(filename)[0]\n\ttmp_dir = os.path.join(tmp_path, filename_without_ext)\n\ttry:\n\t\tos.mkdir(tmp_dir)\n\texcept FileExistsError:\n\t\tpass\n\tarchive_path_with_tmp = os.path.join(tmp_dir, filename)\n\tshutil.copyfile(archive_path, archive_path_with_tmp)\n\tos.chdir(tmp_dir)\n\ttar_archive = tarfile.open(archive_path_with_tmp)\n\ttar_archive.extractall()\n\treturn os.path.join(\"file://\", tmp_dir, filename_without_ext)\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif archive_path.endswith(\".zip\"):\n\t\textract_path = os.path.join(tmp_path, \"extracted_zip\")\n\t\tzip_ref = zipfile.ZipFile(archive_path, \"r\")\n\t\tzip_ref.extractall(extract_path)\n\t\tzip_ref.close()\n\t\tif filename:\n\t\t\treturn os.path.join(extract_path, filename)\n\t\telse:\n\t\t\treturn extract_path\n\telse:\n\t\treturn archive_path\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t \n\ttry:\n\t\tif filename is None:\n\t\t\tfilename = os.path.basename(archive_path)\n\t\tif not os.path.exists(tmp_path):\n\t\t\tos.makedirs(tmp_path)\n\texcept Exception as e:\n\t\traise e\n\n\tif not archive_path.endswith(\".zip\"):\n\t\traise Exception(\"Archive file must be a zip file\")\n\n\tfilename = os.path.join(tmp_path, filename)\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(tmp_path)\n\n\treturn filename\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\t# TODO: this assumes that the archive is a zip file.\n\t\t# Should check for the file type and raise an exception if it is not a zip file.\n\t# Get the file name from the path if not specified\n\tif not filename:\n\t\tfilename = archive_path.name\n\n\t# Uncompress the file\n\ttmp_file_path = PosixPath(tmp_path) / filename.replace(\".zip\", \"\")\n\tif tmp_file_path.exists():\n\t\tshutil.rmtree(str(tmp_file_path))\n\ttmp_file_path.mkdir(parents=True, exist_ok=True)\n\n\t# Unzip the file\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(str(tmp_file_path))\n\t\t# Get the name of the extracted folder\n\t\textracted_file = os.listdir(str(tmp_file_path))[0]\n\t\t# Return the repo url\n\t\treturn f\"{tmp_file_path}/{extracted_file}\"\n/tests/test_setup.py\nimport os\nimport subprocess\nimport tempfile\nfrom pathlib import Path\n\nimport pytest\n\nimport github_actions_toolkit as toolkit\n\n# Test config\nTEST_REPO_URL = \"https://github.com/actions/toolkit", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif not os.path.exists(archive_path):\n\t\traise Exception(f\"File {archive_path} does not exist.\")\n\n\t# Check if filename is provided.\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\t# Extract the file path.\n\textract_path = os.path.join(tmp_path, filename)\n\textract_path = PosixPath(extract_path)\n\n\t# Create the directory if it does not exist.\n\tif not extract_path.parent.exists():\n\t\textract_path.parent.mkdir(parents=True, exist_ok=True)\n\n\t# Extract the file.\n\twith tarfile.open(archive_path, \"r\") as tar:\n\t\ttar.extractall(path=extract_path)\n\n\t# Return the repo url.\n\treturn extract_path.as_uri()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\t# check if the archive exists\n\tif not os.path.exists(archive_path):\n\t\traise Exception(f\"Archive {archive_path} does not exist\")\n\n\t# check if filename exists\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\t# Check if the file already exists\n\tif os.path.exists(tmp_path / filename):\n\t\traise Exception(f\"File {tmp_path / filename} already exists\")\n\n\t# Uncompress the archive\n\twith open(archive_path, \"rb\") as f_in:\n\t\twith open(tmp_path / filename, \"wb\") as f_out:\n\t\t\tshutil.copyfileobj(f_in, f_out)\n\n\trepo_url = f\"file:///{tmp_path / filename}\"\n\treturn repo_url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\t# Create a temporary directory to uncompress the archive into.\n\ttmp_dir = mkdtemp(dir=tmp_path)\n\n\t# Uncompress the archive into the temporary directory.\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(tmp_dir)\n\n\t# Return the repo url of the uncompressed repository.\n\treturn git.Repo.init(tmp_dir).working_tree_dir\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\tassert filename\n\n\ttarget_path = os.path.join(tmp_path, filename)\n\n\tif not os.path.exists(target_path):\n\t\twith open(archive_path, \"rb\") as f:\n\t\t\twith open(target_path, \"wb\") as f_w:\n\t\t\t\tf_w.write(f.read())\n\n\tif not os.path.exists(target_path):\n\t\traise Exception(\"Archive not found: {}\".format(archive_path))\n\n\tif os.path.isdir(target_path):\n\t\traise Exception(\"Target path is a directory: {}\".format(target_path))\n\n\tif not os.path.isfile(target_path):\n\t\traise Exception(\"Target path is not a file: {}\".format(target_path))\n\n\tif not os.path.exists(target_path):\n\t\traise Exception(\"Target path does not exist: {}\".format(target_path))\n\n\t# Uncompress archive\n\tif archive_path.endswith(\".zip\"):\n\t\twith zipfile.ZipFile(target_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(tmp_path)\n\telif archive_path.endswith(\".gz\"):\n\t\twith gzip.open(target_path, \"", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\t# TODO: add exception handling\n\t# TODO: remove the tmp_path hardcoding\n\t# TODO: remove filename\n\t# TODO: remove the hardcoding\n\t# TODO: add type hints\n\t# TODO: add comment\n\t# TODO: remove the hardcoding\n\t# TODO: add exception handling\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t# TODO: remove the filename hardcoding\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\trepo_path = os.path.join(tmp_path, filename)\n\tif not os.path.exists(repo_path):\n\t\trepo_path = os.path.join(tmp_path, \"temp_repo\")\n\n\t# TODO: Implement me\n\n\treturn repo_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\tfilename_without_ext = os.path.splitext(filename)[0]\n\ttmp_dir = os.path.join(tmp_path, filename_without_ext)\n\ttry:\n\t\tos.mkdir(tmp_dir)\n\texcept FileExistsError:\n\t\tpass\n\tarchive_path_with_tmp = os.path.join(tmp_dir, filename)\n\tshutil.copyfile(archive_path, archive_path_with_tmp)\n\tos.chdir(tmp_dir)\n\ttar_archive = tarfile.open(archive_path_with_tmp)\n\ttar_archive.extractall()\n\treturn os.path.join(\"file://\", tmp_dir, filename_without_ext)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\n\tif archive_path.endswith(\".zip\"):\n\t\textract_path = os.path.join(tmp_path, \"extracted_zip\")\n\t\tzip_ref = zipfile.ZipFile(archive_path, \"r\")\n\t\tzip_ref.extractall(extract_path)\n\t\tzip_ref.close()\n\t\tif filename:\n\t\t\treturn os.path.join(extract_path, filename)\n\t\telse:\n\t\t\treturn extract_path\n\telse:\n\t\treturn archive_path\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/flashbake/plugins/ignored.py", "project": "commandline/flashbake", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n#\tgitignored = subprocess.check_output(['git', 'ls-files', '--ignored', '--exclude-standard']).split()\n\tgitignored = subprocess.check_output(['git', 'ls-files', '--cached']).split()\n\tgitignored.sort()\n\tignored += gitignored\n\treturn(ignored)\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tignored = subprocess.check_output(['git', 'ls-files', '--others', '--ignored', '--exclude-standard'])\n\tignored = ignored.splitlines()\n\tignored = sorted(ignored)\n\tignored = ','.join(ignored)\n\n\treturn ignored\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\t# use subprocess to call git command\n\tgitignored = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\"])\n\n\t# make gitignored a list\n\tgitignored = gitignored.split('\\n')\n\n\t# sort gitignored list\n\tgitignored.sort()\n\n\t# turn gitignored list into a string of filenames\n\tgitignored = ','.join(gitignored)\n\n\t# add gitignored to ignored list\n\tignored.append(gitignored)\n\n\treturn ignored\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tignored = subprocess.check_output(\"git ls-files --others --exclude-standard\", shell = True)\n\tignored = ignored.decode(\"utf-8\").split(\"\\n\")\n\tignored.sort()\n\tignored = \",\".join(ignored)\n\n\treturn ignored\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tfile_list = subprocess.check_output([\"git\", \"ls-files\", \"--ignored\", \"--exclude-standard\"]).decode('utf-8').split('\\n')\n\tfile_list.sort()\n\tfile_list = \",\".join(file_list)\n\treturn file_list\n\n#print(addignored())\n/README.md\n# Git-ignore-files\n## 1.Introduction\nThis project aims to achieve the function of using the git command to obtain the file names\uff0cTurn the file names into a list, sort the list for only ignored files\uff0creturn those files as a single string with each filename separated by a comma.\n## 2.The structure of the project\n### 2.1.Gitignore.py\nThis file is the main file.\n### 2.2.Gitignore_test.py\nThis file is used to test Gitignore.py\n### 2.3.test_file\nThis file is used to store test files.\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tignoredfilenames = subprocess.check_output([\"git\", \"ls-files\", \"--ignored\"]).split()\n\tignoredfilenames = sorted(ignoredfilenames)\n\t# print(ignoredfilenames)\n\tignoredfilenames = ','.join(ignoredfilenames)\n\tprint(ignoredfilenames)\n\n\tif len(ignoredfilenames) > 1:\n\t\tignoredfilenames = ignoredfilenames[:-1]\n\n\treturn ignoredfilenames\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t # get the ignored files\n\tignore_text = subprocess.check_output(['git', 'ls-files', '--ignored', '--exclude-standard'])\n\t# turn the files into a list\n\tignored_files = ignore_text.split()\n\t# sort the list\n\tignored_files.sort()\n\t# turn the list into a string with each filename separated by a comma\n\tignored_files = ','.join(ignored_files)\n\t# return the string\n\treturn ignored_files\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\t## Obtain the file names\n\tfiles = os.popen('git ls-files -i -o -z').read()\n\n\t## Separate the file names from the file contents\n\tfiles = files.split('\\0')\n\t\n\t## Delete the first empty string\n\tdel files[0]\n\tfiles.sort()\n\n\t## Print the ignored files\n\tfor file in files:\n\t\tif file in ignored:\n\t\t\tsys.stdout.write(file)\n\t\t\tsys.stdout.write(',')\n\n\t## Add a newline character\n\tsys.stdout.write('\\n')\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\t# If there is no.gitignore file, return an empty string\n\tif not os.path.isfile(os.path.join(os.getcwd(), \".gitignore\")):\n\t\treturn \"\"\n\n\t# If the file is empty, return the empty string\n\twith open(\".gitignore\", 'r') as f:\n\t\tif not f.read(1):\n\t\t\treturn \"\"\n\n\t# If the file is not empty, return a list of all the lines in the file\n\twith open(\".gitignore\", 'r') as f:\n\t\tlines = f.readlines()\n\t\tlines = [x.strip() for x in lines]\n\t\tlines.sort()\n\t\treturn \",\".join(lines)\n\n\n# The following 2 functions are from's solution for \n# http://stackoverflow.com/questions/18066628/how-to-ignore-certain-files-in-git-repo\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t # use git status command to get file names\n\tstatus = \"git status -s\"\n\tp = subprocess.Popen(status, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\toutput, err = p.communicate()\n\tif err:\n\t\tprint err\n\t\treturn None\n\t# split the output and get the names of the files\n\toutput = output.split(\"\\n\")\n\toutput = [x.split(\" \")[1] for x in output if x!= \"\"]\n\t# ignore the files\n\tignored_files = [x for x in output if x in ignored]\n\t# sort the list to get the right order of the files\n\tignored_files.sort()\n\t# turn the list of file names into a single string\n\tignored_files = \",\".join(ignored_files)\n\treturn ignored_files\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n#\tgitignored = subprocess.check_output(['git', 'ls-files', '--ignored', '--exclude-standard']).split()\n\tgitignored = subprocess.check_output(['git', 'ls-files', '--cached']).split()\n\tgitignored.sort()\n\tignored += gitignored\n\treturn(ignored)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tignored = subprocess.check_output(['git', 'ls-files', '--others', '--ignored', '--exclude-standard'])\n\tignored = ignored.splitlines()\n\tignored = sorted(ignored)\n\tignored = ','.join(ignored)\n\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\t# use subprocess to call git command\n\tgitignored = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\"])\n\n\t# make gitignored a list\n\tgitignored = gitignored.split('\\n')\n\n\t# sort gitignored list\n\tgitignored.sort()\n\n\t# turn gitignored list into a string of filenames\n\tgitignored = ','.join(gitignored)\n\n\t# add gitignored to ignored list\n\tignored.append(gitignored)\n\n\treturn ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tignored = subprocess.check_output(\"git ls-files --others --exclude-standard\", shell = True)\n\tignored = ignored.decode(\"utf-8\").split(\"\\n\")\n\tignored.sort()\n\tignored = \",\".join(ignored)\n\n\treturn ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tfile_list = subprocess.check_output([\"git\", \"ls-files\", \"--ignored\", \"--exclude-standard\"]).decode('utf-8').split('\\n')\n\tfile_list.sort()\n\tfile_list = \",\".join(file_list)\n\treturn file_list\n\n#print(addignored())\n/README.md\n# Git-ignore-files\n## 1.Introduction\nThis project aims to achieve the function of using the git command to obtain the file names\uff0cTurn the file names into a list, sort the list for only ignored files\uff0creturn those files as a single string with each filename separated by a comma.\n## 2.The structure of the project\n### 2.1.Gitignore.py\nThis file is the main file.\n### 2.2.Gitignore_test.py\nThis file is used to test Gitignore.py\n### 2.3.test_file\nThis file is used to store test files.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tignoredfilenames = subprocess.check_output([\"git\", \"ls-files\", \"--ignored\"]).split()\n\tignoredfilenames = sorted(ignoredfilenames)\n\t# print(ignoredfilenames)\n\tignoredfilenames = ','.join(ignoredfilenames)\n\tprint(ignoredfilenames)\n\n\tif len(ignoredfilenames) > 1:\n\t\tignoredfilenames = ignoredfilenames[:-1]\n\n\treturn ignoredfilenames\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t # get the ignored files\n\tignore_text = subprocess.check_output(['git', 'ls-files', '--ignored', '--exclude-standard'])\n\t# turn the files into a list\n\tignored_files = ignore_text.split()\n\t# sort the list\n\tignored_files.sort()\n\t# turn the list into a string with each filename separated by a comma\n\tignored_files = ','.join(ignored_files)\n\t# return the string\n\treturn ignored_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\t## Obtain the file names\n\tfiles = os.popen('git ls-files -i -o -z').read()\n\n\t## Separate the file names from the file contents\n\tfiles = files.split('\\0')\n\t\n\t## Delete the first empty string\n\tdel files[0]\n\tfiles.sort()\n\n\t## Print the ignored files\n\tfor file in files:\n\t\tif file in ignored:\n\t\t\tsys.stdout.write(file)\n\t\t\tsys.stdout.write(',')\n\n\t## Add a newline character\n\tsys.stdout.write('\\n')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\t# If there is no.gitignore file, return an empty string\n\tif not os.path.isfile(os.path.join(os.getcwd(), \".gitignore\")):\n\t\treturn \"\"\n\n\t# If the file is empty, return the empty string\n\twith open(\".gitignore\", 'r') as f:\n\t\tif not f.read(1):\n\t\t\treturn \"\"\n\n\t# If the file is not empty, return a list of all the lines in the file\n\twith open(\".gitignore\", 'r') as f:\n\t\tlines = f.readlines()\n\t\tlines = [x.strip() for x in lines]\n\t\tlines.sort()\n\t\treturn \",\".join(lines)\n\n\n# The following 2 functions are from's solution for \n# http://stackoverflow.com/questions/18066628/how-to-ignore-certain-files-in-git-repo\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t # use git status command to get file names\n\tstatus = \"git status -s\"\n\tp = subprocess.Popen(status, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\toutput, err = p.communicate()\n\tif err:\n\t\tprint err\n\t\treturn None\n\t# split the output and get the names of the files\n\toutput = output.split(\"\\n\")\n\toutput = [x.split(\" \")[1] for x in output if x!= \"\"]\n\t# ignore the files\n\tignored_files = [x for x in output if x in ignored]\n\t# sort the list to get the right order of the files\n\tignored_files.sort()\n\t# turn the list of file names into a single string\n\tignored_files = \",\".join(ignored_files)\n\treturn ignored_files\n", "is_pass": false, "return_code": 1}]}
{"file_path": "docopt/__init__.py", "project": "bazaar-projects/docopt-ng", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        dot_pos = filename.find('.')\n        if dot_pos == -1:\n            return False\n\n        return filename[dot_pos:].lower() == '.doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return filename.lower() == 'doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        if filename!= None and filename.lower() == 'doxyfile':\n                return True\n        else:\n                return False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return True if filename.lower() == 'doxyfile' else False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return filename.lower() == 'doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t \n\tif filename.lower() == 'doxyfile':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return filename.lower() == 'doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        if filename.endswith('.doxyfile'):\n\t\treturn True\n\treturn False\n\t\n\t", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n    \tif filename.lower() == 'doxyfile':\n        \treturn True\n    \treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t \n\treturn filename.lower().endswith('doxyfile')\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        dot_pos = filename.find('.')\n        if dot_pos == -1:\n            return False\n\n        return filename[dot_pos:].lower() == '.doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return filename.lower() == 'doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        if filename!= None and filename.lower() == 'doxyfile':\n                return True\n        else:\n                return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return True if filename.lower() == 'doxyfile' else False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return filename.lower() == 'doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t \n\tif filename.lower() == 'doxyfile':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        return filename.lower() == 'doxyfile'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n        if filename.endswith('.doxyfile'):\n\t\treturn True\n\treturn False\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n    \tif filename.lower() == 'doxyfile':\n        \treturn True\n    \treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t \n\treturn filename.lower().endswith('doxyfile')\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/borg/check.py", "project": "witten/borgmatic", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t \n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\telse:\n\t\tunits = frequency.split(\" \")\n\t\tif len(units)!= 2:\n\t\t\traise ValueError('Frequency must be \"number timeunit\"')\n\t\ttry:\n\t\t\tnumber = int(units[0])\n\t\t\tunit = units[1]\n\t\t\tif unit == \"seconds\":\n\t\t\t\treturn datetime.timedelta(seconds = number)\n\t\t\telif unit == \"minutes\":\n\t\t\t\treturn datetime.timedelta(minutes = number)\n\t\t\telif unit == \"hours\":\n\t\t\t\treturn datetime.timedelta(hours = number)\n\t\t\telif unit == \"days\":\n\t\t\t\treturn datetime.timedelta(days = number)\n\t\t\telif unit == \"weeks\":\n\t\t\t\treturn datetime.timedelta(weeks = number)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Unit of time must be one of: seconds, minutes, hours, days, weeks\")\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\t#if frequency is None or frequency.lower() == \"always\":\n\t\t#\treturn None\n\t\t#else:\n\t\t#\tmatch = re.match(r\"(\\d+) (.*)\", frequency)\n\t\t#\tif match is None:\n\t\t#\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\t#\telse:\n\t\t#\t\ttimeunit = match.group(2)\n\t\t#\t\tif timeunit.lower() in [\"second\", \"seconds\"]:\n\t\t#\t\t\ttimeunit = \"S\"\n\t\t#\t\telif timeunit.lower() in [\"minute\", \"minutes\"]:\n\t\t#\t\t\ttimeunit = \"T\"\n\t\t#\t\telif timeunit.lower() in [\"hour\", \"hours\"]:\n\t\t#\t\t\ttimeunit = \"H\"\n\t\t#\t\telse:\n\t\t#\t\t\traise ValueError(\"Unknown time unit: %s\" % timeunit)\n\t\t#\t\treturn datetime.timedelta(**{timeunit: int(match.group(1))})\n\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\telse:\n\t\tmatch = re.match(r\"(\\d+) (.*)\", frequency)\n\t\tif match is None:\n\t\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\telse:\n\t\t\ttimeunit = match", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\t# split the frequency string into a list of two elements\n\t# ['3', 'timeunit']\n\tfreq_list = frequency.split()\n\n\t# convert the first element to an integer\n\t# freq_list[0] is the integer '3'\n\ttry:\n\t\tfrequency_number = int(freq_list[0])\n\texcept ValueError as e:\n\t\traise ValueError(\"the first element of the frequency string cannot be parsed as an integer\")\n\n\t# convert the second element to a valid frequency unit\n\t# freq_list[1] is the string 'timeunit'\n\ttry:\n\t\tfrequency_unit = FrequencyUnit(freq_list[1])\n\texcept ValueError as e:\n\t\traise ValueError(\"the second element of the frequency string cannot be parsed as a frequency unit\")\n\n\tif frequency_unit == FrequencyUnit.day:\n\t\treturn datetime.timedelta(days=frequency_number)\n\telif frequency_unit == FrequencyUnit.week:\n\t\treturn datetime.timedelta(weeks=frequency_number)\n\telif frequency_unit == FrequencyUnit.month:\n\t\treturn datetime.timedelta(days=frequency_number*30)\n\telif frequency_unit == FrequencyUnit.year:\n\t\treturn datetime.timedelta(days=frequency_number*365)", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\nfor i in range(1, 10):\n\tprint(i)\n\tprint(i*4)\n\tprint()/main.py\nfrom datetime import datetime, timedelta\nimport requests\nimport os\nimport pandas as pd\nimport re\nimport json\nfrom flask import Flask, request, render_template, jsonify\nfrom flask_cors import CORS, cross_origin\nfrom flask_sqlalchemy import SQLAlchemy\n\napp = Flask(__name__)\nCORS(app)\nbasedir = os.path.abspath(os.path.dirname(__file__))\napp.config['SQLALCHEMY_DATABASE_URI'] ='sqlite:///' + os.path.join(basedir, 'app.db')\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\ndb = SQLAlchemy(app)\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tif frequency == \"never\":\n\t\treturn datetime.timedelta()\n\n\ttry:\n\t\tnumber, unit = frequency.split()\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\n\tif unit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=number)\n\telse:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\ttime_unit = {'timeunit':1, 'day':1, 'week':7,'month':30, 'year':365}\n\tsplit_frequency = frequency.split()\n\tif len(split_frequency)!= 2:\n\t\traise ValueError(\"Frequency must be a number and a time unit.\" +\n\t\t\t\"For example, '3 timeunit' or '2 week'.\")\n\ttry:\n\t\ttime_unit[split_frequency[1]]\n\texcept KeyError:\n\t\traise ValueError(\"Frequency must be a number and a time unit.\" +\n\t\t\t\"For example, '3 timeunit' or '2 week'.\")\n\n\treturn datetime.timedelta(**{split_frequency[1]: int(split_frequency[0])})\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t/pycfiles/b3-1.10.10-py2.7/test_commands.py\n# uncompyle6 version 3.7.4\n# Python bytecode 2.7 (62211)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: build\\bdist.win32\\egg\\tests\\plugins\\poweradmin\\test_commands.py\n# Compiled at: 2016-03-08 18:42:10\nimport unittest\nfrom mock import Mock, patch\nfrom b3.config import CfgConfigParser\nfrom b3.plugins.poweradmin import PowerAdminPlugin\nfrom tests.plugins.poweradmin import PowerAdminTestCase\nfrom tests.plugins.poweradmin import test_commands_admin\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\n\n\tfrequency = frequency.strip()\n\tif len(frequency) == 0:\n\t\traise ValueError(\"frequency is empty\")\n\tfrequency_list = frequency.split()\n\tif len(frequency_list)!= 2:\n\t\traise ValueError(\"frequency is not a number and timeunit\")\n\ttry:\n\t\tnum = int(frequency_list[0])\n\texcept ValueError:\n\t\traise ValueError(\"frequency is not a number\")\n\ttime_unit = frequency_list[1]\n\tif time_unit == \"second\":\n\t\treturn datetime.timedelta(seconds = num)\n\telif time_unit == \"minute\":\n\t\treturn datetime.timedelta(minutes = num)\n\telif time_unit == \"hour\":\n\t\treturn datetime.timedelta(hours = num)\n\telif time_unit == \"day\":\n\t\treturn datetime.timedelta(days = num)\n\telse:\n\t\traise ValueError(\"frequency is not in the form of 'number timeunit'\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tif frequency == \"never\":\n\t\treturn None\n\n\tn, timeunit = frequency.split()\n\tif not n.isdigit():\n\t\traise ValueError(\"Frequency must be a number\")\n\n\tif timeunit == \"years\":\n\t\treturn datetime.timedelta(n*365)\n\tif timeunit == \"months\":\n\t\treturn datetime.timedelta(n*30)\n\tif timeunit == \"weeks\":\n\t\treturn datetime.timedelta(n*7)\n\tif timeunit == \"days\":\n\t\treturn datetime.timedelta(n)\n\tif timeunit == \"hours\":\n\t\treturn datetime.timedelta(hours=n)\n\tif timeunit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=n)\n\tif timeunit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=n)\n\tif timeunit == \"microseconds\":\n\t\treturn datetime.timedelta(microseconds=n)\n\tif timeunit == \"milliseconds\":\n\t\treturn datetime.timedelta(milliseconds=n)\n\n\traise ValueError(\"Frequency must be in the form 'number timeunit'\")\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency == \"always\":\n\t\treturn None\n\telif frequency is None:\n\t\treturn None\n\n\ttry:\n\t\tnum, unit = frequency.split()\n\t\treturn datetime.timedelta(**{unit: int(num)})\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency %s\" % frequency)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t \n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\telse:\n\t\tunits = frequency.split(\" \")\n\t\tif len(units)!= 2:\n\t\t\traise ValueError('Frequency must be \"number timeunit\"')\n\t\ttry:\n\t\t\tnumber = int(units[0])\n\t\t\tunit = units[1]\n\t\t\tif unit == \"seconds\":\n\t\t\t\treturn datetime.timedelta(seconds = number)\n\t\t\telif unit == \"minutes\":\n\t\t\t\treturn datetime.timedelta(minutes = number)\n\t\t\telif unit == \"hours\":\n\t\t\t\treturn datetime.timedelta(hours = number)\n\t\t\telif unit == \"days\":\n\t\t\t\treturn datetime.timedelta(days = number)\n\t\t\telif unit == \"weeks\":\n\t\t\t\treturn datetime.timedelta(weeks = number)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Unit of time must be one of: seconds, minutes, hours, days, weeks\")\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Could not parse frequency string\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\t#if frequency is None or frequency.lower() == \"always\":\n\t\t#\treturn None\n\t\t#else:\n\t\t#\tmatch = re.match(r\"(\\d+) (.*)\", frequency)\n\t\t#\tif match is None:\n\t\t#\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\t#\telse:\n\t\t#\t\ttimeunit = match.group(2)\n\t\t#\t\tif timeunit.lower() in [\"second\", \"seconds\"]:\n\t\t#\t\t\ttimeunit = \"S\"\n\t\t#\t\telif timeunit.lower() in [\"minute\", \"minutes\"]:\n\t\t#\t\t\ttimeunit = \"T\"\n\t\t#\t\telif timeunit.lower() in [\"hour\", \"hours\"]:\n\t\t#\t\t\ttimeunit = \"H\"\n\t\t#\t\telse:\n\t\t#\t\t\traise ValueError(\"Unknown time unit: %s\" % timeunit)\n\t\t#\t\treturn datetime.timedelta(**{timeunit: int(match.group(1))})\n\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\telse:\n\t\tmatch = re.match(r\"(\\d+) (.*)\", frequency)\n\t\tif match is None:\n\t\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\telse:\n\t\t\ttimeunit = match", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\t# split the frequency string into a list of two elements\n\t# ['3', 'timeunit']\n\tfreq_list = frequency.split()\n\n\t# convert the first element to an integer\n\t# freq_list[0] is the integer '3'\n\ttry:\n\t\tfrequency_number = int(freq_list[0])\n\texcept ValueError as e:\n\t\traise ValueError(\"the first element of the frequency string cannot be parsed as an integer\")\n\n\t# convert the second element to a valid frequency unit\n\t# freq_list[1] is the string 'timeunit'\n\ttry:\n\t\tfrequency_unit = FrequencyUnit(freq_list[1])\n\texcept ValueError as e:\n\t\traise ValueError(\"the second element of the frequency string cannot be parsed as a frequency unit\")\n\n\tif frequency_unit == FrequencyUnit.day:\n\t\treturn datetime.timedelta(days=frequency_number)\n\telif frequency_unit == FrequencyUnit.week:\n\t\treturn datetime.timedelta(weeks=frequency_number)\n\telif frequency_unit == FrequencyUnit.month:\n\t\treturn datetime.timedelta(days=frequency_number*30)\n\telif frequency_unit == FrequencyUnit.year:\n\t\treturn datetime.timedelta(days=frequency_number*365)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\nfor i in range(1, 10):\n\tprint(i)\n\tprint(i*4)\n\tprint()/main.py\nfrom datetime import datetime, timedelta\nimport requests\nimport os\nimport pandas as pd\nimport re\nimport json\nfrom flask import Flask, request, render_template, jsonify\nfrom flask_cors import CORS, cross_origin\nfrom flask_sqlalchemy import SQLAlchemy\n\napp = Flask(__name__)\nCORS(app)\nbasedir = os.path.abspath(os.path.dirname(__file__))\napp.config['SQLALCHEMY_DATABASE_URI'] ='sqlite:///' + os.path.join(basedir, 'app.db')\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\ndb = SQLAlchemy(app)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tif frequency == \"never\":\n\t\treturn datetime.timedelta()\n\n\ttry:\n\t\tnumber, unit = frequency.split()\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\n\tif unit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=number)\n\telse:\n\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\ttime_unit = {'timeunit':1, 'day':1, 'week':7,'month':30, 'year':365}\n\tsplit_frequency = frequency.split()\n\tif len(split_frequency)!= 2:\n\t\traise ValueError(\"Frequency must be a number and a time unit.\" +\n\t\t\t\"For example, '3 timeunit' or '2 week'.\")\n\ttry:\n\t\ttime_unit[split_frequency[1]]\n\texcept KeyError:\n\t\traise ValueError(\"Frequency must be a number and a time unit.\" +\n\t\t\t\"For example, '3 timeunit' or '2 week'.\")\n\n\treturn datetime.timedelta(**{split_frequency[1]: int(split_frequency[0])})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t/pycfiles/b3-1.10.10-py2.7/test_commands.py\n# uncompyle6 version 3.7.4\n# Python bytecode 2.7 (62211)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: build\\bdist.win32\\egg\\tests\\plugins\\poweradmin\\test_commands.py\n# Compiled at: 2016-03-08 18:42:10\nimport unittest\nfrom mock import Mock, patch\nfrom b3.config import CfgConfigParser\nfrom b3.plugins.poweradmin import PowerAdminPlugin\nfrom tests.plugins.poweradmin import PowerAdminTestCase\nfrom tests.plugins.poweradmin import test_commands_admin\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency == None or frequency == \"always\":\n\t\treturn None\n\n\n\tfrequency = frequency.strip()\n\tif len(frequency) == 0:\n\t\traise ValueError(\"frequency is empty\")\n\tfrequency_list = frequency.split()\n\tif len(frequency_list)!= 2:\n\t\traise ValueError(\"frequency is not a number and timeunit\")\n\ttry:\n\t\tnum = int(frequency_list[0])\n\texcept ValueError:\n\t\traise ValueError(\"frequency is not a number\")\n\ttime_unit = frequency_list[1]\n\tif time_unit == \"second\":\n\t\treturn datetime.timedelta(seconds = num)\n\telif time_unit == \"minute\":\n\t\treturn datetime.timedelta(minutes = num)\n\telif time_unit == \"hour\":\n\t\treturn datetime.timedelta(hours = num)\n\telif time_unit == \"day\":\n\t\treturn datetime.timedelta(days = num)\n\telse:\n\t\traise ValueError(\"frequency is not in the form of 'number timeunit'\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tif frequency == \"never\":\n\t\treturn None\n\n\tn, timeunit = frequency.split()\n\tif not n.isdigit():\n\t\traise ValueError(\"Frequency must be a number\")\n\n\tif timeunit == \"years\":\n\t\treturn datetime.timedelta(n*365)\n\tif timeunit == \"months\":\n\t\treturn datetime.timedelta(n*30)\n\tif timeunit == \"weeks\":\n\t\treturn datetime.timedelta(n*7)\n\tif timeunit == \"days\":\n\t\treturn datetime.timedelta(n)\n\tif timeunit == \"hours\":\n\t\treturn datetime.timedelta(hours=n)\n\tif timeunit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=n)\n\tif timeunit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=n)\n\tif timeunit == \"microseconds\":\n\t\treturn datetime.timedelta(microseconds=n)\n\tif timeunit == \"milliseconds\":\n\t\treturn datetime.timedelta(milliseconds=n)\n\n\traise ValueError(\"Frequency must be in the form 'number timeunit'\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\n\tif frequency == \"always\":\n\t\treturn None\n\telif frequency is None:\n\t\treturn None\n\n\ttry:\n\t\tnum, unit = frequency.split()\n\t\treturn datetime.timedelta(**{unit: int(num)})\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency %s\" % frequency)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\t# Check if host is the localhost\n\tif is_localhost(host):\n\t\treturn True\n\t# Check if host is the local ip\n\tif is_ip(host):\n\t\treturn True\n\t# Check if host is the local domain name\n\tif is_valid_domain_name(host):\n\t\treturn True\n\t# Check if host is the local user name\n\tif is_username(host):\n\t\treturn True\n\t# Check if host is the `localhost`\n\tif host == \"localhost\":\n\t\treturn True\n\t# Check if host is the `127.0.0.1`\n\tif host == \"127.0.0.1\":\n\t\treturn True\n\treturn False\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif host in ['localhost', '127.0.0.1']:\n\t\treturn True\n\telif host.startswith('localhost:'):\n\t\treturn True\n\telif host.endswith('.local'):\n\t\treturn True\n\telif host.startswith('.local'):\n\t\treturn True\n\telif host.endswith('.localdomain'):\n\t\treturn True\n\telif host.endswith('.localdomain.'):\n\t\treturn True\n\telif host.startswith('localhost-'):\n\t\treturn True\n\telif host.startswith('localhost-'):\n\t\treturn True\n\telif host.endswith('-localhost'):\n\t\treturn True\n\telif host.endswith('-localhost.'):\n\t\treturn True\n\telif host.startswith('127.0.0.'):\n\t\treturn True\n\telif host.startswith('192.168.0.'):\n\t\treturn True\n\telif host.startswith('10.0.0.'):\n\t\treturn True\n\telif host.startswith('169.254.'):\n\t\treturn True\n\telif host.startswith('172.16.'):\n\t\treturn True\n\telif host.endswith('.localdomain.localdomain.'):\n\t\treturn True\n\telif host.endswith('.localdomain.localdomain'):\n\t\treturn True\n\telif host.endswith('.localdomain-localdomain.'):\n\t\treturn True", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\tif check_host_ip(host):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\treturn (host == \"localhost\") or (host == socket.gethostname()) or (host == socket.gethostbyname(socket.gethostname())) or (host == \"127.0.0.1\")\n/README.md\n# Python-Utils\nA collection of python utilities for quick development\n\n## Installation\n\n```bash\npip install pyutils\n```\n\n## Usage\n\n```python\nimport pyutils\n```\n\n## Available modules\n\n### pyutils.jsonutils\n\n```python\nfrom pyutils.jsonutils import from_json, to_json\n```\n\n### pyutils.netutils\n\n```python\nfrom pyutils.netutils import is_local\n```\n\n### pyutils.processutils\n\n```python\nfrom pyutils.processutils import get_pid_by_name\n```\n\n### pyutils.strutils\n\n```python\nfrom pyutils.strutils import is_empty\n```\n\n### pyutils.timeutils\n\n```python\nfrom pyutils.timeutils import get_timestamp, sleep\n```\n\n## License\n[MIT](https://choosealicense.com/licenses/mit/)\n/pyutils/processutils.py\nimport os\nimport psutil\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\tlocal_ip = get_local_ip()\n\tlocal_domain = get_local_domain()\n\tlocal_name = get_local_username()\n\tlocal_host = 'localhost'\n\tlocal_ip_host = '127.0.0.1'\n\t\n\tif host == local_ip or host == local_domain or host == local_name or host == local_host or host == local_ip_host:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\treturn host in ('localhost', '127.0.0.1')\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif host == 'localhost':\n\t\treturn True\n\telif host == '127.0.0.1':\n\t\treturn True\n\telif host == socket.gethostname():\n\t\treturn True\n\telif host == socket.getfqdn():\n\t\treturn True\n\treturn False\ns(1))\n\tdef test_is_local(self):\n\t\tself.assertTrue(is_local('localhost'))\n\t\tself.assertTrue(is_local('127.0.0.1'))\n\t\tself.assertTrue(is_local(socket.gethostname()))\n\t\tself.assertTrue(is_local(socket.getfqdn()))\n/tests/test_utils.py\nimport unittest\nimport socket\nimport time\n\nfrom..utils import is_local\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif not host:\n\t\treturn False\n\telse:\n\t\tlocal_hosts = [\n\t\t\t'localhost',\n\t\t\t'127.0.0.1',\n\t\t\tsocket.gethostname(),\n\t\t\tsocket.gethostbyname(socket.gethostname())\n\t\t]\n\n\t\treturn True if host in local_hosts else False\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\tif host in ['localhost', '127.0.0.1']:\n\t\treturn True\n\telif host in ['localhost6', '::1']:\n\t\treturn True\n\telif host == socket.gethostname():\n\t\treturn True\n\telse:\n\t\treturn False\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif host in ('localhost', '127.0.0.1'):\n\t\treturn True\n\n\tif socket.gethostname() == host:\n\t\treturn True\n\n\treturn False\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\t# Check if host is the localhost\n\tif is_localhost(host):\n\t\treturn True\n\t# Check if host is the local ip\n\tif is_ip(host):\n\t\treturn True\n\t# Check if host is the local domain name\n\tif is_valid_domain_name(host):\n\t\treturn True\n\t# Check if host is the local user name\n\tif is_username(host):\n\t\treturn True\n\t# Check if host is the `localhost`\n\tif host == \"localhost\":\n\t\treturn True\n\t# Check if host is the `127.0.0.1`\n\tif host == \"127.0.0.1\":\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif host in ['localhost', '127.0.0.1']:\n\t\treturn True\n\telif host.startswith('localhost:'):\n\t\treturn True\n\telif host.endswith('.local'):\n\t\treturn True\n\telif host.startswith('.local'):\n\t\treturn True\n\telif host.endswith('.localdomain'):\n\t\treturn True\n\telif host.endswith('.localdomain.'):\n\t\treturn True\n\telif host.startswith('localhost-'):\n\t\treturn True\n\telif host.startswith('localhost-'):\n\t\treturn True\n\telif host.endswith('-localhost'):\n\t\treturn True\n\telif host.endswith('-localhost.'):\n\t\treturn True\n\telif host.startswith('127.0.0.'):\n\t\treturn True\n\telif host.startswith('192.168.0.'):\n\t\treturn True\n\telif host.startswith('10.0.0.'):\n\t\treturn True\n\telif host.startswith('169.254.'):\n\t\treturn True\n\telif host.startswith('172.16.'):\n\t\treturn True\n\telif host.endswith('.localdomain.localdomain.'):\n\t\treturn True\n\telif host.endswith('.localdomain.localdomain'):\n\t\treturn True\n\telif host.endswith('.localdomain-localdomain.'):\n\t\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\tif check_host_ip(host):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\treturn (host == \"localhost\") or (host == socket.gethostname()) or (host == socket.gethostbyname(socket.gethostname())) or (host == \"127.0.0.1\")\n/README.md\n# Python-Utils\nA collection of python utilities for quick development\n\n## Installation\n\n```bash\npip install pyutils\n```\n\n## Usage\n\n```python\nimport pyutils\n```\n\n## Available modules\n\n### pyutils.jsonutils\n\n```python\nfrom pyutils.jsonutils import from_json, to_json\n```\n\n### pyutils.netutils\n\n```python\nfrom pyutils.netutils import is_local\n```\n\n### pyutils.processutils\n\n```python\nfrom pyutils.processutils import get_pid_by_name\n```\n\n### pyutils.strutils\n\n```python\nfrom pyutils.strutils import is_empty\n```\n\n### pyutils.timeutils\n\n```python\nfrom pyutils.timeutils import get_timestamp, sleep\n```\n\n## License\n[MIT](https://choosealicense.com/licenses/mit/)\n/pyutils/processutils.py\nimport os\nimport psutil\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\tlocal_ip = get_local_ip()\n\tlocal_domain = get_local_domain()\n\tlocal_name = get_local_username()\n\tlocal_host = 'localhost'\n\tlocal_ip_host = '127.0.0.1'\n\t\n\tif host == local_ip or host == local_domain or host == local_name or host == local_host or host == local_ip_host:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\treturn host in ('localhost', '127.0.0.1')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif host == 'localhost':\n\t\treturn True\n\telif host == '127.0.0.1':\n\t\treturn True\n\telif host == socket.gethostname():\n\t\treturn True\n\telif host == socket.getfqdn():\n\t\treturn True\n\treturn False\ns(1))\n\tdef test_is_local(self):\n\t\tself.assertTrue(is_local('localhost'))\n\t\tself.assertTrue(is_local('127.0.0.1'))\n\t\tself.assertTrue(is_local(socket.gethostname()))\n\t\tself.assertTrue(is_local(socket.getfqdn()))\n/tests/test_utils.py\nimport unittest\nimport socket\nimport time\n\nfrom..utils import is_local\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif not host:\n\t\treturn False\n\telse:\n\t\tlocal_hosts = [\n\t\t\t'localhost',\n\t\t\t'127.0.0.1',\n\t\t\tsocket.gethostname(),\n\t\t\tsocket.gethostbyname(socket.gethostname())\n\t\t]\n\n\t\treturn True if host in local_hosts else False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t\n\n\tif host in ['localhost', '127.0.0.1']:\n\t\treturn True\n\telif host in ['localhost6', '::1']:\n\t\treturn True\n\telif host == socket.gethostname():\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\t \n\tif host in ('localhost', '127.0.0.1'):\n\t\treturn True\n\n\tif socket.gethostname() == host:\n\t\treturn True\n\n\treturn False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/borg/list.py", "project": "witten/borgmatic", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\tpattern = '*'\n\t\tfor path in find_paths:\n\t\t\tif path.startswith('*'):\n\t\t\t\tpattern += path\n\t\t\telse:\n\t\t\t\tpattern = path\n\t\treturn pattern\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\tconverted_find_paths = []\n\tfor find_path in find_paths:\n\t\tif find_path.startswith(r'\\\\'):\n\t\t\tconverted_find_paths.append(find_path)\n\t\t\tcontinue\n\t\tconverted_find_paths.append(os.path.join(find_path, '*'))\n\t\t\n\treturn tuple(converted_find_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\treturn [find_path if find_path.endswith(\"*\") else find_path + \"/**\" for find_path in find_paths]\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t \n\tnew_paths = []\n\tfor find_path in find_paths:\n\t\tif os.path.isdir(find_path):\n\t\t\tnew_path = os.path.join(find_path, \"**\")\n\t\telse:\n\t\t\tnew_path = find_path\n\t\tnew_paths.append(new_path)\n\treturn tuple(new_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\tfind_paths = list(find_paths)\n\tfor idx, path in enumerate(find_paths):\n\t\tif is_path_pattern(path):\n\t\t\tfind_paths[idx] = path\n\t\telse:\n\t\t\tfind_paths[idx] = os.path.join(path, '*')\n\treturn tuple(find_paths)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\treturn tuple(glob.escape(p) if p not in find_paths else p for p in find_paths)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t \n\tglob_paths = []\n\tfor path in find_paths:\n\t\tif os.path.isdir(path):\n\t\t\tglob_path = path + '/**'\n\t\telse:\n\t\t\tif '*' in path or '?' in path:\n\t\t\t\tglob_path = path\n\t\t\telse:\n\t\t\t\tglob_path = path + '**'\n\t\tglob_paths.append(glob_path)\n\treturn tuple(glob_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\tpaths = []\n\t\tfor p in find_paths:\n\t\t\tif p.startswith(\"glob:\"):\n\t\t\t\tpaths.append(p)\n\t\t\telse:\n\t\t\t\tpaths.append(\"glob:\" + p)\n\t\treturn tuple(paths)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t \n\tg = []\n\tfor path in find_paths:\n\t\tif path.endswith('.py') or path.endswith('.c') or path.endswith('.h'):\n\t\t\tg.append(path)\n\t\telse:\n\t\t\tg.append(path + '/*')\n\treturn tuple(g)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\tif not find_paths:\n\t\treturn ()\n\tpaths = []\n\tfor path in find_paths:\n\t\tif not os.path.exists(path):\n\t\t\traise Exception('Path not found: %s' % path)\n\t\tif os.path.isdir(path):\n\t\t\tpath = os.path.join(path, '**')\n\t\tpaths.append(path)\n\treturn tuple(paths)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\tpattern = '*'\n\t\tfor path in find_paths:\n\t\t\tif path.startswith('*'):\n\t\t\t\tpattern += path\n\t\t\telse:\n\t\t\t\tpattern = path\n\t\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\tconverted_find_paths = []\n\tfor find_path in find_paths:\n\t\tif find_path.startswith(r'\\\\'):\n\t\t\tconverted_find_paths.append(find_path)\n\t\t\tcontinue\n\t\tconverted_find_paths.append(os.path.join(find_path, '*'))\n\t\t\n\treturn tuple(converted_find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\treturn [find_path if find_path.endswith(\"*\") else find_path + \"/**\" for find_path in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t \n\tnew_paths = []\n\tfor find_path in find_paths:\n\t\tif os.path.isdir(find_path):\n\t\t\tnew_path = os.path.join(find_path, \"**\")\n\t\telse:\n\t\t\tnew_path = find_path\n\t\tnew_paths.append(new_path)\n\treturn tuple(new_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\tfind_paths = list(find_paths)\n\tfor idx, path in enumerate(find_paths):\n\t\tif is_path_pattern(path):\n\t\t\tfind_paths[idx] = path\n\t\telse:\n\t\t\tfind_paths[idx] = os.path.join(path, '*')\n\treturn tuple(find_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\treturn tuple(glob.escape(p) if p not in find_paths else p for p in find_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t \n\tglob_paths = []\n\tfor path in find_paths:\n\t\tif os.path.isdir(path):\n\t\t\tglob_path = path + '/**'\n\t\telse:\n\t\t\tif '*' in path or '?' in path:\n\t\t\t\tglob_path = path\n\t\t\telse:\n\t\t\t\tglob_path = path + '**'\n\t\tglob_paths.append(glob_path)\n\treturn tuple(glob_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\tpaths = []\n\t\tfor p in find_paths:\n\t\t\tif p.startswith(\"glob:\"):\n\t\t\t\tpaths.append(p)\n\t\t\telse:\n\t\t\t\tpaths.append(\"glob:\" + p)\n\t\treturn tuple(paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t \n\tg = []\n\tfor path in find_paths:\n\t\tif path.endswith('.py') or path.endswith('.c') or path.endswith('.h'):\n\t\t\tg.append(path)\n\t\telse:\n\t\t\tg.append(path + '/*')\n\treturn tuple(g)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\t\n\n\tif not find_paths:\n\t\treturn ()\n\tpaths = []\n\tfor path in find_paths:\n\t\tif not os.path.exists(path):\n\t\t\traise Exception('Path not found: %s' % path)\n\t\tif os.path.isdir(path):\n\t\t\tpath = os.path.join(path, '**')\n\t\tpaths.append(path)\n\treturn tuple(paths)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n \treturn sys.platform.startswith('win') and os.environ.get('MSYSTEM') == 'MINGW64'\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n\n\tif os.name == 'posix':\n\t\treturn False\n\telse:\n\t\treturn 'bash' in sys.executable.lower()\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\ttry:\n\t\tif os.environ['GIT_TERMINAL_TITLE']:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\nif is_gitbash():\n    print(\"gitbash\")\nelse:\n    print(\"not gitbash\")\n/src/main/java/com/example/demo/model/User.java\npackage com.example.demo.model;\r\n\r\nimport java.util.Collection;\r\n\r\nimport javax.persistence.Column;\r\nimport javax.persistence.Entity;\r\nimport javax.persistence.GeneratedValue;\r\nimport javax.persistence.GenerationType;\r\nimport javax.persistence.Id;\r\nimport javax.persistence.ManyToMany;\r\nimport javax.persistence.Table;\r\nimport javax.validation.constraints.Size;\r\n\r\nimport org.springframework.security.core.GrantedAuthority;\r\nimport org.springframework.security.core.userdetails.UserDetails;\r\n\r\n@Entity\r\n@Table(name = \"Users\")\r\npublic class User implements UserDetails {\r\n\r\n\tprivate static final long serialVersionUID = 1L;\r\n\r\n\t@Id\r\n\t@GeneratedValue(strategy = GenerationType.IDENTITY)\r\n\t@Column(name = \"id\")\r\n\tprivate long id;\r\n\r\n\t@Column(name = \"username\")\r\n\tprivate String username;\r\n\r\n\t@Column(name = \"\")\r\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\treturn sys.platform == \"win32\"\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\treturn os.environ.get(\"TERM\") == \"cygwin\"\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n\n\treturn sys.platform == \"win32\"\n/README.md\n# d-py\n\n[![Build Status](https://travis-ci.org/d-py/d-py.svg?branch=master)](https://travis-ci.org/d-py/d-py)\n[![PyPI version](https://badge.fury.io/py/d-py.svg)](https://badge.fury.io/py/d-py)\n\n## Overview\nThis project is a Python library for the [d-py](https://github.com/d-py/d-py) framework.\n\n## Installation\n\nThe easiest way to install d-py is to use pip:\n\n    pip install d-py\n\n## Usage\n\n### CLI\n\n```\nUsage: d-py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  create-project  Create a new project.\n  serve           Run the project.\n```\n\n### Create a project\n\n```\nd-py create-project [project_name]\n```\n\n### Run a project\n\n```\nd-py serve\n```\n\n### Running a project in development mode\n\nTo run a project in development mode you can run the following command:\n\n```\nd-py serve --dev\n```", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\treturn \"GIT_PS1_SHOWDIRTYSTATE=true\" in os.environ.get('PS1', '') and \\\n           \"GIT_PS1_SHOWSTASHSTATE=true\" in os.environ.get('PS1', '') and \\\n           \"GIT_PS1_SHOWUNTRACKEDFILES=true\" in os.environ.get('PS1', '') and \\\n           \"GIT_PS1_SHOWUPSTREAM=\" in os.environ.get('PS1', '')\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n        if os.name == 'nt' and os.environ.get('TERM') == 'xterm':\n            return True\n        else:\n            return False\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t  \n\ttry:\n\t\treturn os.environ['MSYSTEM'] == 'MINGW64'\n\texcept KeyError:\n\t\treturn False\n\t\t", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n\n\treturn os.name == 'nt'\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n \treturn sys.platform.startswith('win') and os.environ.get('MSYSTEM') == 'MINGW64'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n\n\tif os.name == 'posix':\n\t\treturn False\n\telse:\n\t\treturn 'bash' in sys.executable.lower()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\ttry:\n\t\tif os.environ['GIT_TERMINAL_TITLE']:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\nif is_gitbash():\n    print(\"gitbash\")\nelse:\n    print(\"not gitbash\")\n/src/main/java/com/example/demo/model/User.java\npackage com.example.demo.model;\r\n\r\nimport java.util.Collection;\r\n\r\nimport javax.persistence.Column;\r\nimport javax.persistence.Entity;\r\nimport javax.persistence.GeneratedValue;\r\nimport javax.persistence.GenerationType;\r\nimport javax.persistence.Id;\r\nimport javax.persistence.ManyToMany;\r\nimport javax.persistence.Table;\r\nimport javax.validation.constraints.Size;\r\n\r\nimport org.springframework.security.core.GrantedAuthority;\r\nimport org.springframework.security.core.userdetails.UserDetails;\r\n\r\n@Entity\r\n@Table(name = \"Users\")\r\npublic class User implements UserDetails {\r\n\r\n\tprivate static final long serialVersionUID = 1L;\r\n\r\n\t@Id\r\n\t@GeneratedValue(strategy = GenerationType.IDENTITY)\r\n\t@Column(name = \"id\")\r\n\tprivate long id;\r\n\r\n\t@Column(name = \"username\")\r\n\tprivate String username;\r\n\r\n\t@Column(name = \"\")\r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\treturn sys.platform == \"win32\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\treturn os.environ.get(\"TERM\") == \"cygwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n\n\treturn sys.platform == \"win32\"\n/README.md\n# d-py\n\n[![Build Status](https://travis-ci.org/d-py/d-py.svg?branch=master)](https://travis-ci.org/d-py/d-py)\n[![PyPI version](https://badge.fury.io/py/d-py.svg)](https://badge.fury.io/py/d-py)\n\n## Overview\nThis project is a Python library for the [d-py](https://github.com/d-py/d-py) framework.\n\n## Installation\n\nThe easiest way to install d-py is to use pip:\n\n    pip install d-py\n\n## Usage\n\n### CLI\n\n```\nUsage: d-py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  create-project  Create a new project.\n  serve           Run the project.\n```\n\n### Create a project\n\n```\nd-py create-project [project_name]\n```\n\n### Run a project\n\n```\nd-py serve\n```\n\n### Running a project in development mode\n\nTo run a project in development mode you can run the following command:\n\n```\nd-py serve --dev\n```", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t \n\treturn \"GIT_PS1_SHOWDIRTYSTATE=true\" in os.environ.get('PS1', '') and \\\n           \"GIT_PS1_SHOWSTASHSTATE=true\" in os.environ.get('PS1', '') and \\\n           \"GIT_PS1_SHOWUNTRACKEDFILES=true\" in os.environ.get('PS1', '') and \\\n           \"GIT_PS1_SHOWUPSTREAM=\" in os.environ.get('PS1', '')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n        if os.name == 'nt' and os.environ.get('TERM') == 'xterm':\n            return True\n        else:\n            return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t  \n\ttry:\n\t\treturn os.environ['MSYSTEM'] == 'MINGW64'\n\texcept KeyError:\n\t\treturn False\n\t\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\t\n\n\treturn os.name == 'nt'\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/generate.py", "project": "witten/borgmatic", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tprint(f\"File {config_filename} already exists. Aborting.\")\n\t\treturn\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\t# TODO: Implement me!\n\tpass\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t # Abort and raise error if the file already exists and overwrite is False\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise FileExistsError(f\"Target file {config_filename} already exists, use --overwrite\")\n\telse:\n\t\tprint(f\"Writing to {config_filename}\")\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t \n\tif not overwrite and os.path.exists(config_filename):\n\t\traise Exception(\n\t\t\t\"File exists and overwrite is disabled. \"\n\t\t\t\"Refusing to write to %s\" % config_filename\n\t\t)\n\telse:\n\t\twith open(config_filename, \"wt\") as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n \tif os.path.exists(config_filename) and overwrite is False:\n\t\tprint('Aborting write of configuration file: %s' % config_filename)\n\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tyaml.dump(rendered_config, f, default_flow_style=False)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif os.path.exists(config_filename):\n\t\tif not overwrite:\n\t\t\traise Exception(\"File already exists and overwrite is False.\")\n\t\telse:\n\t\t\twith open(config_filename, 'w') as f:\n\t\t\t\tf.write(rendered_config)\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif overwrite or not os.path.exists(config_filename):\n\t\twith open(config_filename, 'w') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n\telse:\n\t\tprint(\"Configuration file exists. Please choose another name or delete existing file.\")\n\t\tsys.exit(-1)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t \n\tif os.path.isfile(config_filename) and not overwrite:\n\t\tprint(\"File {} exists, not overwriting\".format(config_filename))\n\t\traise FileExistsError\n\telse:\n\t\tprint(\"Writing to {0} with mode {1:o}\".format(config_filename, mode))\n\t\twith open(config_filename, 'w') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif overwrite or not os.path.exists(config_filename):\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\telse:\n\t\tprint(\"Configuration file {} exists already. Not overwriting.\".format(config_filename))\n\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise Exception('File exists and overwrite is False: {}'.format(config_filename))\n\n\tif not os.path.exists(os.path.dirname(config_filename)):\n\t\tos.makedirs(os.path.dirname(config_filename))\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tf.flush()\n\t\tos.fsync(f.fileno())\n\n\tos.chmod(config_filename, mode)\n/config/config.ini\n[global]\nenv = prod\n\n[app:main]\nhost = 127.0.0.1\nport = 25565\n\n[app:admin]\nhost = 127.0.0.1\nport = 8080\n\n[app:static]\nhost = 127.0.0.1\nport = 8000\n\n[app:nginx]\nhost = 127.0.0.1\nport = 8001\n\n[app:postgresql]\nhost = 127.0.0.1\nport = 5432\n\n[app:redis]\nhost = 127.0."], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tprint(f\"File {config_filename} already exists. Aborting.\")\n\t\treturn\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\t# TODO: Implement me!\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t # Abort and raise error if the file already exists and overwrite is False\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise FileExistsError(f\"Target file {config_filename} already exists, use --overwrite\")\n\telse:\n\t\tprint(f\"Writing to {config_filename}\")\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t \n\tif not overwrite and os.path.exists(config_filename):\n\t\traise Exception(\n\t\t\t\"File exists and overwrite is disabled. \"\n\t\t\t\"Refusing to write to %s\" % config_filename\n\t\t)\n\telse:\n\t\twith open(config_filename, \"wt\") as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n \tif os.path.exists(config_filename) and overwrite is False:\n\t\tprint('Aborting write of configuration file: %s' % config_filename)\n\t\treturn\n\twith open(config_filename, 'w') as f:\n\t\tyaml.dump(rendered_config, f, default_flow_style=False)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif os.path.exists(config_filename):\n\t\tif not overwrite:\n\t\t\traise Exception(\"File already exists and overwrite is False.\")\n\t\telse:\n\t\t\twith open(config_filename, 'w') as f:\n\t\t\t\tf.write(rendered_config)\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif overwrite or not os.path.exists(config_filename):\n\t\twith open(config_filename, 'w') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n\telse:\n\t\tprint(\"Configuration file exists. Please choose another name or delete existing file.\")\n\t\tsys.exit(-1)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t \n\tif os.path.isfile(config_filename) and not overwrite:\n\t\tprint(\"File {} exists, not overwriting\".format(config_filename))\n\t\traise FileExistsError\n\telse:\n\t\tprint(\"Writing to {0} with mode {1:o}\".format(config_filename, mode))\n\t\twith open(config_filename, 'w') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif overwrite or not os.path.exists(config_filename):\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\telse:\n\t\tprint(\"Configuration file {} exists already. Not overwriting.\".format(config_filename))\n\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\t\n\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise Exception('File exists and overwrite is False: {}'.format(config_filename))\n\n\tif not os.path.exists(os.path.dirname(config_filename)):\n\t\tos.makedirs(os.path.dirname(config_filename))\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tf.flush()\n\t\tos.fsync(f.fileno())\n\n\tos.chmod(config_filename, mode)\n/config/config.ini\n[global]\nenv = prod\n\n[app:main]\nhost = 127.0.0.1\nport = 25565\n\n[app:admin]\nhost = 127.0.0.1\nport = 8080\n\n[app:static]\nhost = 127.0.0.1\nport = 8000\n\n[app:nginx]\nhost = 127.0.0.1\nport = 8001\n\n[app:postgresql]\nhost = 127.0.0.1\nport = 5432\n\n[app:redis]\nhost = 127.0.", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/Shell.py", "project": "cloudmesh/cloudmesh-common", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        if seperator is None:\n                seperator = \" && \"\n\n        if seperator == \"\":\n                seperator = \" \"\n\n        for i, line in enumerate(script.split(\"\\n\")):\n                if line.strip()!= \"\":\n                        script = script.replace(line, \"echo -n '{}'\".format(line))\n\n        script = script.replace(\"\\n\", seperator)\n        script = script.replace(\"''\", \"'\")\n        return script\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join([i for i in script.splitlines() if i!= \"\"])\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        script = script.replace(\"\\n\", seperator)\n        script = script.replace(\"\\r\", seperator)\n\treturn script\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.split(\"\\n\")).strip()\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        assert isinstance(script, str)\n        assert isinstance(seperator, str)\n        return seperator.join([ s for s in script.splitlines() if s ])\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        script = script.strip()\n        if script[-1] == \";\":\n                script = script[:-1]\n        if len(script) == 0:\n                return script\n        lines = script.split(\"\\n\")\n        cmd = [lines[0].strip()]\n        for line in lines[1:]:\n                if line.strip():\n                        cmd.append(seperator + line.strip())\n        return \" \".join(cmd)\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        if seperator is None:\n                seperator = \" && \"\n\n        if seperator == \"\":\n                seperator = \" \"\n\n        for i, line in enumerate(script.split(\"\\n\")):\n                if line.strip()!= \"\":\n                        script = script.replace(line, \"echo -n '{}'\".format(line))\n\n        script = script.replace(\"\\n\", seperator)\n        script = script.replace(\"''\", \"'\")\n        return script\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join([i for i in script.splitlines() if i!= \"\"])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        script = script.replace(\"\\n\", seperator)\n        script = script.replace(\"\\r\", seperator)\n\treturn script\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.split(\"\\n\")).strip()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        assert isinstance(script, str)\n        assert isinstance(seperator, str)\n        return seperator.join([ s for s in script.splitlines() if s ])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        script = script.strip()\n        if script[-1] == \";\":\n                script = script[:-1]\n        if len(script) == 0:\n                return script\n        lines = script.split(\"\\n\")\n        cmd = [lines[0].strip()]\n        for line in lines[1:]:\n                if line.strip():\n                        cmd.append(seperator + line.strip())\n        return \" \".join(cmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\t\n        return seperator.join(script.splitlines())\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lib/matplotlib/testing/__init__.py", "project": "rougier/matplotlib", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        p = subprocess.Popen([sys.executable, '-c', 'import sys, importlib; sys.exit(importlib.import_module(\"{}\").{}(*sys.argv[1:]))'.format(func.__module__, func.__name__), *args], env=extra_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return p.communicate(timeout=timeout)\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        # Run the command in a subprocess\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\ttry:\n\t\tproc = subprocess.run(\n\t\t\t[sys.executable, \"-m\", \"main.py\"] + list(args),\n\t\t\ttimeout=timeout,\n\t\t\tenv=env\n\t\t)\n\texcept subprocess.TimeoutExpired as e:\n\t\tproc = e.process\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise e\n\n\tif proc.returncode!= 0:\n\t\traise Exception(\"Non-zero return code: \" + str(proc.returncode))\n\treturn proc\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        temp_dir = tempfile.mkdtemp()\n        if extra_env is None:\n                extra_env = {}\n        extra_env['PATH'] = os.pathsep.join([temp_dir, extra_env.get('PATH', '')])\n        completed_process = subprocess.run([sys.executable, '-m', func.__module__, func.__name__] + list(args),\n                                           timeout=timeout,\n                                           env=os.environ.copy(),\n                                           **extra_env)\n        shutil.rmtree(temp_dir)\n        return completed_process\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        env = os.environ.copy()\n        if extra_env is not None:\n            env.update(extra_env)\n        return subprocess.run([sys.executable, \"-m\", func] + list(args), env=env, timeout=timeout)\n\n\n# @TODO: check this works", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n\n\t# Get the location of the module\n\tmodule_path = func.__module__\n\tassert module_path!= '__main__', 'The main module cannot be run in a subprocess.'\n\n\t# Get the name of the function\n\tfunc_name = func.__name__\n\t\n\t# Get the location of the module\n\tmodule_path = func.__module__\n\n\t# The path to the script running the subprocess\n\tscript_path = os.path.dirname(__file__)\n\t\n\t# The path to the module we want to import\n\tmodule_path = os.path.abspath(os.path.join(script_path, '..', module_path))\n\n\t# The path to the file containing the function we want to import\n\tmodule_path = '{}.py'.format(module_path)\n\n\t# The absolute path to the function we want to import\n\tfunc_path = '{}.{}'.format(module_path, func_name)\n\n\t# The command that will be run in the subprocess\n\tcommand = ['python', '-m','subprocess_run.run', func_path, *args]\n\n\t# The environment variables to be set in the subprocess\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\n\t# Run the subprocess\n\tprocess = subprocess.run(command, env=env, timeout=timeout)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\t# NOTE: the'shell' flag must be set to True or this will not work on Windows\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n\n\t# Add the python executable to the path so we can import the module\n\t# This is needed because the module is in a different location on the VM\n\t# from the python executable\n\tenv = os.environ.copy()\n\tenv[\"PATH\"] = f\"{env['PATH']}:{sys.executable}\"\n\n\t# Get the current directory\n\tcurrent_dir = os.getcwd()\n\n\t# Change directory to the directory of the module\n\tos.chdir(os.path.dirname(inspect.getfile(func)))\n\n\t# Run the function in a subprocess with the specified arguments\n\tresult = subprocess.run([sys.executable, '-m', func.__module__] + list(args), env=env, timeout=timeout, check=True)\n\n\t# Change back to the original directory\n\tos.chdir(current_dir)\n\n\treturn result\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        # Set up environment variables for subprocess\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n        # Run the subprocess\n\ttry:\n\t\tresult = func(*args, env=env, timeout=timeout)\n\texcept TimeoutExpired as e:\n\t\tprint('Process timed out after {} seconds'.format(timeout))\n\t\tresult = e.output\n\texcept Exception as e:\n\t\tprint('Process failed with exception: {}'.format(e))\n\t\tresult = e.output\n\t\n        # Return the result\n\treturn result\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        assert isinstance(func, str)\n\n        # Determine the module and function name\n        module_name, func_name = func.rsplit('.', 1)\n\n        # Build the command\n        cmd = [\"python\", \"-m\", module_name] + list(args)\n        print(cmd, extra_env)\n        # Run the command\n        completed_process = subprocess.run(cmd, env=extra_env, timeout=timeout)\n\n        # Check for errors\n        if completed_process.returncode:\n            raise subprocess.CalledProcessError(completed_process.returncode, cmd)\n        return completed_process\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n\n\tenv = os.environ.copy()\n\tenv.update(extra_env or {})\n\treturn subprocess.run(\n\t\t[sys.executable, \"-m\", \"ccl_testing.subprocess_helper\", func, *args],\n\t\tcheck=True,\n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        p = subprocess.Popen([sys.executable, '-c', 'import sys, importlib; sys.exit(importlib.import_module(\"{}\").{}(*sys.argv[1:]))'.format(func.__module__, func.__name__), *args], env=extra_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return p.communicate(timeout=timeout)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        # Run the command in a subprocess\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\ttry:\n\t\tproc = subprocess.run(\n\t\t\t[sys.executable, \"-m\", \"main.py\"] + list(args),\n\t\t\ttimeout=timeout,\n\t\t\tenv=env\n\t\t)\n\texcept subprocess.TimeoutExpired as e:\n\t\tproc = e.process\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise e\n\n\tif proc.returncode!= 0:\n\t\traise Exception(\"Non-zero return code: \" + str(proc.returncode))\n\treturn proc\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        temp_dir = tempfile.mkdtemp()\n        if extra_env is None:\n                extra_env = {}\n        extra_env['PATH'] = os.pathsep.join([temp_dir, extra_env.get('PATH', '')])\n        completed_process = subprocess.run([sys.executable, '-m', func.__module__, func.__name__] + list(args),\n                                           timeout=timeout,\n                                           env=os.environ.copy(),\n                                           **extra_env)\n        shutil.rmtree(temp_dir)\n        return completed_process\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        env = os.environ.copy()\n        if extra_env is not None:\n            env.update(extra_env)\n        return subprocess.run([sys.executable, \"-m\", func] + list(args), env=env, timeout=timeout)\n\n\n# @TODO: check this works", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n\n\t# Get the location of the module\n\tmodule_path = func.__module__\n\tassert module_path!= '__main__', 'The main module cannot be run in a subprocess.'\n\n\t# Get the name of the function\n\tfunc_name = func.__name__\n\t\n\t# Get the location of the module\n\tmodule_path = func.__module__\n\n\t# The path to the script running the subprocess\n\tscript_path = os.path.dirname(__file__)\n\t\n\t# The path to the module we want to import\n\tmodule_path = os.path.abspath(os.path.join(script_path, '..', module_path))\n\n\t# The path to the file containing the function we want to import\n\tmodule_path = '{}.py'.format(module_path)\n\n\t# The absolute path to the function we want to import\n\tfunc_path = '{}.{}'.format(module_path, func_name)\n\n\t# The command that will be run in the subprocess\n\tcommand = ['python', '-m','subprocess_run.run', func_path, *args]\n\n\t# The environment variables to be set in the subprocess\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\n\t# Run the subprocess\n\tprocess = subprocess.run(command, env=env, timeout=timeout)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\t# NOTE: the'shell' flag must be set to True or this will not work on Windows\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE: on Windows, the shell flag must be set to True, otherwise the path to python is not found\n\t# NOTE", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n\n\t# Add the python executable to the path so we can import the module\n\t# This is needed because the module is in a different location on the VM\n\t# from the python executable\n\tenv = os.environ.copy()\n\tenv[\"PATH\"] = f\"{env['PATH']}:{sys.executable}\"\n\n\t# Get the current directory\n\tcurrent_dir = os.getcwd()\n\n\t# Change directory to the directory of the module\n\tos.chdir(os.path.dirname(inspect.getfile(func)))\n\n\t# Run the function in a subprocess with the specified arguments\n\tresult = subprocess.run([sys.executable, '-m', func.__module__] + list(args), env=env, timeout=timeout, check=True)\n\n\t# Change back to the original directory\n\tos.chdir(current_dir)\n\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        # Set up environment variables for subprocess\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n        # Run the subprocess\n\ttry:\n\t\tresult = func(*args, env=env, timeout=timeout)\n\texcept TimeoutExpired as e:\n\t\tprint('Process timed out after {} seconds'.format(timeout))\n\t\tresult = e.output\n\texcept Exception as e:\n\t\tprint('Process failed with exception: {}'.format(e))\n\t\tresult = e.output\n\t\n        # Return the result\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n        assert isinstance(func, str)\n\n        # Determine the module and function name\n        module_name, func_name = func.rsplit('.', 1)\n\n        # Build the command\n        cmd = [\"python\", \"-m\", module_name] + list(args)\n        print(cmd, extra_env)\n        # Run the command\n        completed_process = subprocess.run(cmd, env=extra_env, timeout=timeout)\n\n        # Check for errors\n        if completed_process.returncode:\n            raise subprocess.CalledProcessError(completed_process.returncode, cmd)\n        return completed_process\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\t\n\n\tenv = os.environ.copy()\n\tenv.update(extra_env or {})\n\treturn subprocess.run(\n\t\t[sys.executable, \"-m\", \"ccl_testing.subprocess_helper\", func, *args],\n\t\tcheck=True,\n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/override.py", "project": "witten/borgmatic", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tname = matcher['string']\n\tdefault = None\n\tif 'default' in matcher:\n\t\tdefault = matcher['default']\n\n\treturn os.environ.get(name, default)\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tvalue = os.environ[matcher.group('name')]\n\texcept KeyError:\n\t\tif matcher.group('default') is None:\n\t\t\traise Error('Environment variable %s is not defined' % matcher.group('name'))\n\t\tvalue = matcher.group('default')\n\treturn value\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tdef get_string_value(s, default=None):\n\t\t\"\"\"\n\t\tGet the value of the variable if defined in environment, or the default value if not.\n\t\t\"\"\"\n\t\tif default:\n\t\t\treturn os.environ.get(s, default)\n\t\treturn os.environ.get(s)\n\n\tif matcher.group(1) is not None:\n\t\treturn get_string_value(matcher.group(1), matcher.group(2))\n\treturn get_string_value(matcher.group(2))\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tif not matcher.groupdict()['default']:\n\t\t\tif not os.environ.get(matcher.groupdict()['name'], None):\n\t\t\t\traise ValueError('Missing environment variable: {}'.format(matcher.groupdict()['name']))\n\t\t\treturn os.environ.get(matcher.groupdict()['name'])\n\t\telse:\n\t\t\treturn matcher.groupdict()['default']\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tif matcher.groupdict().get('default') is None and not os.environ.get(matcher.groupdict()['name']):\n\t\traise Error('Environment variable \"{}\" not defined and no default value provided.'.format(matcher.groupdict()['name']))\n\n\treturn os.environ.get(matcher.groupdict()['name'], matcher.groupdict()['default'])\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n    \tvalue = os.environ.get(matcher.group(1), matcher.group(2))\n\t\n    \tif not value:\n        \traise Error('Variable \"%s\" not defined in environment' % matcher.group(1))\n\t\n    \treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t \n\tif matcher:\n\t\tname = matcher.group(\"name\")\n\t\tdefault = matcher.group(\"default\")\n\t\tif name in os.environ:\n\t\t\treturn os.environ[name]\n\t\telif default:\n\t\t\treturn default\n\t\telse:\n\t\t\traise Error(\"Variable %s is not defined in environment\" % name)\n\telse:\n\t\traise Error(\"No valid value in variable\")\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tname = matcher.group(1)\n\tdefault = matcher.group(2)\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default:\n\t\treturn default\n\telse:\n\t\traise EnvironmentError('Environment variable {} is not defined!'.format(name))\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tif matcher.group('default'):\n\t\tdefault = matcher.group('default')\n\t\tif default[0] == '$':\n\t\t\tdefault = os.environ[default[1:]]\n\telse:\n\t\tdefault = None\n\n\tif matcher.group('name') in os.environ:\n\t\tvalue = os.environ[matcher.group('name')]\n\telif default:\n\t\tvalue = default\n\telse:\n\t\traise Exception('Variable ${%s} is not defined in environment' % matcher.group('name'))\n\n\treturn value\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tif matcher.group('default'):\n\t\t\treturn matcher.group('default')\n\t\telse:\n\t\t\traise Error('Environment variable {0} not defined'.format(matcher.group('env')))\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tname = matcher['string']\n\tdefault = None\n\tif 'default' in matcher:\n\t\tdefault = matcher['default']\n\n\treturn os.environ.get(name, default)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tvalue = os.environ[matcher.group('name')]\n\texcept KeyError:\n\t\tif matcher.group('default') is None:\n\t\t\traise Error('Environment variable %s is not defined' % matcher.group('name'))\n\t\tvalue = matcher.group('default')\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tdef get_string_value(s, default=None):\n\t\t\"\"\"\n\t\tGet the value of the variable if defined in environment, or the default value if not.\n\t\t\"\"\"\n\t\tif default:\n\t\t\treturn os.environ.get(s, default)\n\t\treturn os.environ.get(s)\n\n\tif matcher.group(1) is not None:\n\t\treturn get_string_value(matcher.group(1), matcher.group(2))\n\treturn get_string_value(matcher.group(2))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tif not matcher.groupdict()['default']:\n\t\t\tif not os.environ.get(matcher.groupdict()['name'], None):\n\t\t\t\traise ValueError('Missing environment variable: {}'.format(matcher.groupdict()['name']))\n\t\t\treturn os.environ.get(matcher.groupdict()['name'])\n\t\telse:\n\t\t\treturn matcher.groupdict()['default']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tif matcher.groupdict().get('default') is None and not os.environ.get(matcher.groupdict()['name']):\n\t\traise Error('Environment variable \"{}\" not defined and no default value provided.'.format(matcher.groupdict()['name']))\n\n\treturn os.environ.get(matcher.groupdict()['name'], matcher.groupdict()['default'])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n    \tvalue = os.environ.get(matcher.group(1), matcher.group(2))\n\t\n    \tif not value:\n        \traise Error('Variable \"%s\" not defined in environment' % matcher.group(1))\n\t\n    \treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t \n\tif matcher:\n\t\tname = matcher.group(\"name\")\n\t\tdefault = matcher.group(\"default\")\n\t\tif name in os.environ:\n\t\t\treturn os.environ[name]\n\t\telif default:\n\t\t\treturn default\n\t\telse:\n\t\t\traise Error(\"Variable %s is not defined in environment\" % name)\n\telse:\n\t\traise Error(\"No valid value in variable\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tname = matcher.group(1)\n\tdefault = matcher.group(2)\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default:\n\t\treturn default\n\telse:\n\t\traise EnvironmentError('Environment variable {} is not defined!'.format(name))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tif matcher.group('default'):\n\t\tdefault = matcher.group('default')\n\t\tif default[0] == '$':\n\t\t\tdefault = os.environ[default[1:]]\n\telse:\n\t\tdefault = None\n\n\tif matcher.group('name') in os.environ:\n\t\tvalue = os.environ[matcher.group('name')]\n\telif default:\n\t\tvalue = default\n\telse:\n\t\traise Exception('Variable ${%s} is not defined in environment' % matcher.group('name'))\n\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tif matcher.group('default'):\n\t\t\treturn matcher.group('default')\n\t\telse:\n\t\t\traise Error('Environment variable {0} not defined'.format(matcher.group('env')))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cinder/image/glance.py", "project": "openstack/cinder", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tparts = image_href.split('://')\n\tif len(parts)!= 2:\n\t\traise ValueError(f\"Expected 2 parts in image href, got {parts}\")\n\n\tnetloc = parts[1]\n\tprotocol = parts[0]\n\n\tuse_ssl = protocol == 'https'\n\n\tparts = netloc.split('/')\n\tif len(parts)!= 2:\n\t\traise ValueError(f\"Expected 2 parts in netloc, got {parts}\")\n\n\timage_id = parts[0]\n\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tparts = urlparse(image_href)\n\n\tif parts.scheme not in [\"http\", \"https\"]:\n\t\traise ValueError(\"Image href must use http or https protocol\")\n\n\tif parts.netloc not in [\"images.contentstack.io\", \"images.contentstack.com\"]:\n\t\traise ValueError(\"Image href must use images.contentstack.io or images.contentstack.com domain\")\n\n\timage_id = parts.path.split(\"/\")[1]\n\tuse_ssl = parts.scheme == \"https\"\n\n\treturn image_id, parts.netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\t# TODO: need to parse out image_id and image_href and then import urllib.parse.urlparse(image_href).netloc\n\n\treturn image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\t# Parse the URL into its parts\n\turl = urllib.parse.urlparse(image_href)\n\n\t# Get the image id and the netloc\n\timage_id = url.path.split('/')[-1]\n\tnetloc = url.netloc\n\n\t# Determine whether or not to use SSL\n\tuse_ssl = url.scheme == 'https'\n\n\treturn image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\t# TODO: check if image_href is a valid URL\n\t# TODO: check if image_href is of the form http://image_id.netloc/...\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\t# parse the url\n\tp = urlparse(image_href)\n\t\n\t# check that there is a scheme\n\tif not p.scheme:\n\t\traise ValueError(f\"No scheme in {image_href!r}\")\n\t\n\t# check that there is a hostname\n\tif not p.netloc:\n\t\traise ValueError(f\"No hostname in {image_href!r}\")\n\t\n\t# check that there is a path\n\tif not p.path:\n\t\traise ValueError(f\"No path in {image_href!r}\")\n\t\n\t# check that there is a filename\n\tif not p.path.rsplit(\"/\", 1)[-1]:\n\t\traise ValueError(f\"No filename in {image_href!r}\")\n\t\n\t# check that the scheme is either https or http\n\tif p.scheme not in (\"http\", \"https\"):\n\t\traise ValueError(f\"Scheme {p.scheme!r} not http or https in {image_href!r}\")\n\t\n\t# check that the path starts with a /\n\tif not p.path.startswith(\"/\"):\n\t\traise ValueError(f\"Path {p.path!r} does not start with / in {image_href!r}\")\n\t\n\t# check that the path is not a directory\n\tif p.path.endswith(\"/\"):\n\t\traise ValueError(f\"Path {p.path!", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tif not image_href:\n\t\traise ValueError(\"Image href must be non-empty\")\n\n\t# parse the URI into a \"parsed\" object\n\tparsed = urlparse(image_href)\n\tif not parsed.scheme or not parsed.netloc:\n\t\traise ValueError(\"Image href must be a valid URL\")\n\n\t# convert the scheme to lowercase\n\tuse_ssl = False\n\tif parsed.scheme == \"https\":\n\t\tuse_ssl = True\n\telif parsed.scheme!= \"http\":\n\t\traise ValueError(\"Image href must be an HTTP or HTTPS URL\")\n\n\t# return the image ID and netloc\n\treturn parsed.path[1:], parsed.netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n#\tif not image_href.startswith('http://') and not image_href.startswith('https://'):\n#\t\traise ValueError(\"image_href is not a valid image href\")\n\n\t# parse the url\n\timage_href = image_href.replace('http://', 'https://')\n\tparsed_url = urlparse(image_href)\n\n\t# make sure it's a valid image\n\tif parsed_url.path.endswith('.png') or parsed_url.path.endswith('.jpg'):\n\t\timage_id = parsed_url.path.split('/')[-1]\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = True if parsed_url.scheme == 'https' else False\n\telse:\n\t\traise ValueError(\"image_href is not a valid image href\")\n\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n        # remove the protocol from the url\n\timage_href = image_href.replace('http://', '').replace('https://', '')\n\tat_index = image_href.find('@')\n\t\n\tif at_index == -1:\n\t\traise ValueError(\"Could not parse image href: %s\" % image_href)\n\n        # extract the image id\n\timage_id = image_href[:at_index]\n\n        # extract the netloc\n\tnetloc = image_href[at_index + 1:]\n\tat_index = netloc.find('@')\n\tif at_index == -1:\n\t\traise ValueError(\"Could not parse image href: %s\" % image_href)\n\n\tnetloc = netloc[:at_index]\n\n        # determine if the netloc is using ssl\n\tuse_ssl = netloc.startswith('https://')\n\n\treturn image_id, netloc, use_ssl\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tif not image_href.startswith(\"image/\"):\n\t\traise ValueError(f\"{image_href} does not start with 'image/'\")\n\n\tnetloc = image_href[6:]\n\tif not netloc.startswith(\"http\"):\n\t\traise ValueError(f\"{netloc} does not start with 'http'\")\n\n\tuse_ssl = netloc.startswith(\"https\")\n\treturn netloc[len(\"http://\") if use_ssl else len(\"http://\")], netloc, use_ssl\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tparts = image_href.split('://')\n\tif len(parts)!= 2:\n\t\traise ValueError(f\"Expected 2 parts in image href, got {parts}\")\n\n\tnetloc = parts[1]\n\tprotocol = parts[0]\n\n\tuse_ssl = protocol == 'https'\n\n\tparts = netloc.split('/')\n\tif len(parts)!= 2:\n\t\traise ValueError(f\"Expected 2 parts in netloc, got {parts}\")\n\n\timage_id = parts[0]\n\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tparts = urlparse(image_href)\n\n\tif parts.scheme not in [\"http\", \"https\"]:\n\t\traise ValueError(\"Image href must use http or https protocol\")\n\n\tif parts.netloc not in [\"images.contentstack.io\", \"images.contentstack.com\"]:\n\t\traise ValueError(\"Image href must use images.contentstack.io or images.contentstack.com domain\")\n\n\timage_id = parts.path.split(\"/\")[1]\n\tuse_ssl = parts.scheme == \"https\"\n\n\treturn image_id, parts.netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\t# TODO: need to parse out image_id and image_href and then import urllib.parse.urlparse(image_href).netloc\n\n\treturn image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\t# Parse the URL into its parts\n\turl = urllib.parse.urlparse(image_href)\n\n\t# Get the image id and the netloc\n\timage_id = url.path.split('/')[-1]\n\tnetloc = url.netloc\n\n\t# Determine whether or not to use SSL\n\tuse_ssl = url.scheme == 'https'\n\n\treturn image_id, netloc, use_ssl\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\t# TODO: check if image_href is a valid URL\n\t# TODO: check if image_href is of the form http://image_id.netloc/...\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if the image_id is a valid image_id\n\t# TODO: check if", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\t# parse the url\n\tp = urlparse(image_href)\n\t\n\t# check that there is a scheme\n\tif not p.scheme:\n\t\traise ValueError(f\"No scheme in {image_href!r}\")\n\t\n\t# check that there is a hostname\n\tif not p.netloc:\n\t\traise ValueError(f\"No hostname in {image_href!r}\")\n\t\n\t# check that there is a path\n\tif not p.path:\n\t\traise ValueError(f\"No path in {image_href!r}\")\n\t\n\t# check that there is a filename\n\tif not p.path.rsplit(\"/\", 1)[-1]:\n\t\traise ValueError(f\"No filename in {image_href!r}\")\n\t\n\t# check that the scheme is either https or http\n\tif p.scheme not in (\"http\", \"https\"):\n\t\traise ValueError(f\"Scheme {p.scheme!r} not http or https in {image_href!r}\")\n\t\n\t# check that the path starts with a /\n\tif not p.path.startswith(\"/\"):\n\t\traise ValueError(f\"Path {p.path!r} does not start with / in {image_href!r}\")\n\t\n\t# check that the path is not a directory\n\tif p.path.endswith(\"/\"):\n\t\traise ValueError(f\"Path {p.path!", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tif not image_href:\n\t\traise ValueError(\"Image href must be non-empty\")\n\n\t# parse the URI into a \"parsed\" object\n\tparsed = urlparse(image_href)\n\tif not parsed.scheme or not parsed.netloc:\n\t\traise ValueError(\"Image href must be a valid URL\")\n\n\t# convert the scheme to lowercase\n\tuse_ssl = False\n\tif parsed.scheme == \"https\":\n\t\tuse_ssl = True\n\telif parsed.scheme!= \"http\":\n\t\traise ValueError(\"Image href must be an HTTP or HTTPS URL\")\n\n\t# return the image ID and netloc\n\treturn parsed.path[1:], parsed.netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n#\tif not image_href.startswith('http://') and not image_href.startswith('https://'):\n#\t\traise ValueError(\"image_href is not a valid image href\")\n\n\t# parse the url\n\timage_href = image_href.replace('http://', 'https://')\n\tparsed_url = urlparse(image_href)\n\n\t# make sure it's a valid image\n\tif parsed_url.path.endswith('.png') or parsed_url.path.endswith('.jpg'):\n\t\timage_id = parsed_url.path.split('/')[-1]\n\t\tnetloc = parsed_url.netloc\n\t\tuse_ssl = True if parsed_url.scheme == 'https' else False\n\telse:\n\t\traise ValueError(\"image_href is not a valid image href\")\n\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n        # remove the protocol from the url\n\timage_href = image_href.replace('http://', '').replace('https://', '')\n\tat_index = image_href.find('@')\n\t\n\tif at_index == -1:\n\t\traise ValueError(\"Could not parse image href: %s\" % image_href)\n\n        # extract the image id\n\timage_id = image_href[:at_index]\n\n        # extract the netloc\n\tnetloc = image_href[at_index + 1:]\n\tat_index = netloc.find('@')\n\tif at_index == -1:\n\t\traise ValueError(\"Could not parse image href: %s\" % image_href)\n\n\tnetloc = netloc[:at_index]\n\n        # determine if the netloc is using ssl\n\tuse_ssl = netloc.startswith('https://')\n\n\treturn image_id, netloc, use_ssl\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\t\n\n\tif not image_href.startswith(\"image/\"):\n\t\traise ValueError(f\"{image_href} does not start with 'image/'\")\n\n\tnetloc = image_href[6:]\n\tif not netloc.startswith(\"http\"):\n\t\traise ValueError(f\"{netloc} does not start with 'http'\")\n\n\tuse_ssl = netloc.startswith(\"https\")\n\treturn netloc[len(\"http://\") if use_ssl else len(\"http://\")], netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif not input_list:\n\t\treturn []\n\n\toutput_list = []\n\tfor i in input_list:\n\t\tif len(i) > 1 and i[-1] == os.sep:\n\t\t\toutput_list.append(i[:-1])\n\t\telse:\n\t\t\toutput_list.append(i)\n\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\treturn [i[:-1] if i and i[-1] == os.sep else i for i in input_list]\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\toutput_list = []\n\t\tfor item in input_list:\n\t\t\tif len(item) > 1:\n\t\t\t\tif item[-1] == os.sep:\n\t\t\t\t\toutput_list.append(item[:-1])\n\t\t\t\telse:\n\t\t\t\t\toutput_list.append(item)\n\t\t\telse:\n\t\t\t\toutput_list.append(item)\n\treturn output_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor item in input_list:\n\t\toutput_list.append(item)\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\toutput_list[-1] = item[:-1]\n\n\treturn output_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t  \n\tif input_list is None:\n\t\tinput_list = []\n\n\toutput_list = []\n\n\tfor s in input_list:\n\t\tif len(s) > 1 and s[-1] == os.path.sep:\n\t\t\toutput_list.append(s[:-1])\n\t\telse:\n\t\t\toutput_list.append(s)\n\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\toutput_list.append(item[:-1])\n\t\telse:\n\t\t\toutput_list.append(item)\n\t\t\t\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\n\t# if input list is None, return []\n\tif input_list is None:\n\t\treturn []\n\n\t# iterate over list\n\tfor i in range(len(input_list)):\n\n\t\t# if last character is the pathname seperator, remove it\n\t\tif len(input_list[i]) > 1:\n\t\t\tif input_list[i][-1] == os.path.sep:\n\t\t\t\tinput_list[i] = input_list[i][:-1]\n\n\treturn input_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\tfor i in range(len(input_list)):\n\t\t\tif len(input_list[i]) > 1 and input_list[i][-1] == os.sep:\n\t\t\t\tinput_list[i] = input_list[i][:-1]\n\t\treturn input_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\treturn_list = []\n\t\tfor item in input_list:\n\t\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\t\treturn_list.append(item[:-1])\n\t\t\telse:\n\t\t\t\treturn_list.append(item)\n\t\treturn return_list\n\t\t", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\n\tif not input_list:\n\t\treturn []\n\t\t#return [input_list]\n\n\toutput_list = []\n\n\tfor i in input_list:\n\t\tif len(i) > 1:\n\t\t\tif i[-1] == os.sep:\n\t\t\t\toutput_list.append(i[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(i)\n\t\telse:\n\t\t\toutput_list.append(i)\n\treturn output_list\n\t"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif not input_list:\n\t\treturn []\n\n\toutput_list = []\n\tfor i in input_list:\n\t\tif len(i) > 1 and i[-1] == os.sep:\n\t\t\toutput_list.append(i[:-1])\n\t\telse:\n\t\t\toutput_list.append(i)\n\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\treturn [i[:-1] if i and i[-1] == os.sep else i for i in input_list]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\toutput_list = []\n\t\tfor item in input_list:\n\t\t\tif len(item) > 1:\n\t\t\t\tif item[-1] == os.sep:\n\t\t\t\t\toutput_list.append(item[:-1])\n\t\t\t\telse:\n\t\t\t\t\toutput_list.append(item)\n\t\t\telse:\n\t\t\t\toutput_list.append(item)\n\treturn output_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\tfor item in input_list:\n\t\toutput_list.append(item)\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\toutput_list[-1] = item[:-1]\n\n\treturn output_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t  \n\tif input_list is None:\n\t\tinput_list = []\n\n\toutput_list = []\n\n\tfor s in input_list:\n\t\tif len(s) > 1 and s[-1] == os.path.sep:\n\t\t\toutput_list.append(s[:-1])\n\t\telse:\n\t\t\toutput_list.append(s)\n\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\t\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\toutput_list.append(item[:-1])\n\t\telse:\n\t\t\toutput_list.append(item)\n\t\t\t\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\n\t# if input list is None, return []\n\tif input_list is None:\n\t\treturn []\n\n\t# iterate over list\n\tfor i in range(len(input_list)):\n\n\t\t# if last character is the pathname seperator, remove it\n\t\tif len(input_list[i]) > 1:\n\t\t\tif input_list[i][-1] == os.path.sep:\n\t\t\t\tinput_list[i] = input_list[i][:-1]\n\n\treturn input_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\tfor i in range(len(input_list)):\n\t\t\tif len(input_list[i]) > 1 and input_list[i][-1] == os.sep:\n\t\t\t\tinput_list[i] = input_list[i][:-1]\n\t\treturn input_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t \n\tif input_list is None:\n\t\treturn []\n\telse:\n\t\treturn_list = []\n\t\tfor item in input_list:\n\t\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\t\treturn_list.append(item[:-1])\n\t\t\telse:\n\t\t\t\treturn_list.append(item)\n\t\treturn return_list\n\t\t", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\n\tif not input_list:\n\t\treturn []\n\t\t#return [input_list]\n\n\toutput_list = []\n\n\tfor i in input_list:\n\t\tif len(i) > 1:\n\t\t\tif i[-1] == os.sep:\n\t\t\t\toutput_list.append(i[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(i)\n\t\telse:\n\t\t\toutput_list.append(i)\n\treturn output_list\n\t", "is_pass": true, "return_code": 0}]}
{"file_path": "shconfparser/search.py", "project": "kirankotari/shconfparser", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if isinstance(pattern, str):\n            pattern = re.compile(pattern)\n\n        if strip:\n            pattern = re.compile(pattern.pattern.replace(\" \", r\"\\s*\"))\n\n        return pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t  \n\tif not isinstance(pattern, re.Pattern):\n\t\tpattern = re.compile(pattern, re.IGNORECASE) if pattern else None\n\tif strip:\n\t\tpattern = re.compile(r'^[ \\t]+|[ \\t]+$', re.IGNORECASE) if pattern else None\n\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t \n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.VERBOSE)\n\n\tif strip:\n\t\tpattern = re.compile(r'^\\s*(.*?)\\s*$')\n\t\tpattern = pattern.sub(r'\\1', str(pattern))\n\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n# \tif isinstance(pattern, str):\n# \t\tpattern = re.compile(pattern)\n\tif strip:\n\t\tpattern = re.compile(pattern, re.IGNORECASE | re.DOTALL)\n\telse:\n\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if isinstance(pattern, str):\n            pattern = re.compile(pattern, re.IGNORECASE)\n\n        if strip:\n            pattern = re.compile(re.escape(pattern.pattern), re.IGNORECASE)\n        return pattern\n\t", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if isinstance(pattern, str):\n            if strip:\n                pattern = pattern.strip()\n            pattern = re.compile(pattern)\n        return pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n\n\n\tif type(pattern) is not re._pattern_type:\n\t\t# If the pattern is not a regex pattern object\n\t\tpattern = pattern.replace(\" \", \"\")\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\telse:\n\t\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n    \tif isinstance(pattern, str):\n    \t\tif strip:\n    \t\t\tpattern = pattern.lstrip().rstrip()\n    \t\tpattern = re.compile(pattern)\n    \treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t \n\tif isinstance(pattern, type('')):\n\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\n\telif not isinstance(pattern, re._pattern_type):\n\t\traise ValueError('invalid pattern type')\n\n\tif strip:\n\t\tpattern = re.compile('^\\\\s*(.*?)\\\\s*$', re.IGNORECASE)\n\t\tpattern = pattern.sub(r'\\1', str(pattern))\n\n\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if not isinstance(pattern, str):\n            return pattern\n\n        if strip:\n            pattern = pattern.strip()\n\n        pattern = pattern.replace(\" \", \"\\s\")\n        pattern = pattern.replace(\"\\r\", \"\\n\")\n        pattern = pattern.replace(\"\\n\", \"\\\\\\n\")\n\n        return re.compile(pattern)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if isinstance(pattern, str):\n            pattern = re.compile(pattern)\n\n        if strip:\n            pattern = re.compile(pattern.pattern.replace(\" \", r\"\\s*\"))\n\n        return pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t  \n\tif not isinstance(pattern, re.Pattern):\n\t\tpattern = re.compile(pattern, re.IGNORECASE) if pattern else None\n\tif strip:\n\t\tpattern = re.compile(r'^[ \\t]+|[ \\t]+$', re.IGNORECASE) if pattern else None\n\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t \n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.VERBOSE)\n\n\tif strip:\n\t\tpattern = re.compile(r'^\\s*(.*?)\\s*$')\n\t\tpattern = pattern.sub(r'\\1', str(pattern))\n\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n# \tif isinstance(pattern, str):\n# \t\tpattern = re.compile(pattern)\n\tif strip:\n\t\tpattern = re.compile(pattern, re.IGNORECASE | re.DOTALL)\n\telse:\n\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if isinstance(pattern, str):\n            pattern = re.compile(pattern, re.IGNORECASE)\n\n        if strip:\n            pattern = re.compile(re.escape(pattern.pattern), re.IGNORECASE)\n        return pattern\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if isinstance(pattern, str):\n            if strip:\n                pattern = pattern.strip()\n            pattern = re.compile(pattern)\n        return pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n\n\n\tif type(pattern) is not re._pattern_type:\n\t\t# If the pattern is not a regex pattern object\n\t\tpattern = pattern.replace(\" \", \"\")\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\treturn re.compile(pattern)\n\telse:\n\t\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n    \tif isinstance(pattern, str):\n    \t\tif strip:\n    \t\t\tpattern = pattern.lstrip().rstrip()\n    \t\tpattern = re.compile(pattern)\n    \treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t \n\tif isinstance(pattern, type('')):\n\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\n\telif not isinstance(pattern, re._pattern_type):\n\t\traise ValueError('invalid pattern type')\n\n\tif strip:\n\t\tpattern = re.compile('^\\\\s*(.*?)\\\\s*$', re.IGNORECASE)\n\t\tpattern = pattern.sub(r'\\1', str(pattern))\n\n\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n        if not isinstance(pattern, str):\n            return pattern\n\n        if strip:\n            pattern = pattern.strip()\n\n        pattern = pattern.replace(\" \", \"\\s\")\n        pattern = pattern.replace(\"\\r\", \"\\n\")\n        pattern = pattern.replace(\"\\n\", \"\\\\\\n\")\n\n        return re.compile(pattern)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/core.py", "project": "burgerbecky/makeprojects", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tcommand = commands[0]\n\t\tif isinstance(args, list):\n\t\t\targs =''.join(args)\n\t\telse:\n\t\t\targs = str(args)\n\t\tif verbose:\n\t\t\tprint(\"Running command: {0} {1}\".format(command, args))\n\t\tif hide_stderr:\n\t\t\tproc = subprocess.Popen([command, args], stdout=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, cwd=cwd, env=env)\n\t\telse:\n\t\t\tproc = subprocess.Popen([command, args], stdout=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, cwd=cwd, stderr=subprocess.PIPE, env=env)\n\t\treturn_code = proc.wait()\n\t\tstdout, stderr = proc.communicate()\n\t\tif verbose:\n\t\t\tprint(\"Command finished successfully.\")\n\t\t\tif return_code:\n\t\t\t\tprint(\"Exit code: {0}\\n\".format(return_code))\n\t\t\tif stdout:\n\t\t\t\tprint(\"Output:\\n{0}\\n\".format(stdout))\n\t\t\tif stderr:\n\t\t\t\tprint(\"Error:\\n{0}\\n\".format(stderr))\n\t\treturn (stdout, stderr, return_code)\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\t# Add stdin=PIPE to the popen args if stdin is not None\n\t# if stdin is not None:\n\t# \tpopen_args['stdin'] = subprocess.PIPE\n\t\n\tif verbose:\n\t\tprint 'Running Commands: {}'.format(commands)\n\t\tprint 'in directory: {}'.format(cwd)\n\t\tprint 'with arguments: {}'.format(args)\n\t\tprint 'with environment: {}'.format(env)\n\t\n\tif verbose and hide_stderr:\n\t\tpopen_args['stdout'] = subprocess.PIPE\n\t\tpopen_args['stderr'] = subprocess.PIPE\n\telse:\n\t\tpopen_args['stdout'] = subprocess.PIPE\n\t\n\tpopen_args['cwd'] = cwd\n\tpopen_args['env'] = env\n\tpopen_args['shell'] = True\n\tpopen_args['universal_newlines'] = True\n\t\n\tproc = subprocess.Popen(commands, **popen_args)\n\tstdout = proc.communicate()\n\tretcode = proc.returncode\n\t\n\tif retcode:\n\t\traise subprocess.CalledProcessError(retcode, commands)\n\t\n\treturn stdout\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tassert isinstance(commands, list)\n\t\tassert isinstance(args, list)\n\t\tprocess = subprocess.Popen(commands + args, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None), cwd=cwd, env=env)\n\t\tstdoutput, erroroutput = process.communicate()\n\t\texit_code = process.returncode\n\t\treturn stdoutput, erroroutput, exit_code\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\t# For debugging\n\tif verbose:\n\t\tprint(\"running: \" + str(commands) + \" with \" + str(args), flush=True)\n\n\tresult = None\n\ttry:\n\t\tresult = subprocess.run(commands, args, cwd=cwd, env=env, capture_output=False, shell=False, check=True, text=True)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(\"error running command: \" + str(commands) + \" with \" + str(args) + \"\\n\" + str(e.output), flush=True)\n\t\tresult = e.returncode\n\texcept OSError as e:\n\t\tprint(\"error running command: \" + str(commands) + \" with \" + str(args) + \"\\n\" + str(e), flush=True)\n\t\tresult = 127\n\n\tif verbose:\n\t\tprint(\"result code: \" + str(result.returncode), flush=True)\n\n\treturn result\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tstdout = []\n\t\tstderr = []\n\t\tprocess = None\n\t\tdef data_handler(fd, data):\n\t\t\tif fd == 0:\n\t\t\t\tstdout.append(data)\n\t\t\telse:\n\t\t\t\tstderr.append(data)\n\t\tif not isinstance(commands, list):\n\t\t\tcommands = [commands]\n\t\tif verbose:\n\t\t\tprint(\"RUN COMMANDS:\")\n\t\t\tprint(\" \".join(commands))\n\t\t\tprint(\" \".join(args))\n\t\t\tprint(\"CWD: \" + str(cwd))\n\t\ttry:\n\t\t\tprocess = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, cwd=cwd, args=args, env=env)\n\t\t\tif process.stdout:\n\t\t\t\tprocess.stdout.read(1)\n\t\t\t\tprocess.stdout.flush()\n\t\t\t\tprocess.stdout.seek(0, os.SEEK_END)\n\t\t\tif process.stderr:\n\t\t\t\tprocess.stderr.read(1)\n\t\t\t\tprocess.stderr.flush()\n\t\t\t\tprocess.stderr.seek(0, os.SEEK_END)\n\t\t\tif not hide_stderr:\n\t\t\t\tpoll_fd(process.stdout.fileno(), data_handler)\n\t\t\t\tpoll_fd(process.stderr.fileno(), data_handler)\n\t\t\twhile True:\n\t\t\t\tif process.poll() is not None:\n\t\t\t\t\t", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tif verbose:\n\t\t\tprint(\"Running command: {0} {1}\".format(commands, args))\n\n\t\tif hide_stderr:\n\t\t\tdevnull = open(os.devnull, 'wb')\n\t\telse:\n\t\t\tdevnull = None\n\n\t\ttry:\n\t\t\tif cwd is None:\n\t\t\t\treturn subprocess.check_output(shlex.split(commands) + args, env=env, stderr=devnull)\n\t\t\telse:\n\t\t\t\treturn subprocess.check_output(shlex.split(commands) + args, cwd=cwd, env=env, stderr=devnull)\n\t\texcept KeyboardInterrupt:\n\t\t\tsys.exit(1)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tsys.exit(\"Command: {0} {1} failed with returncode {2} and error message: {3}\".format(commands, args, e.returncode, e.output))\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\n\n\tif hide_stderr:\n\t\tdevnull = open(os.devnull, 'w')\n\telse:\n\t\tdevnull = None\n\ttry:\n\t\twith open(os.devnull, 'w') as f:\n\t\t\tif verbose:\n\t\t\t\tproc = subprocess.Popen(commands, stdout=f, stderr=f, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tproc = subprocess.Popen(commands, stdout=f, stderr=f, cwd=cwd, env=env)\n\t\t\t(out, err) = proc.communicate()\n\t\treturncode = proc.returncode\n\t\tif returncode:\n\t\t\traise ProcessException(commands, returncode)\n\t\telse:\n\t\t\treturn (out, returncode)\n\tfinally:\n\t\tif hide_stderr:\n\t\t\tdevnull.close()\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\n\n\tresult = []\n\tfor command in commands:\n\t\tresult.append(run_command_single(command, args, cwd, verbose, hide_stderr, env))\n\treturn result\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tassert(len(commands) > 0)\n\tif verbose:\n\t\tprint \"Running command: \" + str(commands) + str(args)\n\t\n\tprocess = subprocess.Popen(str(commands) + str(args), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tstdout, stderr = process.communicate()\n\tif process.returncode!= 0 and not hide_stderr:\n\t\t#error_text = \"Error code: \" + str(process.returncode) + \"\\nOutput: \" + stdout + \"\\nError: \" + stderr\n\t\terror_text = \"Error code: \" + str(process.returncode) + \"\\nError: \" + stderr\n\t\traise Exception(error_text)\n\treturn stdout, process.returncode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t \n\tresult = None\n\ttry:\n\t\t#print \"commands: \", commands\n\t\t#print \"args: \", args\n\t\tproc = subprocess.Popen(commands, args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None))\n\t\tout, err = proc.communicate()\n\t\tresult = Result(out, proc.returncode)\n\t\tif verbose:\n\t\t\tprint \"out: \", out\n\t\t\tprint \"err: \", err\n\t\t\tprint \"result: \", result\n\texcept:\n\t\tresult = Result(\"Error with the command: \" + str(commands), -1)\n\t\tif verbose:\n\t\t\tprint \"Error with the command: \" + str(commands)\n\treturn result\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tcommand = commands[0]\n\t\tif isinstance(args, list):\n\t\t\targs =''.join(args)\n\t\telse:\n\t\t\targs = str(args)\n\t\tif verbose:\n\t\t\tprint(\"Running command: {0} {1}\".format(command, args))\n\t\tif hide_stderr:\n\t\t\tproc = subprocess.Popen([command, args], stdout=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, cwd=cwd, env=env)\n\t\telse:\n\t\t\tproc = subprocess.Popen([command, args], stdout=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, cwd=cwd, stderr=subprocess.PIPE, env=env)\n\t\treturn_code = proc.wait()\n\t\tstdout, stderr = proc.communicate()\n\t\tif verbose:\n\t\t\tprint(\"Command finished successfully.\")\n\t\t\tif return_code:\n\t\t\t\tprint(\"Exit code: {0}\\n\".format(return_code))\n\t\t\tif stdout:\n\t\t\t\tprint(\"Output:\\n{0}\\n\".format(stdout))\n\t\t\tif stderr:\n\t\t\t\tprint(\"Error:\\n{0}\\n\".format(stderr))\n\t\treturn (stdout, stderr, return_code)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\t# Add stdin=PIPE to the popen args if stdin is not None\n\t# if stdin is not None:\n\t# \tpopen_args['stdin'] = subprocess.PIPE\n\t\n\tif verbose:\n\t\tprint 'Running Commands: {}'.format(commands)\n\t\tprint 'in directory: {}'.format(cwd)\n\t\tprint 'with arguments: {}'.format(args)\n\t\tprint 'with environment: {}'.format(env)\n\t\n\tif verbose and hide_stderr:\n\t\tpopen_args['stdout'] = subprocess.PIPE\n\t\tpopen_args['stderr'] = subprocess.PIPE\n\telse:\n\t\tpopen_args['stdout'] = subprocess.PIPE\n\t\n\tpopen_args['cwd'] = cwd\n\tpopen_args['env'] = env\n\tpopen_args['shell'] = True\n\tpopen_args['universal_newlines'] = True\n\t\n\tproc = subprocess.Popen(commands, **popen_args)\n\tstdout = proc.communicate()\n\tretcode = proc.returncode\n\t\n\tif retcode:\n\t\traise subprocess.CalledProcessError(retcode, commands)\n\t\n\treturn stdout\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tassert isinstance(commands, list)\n\t\tassert isinstance(args, list)\n\t\tprocess = subprocess.Popen(commands + args, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None), cwd=cwd, env=env)\n\t\tstdoutput, erroroutput = process.communicate()\n\t\texit_code = process.returncode\n\t\treturn stdoutput, erroroutput, exit_code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\t# For debugging\n\tif verbose:\n\t\tprint(\"running: \" + str(commands) + \" with \" + str(args), flush=True)\n\n\tresult = None\n\ttry:\n\t\tresult = subprocess.run(commands, args, cwd=cwd, env=env, capture_output=False, shell=False, check=True, text=True)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(\"error running command: \" + str(commands) + \" with \" + str(args) + \"\\n\" + str(e.output), flush=True)\n\t\tresult = e.returncode\n\texcept OSError as e:\n\t\tprint(\"error running command: \" + str(commands) + \" with \" + str(args) + \"\\n\" + str(e), flush=True)\n\t\tresult = 127\n\n\tif verbose:\n\t\tprint(\"result code: \" + str(result.returncode), flush=True)\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tstdout = []\n\t\tstderr = []\n\t\tprocess = None\n\t\tdef data_handler(fd, data):\n\t\t\tif fd == 0:\n\t\t\t\tstdout.append(data)\n\t\t\telse:\n\t\t\t\tstderr.append(data)\n\t\tif not isinstance(commands, list):\n\t\t\tcommands = [commands]\n\t\tif verbose:\n\t\t\tprint(\"RUN COMMANDS:\")\n\t\t\tprint(\" \".join(commands))\n\t\t\tprint(\" \".join(args))\n\t\t\tprint(\"CWD: \" + str(cwd))\n\t\ttry:\n\t\t\tprocess = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, cwd=cwd, args=args, env=env)\n\t\t\tif process.stdout:\n\t\t\t\tprocess.stdout.read(1)\n\t\t\t\tprocess.stdout.flush()\n\t\t\t\tprocess.stdout.seek(0, os.SEEK_END)\n\t\t\tif process.stderr:\n\t\t\t\tprocess.stderr.read(1)\n\t\t\t\tprocess.stderr.flush()\n\t\t\t\tprocess.stderr.seek(0, os.SEEK_END)\n\t\t\tif not hide_stderr:\n\t\t\t\tpoll_fd(process.stdout.fileno(), data_handler)\n\t\t\t\tpoll_fd(process.stderr.fileno(), data_handler)\n\t\t\twhile True:\n\t\t\t\tif process.poll() is not None:\n\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tif verbose:\n\t\t\tprint(\"Running command: {0} {1}\".format(commands, args))\n\n\t\tif hide_stderr:\n\t\t\tdevnull = open(os.devnull, 'wb')\n\t\telse:\n\t\t\tdevnull = None\n\n\t\ttry:\n\t\t\tif cwd is None:\n\t\t\t\treturn subprocess.check_output(shlex.split(commands) + args, env=env, stderr=devnull)\n\t\t\telse:\n\t\t\t\treturn subprocess.check_output(shlex.split(commands) + args, cwd=cwd, env=env, stderr=devnull)\n\t\texcept KeyboardInterrupt:\n\t\t\tsys.exit(1)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tsys.exit(\"Command: {0} {1} failed with returncode {2} and error message: {3}\".format(commands, args, e.returncode, e.output))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\n\n\tif hide_stderr:\n\t\tdevnull = open(os.devnull, 'w')\n\telse:\n\t\tdevnull = None\n\ttry:\n\t\twith open(os.devnull, 'w') as f:\n\t\t\tif verbose:\n\t\t\t\tproc = subprocess.Popen(commands, stdout=f, stderr=f, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tproc = subprocess.Popen(commands, stdout=f, stderr=f, cwd=cwd, env=env)\n\t\t\t(out, err) = proc.communicate()\n\t\treturncode = proc.returncode\n\t\tif returncode:\n\t\t\traise ProcessException(commands, returncode)\n\t\telse:\n\t\t\treturn (out, returncode)\n\tfinally:\n\t\tif hide_stderr:\n\t\t\tdevnull.close()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\n\n\tresult = []\n\tfor command in commands:\n\t\tresult.append(run_command_single(command, args, cwd, verbose, hide_stderr, env))\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t\tassert(len(commands) > 0)\n\tif verbose:\n\t\tprint \"Running command: \" + str(commands) + str(args)\n\t\n\tprocess = subprocess.Popen(str(commands) + str(args), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\tstdout, stderr = process.communicate()\n\tif process.returncode!= 0 and not hide_stderr:\n\t\t#error_text = \"Error code: \" + str(process.returncode) + \"\\nOutput: \" + stdout + \"\\nError: \" + stderr\n\t\terror_text = \"Error code: \" + str(process.returncode) + \"\\nError: \" + stderr\n\t\traise Exception(error_text)\n\treturn stdout, process.returncode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\t \n\tresult = None\n\ttry:\n\t\t#print \"commands: \", commands\n\t\t#print \"args: \", args\n\t\tproc = subprocess.Popen(commands, args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None))\n\t\tout, err = proc.communicate()\n\t\tresult = Result(out, proc.returncode)\n\t\tif verbose:\n\t\t\tprint \"out: \", out\n\t\t\tprint \"err: \", err\n\t\t\tprint \"result: \", result\n\texcept:\n\t\tresult = Result(\"Error with the command: \" + str(commands), -1)\n\t\tif verbose:\n\t\t\tprint \"Error with the command: \" + str(commands)\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "gopad/rest.py", "project": "gopad/gopad-python", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n  try:\n    ipaddress.ip_address(target)\n    return True\n  except ValueError:\n    return False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\ttry:\n\t\t\tipaddress.IPv4Address(target)\n\t\t\treturn True\n\t\texcept ipaddress.AddressValueError:\n\t\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n   try:\n      socket.inet_aton(target)\n      return True\n   except socket.error:\n      return False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn str(ipaddress.ip_address(target)) == target\n\texcept ValueError:\n\t\treturn False\n\t", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn str(ipaddress.IPv4Address(unicode(target)))\n\texcept:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t try:\n\t\t socket.inet_aton(target)\n\t\t return True\n\t except socket.error:\n\t\t return False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\tipaddress.IPv4Address(target)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n  try:\n    ipaddress.ip_address(target)\n    return True\n  except ValueError:\n    return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\ttry:\n\t\t\tipaddress.IPv4Address(target)\n\t\t\treturn True\n\t\texcept ipaddress.AddressValueError:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n   try:\n      socket.inet_aton(target)\n      return True\n   except socket.error:\n      return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn str(ipaddress.ip_address(target)) == target\n\texcept ValueError:\n\t\treturn False\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn str(ipaddress.IPv4Address(unicode(target)))\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t try:\n\t\t socket.inet_aton(target)\n\t\t return True\n\t except socket.error:\n\t\t return False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\tipaddress.IPv4Address(target)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t \n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rdflib/util.py", "project": "mwatts15/rdflib", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t \n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.nodes:\n\t\tif node not in roots and not graph.transitive_parent(node, prop):\n\t\t\troots.add(node)\n\treturn roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\t# if no roots are given, then any node in the graph is a root\n\t\tif roots is None:\n\t\t\troots = set(graph.nodes)\n\t\t# otherwise, we need to remove the roots from the graph\n\t\telse:\n\t\t\tgraph.nodes.difference_update(roots)\n\t\t# we now iterate through the graph, and find nodes with no parents,\n\t\t# and add them to the roots set\n\t\tfor child in roots.copy():\n\t\t\tif not any(\n\t\t\t\tgraph.triples((child, prop, parent)) for parent in graph.nodes\n\t\t\t):\n\t\t\t\troots.add(child)\n\t\t\t\troots.remove(child)\n\t\treturn roots\n/tests/test_graph_tools.py\nimport rdflib\nimport pytest\n\nfrom rdflib import Graph\nfrom rdflib.namespace import RDF, RDFS, SKOS, OWL\nfrom rdflib_hierarchy import graph_tools as gt\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t # if the roots are not provided, we'll find them here\n\tif not roots:\n\t\troots = set()\n\t\tfor s, _, o in graph.triples((None, prop, None)):\n\t\t\troots.add(s)\n\n\t# keep track of which nodes have been visited\n\tvisited = set()\n\n\t# keep track of the roots that we find\n\tnew_roots = set()\n\n\twhile roots:\n\t\t# pop a root from the list\n\t\troot = roots.pop()\n\n\t\t# keep track of the nodes that are visited\n\t\tvisited.add(root)\n\n\t\t# find the children of this root\n\t\tfor _, _, child in graph.triples((root, prop, None)):\n\t\t\t# if this child hasn't been visited, add it to the list of roots\n\t\t\tif child not in visited:\n\t\t\t\tnew_roots.add(child)\n\n\t\t# add the new roots to the list of roots\n\t\troots.update(new_roots)\n\t\tnew_roots = set()\n\n\treturn visited\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\tif roots is None:\n\t\t\troots = set()\n\t\tfor node in graph.nodes:\n\t\t\tif graph.out_degree(node) == 0 and node not in roots:\n\t\t\t\troots.add(node)\n\t\t\t\tfor neighbor in graph.neighbors(node):\n\t\t\t\t\tfind_roots(graph, prop, roots)\n\t\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\t# Use the transitive closure of the graph to find the roots.\n\t\t# This is the set of all nodes that have no incoming edges\n\t\t# into the graph.\n\t\t# TODO: use a SPARQL query\n\tif roots is None:\n\t\troots = set(graph.nodes())\n\telse:\n\t\troots = set(roots)\n\t# Use a breadth-first traversal of the graph, visiting nodes that have\n\t# no incoming edges.\n\tnext_roots = set()\n\twhile roots:\n\t\tnext_roots.clear()\n\t\tfor node in roots:\n\t\t\tfor n, r in graph.in_edges(node):\n\t\t\t\tif r == prop:\n\t\t\t\t\tnext_roots.add(r)\n\t\troots = next_roots\n\treturn roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\tif roots is None:\n\t\t\troots = set()\n\t\tif roots is not None and not isinstance(roots, set):\n\t\t\traise TypeError(\"roots must be of type set\")\n\t\tif prop is None:\n\t\t\traise ValueError(\"argument prop must not be None\")\n\t\tif not isinstance(prop, URIRef):\n\t\t\traise TypeError(\"argument prop must be of type URIRef\")\n\t\tchildren = graph.transitive_subjects(prop, 1)\n\t\tif children is None:\n\t\t\treturn set()\n\t\tchildren = children - set(graph.transitive_subjects(prop, 2))\n\t\troots = roots.union(children)\n\t\tfor child in children:\n\t\t\troots = roots.union(find_roots(graph, prop, roots))\n\t\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t # roots is a set with nodes\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t \n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\t# if not roots:\n\t\t#     roots = set(graph.subjects(RDF.type, RDFS.Class))\n\t\t#     roots = roots.union(graph.subjects(RDF.type, SKOS.ConceptScheme))\n\n\t\t#     if not roots:\n\t\t#         return set()\n\n\tif not roots:\n\t\troots = set(graph.subjects(RDF.type, RDFS.Class))\n\t\troots = roots.union(graph.subjects(RDF.type, SKOS.ConceptScheme))\n\n\t\tif not roots:\n\t\t\treturn set()\n\n\treturn set(\n\t\tnode\n\t\tfor node in roots\n\t\tif not graph.transitive_subjects(node, prop, node)\n\t)\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\tif roots is None:\n\t\t\treturn set()\n\t\troots = set(roots)\n\t\tfor node in graph:\n\t\t\tif prop in graph.transitive_objects(node):\n\t\t\t\troots.add(node)\n\t\treturn roots\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t \n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.nodes:\n\t\tif node not in roots and not graph.transitive_parent(node, prop):\n\t\t\troots.add(node)\n\treturn roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\t# if no roots are given, then any node in the graph is a root\n\t\tif roots is None:\n\t\t\troots = set(graph.nodes)\n\t\t# otherwise, we need to remove the roots from the graph\n\t\telse:\n\t\t\tgraph.nodes.difference_update(roots)\n\t\t# we now iterate through the graph, and find nodes with no parents,\n\t\t# and add them to the roots set\n\t\tfor child in roots.copy():\n\t\t\tif not any(\n\t\t\t\tgraph.triples((child, prop, parent)) for parent in graph.nodes\n\t\t\t):\n\t\t\t\troots.add(child)\n\t\t\t\troots.remove(child)\n\t\treturn roots\n/tests/test_graph_tools.py\nimport rdflib\nimport pytest\n\nfrom rdflib import Graph\nfrom rdflib.namespace import RDF, RDFS, SKOS, OWL\nfrom rdflib_hierarchy import graph_tools as gt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t # if the roots are not provided, we'll find them here\n\tif not roots:\n\t\troots = set()\n\t\tfor s, _, o in graph.triples((None, prop, None)):\n\t\t\troots.add(s)\n\n\t# keep track of which nodes have been visited\n\tvisited = set()\n\n\t# keep track of the roots that we find\n\tnew_roots = set()\n\n\twhile roots:\n\t\t# pop a root from the list\n\t\troot = roots.pop()\n\n\t\t# keep track of the nodes that are visited\n\t\tvisited.add(root)\n\n\t\t# find the children of this root\n\t\tfor _, _, child in graph.triples((root, prop, None)):\n\t\t\t# if this child hasn't been visited, add it to the list of roots\n\t\t\tif child not in visited:\n\t\t\t\tnew_roots.add(child)\n\n\t\t# add the new roots to the list of roots\n\t\troots.update(new_roots)\n\t\tnew_roots = set()\n\n\treturn visited\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\tif roots is None:\n\t\t\troots = set()\n\t\tfor node in graph.nodes:\n\t\t\tif graph.out_degree(node) == 0 and node not in roots:\n\t\t\t\troots.add(node)\n\t\t\t\tfor neighbor in graph.neighbors(node):\n\t\t\t\t\tfind_roots(graph, prop, roots)\n\t\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\t# Use the transitive closure of the graph to find the roots.\n\t\t# This is the set of all nodes that have no incoming edges\n\t\t# into the graph.\n\t\t# TODO: use a SPARQL query\n\tif roots is None:\n\t\troots = set(graph.nodes())\n\telse:\n\t\troots = set(roots)\n\t# Use a breadth-first traversal of the graph, visiting nodes that have\n\t# no incoming edges.\n\tnext_roots = set()\n\twhile roots:\n\t\tnext_roots.clear()\n\t\tfor node in roots:\n\t\t\tfor n, r in graph.in_edges(node):\n\t\t\t\tif r == prop:\n\t\t\t\t\tnext_roots.add(r)\n\t\troots = next_roots\n\treturn roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\tif roots is None:\n\t\t\troots = set()\n\t\tif roots is not None and not isinstance(roots, set):\n\t\t\traise TypeError(\"roots must be of type set\")\n\t\tif prop is None:\n\t\t\traise ValueError(\"argument prop must not be None\")\n\t\tif not isinstance(prop, URIRef):\n\t\t\traise TypeError(\"argument prop must be of type URIRef\")\n\t\tchildren = graph.transitive_subjects(prop, 1)\n\t\tif children is None:\n\t\t\treturn set()\n\t\tchildren = children - set(graph.transitive_subjects(prop, 2))\n\t\troots = roots.union(children)\n\t\tfor child in children:\n\t\t\troots = roots.union(find_roots(graph, prop, roots))\n\t\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t # roots is a set with nodes\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t \n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\t# if not roots:\n\t\t#     roots = set(graph.subjects(RDF.type, RDFS.Class))\n\t\t#     roots = roots.union(graph.subjects(RDF.type, SKOS.ConceptScheme))\n\n\t\t#     if not roots:\n\t\t#         return set()\n\n\tif not roots:\n\t\troots = set(graph.subjects(RDF.type, RDFS.Class))\n\t\troots = roots.union(graph.subjects(RDF.type, SKOS.ConceptScheme))\n\n\t\tif not roots:\n\t\t\treturn set()\n\n\treturn set(\n\t\tnode\n\t\tfor node in roots\n\t\tif not graph.transitive_subjects(node, prop, node)\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\t\tif roots is None:\n\t\t\treturn set()\n\t\troots = set(roots)\n\t\tfor node in graph:\n\t\t\tif prop in graph.transitive_objects(node):\n\t\t\t\troots.add(node)\n\t\treturn roots\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/unit/mock/yaml_helper.py", "project": "ansible-security/ansible_collections.ibm.qradar", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t if dumper:\n\t     return dumper.represent_scalar('tag:yaml.org,2002:str', repr(obj))\n\t else:\n\t     return repr(obj)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = Dumper\n\t\tdumper.open()\n\t\tdumper.scalar_flow_style = False\n\n\tif isinstance(obj, str):\n\t\tdumper.represent_str(obj)\n\telif isinstance(obj, unicode):\n\t\tdumper.represent_str(str(obj))\n\telse:\n\t\traise TypeError(\"can't dump %s\" % type(obj))\n\n\treturn dumper\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\ttry:\n\t\ts = dumper(obj, Dumper=dumper)\n\t\ts.open()\n\t\treturn s\n\texcept:\n\t\traise YAMLDumperError(\"Could not serialize: %s\" %(obj))\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = yaml.SafeDumper\n\treturn dumper.represent_data(obj)\n\n/src/pyyaml_tools/tests/test_yaml_tools.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\ntest_yaml_tools\n----------------------------------\n\nTests for `yaml_tools` module.\n\"\"\"\n\nimport sys\nimport unittest\nimport os\nimport yaml\nimport yaml_tools as yt\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        if isinstance(obj, str):\n            return obj\n        elif isinstance(obj, list):\n            return [_dump_string(item, dumper=dumper) for item in obj]\n        elif isinstance(obj, dict):\n            return dict((_dump_string(key, dumper=dumper), _dump_string(value, dumper=dumper)) for key, value in obj.items())\n        else:\n            raise TypeError(repr(obj) + \" is not JSON serializable\")\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        #if dumper is None:\n        #    dumper = Dumper(stream)\n        #dumper.represent_str = represent_str\n        return obj\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = yaml.dumper.SafeDumper\n\tif isinstance(obj, unicode):\n\t\treturn dumper.represent_unicode(obj)\n\telif isinstance(obj, date):\n\t\treturn dumper.represent_data(obj)\n\telse:\n\t\traise TypeError(u\"Unknown type %r\" % type(obj))\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        if dumper is None:\n            dumper = Dumper\n        return dumper.represent_data(obj)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t \n\tif dumper is None:\n\t\tdumper = Dumper\n\treturn dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        if dumper is None:\n            dumper = Dumper\n\n        # Dumper does not support encoding/decoding strings in a\n        # consistent manner, we need to explicitly add tags\n        if isinstance(obj, str):\n            return dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n        elif isinstance(obj, bytes):\n            return dumper.represent_scalar('tag:yaml.org,2002:binary', obj)\n        else:\n            return dumper.represent_object(obj)\n\n    yaml.add_representer(str, _dump_string)\n    yaml.add_representer(bytes, _dump_string)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t if dumper:\n\t     return dumper.represent_scalar('tag:yaml.org,2002:str', repr(obj))\n\t else:\n\t     return repr(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = Dumper\n\t\tdumper.open()\n\t\tdumper.scalar_flow_style = False\n\n\tif isinstance(obj, str):\n\t\tdumper.represent_str(obj)\n\telif isinstance(obj, unicode):\n\t\tdumper.represent_str(str(obj))\n\telse:\n\t\traise TypeError(\"can't dump %s\" % type(obj))\n\n\treturn dumper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\ttry:\n\t\ts = dumper(obj, Dumper=dumper)\n\t\ts.open()\n\t\treturn s\n\texcept:\n\t\traise YAMLDumperError(\"Could not serialize: %s\" %(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = yaml.SafeDumper\n\treturn dumper.represent_data(obj)\n\n/src/pyyaml_tools/tests/test_yaml_tools.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\ntest_yaml_tools\n----------------------------------\n\nTests for `yaml_tools` module.\n\"\"\"\n\nimport sys\nimport unittest\nimport os\nimport yaml\nimport yaml_tools as yt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        if isinstance(obj, str):\n            return obj\n        elif isinstance(obj, list):\n            return [_dump_string(item, dumper=dumper) for item in obj]\n        elif isinstance(obj, dict):\n            return dict((_dump_string(key, dumper=dumper), _dump_string(value, dumper=dumper)) for key, value in obj.items())\n        else:\n            raise TypeError(repr(obj) + \" is not JSON serializable\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        #if dumper is None:\n        #    dumper = Dumper(stream)\n        #dumper.represent_str = represent_str\n        return obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\n\tif dumper is None:\n\t\tdumper = yaml.dumper.SafeDumper\n\tif isinstance(obj, unicode):\n\t\treturn dumper.represent_unicode(obj)\n\telif isinstance(obj, date):\n\t\treturn dumper.represent_data(obj)\n\telse:\n\t\traise TypeError(u\"Unknown type %r\" % type(obj))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        if dumper is None:\n            dumper = Dumper\n        return dumper.represent_data(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t \n\tif dumper is None:\n\t\tdumper = Dumper\n\treturn dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n        if dumper is None:\n            dumper = Dumper\n\n        # Dumper does not support encoding/decoding strings in a\n        # consistent manner, we need to explicitly add tags\n        if isinstance(obj, str):\n            return dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n        elif isinstance(obj, bytes):\n            return dumper.represent_scalar('tag:yaml.org,2002:binary', obj)\n        else:\n            return dumper.represent_object(obj)\n\n    yaml.add_representer(str, _dump_string)\n    yaml.add_representer(bytes, _dump_string)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "apphelpers/loggers.py", "project": "scrolltech/apphelpers", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n\n\t# create logger\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(ch)\n\tlogger.addHandler(fh)\n\n\treturn logger\n/app/config/default.py\n# -*- coding: utf-8 -*-\n\n# import logging\n\n# ##############################################################################\n# # General configuration\n# ##############################################################################\n#\n# DEBUG = True\n# SECRET_KEY = ''\n#\n# # ##############################################################################\n# # Security\n# # ##############################################################################\n# #\n# CSRF_ENABLED = True\n# CSRF_SESSION_KEY = ''\n#\n# # ##############################################################################\n# # Database configuration\n# # ##############################################################################\n# #\n# # SQLAL", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\n\tlogger.addHandler(handler)\n\n\tif debug:\n\t\tlogger.debug('Debug mode is ON!')\n\t\tlogger.info('Debug mode is ON!')\n\t\tlogger.warning('Debug mode is ON!')\n\t\tlogger.error('Debug mode is ON!')\n\t\tlogger.critical('Debug mode is ON!')\n\n\treturn logger\n\nif __name__ == '__main__':\n\tapp_logger = build_app_logger(name='app', logfile='app.log', debug=True)\n\tapp_logger.debug('Debug mode is ON!')\n\tapp_logger.info('Debug mode is ON!')\n\tapp_logger.warning('Debug mode is ON!')\n\tapp_logger.error('Debug mode is ON!')\n\tapp_logger.critical('Debug mode is ON!')\nimport random\nimport os\nimport logging\nimport datetime\nfrom flask import Flask, request, jsonify, session\nfrom flask_cors import CORS\nfrom flask_socketio import SocketIO, emit, join_room", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif debug is True:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t#File Handler\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\t#Stream Handler\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.DEBUG)\n\tstream_handler.setFormatter(formatter)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\t# Create logger\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# Create console handler and set level to debug\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# Create file handler and set level to debug\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# Create formatter\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t# Add formatter to ch and fh\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# Add ch and fh to logger\n\tlogger.addHandler(ch)\n\tlogger.addHandler(fh)\n\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n\n\t# create logger with'spam_application'\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n\n\t# File handler\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\t# Stream Handler\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setFormatter(formatter)\n\tlogger.addHandler(stream_handler)\n\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\n\treturn logger\n/src/backend/server/src/main/java/com/dev/backend/server/dto/BookDTO.java\npackage com.dev.backend.server.dto;\n\nimport lombok.Data;\n\n@Data\npublic class BookDTO {\n\n    private Long id;\n    private String title;\n    private String isbn;\n    private String author;\n    private String publisher;\n    private Integer year;\n    private String category;\n    private String description;\n    private String image;\n    private String status;\n    private Long totalQuantity;\n    private Long quantity;\n    private Long totalSold;\n    private Long sold;\n   ", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.addHandler(handler)\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n        logger = logging.getLogger(name)\n        if not logger.handlers:\n            logger.setLevel(logging.DEBUG)\n            handler = RotatingFileHandler(logfile, maxBytes=1000000, backupCount=5)\n            handler.setLevel(logging.DEBUG)\n            formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n        return logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(fmt=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\")\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n# \tif debug:\n# \t\tlogging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p',\n# \t\t\t\t\t\t\tfilename=logfile, level=logging.DEBUG)\n# \telse:\n# \t\tlogging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p',\n# \t\t\t\t\t\t\tfilename=logfile, level=logging.INFO)\n# \treturn logging.getLogger(name)\n# \treturn logging.getLogger(name)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n\n\t# create logger\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(ch)\n\tlogger.addHandler(fh)\n\n\treturn logger\n/app/config/default.py\n# -*- coding: utf-8 -*-\n\n# import logging\n\n# ##############################################################################\n# # General configuration\n# ##############################################################################\n#\n# DEBUG = True\n# SECRET_KEY = ''\n#\n# # ##############################################################################\n# # Security\n# # ##############################################################################\n# #\n# CSRF_ENABLED = True\n# CSRF_SESSION_KEY = ''\n#\n# # ##############################################################################\n# # Database configuration\n# # ##############################################################################\n# #\n# # SQLAL", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\n\tlogger.addHandler(handler)\n\n\tif debug:\n\t\tlogger.debug('Debug mode is ON!')\n\t\tlogger.info('Debug mode is ON!')\n\t\tlogger.warning('Debug mode is ON!')\n\t\tlogger.error('Debug mode is ON!')\n\t\tlogger.critical('Debug mode is ON!')\n\n\treturn logger\n\nif __name__ == '__main__':\n\tapp_logger = build_app_logger(name='app', logfile='app.log', debug=True)\n\tapp_logger.debug('Debug mode is ON!')\n\tapp_logger.info('Debug mode is ON!')\n\tapp_logger.warning('Debug mode is ON!')\n\tapp_logger.error('Debug mode is ON!')\n\tapp_logger.critical('Debug mode is ON!')\nimport random\nimport os\nimport logging\nimport datetime\nfrom flask import Flask, request, jsonify, session\nfrom flask_cors import CORS\nfrom flask_socketio import SocketIO, emit, join_room", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif debug is True:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t#File Handler\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\t#Stream Handler\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.DEBUG)\n\tstream_handler.setFormatter(formatter)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\t# Create logger\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# Create console handler and set level to debug\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# Create file handler and set level to debug\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# Create formatter\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\t# Add formatter to ch and fh\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# Add ch and fh to logger\n\tlogger.addHandler(ch)\n\tlogger.addHandler(fh)\n\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n\n\t# create logger with'spam_application'\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n\n\t# File handler\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\t# Stream Handler\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setFormatter(formatter)\n\tlogger.addHandler(stream_handler)\n\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\n\treturn logger\n/src/backend/server/src/main/java/com/dev/backend/server/dto/BookDTO.java\npackage com.dev.backend.server.dto;\n\nimport lombok.Data;\n\n@Data\npublic class BookDTO {\n\n    private Long id;\n    private String title;\n    private String isbn;\n    private String author;\n    private String publisher;\n    private Integer year;\n    private String category;\n    private String description;\n    private String image;\n    private String status;\n    private Long totalQuantity;\n    private Long quantity;\n    private Long totalSold;\n    private Long sold;\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.addHandler(handler)\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n        logger = logging.getLogger(name)\n        if not logger.handlers:\n            logger.setLevel(logging.DEBUG)\n            handler = RotatingFileHandler(logfile, maxBytes=1000000, backupCount=5)\n            handler.setLevel(logging.DEBUG)\n            formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n        return logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t \n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(fmt=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\")\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\t\n# \tif debug:\n# \t\tlogging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p',\n# \t\t\t\t\t\t\tfilename=logfile, level=logging.DEBUG)\n# \telse:\n# \t\tlogging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p',\n# \t\t\t\t\t\t\tfilename=logfile, level=logging.INFO)\n# \treturn logging.getLogger(name)\n# \treturn logging.getLogger(name)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "radiospectra/spectrogram.py", "project": "sunpy/radiospectra", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.array(np.zeros(shape), dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        if not isinstance(shape, tuple):\n                raise ValueError(\"shape must be a tuple\")\n        if isinstance(shape, np.ndarray):\n                shape = shape.tolist()\n        if not isinstance(shape, tuple):\n                raise ValueError(\"shape must be a tuple\")\n        if dtype is None:\n                dtype = np.dtype(\"float32\")\n        if not isinstance(dtype, np.dtype):\n                raise ValueError(\"dtype must be a numpy.dtype\")\n        return np.empty(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        # TODO: return a 0-filled array of the given shape and type\n\treturn np.zeros(shape, dtype=dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\tarray = np.zeros(shape, dtype)\n\t\treturn array\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape, dtype)\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        import numpy as np\n        return np.zeros(shape, dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape, dtype=dtype)\n\t", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape=shape, dtype=dtype, order=\"C\")\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        #return np.zeros(shape, dtype=dtype)\n\treturn np.zeros(shape, dtype=dtype)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.array(np.zeros(shape), dtype=dtype)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        if not isinstance(shape, tuple):\n                raise ValueError(\"shape must be a tuple\")\n        if isinstance(shape, np.ndarray):\n                shape = shape.tolist()\n        if not isinstance(shape, tuple):\n                raise ValueError(\"shape must be a tuple\")\n        if dtype is None:\n                dtype = np.dtype(\"float32\")\n        if not isinstance(dtype, np.dtype):\n                raise ValueError(\"dtype must be a numpy.dtype\")\n        return np.empty(shape, dtype=dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        # TODO: return a 0-filled array of the given shape and type\n\treturn np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\tarray = np.zeros(shape, dtype)\n\t\treturn array\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape, dtype=dtype)", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape, dtype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        import numpy as np\n        return np.zeros(shape, dtype)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape, dtype=dtype)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        return np.zeros(shape=shape, dtype=dtype, order=\"C\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\t\n        #return np.zeros(shape, dtype=dtype)\n\treturn np.zeros(shape, dtype=dtype)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "concert/tests/unit/devices/test_monochromator.py", "project": "ufo-kit/concert", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t return 1.0 / (2.0 * math.pi * 0.1 * 0.1) * math.exp(-1.0 / (2.0 * 0.1 * 0.1) * (x - 0.2) ** 2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t1/(2*pi*sigma**2)**0.5*exp(-(x-u)**2/2*sigma**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\n\n\treturn np.exp(-0.5*((x-0.2)/0.1)**2)\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t3 * np.exp(-((x-0.2)/0.1)**2)\n\nplt.plot(x, gaussian(x))\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 3.0)\nplt.xlabel('x')\nplt.ylabel('gaussian')\nplt.show()\n\n\"\"\"\nQ5.2\n\"\"\"\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\treturn 1/(sigma*np.sqrt(2*np.pi))*np.exp(-(x-u)**2/(2*sigma**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\n#\tx = np.array(x, dtype=float)\n#\tn = len(x)\n#\tu = 0.2\n#\tsigma = 0.1\n#\ty = np.exp(-((x-u)**2)/(2*sigma**2))\n#\treturn y\n\treturn np.exp(-x**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\treturn np.exp(-(x-0.2)**2/0.1**2)\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\n\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-np.power(x - u, 2.) / (2 * np.power(sigma, 2.)))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\treturn 1/(np.sqrt(2*np.pi*0.1**2))*np.exp(-(x-0.2)**2/2/0.1**2)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t return 1.0 / (2.0 * math.pi * 0.1 * 0.1) * math.exp(-1.0 / (2.0 * 0.1 * 0.1) * (x - 0.2) ** 2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t1/(2*pi*sigma**2)**0.5*exp(-(x-u)**2/2*sigma**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\n\n\treturn np.exp(-0.5*((x-0.2)/0.1)**2)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t3 * np.exp(-((x-0.2)/0.1)**2)\n\nplt.plot(x, gaussian(x))\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 3.0)\nplt.xlabel('x')\nplt.ylabel('gaussian')\nplt.show()\n\n\"\"\"\nQ5.2\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\treturn 1/(sigma*np.sqrt(2*np.pi))*np.exp(-(x-u)**2/(2*sigma**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\n#\tx = np.array(x, dtype=float)\n#\tn = len(x)\n#\tu = 0.2\n#\tsigma = 0.1\n#\ty = np.exp(-((x-u)**2)/(2*sigma**2))\n#\treturn y\n\treturn np.exp(-x**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\treturn np.exp(-(x-0.2)**2/0.1**2)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\n\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-np.power(x - u, 2.) / (2 * np.power(sigma, 2.)))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\t\treturn 1/(np.sqrt(2*np.pi*0.1**2))*np.exp(-(x-0.2)**2/2/0.1**2)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/borgmatic.py", "project": "witten/borgmatic", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# Load configurations and validate them\n\tconfigurations = dict()\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\t\t# Load the configuration file\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as config_file:\n\t\t\t\t# Parse the configuration file\n\t\t\t\ttry:\n\t\t\t\t\tconfig = yaml.load(config_file)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\terrors.append(logging.LogRecord(\n\t\t\t\t\t\tname='Config file',\n\t\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\t\tmsg='Error parsing %s : %s' % (config_filename, e),\n\t\t\t\t\t\targs=None,\n\t\t\t\t\t\texc_info=None,\n\t\t\t\t\t\tfunc=None,\n\t\t\t\t\t\tlineno=None,\n\t\t\t\t\t\tmodule=None,\n\t\t\t\t\t\tpathname=None,\n\t\t\t\t\t\tprocess=None,\n\t\t\t\t\t\tprocessName=None,\n\t\t\t\t\t\trelativeCreated=None,\n\t\t\t\t\t\tthread=None,\n\t\t\t\t\t\tthreadName=None))\n\t\t\t\t\tcontinue\n\t\t\t# Validate the configuration\n\t\t\ttry:\n\t\t\t\tvalidate_configuration(config)\n\t\t\texcept Exception as e:\n\t\t\t\terrors.append(logging.LogRecord(\n\t\t\t\t\tname='Config file',\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tmsg='Error validating %s : %s' % (config_filename, e),\n\t\t\t\t\targs=None,\n\t\t\t\t\texc_info=None,\n\t\t\t\t\tfunc=None,\n\t\t\t\t\t", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\trecords = []\n\t\tconfigs = {}\n\t\tfor filename in config_filenames:\n\t\t\ttry:\n\t\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\t\texcept Exception as e:\n\t\t\t\trecords.append(logging.LogRecord('config', logging.ERROR, None, 1,\n\t\t\t\t\t'Unable to load configuration file %s: %s' % (filename, str(e)),\n\t\t\t\t\tNone, None))\n\t\t\telse:\n\t\t\t\tconfigs[filename] = config\n\t\treturn configs, records\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\terrors = []\n\t\tconfigs = {}\n\t\tfor filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(filename, 'r') as config_file:\n\t\t\t\t\tconfig = configparser.SafeConfigParser(overrides)\n\t\t\t\t\tconfig.readfp(config_file)\n\t\t\t\tconfig_dict = dict(config.items('general'))\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig_dict = resolve_env_vars(config_dict)\n\t\t\t\tconfigs[filename] = config_dict\n\t\t\texcept IOError as exception:\n\t\t\t\terrors.append(_error_log_record('config_file', filename, exception))\n\t\t\texcept configparser.Error as exception:\n\t\t\t\terrors.append(_error_log_record('config_file', filename, exception))\n\t\treturn configs, errors\n```\n\nconfigparser\u6a21\u5757\u662fpython\u81ea\u5e26\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u914d\u7f6e\u6587\u4ef6\u7684\u8bfb\u5199\u64cd\u4f5c\u3002\n\nconfigparser.SafeConfigParser()\u65b9\u6cd5\u521b\u5efaSafeConfigParser\u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u662fConfigParser\u5bf9\u8c61\u7684\u5b50\u7c7b\u3002SafeConfigParser\u7684\u884c\u4e3a\u548cConfigParser\u662f\u4e00\u6837\u7684\uff0c\u53ea\u662f\u5f53\u914d\u7f6e\u6587\u4ef6\u4e2d\u51fa\u73b0\u672a\u77e5\u7684section\u6216option\u65f6\uff0cSafeConfigParser\u5bf9\u8c61\u4e0d\u4f1a\u629b\u51fa\u5f02\u5e38\uff0c\u800c\u662f\u7ed9\u51fa\u8b66\u544a\u3002\n\nSafeConfigParser\u5bf9\u8c61\u901a\u8fc7SafeConfigParser.read()\u65b9\u6cd5\u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u3002read(filenames)\u65b9\u6cd5\u53ef\u4ee5\u662f\u591a\u4e2a\u6587\u4ef6\u540d\uff0c\u6216\u8005\u662f\u6587\u4ef6\u5bf9\u8c61\u3002\n\nSafeConfigParser\u5bf9\u8c61\u901a\u8fc7SafeConfigParser.get(section, option)\u65b9\u6cd5", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tconfigs = {}\n\t\terrors = []\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tconfig = yaml.load(f)\n\t\t\t\t\texcept yaml.YAMLError as e:\n\t\t\t\t\t\terrors.append(e)\n\t\t\t\t\t\tcontinue\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = resolve_env_vars(config)\n\t\t\t\tif overrides is not None:\n\t\t\t\t\tconfig = override_config(config, overrides)\n\t\t\t\tvalidate_config(config)\n\t\t\t\tconfigs[config_filename] = config\n\t\t\texcept IOError as e:\n\t\t\t\terrors.append(e)\n\t\treturn configs, errors\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tconfig_filenames = [config_filenames] if isinstance(config_filenames, str) else config_filenames\n\t\tconfigs = {}\n\t\terrors = []\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(config_filename) as config_file:\n\t\t\t\t\tconfig_dict = yaml.load(config_file)\n\t\t\t\tconfig = Config(config_dict)\n\t\t\t\tconfig.resolve_env(resolve_env)\n\t\t\t\tconfig.resolve_overrides(overrides)\n\t\t\t\tconfig.validate()\n\t\t\t\tconfigs[config_filename] = config\n\t\t\texcept (IOError, KeyError) as e:\n\t\t\t\t# If the configuration file does not exist or is unreadable, do not log this error.\n\t\t\t\t# If the configuration file is readable but is not valid, log this error.\n\t\t\t\tlogging.warning(e.message)\n\t\t\texcept Exception as e:\n\t\t\t\terrors.append(e)\n\t\treturn configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# the configuration data is stored in this dictionary, with configuration filename as key\n\t\tconfigurations = {}\n\t\t# load the configuration files\n\t\tif not isinstance(config_filenames, (list, tuple)):\n\t\t\tconfig_filenames = [config_filenames]\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(config_filename, 'rt') as config_file:\n\t\t\t\t\tconfig = yaml.safe_load(config_file) or {}\n\t\t\texcept OSError as e:\n\t\t\t\tlogging.exception(\"failed to load configuration file %s: %s\", config_filename, e)\n\t\t\t\tcontinue\n\t\t\texcept Exception as e:\n\t\t\t\tlogging.exception(\"failed to read configuration file %s: %s\", config_filename, e)\n\t\t\t\tcontinue\n\t\t\t# apply the configuration overrides\n\t\t\tif overrides:\n\t\t\t\tfor key, value in overrides.items():\n\t\t\t\t\tif key in config:\n\t\t\t\t\t\tconfig[key] = value\n\t\t\t# validate the configuration\n\t\t\ttry:\n\t\t\t\tvalidate_configuration(config)\n\t\t\texcept ConfigurationError as e:\n\t\t\t\tlogging.exception(\"invalid configuration file %s: %s\", config_filename, e)\n\t\t\t\tcontinue\n\t\t\t# resolve environment variables\n\t\t\tif resolve_env:\n\t\t\t\tconfig = dict((key, os.path.expandvars(value)) for key, value in config.items())\n\t\t\tconfigurations[", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# We need to be able to read the configuration file.\n\tif not os.access(config_filenames, os.R_OK):\n\t\treturn None, [logging.LogRecord(loggerName='config_loader', level=logging.ERROR, msg=\"Unable to read configuration file: {}\".format(config_filenames), args=None, exc_info=None)]\n\tconfig_files = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig_files.append(load_configuration_file(filename, overrides, resolve_env))\n\t\texcept Exception as ex:\n\t\t\treturn None, [logging.LogRecord(loggerName='config_loader', level=logging.ERROR, msg=\"Error while parsing configuration file: {}\".format(filename), args=None, exc_info=ex)]\n\treturn dict([(cf['filename'], cf) for cf in config_files]), []", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tconfig = ConfigParser.SafeConfigParser()\n\t\tconfig.read(config_filenames)\n\t\terror_records = []\n\t\terrors = []\n\t\tconfig_dict = {}\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\tconfig_dict[config_filename] = parse_configuration(config, overrides, resolve_env)\n\t\t\texcept ConfigParser.ParsingError as e:\n\t\t\t\terror_records.append(e.record)\n\t\t\t\terrors.append(e.message)\n\t\tif errors:\n\t\t\traise ConfigError(errors, error_records)\n\t\treturn config_dict, error_records\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t/pycfiles/djay-0.0.8-py2.py3-none-any/models.cpython-36.py\n# uncompyle6 version 3.7.4\n# Python bytecode 3.6 (3379)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: /tmp/pip-install-m_4qh6p6/Pygments/pygments/lexers/models.py\n# Compiled at: 2019-07-30 18:47:11\n# Size of source mod 2**32: 3734 bytes\n\"\"\"\n    pygments.lexers.models\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for models.\n\n    :copyright: Copyright 2006-2019 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\nimport re\nfrom pygments.lexer import RegexLexer, include, bygroups, using\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation, Error\n__all__ = [\n 'ModelLexer']\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\treturn _load_configurations(config_filenames, overrides, resolve_env)"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# Load configurations and validate them\n\tconfigurations = dict()\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\t\t# Load the configuration file\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as config_file:\n\t\t\t\t# Parse the configuration file\n\t\t\t\ttry:\n\t\t\t\t\tconfig = yaml.load(config_file)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\terrors.append(logging.LogRecord(\n\t\t\t\t\t\tname='Config file',\n\t\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\t\tmsg='Error parsing %s : %s' % (config_filename, e),\n\t\t\t\t\t\targs=None,\n\t\t\t\t\t\texc_info=None,\n\t\t\t\t\t\tfunc=None,\n\t\t\t\t\t\tlineno=None,\n\t\t\t\t\t\tmodule=None,\n\t\t\t\t\t\tpathname=None,\n\t\t\t\t\t\tprocess=None,\n\t\t\t\t\t\tprocessName=None,\n\t\t\t\t\t\trelativeCreated=None,\n\t\t\t\t\t\tthread=None,\n\t\t\t\t\t\tthreadName=None))\n\t\t\t\t\tcontinue\n\t\t\t# Validate the configuration\n\t\t\ttry:\n\t\t\t\tvalidate_configuration(config)\n\t\t\texcept Exception as e:\n\t\t\t\terrors.append(logging.LogRecord(\n\t\t\t\t\tname='Config file',\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tmsg='Error validating %s : %s' % (config_filename, e),\n\t\t\t\t\targs=None,\n\t\t\t\t\texc_info=None,\n\t\t\t\t\tfunc=None,\n\t\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\trecords = []\n\t\tconfigs = {}\n\t\tfor filename in config_filenames:\n\t\t\ttry:\n\t\t\t\tconfig = load_configuration(filename, overrides, resolve_env)\n\t\t\texcept Exception as e:\n\t\t\t\trecords.append(logging.LogRecord('config', logging.ERROR, None, 1,\n\t\t\t\t\t'Unable to load configuration file %s: %s' % (filename, str(e)),\n\t\t\t\t\tNone, None))\n\t\t\telse:\n\t\t\t\tconfigs[filename] = config\n\t\treturn configs, records\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\terrors = []\n\t\tconfigs = {}\n\t\tfor filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(filename, 'r') as config_file:\n\t\t\t\t\tconfig = configparser.SafeConfigParser(overrides)\n\t\t\t\t\tconfig.readfp(config_file)\n\t\t\t\tconfig_dict = dict(config.items('general'))\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig_dict = resolve_env_vars(config_dict)\n\t\t\t\tconfigs[filename] = config_dict\n\t\t\texcept IOError as exception:\n\t\t\t\terrors.append(_error_log_record('config_file', filename, exception))\n\t\t\texcept configparser.Error as exception:\n\t\t\t\terrors.append(_error_log_record('config_file', filename, exception))\n\t\treturn configs, errors\n```\n\nconfigparser\u6a21\u5757\u662fpython\u81ea\u5e26\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u914d\u7f6e\u6587\u4ef6\u7684\u8bfb\u5199\u64cd\u4f5c\u3002\n\nconfigparser.SafeConfigParser()\u65b9\u6cd5\u521b\u5efaSafeConfigParser\u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u662fConfigParser\u5bf9\u8c61\u7684\u5b50\u7c7b\u3002SafeConfigParser\u7684\u884c\u4e3a\u548cConfigParser\u662f\u4e00\u6837\u7684\uff0c\u53ea\u662f\u5f53\u914d\u7f6e\u6587\u4ef6\u4e2d\u51fa\u73b0\u672a\u77e5\u7684section\u6216option\u65f6\uff0cSafeConfigParser\u5bf9\u8c61\u4e0d\u4f1a\u629b\u51fa\u5f02\u5e38\uff0c\u800c\u662f\u7ed9\u51fa\u8b66\u544a\u3002\n\nSafeConfigParser\u5bf9\u8c61\u901a\u8fc7SafeConfigParser.read()\u65b9\u6cd5\u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u3002read(filenames)\u65b9\u6cd5\u53ef\u4ee5\u662f\u591a\u4e2a\u6587\u4ef6\u540d\uff0c\u6216\u8005\u662f\u6587\u4ef6\u5bf9\u8c61\u3002\n\nSafeConfigParser\u5bf9\u8c61\u901a\u8fc7SafeConfigParser.get(section, option)\u65b9\u6cd5", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tconfigs = {}\n\t\terrors = []\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tconfig = yaml.load(f)\n\t\t\t\t\texcept yaml.YAMLError as e:\n\t\t\t\t\t\terrors.append(e)\n\t\t\t\t\t\tcontinue\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = resolve_env_vars(config)\n\t\t\t\tif overrides is not None:\n\t\t\t\t\tconfig = override_config(config, overrides)\n\t\t\t\tvalidate_config(config)\n\t\t\t\tconfigs[config_filename] = config\n\t\t\texcept IOError as e:\n\t\t\t\terrors.append(e)\n\t\treturn configs, errors\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tconfig_filenames = [config_filenames] if isinstance(config_filenames, str) else config_filenames\n\t\tconfigs = {}\n\t\terrors = []\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(config_filename) as config_file:\n\t\t\t\t\tconfig_dict = yaml.load(config_file)\n\t\t\t\tconfig = Config(config_dict)\n\t\t\t\tconfig.resolve_env(resolve_env)\n\t\t\t\tconfig.resolve_overrides(overrides)\n\t\t\t\tconfig.validate()\n\t\t\t\tconfigs[config_filename] = config\n\t\t\texcept (IOError, KeyError) as e:\n\t\t\t\t# If the configuration file does not exist or is unreadable, do not log this error.\n\t\t\t\t# If the configuration file is readable but is not valid, log this error.\n\t\t\t\tlogging.warning(e.message)\n\t\t\texcept Exception as e:\n\t\t\t\terrors.append(e)\n\t\treturn configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# the configuration data is stored in this dictionary, with configuration filename as key\n\t\tconfigurations = {}\n\t\t# load the configuration files\n\t\tif not isinstance(config_filenames, (list, tuple)):\n\t\t\tconfig_filenames = [config_filenames]\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\twith open(config_filename, 'rt') as config_file:\n\t\t\t\t\tconfig = yaml.safe_load(config_file) or {}\n\t\t\texcept OSError as e:\n\t\t\t\tlogging.exception(\"failed to load configuration file %s: %s\", config_filename, e)\n\t\t\t\tcontinue\n\t\t\texcept Exception as e:\n\t\t\t\tlogging.exception(\"failed to read configuration file %s: %s\", config_filename, e)\n\t\t\t\tcontinue\n\t\t\t# apply the configuration overrides\n\t\t\tif overrides:\n\t\t\t\tfor key, value in overrides.items():\n\t\t\t\t\tif key in config:\n\t\t\t\t\t\tconfig[key] = value\n\t\t\t# validate the configuration\n\t\t\ttry:\n\t\t\t\tvalidate_configuration(config)\n\t\t\texcept ConfigurationError as e:\n\t\t\t\tlogging.exception(\"invalid configuration file %s: %s\", config_filename, e)\n\t\t\t\tcontinue\n\t\t\t# resolve environment variables\n\t\t\tif resolve_env:\n\t\t\t\tconfig = dict((key, os.path.expandvars(value)) for key, value in config.items())\n\t\t\tconfigurations[", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# We need to be able to read the configuration file.\n\tif not os.access(config_filenames, os.R_OK):\n\t\treturn None, [logging.LogRecord(loggerName='config_loader', level=logging.ERROR, msg=\"Unable to read configuration file: {}\".format(config_filenames), args=None, exc_info=None)]\n\tconfig_files = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig_files.append(load_configuration_file(filename, overrides, resolve_env))\n\t\texcept Exception as ex:\n\t\t\treturn None, [logging.LogRecord(loggerName='config_loader', level=logging.ERROR, msg=\"Error while parsing configuration file: {}\".format(filename), args=None, exc_info=ex)]\n\treturn dict([(cf['filename'], cf) for cf in config_files]), []", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tconfig = ConfigParser.SafeConfigParser()\n\t\tconfig.read(config_filenames)\n\t\terror_records = []\n\t\terrors = []\n\t\tconfig_dict = {}\n\t\tfor config_filename in config_filenames:\n\t\t\ttry:\n\t\t\t\tconfig_dict[config_filename] = parse_configuration(config, overrides, resolve_env)\n\t\t\texcept ConfigParser.ParsingError as e:\n\t\t\t\terror_records.append(e.record)\n\t\t\t\terrors.append(e.message)\n\t\tif errors:\n\t\t\traise ConfigError(errors, error_records)\n\t\treturn config_dict, error_records\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t/pycfiles/djay-0.0.8-py2.py3-none-any/models.cpython-36.py\n# uncompyle6 version 3.7.4\n# Python bytecode 3.6 (3379)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: /tmp/pip-install-m_4qh6p6/Pygments/pygments/lexers/models.py\n# Compiled at: 2019-07-30 18:47:11\n# Size of source mod 2**32: 3734 bytes\n\"\"\"\n    pygments.lexers.models\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for models.\n\n    :copyright: Copyright 2006-2019 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\nimport re\nfrom pygments.lexer import RegexLexer, include, bygroups, using\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation, Error\n__all__ = [\n 'ModelLexer']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\treturn _load_configurations(config_filenames, overrides, resolve_env)", "is_pass": false, "return_code": 1}]}
{"file_path": "o2sclpy/utils.py", "project": "awsteiner/o2sclpy", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\tif isinstance(obj, (np.bytes_, bytes)):\n\t\t\treturn obj.decode('utf-8')\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj\n\telse:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, (numpy.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n/data_loader/loaders/video_loader.py\n\"\"\"\nVideo Loader\n\"\"\"\n\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nfrom data_loader.image_loader import ImageLoader\nfrom data_loader.video_loader import VideoLoader\nfrom data_loader.utils import force_string\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\ttry:\n\t\t\treturn obj.decode('utf-8')\n\t\texcept UnicodeDecodeError:\n\t\t\treturn obj\n\telse:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\n\tif isinstance(obj, np.ndarray):\n\t\tobj = obj.tostring()\n\tif isinstance(obj, bytes):\n\t\tobj = obj.decode(\"utf-8\")\n\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\tif isinstance(obj, (bytes, numpy.bytes)):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\tif isinstance(obj, (np.bytes_, bytes)):\n\t\t\treturn obj.decode('utf-8')\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, bytes):\n\t\treturn obj\n\telse:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, (numpy.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n/data_loader/loaders/video_loader.py\n\"\"\"\nVideo Loader\n\"\"\"\n\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nfrom data_loader.image_loader import ImageLoader\nfrom data_loader.video_loader import VideoLoader\nfrom data_loader.utils import force_string\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\ttry:\n\t\t\treturn obj.decode('utf-8')\n\t\texcept UnicodeDecodeError:\n\t\t\treturn obj\n\telse:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t \n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\n\tif isinstance(obj, np.ndarray):\n\t\tobj = obj.tostring()\n\tif isinstance(obj, bytes):\n\t\tobj = obj.decode(\"utf-8\")\n\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\tif isinstance(obj, (bytes, numpy.bytes)):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\treturn cls(int(ticks/10000), int(ticks/100)%100, int(ticks%100), tz=tz)\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not (0 <= ticks < 86400000000000):\n\t\t\traise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000000)\")\n\t\treturn cls(ticks // 1000000, ticks % 1000000, tz)\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = datetime.timezone.utc\n\t\tif ticks < 0:\n\t\t\traise ValueError(\"negative ticks\")\n\t\tif ticks >= 86400000000000:\n\t\t\traise ValueError(\"more than 24 hours\")\n\t\treturn cls(datetime.datetime.fromtimestamp(ticks / 1e9, tz=tz))\n\n\t@classmethod\n\tdef from_seconds(cls, seconds, tz=None):\n\t\"\"\"\n\tCreate a time from seconds (seconds since midnight).\n\n:param seconds: seconds since midnight\n:type seconds: float\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if seconds is out of bounds\n    (0 <= seconds < 86400)\n\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = datetime.timezone.utc\n\t\tif seconds < 0:\n\t\t\traise ValueError(\"negative seconds\")\n\t\tif seconds >= 86400:\n\t\t\traise ValueError(\"more than 24 hours\")\n\t\treturn cls(datetime.datetime.fromtimestamp(seconds, tz=tz))\n\n\t@classmethod\n\tdef from_offset(cls, offset, tz=None):\n\t\"\"\"\n\tCreate a time from an offset (timedelta).\n\n:param offset", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError('ticks out of bounds')\n\t\treturn cls(ticks // 1000000000, ticks % 1000000000, tz)\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif ticks < 0 or ticks >= 86400000000000:\n\t\t\traise ValueError(f'{ticks} is out of bounds')\n\t\treturn cls(ticks // 1000000000, ticks % 1000000000, tz=tz)\n\n@classmethod", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError(\"ticks must be in [0, 86400000000000)\")\n\t\treturn cls(ticks, tz)\n\n    @classmethod\n    def from_time(cls, t, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.time object.\n\n:param t: A time.time() object.\n:type t: datetime.time\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\t\"\"\"\n\t\treturn cls(t.hour * 3600 + t.minute * 60 + t.second, tz)\n\n    @classmethod\n    def from_datetime(cls, dt, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.datetime object.\n\n:param dt: A datetime.datetime() object.\n:type dt: datetime.datetime\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\t\"\"\"\n\t\treturn cls(dt.hour * 3600 + dt.minute * 60 + dt.second, tz)\n\n    @classmethod\n    def from_date(cls, d, tz=None):\n\t\"\"\"\n\tCreate a time", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError('ticks out of bounds')\n\t\treturn cls(0, 0, 0, ticks, tz)\n\n@classmethod", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError('ticks must be in range 0 <= ticks < 86400000000000')\n\t\treturn cls(ticks, tz)\n\n@classmethod", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\treturn Time.from_datetime(datetime.datetime.fromtimestamp(ticks/1000000000, tz=tz), tz=tz)\n\n@classmethod", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < Time.MAX_TICKS:\n\t\t\traise ValueError(\"Invalid ticks: %d\" % ticks)\n\t\tif tz:\n\t\t\treturn cls(ticks, tz)\n\t\telse:\n\t\t\treturn cls(ticks)\n\n\t@classmethod\n\tdef from_time(cls, time):\n\t\t\"\"\"\n\t\tCreate a Time from a time.time() value (seconds since midnight).\n\n:param time: time.time() value\n:type time: float\n\n:rtype: Time\n\n:raises ValueError: if time is out of bounds (0 <= time < 86400)\n\t\"\"\"\n\t\tif not 0 <= time < Time.MAX_TIME:\n\t\t\traise ValueError(\"Invalid time: %f\" % time)\n\t\treturn cls(time)\n\n\t@classmethod\n\tdef from_string(cls, string, tz=None):\n\t\t\"\"\"\n\t\tCreate a Time from a string in ISO 8601 format.\n\n:param string: time string in ISO 8601 format\n:type string: str\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if string is in invalid format\n\t\"\"\"\n\t\tif not Time.RE.match(string):\n\t\t\traise ValueError(\"Invalid string: %s\" % string"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\treturn cls(int(ticks/10000), int(ticks/100)%100, int(ticks%100), tz=tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not (0 <= ticks < 86400000000000):\n\t\t\traise ValueError(\"ticks out of bounds (0 <= ticks < 86400000000000)\")\n\t\treturn cls(ticks // 1000000, ticks % 1000000, tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = datetime.timezone.utc\n\t\tif ticks < 0:\n\t\t\traise ValueError(\"negative ticks\")\n\t\tif ticks >= 86400000000000:\n\t\t\traise ValueError(\"more than 24 hours\")\n\t\treturn cls(datetime.datetime.fromtimestamp(ticks / 1e9, tz=tz))\n\n\t@classmethod\n\tdef from_seconds(cls, seconds, tz=None):\n\t\"\"\"\n\tCreate a time from seconds (seconds since midnight).\n\n:param seconds: seconds since midnight\n:type seconds: float\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if seconds is out of bounds\n    (0 <= seconds < 86400)\n\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = datetime.timezone.utc\n\t\tif seconds < 0:\n\t\t\traise ValueError(\"negative seconds\")\n\t\tif seconds >= 86400:\n\t\t\traise ValueError(\"more than 24 hours\")\n\t\treturn cls(datetime.datetime.fromtimestamp(seconds, tz=tz))\n\n\t@classmethod\n\tdef from_offset(cls, offset, tz=None):\n\t\"\"\"\n\tCreate a time from an offset (timedelta).\n\n:param offset", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError('ticks out of bounds')\n\t\treturn cls(ticks // 1000000000, ticks % 1000000000, tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif ticks < 0 or ticks >= 86400000000000:\n\t\t\traise ValueError(f'{ticks} is out of bounds')\n\t\treturn cls(ticks // 1000000000, ticks % 1000000000, tz=tz)\n\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError(\"ticks must be in [0, 86400000000000)\")\n\t\treturn cls(ticks, tz)\n\n    @classmethod\n    def from_time(cls, t, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.time object.\n\n:param t: A time.time() object.\n:type t: datetime.time\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\t\"\"\"\n\t\treturn cls(t.hour * 3600 + t.minute * 60 + t.second, tz)\n\n    @classmethod\n    def from_datetime(cls, dt, tz=None):\n\t\"\"\"\n\tCreate a time from a datetime.datetime object.\n\n:param dt: A datetime.datetime() object.\n:type dt: datetime.datetime\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\t\"\"\"\n\t\treturn cls(dt.hour * 3600 + dt.minute * 60 + dt.second, tz)\n\n    @classmethod\n    def from_date(cls, d, tz=None):\n\t\"\"\"\n\tCreate a time", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError('ticks out of bounds')\n\t\treturn cls(0, 0, 0, ticks, tz)\n\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < 86400000000000:\n\t\t\traise ValueError('ticks must be in range 0 <= ticks < 86400000000000')\n\t\treturn cls(ticks, tz)\n\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\treturn Time.from_datetime(datetime.datetime.fromtimestamp(ticks/1000000000, tz=tz), tz=tz)\n\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not 0 <= ticks < Time.MAX_TICKS:\n\t\t\traise ValueError(\"Invalid ticks: %d\" % ticks)\n\t\tif tz:\n\t\t\treturn cls(ticks, tz)\n\t\telse:\n\t\t\treturn cls(ticks)\n\n\t@classmethod\n\tdef from_time(cls, time):\n\t\t\"\"\"\n\t\tCreate a Time from a time.time() value (seconds since midnight).\n\n:param time: time.time() value\n:type time: float\n\n:rtype: Time\n\n:raises ValueError: if time is out of bounds (0 <= time < 86400)\n\t\"\"\"\n\t\tif not 0 <= time < Time.MAX_TIME:\n\t\t\traise ValueError(\"Invalid time: %f\" % time)\n\t\treturn cls(time)\n\n\t@classmethod\n\tdef from_string(cls, string, tz=None):\n\t\t\"\"\"\n\t\tCreate a Time from a string in ISO 8601 format.\n\n:param string: time string in ISO 8601 format\n:type string: str\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if string is in invalid format\n\t\"\"\"\n\t\tif not Time.RE.match(string):\n\t\t\traise ValueError(\"Invalid string: %s\" % string", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\t# TODO: Remove this when we remove ProtocolV1\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be a tuple.\")\n\t# TODO: Remove this when we remove ProtocolV1\n\tif protocol_version is None or protocol_version == (1, 0):\n\t\treturn {ProtocolV1.protocol_version: ProtocolV1}\n\treturn {ProtocolV2.protocol_version: ProtocolV2}\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\tif protocol_version is None:\n\t\t\tavailable = cls.protocol_versions\n\t\telse:\n\t\t\tavailable = [protocol_version]\n\t\treturn dict((p, cls.protocol_handlers[p]) for p in available if p in cls.protocol_handlers)\n\n```\n\n### 6.2 \u603b\u7ed3\n\n#### 6.2.1 \u4ec0\u4e48\u662f\u5143\u7c7b\uff1f\n\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n\n#### 6.2.2 \u4ec0\u4e48\u662f\u7c7b\u88c5\u9970\u5668\uff1f\n\n- \u7c7b\u88c5\u9970\u5668\u662f\u7528\u6765\u4fee\u9970\u7c7b\u7684\u88c5\u9970\u5668\n- \u7c7b\u88c5\u9970\u5668\u662f\u7528\u6765\u4fee\u9970\u7c7b\u7684\u88c5\u9970\u5668\n\n#### 6.2.3 \u4ec0\u4e48\u662f\u7c7b\uff1f\n\n- \u7c7b\u662f\u7528\u6765\u521b\u5efa\u5bf9\u8c61\u7684\u7c7b\n- \u7c7b\u662f\u7528\u6765\u521b\u5efa\u5bf9\u8c61\u7684\u7c7b\n- \u7c7b\u662f\u7528\u6765\u521b\u5efa\u5bf9\u8c61\u7684\u7c7b\n\n#### 6.2.4 \u5143\u7c7b\u662f\u4ec0\u4e48\uff1f\n\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n\n#### 6.2.5 \u7c7b\u88c5\u9970\u5668\u662f\u4ec0\u4e48", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None:\n\t\t\tif not isinstance(protocol_version, tuple):\n\t\t\t\traise TypeError('protocol version must be a tuple')\n\t\t\thandler_class = cls._get_handler_class(protocol_version)\n\t\t\tif handler_class is None:\n\t\t\t\traise ValueError('unsupported protocol version %s' % protocol_version)\n\t\t\treturn {protocol_version: handler_class}\n\t\tversions = set()\n\t\tfor handler in cls._get_all_handler_classes():\n\t\t\tversions.add(handler.protocol_version)\n\t\treturn dict((version, cls._get_handler_class(version)) for version in versions)\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n:meth:`~neo4j.bolt.v1.connection.Session.run`\n\n.. code-block:: python\n\n    query = \"MATCH (a:Person)-[:KNOWS]->(b:Person) RETURN a.name\"\n    result = session.run(query)\n\nThe result object returned by the run method is an instance of\n:class:`neo4j.bolt.v1.result.Result` class.\n\n:class:`~neo4j.bolt.v1.result.Result` instances have the following methods:\n\n:meth:`~neo4j.bolt.v1.result.Result.consume`\n\n.. code-block:: python\n\n    while result.consume():\n        print(result.summary())\n\n:meth:`~neo4j.bolt.v1.result.Result.peek`\n\n.. code-block:: python\n\n    while result.peek():\n        print(result.next())\n\n:meth:`~neo4j.bolt.v1.result.Result.next`\n\n.. code-block:: python\n\n    while result.next():\n        print(result.next())\n\n:meth:`~neo4j.bolt.v1.result.Result.summary`\n\n.. code-block:: python\n\n    while result.summary():\n        print(result.summary())\n\n:meth:`~neo4j.bolt.v1.result.Result.stream", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls._protocol_handlers\n\t\telif isinstance(protocol_version, tuple):\n\t\t\tif protocol_version in cls._protocol_handlers:\n\t\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\t\telse:\n\t\t\t\treturn {}\n\t\telse:\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None and not isinstance(protocol_version, (tuple, list)):\n\t\t\traise TypeError('protocol_version must be a tuple or list')\n\t\tif protocol_version is None:\n\t\t\treturn cls._all_handlers\n\t\telse:\n\t\t\treturn dict(cls._all_handlers.items() if protocol_version is None else dict(filter(lambda item: item[0] == protocol_version, cls._all_handlers.items())))\n\n```\n\nThe above function is part of the `BoltProtocol` class.\n\n```", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\t\traise TypeError('protocol version must be passed in as a tuple')\n\n\t\tif protocol_version is None:\n\t\t\treturn {k: v for k, v in cls.__handlers__.items() if v.is_supported()}\n\t\telse:\n\t\t\tif protocol_version not in cls.__handlers__:\n\t\t\t\traise ValueError('no handler registered for protocol version {}'.format(protocol_version))\n\t\t\treturn {protocol_version: cls.__handlers__[protocol_version]}\n\n/bolt/bolt_protocol_handler.py\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom bolt.bolt_protocol import BoltProtocol\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n:param protocol_version: tuple identifying a specific protocol\n\tversion (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n\trelevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple\")\n\thandlers = {}\n\tfor name in dir(cls):\n\t\thandler = getattr(cls, name)\n\t\tif not hasattr(handler, \"__bolt_protocol_version__\"):\n\t\t\tcontinue\n\t\tif protocol_version is not None:\n\t\t\tif protocol_version == handler.__bolt_protocol_version__:\n\t\t\t\thandlers[protocol_version] = handler\n\t\t\tcontinue\n\t\tif handler.__bolt_protocol_version__ not in handlers:\n\t\t\thandlers[handler.__bolt_protocol_version__] = handler\n\treturn handlers\n\n```\n\n### 3.1.1\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.2\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.3\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.4\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.5\n\n- [x] \u539f\u751f", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None:\n\t\t\tif not isinstance(protocol_version, tuple):\n\t\t\t\traise TypeError('protocol_version must be a tuple')\n\t\t\treturn {protocol_version: cls.get_handler(protocol_version)}\n\t\thandlers = {}\n\t\tfor protocol_version in cls.get_supported_versions():\n\t\t\thandlers[protocol_version] = cls.get_handler(protocol_version)\n\t\treturn handlers\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\n\t\t\t\t\"protocol_version must be a tuple, not {}\".format(\n\t\t\t\t\ttype(protocol_version)\n\t\t\t\t)\n\t\t\t)\n\t\tif protocol_version is not None:\n\t\t\tversions = [protocol_version]\n\t\telse:\n\t\t\tversions = cls.PROTOCOLS.keys()\n\t\treturn {version: cls.PROTOCOLS[version] for version in versions}\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\t# TODO: Remove this when we remove ProtocolV1\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be a tuple.\")\n\t# TODO: Remove this when we remove ProtocolV1\n\tif protocol_version is None or protocol_version == (1, 0):\n\t\treturn {ProtocolV1.protocol_version: ProtocolV1}\n\treturn {ProtocolV2.protocol_version: ProtocolV2}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\tif protocol_version is None:\n\t\t\tavailable = cls.protocol_versions\n\t\telse:\n\t\t\tavailable = [protocol_version]\n\t\treturn dict((p, cls.protocol_handlers[p]) for p in available if p in cls.protocol_handlers)\n\n```\n\n### 6.2 \u603b\u7ed3\n\n#### 6.2.1 \u4ec0\u4e48\u662f\u5143\u7c7b\uff1f\n\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n\n#### 6.2.2 \u4ec0\u4e48\u662f\u7c7b\u88c5\u9970\u5668\uff1f\n\n- \u7c7b\u88c5\u9970\u5668\u662f\u7528\u6765\u4fee\u9970\u7c7b\u7684\u88c5\u9970\u5668\n- \u7c7b\u88c5\u9970\u5668\u662f\u7528\u6765\u4fee\u9970\u7c7b\u7684\u88c5\u9970\u5668\n\n#### 6.2.3 \u4ec0\u4e48\u662f\u7c7b\uff1f\n\n- \u7c7b\u662f\u7528\u6765\u521b\u5efa\u5bf9\u8c61\u7684\u7c7b\n- \u7c7b\u662f\u7528\u6765\u521b\u5efa\u5bf9\u8c61\u7684\u7c7b\n- \u7c7b\u662f\u7528\u6765\u521b\u5efa\u5bf9\u8c61\u7684\u7c7b\n\n#### 6.2.4 \u5143\u7c7b\u662f\u4ec0\u4e48\uff1f\n\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n- \u5143\u7c7b\u662f\u7528\u6765\u521b\u5efa\u7c7b\u7684\u7c7b\n\n#### 6.2.5 \u7c7b\u88c5\u9970\u5668\u662f\u4ec0\u4e48", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None:\n\t\t\tif not isinstance(protocol_version, tuple):\n\t\t\t\traise TypeError('protocol version must be a tuple')\n\t\t\thandler_class = cls._get_handler_class(protocol_version)\n\t\t\tif handler_class is None:\n\t\t\t\traise ValueError('unsupported protocol version %s' % protocol_version)\n\t\t\treturn {protocol_version: handler_class}\n\t\tversions = set()\n\t\tfor handler in cls._get_all_handler_classes():\n\t\t\tversions.add(handler.protocol_version)\n\t\treturn dict((version, cls._get_handler_class(version)) for version in versions)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n:meth:`~neo4j.bolt.v1.connection.Session.run`\n\n.. code-block:: python\n\n    query = \"MATCH (a:Person)-[:KNOWS]->(b:Person) RETURN a.name\"\n    result = session.run(query)\n\nThe result object returned by the run method is an instance of\n:class:`neo4j.bolt.v1.result.Result` class.\n\n:class:`~neo4j.bolt.v1.result.Result` instances have the following methods:\n\n:meth:`~neo4j.bolt.v1.result.Result.consume`\n\n.. code-block:: python\n\n    while result.consume():\n        print(result.summary())\n\n:meth:`~neo4j.bolt.v1.result.Result.peek`\n\n.. code-block:: python\n\n    while result.peek():\n        print(result.next())\n\n:meth:`~neo4j.bolt.v1.result.Result.next`\n\n.. code-block:: python\n\n    while result.next():\n        print(result.next())\n\n:meth:`~neo4j.bolt.v1.result.Result.summary`\n\n.. code-block:: python\n\n    while result.summary():\n        print(result.summary())\n\n:meth:`~neo4j.bolt.v1.result.Result.stream", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\treturn cls._protocol_handlers\n\t\telif isinstance(protocol_version, tuple):\n\t\t\tif protocol_version in cls._protocol_handlers:\n\t\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\t\telse:\n\t\t\t\treturn {}\n\t\telse:\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None and not isinstance(protocol_version, (tuple, list)):\n\t\t\traise TypeError('protocol_version must be a tuple or list')\n\t\tif protocol_version is None:\n\t\t\treturn cls._all_handlers\n\t\telse:\n\t\t\treturn dict(cls._all_handlers.items() if protocol_version is None else dict(filter(lambda item: item[0] == protocol_version, cls._all_handlers.items())))\n\n```\n\nThe above function is part of the `BoltProtocol` class.\n\n```", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\t\traise TypeError('protocol version must be passed in as a tuple')\n\n\t\tif protocol_version is None:\n\t\t\treturn {k: v for k, v in cls.__handlers__.items() if v.is_supported()}\n\t\telse:\n\t\t\tif protocol_version not in cls.__handlers__:\n\t\t\t\traise ValueError('no handler registered for protocol version {}'.format(protocol_version))\n\t\t\treturn {protocol_version: cls.__handlers__[protocol_version]}\n\n/bolt/bolt_protocol_handler.py\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom bolt.bolt_protocol import BoltProtocol\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n:param protocol_version: tuple identifying a specific protocol\n\tversion (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n\trelevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple\")\n\thandlers = {}\n\tfor name in dir(cls):\n\t\thandler = getattr(cls, name)\n\t\tif not hasattr(handler, \"__bolt_protocol_version__\"):\n\t\t\tcontinue\n\t\tif protocol_version is not None:\n\t\t\tif protocol_version == handler.__bolt_protocol_version__:\n\t\t\t\thandlers[protocol_version] = handler\n\t\t\tcontinue\n\t\tif handler.__bolt_protocol_version__ not in handlers:\n\t\t\thandlers[handler.__bolt_protocol_version__] = handler\n\treturn handlers\n\n```\n\n### 3.1.1\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.2\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.3\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.4\n\n- [x] \u539f\u751f\u5b9e\u73b0\n\n### 3.5\n\n- [x] \u539f\u751f", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None:\n\t\t\tif not isinstance(protocol_version, tuple):\n\t\t\t\traise TypeError('protocol_version must be a tuple')\n\t\t\treturn {protocol_version: cls.get_handler(protocol_version)}\n\t\thandlers = {}\n\t\tfor protocol_version in cls.get_supported_versions():\n\t\t\thandlers[protocol_version] = cls.get_handler(protocol_version)\n\t\treturn handlers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\n\t\t\t\t\"protocol_version must be a tuple, not {}\".format(\n\t\t\t\t\ttype(protocol_version)\n\t\t\t\t)\n\t\t\t)\n\t\tif protocol_version is not None:\n\t\t\tversions = [protocol_version]\n\t\telse:\n\t\t\tversions = cls.PROTOCOLS.keys()\n\t\treturn {version: cls.PROTOCOLS[version] for version in versions}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tbookmarks = cls()\n\t\tbookmarks.bookmarks = [ cls.Bookmark.from_raw_value(b) for b in values ]\n\t\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tbookmarks = Bookmarks()\n\t\tfor value in values:\n\t\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\t\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\treturn cls(values)\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tbookmarks = cls()\n\t\tbookmarks._bookmarks = values\n\t\treturn bookmarks\n\t\t", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\n\n\tif len(values) % 2!= 0:\n\t\traise ValueError(\"Bookmark values must be pairs of strings.\")\n\n\tbookmarks = cls()\n\n\tfor i in range(0, len(values), 2):\n\t\tname = values[i]\n\t\turl = values[i+1]\n\t\tbookmarks.add(name, url)\n\n\treturn bookmarks\n\t\n\n#\n# Bookmark class\n#\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\treturn cls(values)\n\n# Methods for Bookmarks", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tl = []\n\t\tfor v in values:\n\t\t\tif v is None:\n\t\t\t\tcontinue\n\t\t\tl.append(cls.from_raw_value(v))\n\t\treturn cls(l)\n\n\tdef to_raw_values(self):\n\t\t\"\"\"\n\t\tReturn a list of raw bookmark string values.\n\t\t\"\"\"\n\t\treturn [v.to_raw_value() for v in self]\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t_path, 'r') as fp:\n\t\treturn json.load(fp)import json\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\treturn cls([Bookmark(value) for value in values])\n# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file'mainwindow.ui'\n#\n# Created by: PyQt5 UI code generator 5.15.0\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic5 is\n# run again.  Do not edit this file unless you know what you are doing.\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tbookmarks = cls()\n\t\tbookmarks.bookmarks = [ cls.Bookmark.from_raw_value(b) for b in values ]\n\t\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tbookmarks = Bookmarks()\n\t\tfor value in values:\n\t\t\tbookmarks.append(Bookmark.from_raw_value(value))\n\t\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\treturn cls(values)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tbookmarks = cls()\n\t\tbookmarks._bookmarks = values\n\t\treturn bookmarks\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\n\n\tif len(values) % 2!= 0:\n\t\traise ValueError(\"Bookmark values must be pairs of strings.\")\n\n\tbookmarks = cls()\n\n\tfor i in range(0, len(values), 2):\n\t\tname = values[i]\n\t\turl = values[i+1]\n\t\tbookmarks.add(name, url)\n\n\treturn bookmarks\n\t\n\n#\n# Bookmark class\n#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\treturn cls(values)\n\n# Methods for Bookmarks", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\tl = []\n\t\tfor v in values:\n\t\t\tif v is None:\n\t\t\t\tcontinue\n\t\t\tl.append(cls.from_raw_value(v))\n\t\treturn cls(l)\n\n\tdef to_raw_values(self):\n\t\t\"\"\"\n\t\tReturn a list of raw bookmark string values.\n\t\t\"\"\"\n\t\treturn [v.to_raw_value() for v in self]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t_path, 'r') as fp:\n\t\treturn json.load(fp)import json\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\treturn cls([Bookmark(value) for value in values])\n# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file'mainwindow.ui'\n#\n# Created by: PyQt5 UI code generator 5.15.0\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic5 is\n# run again.  Do not edit this file unless you know what you are doing.\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None and isinstance(seq, (list, tuple)):\n\t\t\tbufsize = len(seq)\n\t\treturn seq, type(seq)\n\telse:\n\t\treturn seq, type(seq)\n\t\n\tdef _is_contiguous(seq):\n\t\t\"\"\"\n\t\tReturn True for sequences that are guaranteed to be contiguous in memory.\n\t\t\"\"\"\n\t\t# XXX I'm not sure this is right.\n\t\treturn isinstance(seq, (list, tuple))\n\tdef _is_contiguous_and_ordered(seq):\n\t\t\"\"\"\n\t\tReturn True for sequences that are guaranteed to be contiguous in memory,\n\t\tand for which the order of items is meaningful.\n\t\t\"\"\"\n\t\t# XXX I'm not sure this is right.\n\t\treturn isinstance(seq, (list, tuple)) or _is_contiguous(seq)\n\tdef _is_list(seq):\n\t\treturn isinstance(seq, list)\n\tdef _is_tuple(seq):\n\t\treturn isinstance(seq, tuple)\n\tdef _is_set(seq):\n\t\treturn isinstance(seq, set)\n\tdef _is_dict(seq):\n\t\treturn isinstance(seq, dict)\n\tdef _is_ordered(seq):\n\t\t\"\"\"\n\t\tReturn True for sequences that have a defined order of items.\n\t\t\"\"\"\n\t\treturn _is_list(seq) or _is", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif issubclass(type(seq), Sequence):\n\t\treturn seq, type(seq)\n\treturn [seq], list\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tseq = seq if isinstance(seq, types.SequenceType) else (seq,)\n\t\treturn seq, type(seq[0] if seq else None)", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif seq is None:\n\t\t\treturn None, None\n\t\telif isinstance(seq, str):\n\t\t\treturn seq, str\n\t\telif isinstance(seq, bytes):\n\t\t\treturn seq, bytes\n\t\telif isinstance(seq, bytearray):\n\t\t\treturn bytes(seq), bytes\n\t\telif isinstance(seq, memoryview):\n\t\t\treturn seq.tobytes(), bytes\n\t\telif isinstance(seq, collections.Sequence):\n\t\t\tif isinstance(seq, str):\n\t\t\t\treturn seq, str\n\t\t\telif isinstance(seq, bytes):\n\t\t\t\treturn seq, bytes\n\t\t\telif isinstance(seq, bytearray):\n\t\t\t\treturn bytes(seq), bytes\n\t\t\telse:\n\t\t\t\treturn bytes(seq), bytes\n\t\telse:\n\t\t\traise TypeError(f\"Expected sequence type, got: {seq!r}\")\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\treturn seq, type(seq)\n\t\tif isinstance(seq, (list, tuple)):\n\t\t\treturn seq, _get_seq_type(seq)\n\t\telse:\n\t\t\treturn seq, type(seq)\n\tif bufsize is None:\n\t\treturn seq\n\telse:\n\t\tbufsize = _get_bufsize(bufsize)\n\t\tif bufsize < 0:\n\t\t\treturn seq\n\t\tif isinstance(seq, str):\n\t\t\treturn seq[:bufsize]\n\t\telse:\n\t\t\treturn seq[:bufsize]", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif isinstance(seq, (str, unicode)):\n\t\tseq = [seq]\n\telif isinstance(seq, (tuple, list)):\n\tseq = seq\n\telse:\n\tseq = list(seq)\n\treturn seq, type(seq[0])\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif isinstance(seq, bytes):\n\t\t\treturn seq, btype\n\t\telif isinstance(seq, str):\n\t\t\treturn seq, stype\n\t\telif isinstance(seq, collections.Sequence):\n\t\t\treturn seq, type(seq)\n\t\telif isinstance(seq, (memoryview, bytearray)):\n\t\t\treturn seq, type(seq)\n\t\telse:\n\t\t\treturn seq, type(seq)\n\n\tdef _get_byte_buffer(bufsize=None):\n\t\t\"\"\"\n\t\tReturn a buffer-like object of at least *bufsize* bytes.\n\t\t\"\"\"\n\t\t# We use the buffer() builtin function to create the buffer.\n\t\t# It will return a memoryview if the buffer size is big\n\t\t# enough, or if the buffer is backed by a bytearray and\n\t\t# the bytearray is big enough.\n\t\t# Otherwise, it will return a bytearray.\n\t\tif bufsize is None:\n\t\t\tbufsize = 32\n\t\ttry:\n\t\t\t# Python 3.3+\n\t\t\treturn bytearray(bufsize)\n\t\texcept TypeError:\n\t\t\t# Python 3.2 or earlier\n\t\t\treturn buffer(bufsize)\n\n\tdef _get_byte_buffer_with_type(bufsize=None):\n\t\treturn _get_seq_with_type(_get_byte_buffer(bufsize))\n\n\tdef _get_", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\t# TODO: Add support for other sequence types.\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\tif not bufsize:\n\t\t\treturn None, None\n\tseq = seq[:bufsize]\n\tfor (type, func) in _SEQ_TYPES:\n\t\tif isinstance(seq, type):\n\t\t\treturn (seq, type), func\n\t\t\treturn (seq, type)\n\treturn None, None\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is not None and bufsize < 1:\n\t\t\traise ValueError('bufsize must be at least 1')\n\t\t# Get the type of the sequence.\n\t\tseq_type = type(seq)\n\t\tif issubclass(seq_type, (tuple, list)):\n\t\t\t# The sequence is already a sequence type.\n\t\t\treturn seq, seq_type\n\t\t# Get the type of the elements in the sequence.\n\t\tseq_iter = iter(seq)\n\t\ttry:\n\t\t\tfirst_element = next(seq_iter)\n\t\texcept StopIteration:\n\t\t\traise ValueError('sequence cannot be empty')\n\t\telem_type = type(first_element)\n\t\tbuf = []\n\t\t# Fill in the buffer with elements of the same type.\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tbuf.append(next(seq_iter))\n\t\t\texcept StopIteration:\n\t\t\t\tbreak\n\t\t\tif elem_type is not type(buf[-1]):\n\t\t\t\traise TypeError('sequence element type is inconsistent')\n\t\tif bufsize is not None and len(buf) > bufsize:\n\t\t\traise ValueError('sequence is too long')\n\t\t# Return the buffer as the sequence and its type.\n\t\treturn buf, seq_type\n\n\tdef _get_buffer_from_seq(seq, bufsize=None):\n\t\t\"\"\"\n\t\tReturn a buffer of elements"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None and isinstance(seq, (list, tuple)):\n\t\t\tbufsize = len(seq)\n\t\treturn seq, type(seq)\n\telse:\n\t\treturn seq, type(seq)\n\t\n\tdef _is_contiguous(seq):\n\t\t\"\"\"\n\t\tReturn True for sequences that are guaranteed to be contiguous in memory.\n\t\t\"\"\"\n\t\t# XXX I'm not sure this is right.\n\t\treturn isinstance(seq, (list, tuple))\n\tdef _is_contiguous_and_ordered(seq):\n\t\t\"\"\"\n\t\tReturn True for sequences that are guaranteed to be contiguous in memory,\n\t\tand for which the order of items is meaningful.\n\t\t\"\"\"\n\t\t# XXX I'm not sure this is right.\n\t\treturn isinstance(seq, (list, tuple)) or _is_contiguous(seq)\n\tdef _is_list(seq):\n\t\treturn isinstance(seq, list)\n\tdef _is_tuple(seq):\n\t\treturn isinstance(seq, tuple)\n\tdef _is_set(seq):\n\t\treturn isinstance(seq, set)\n\tdef _is_dict(seq):\n\t\treturn isinstance(seq, dict)\n\tdef _is_ordered(seq):\n\t\t\"\"\"\n\t\tReturn True for sequences that have a defined order of items.\n\t\t\"\"\"\n\t\treturn _is_list(seq) or _is", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif issubclass(type(seq), Sequence):\n\t\treturn seq, type(seq)\n\treturn [seq], list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tseq = seq if isinstance(seq, types.SequenceType) else (seq,)\n\t\treturn seq, type(seq[0] if seq else None)", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif seq is None:\n\t\t\treturn None, None\n\t\telif isinstance(seq, str):\n\t\t\treturn seq, str\n\t\telif isinstance(seq, bytes):\n\t\t\treturn seq, bytes\n\t\telif isinstance(seq, bytearray):\n\t\t\treturn bytes(seq), bytes\n\t\telif isinstance(seq, memoryview):\n\t\t\treturn seq.tobytes(), bytes\n\t\telif isinstance(seq, collections.Sequence):\n\t\t\tif isinstance(seq, str):\n\t\t\t\treturn seq, str\n\t\t\telif isinstance(seq, bytes):\n\t\t\t\treturn seq, bytes\n\t\t\telif isinstance(seq, bytearray):\n\t\t\t\treturn bytes(seq), bytes\n\t\t\telse:\n\t\t\t\treturn bytes(seq), bytes\n\t\telse:\n\t\t\traise TypeError(f\"Expected sequence type, got: {seq!r}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\treturn seq, type(seq)\n\t\tif isinstance(seq, (list, tuple)):\n\t\t\treturn seq, _get_seq_type(seq)\n\t\telse:\n\t\t\treturn seq, type(seq)\n\tif bufsize is None:\n\t\treturn seq\n\telse:\n\t\tbufsize = _get_bufsize(bufsize)\n\t\tif bufsize < 0:\n\t\t\treturn seq\n\t\tif isinstance(seq, str):\n\t\t\treturn seq[:bufsize]\n\t\telse:\n\t\t\treturn seq[:bufsize]", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif isinstance(seq, (str, unicode)):\n\t\tseq = [seq]\n\telif isinstance(seq, (tuple, list)):\n\tseq = seq\n\telse:\n\tseq = list(seq)\n\treturn seq, type(seq[0])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif isinstance(seq, bytes):\n\t\t\treturn seq, btype\n\t\telif isinstance(seq, str):\n\t\t\treturn seq, stype\n\t\telif isinstance(seq, collections.Sequence):\n\t\t\treturn seq, type(seq)\n\t\telif isinstance(seq, (memoryview, bytearray)):\n\t\t\treturn seq, type(seq)\n\t\telse:\n\t\t\treturn seq, type(seq)\n\n\tdef _get_byte_buffer(bufsize=None):\n\t\t\"\"\"\n\t\tReturn a buffer-like object of at least *bufsize* bytes.\n\t\t\"\"\"\n\t\t# We use the buffer() builtin function to create the buffer.\n\t\t# It will return a memoryview if the buffer size is big\n\t\t# enough, or if the buffer is backed by a bytearray and\n\t\t# the bytearray is big enough.\n\t\t# Otherwise, it will return a bytearray.\n\t\tif bufsize is None:\n\t\t\tbufsize = 32\n\t\ttry:\n\t\t\t# Python 3.3+\n\t\t\treturn bytearray(bufsize)\n\t\texcept TypeError:\n\t\t\t# Python 3.2 or earlier\n\t\t\treturn buffer(bufsize)\n\n\tdef _get_byte_buffer_with_type(bufsize=None):\n\t\treturn _get_seq_with_type(_get_byte_buffer(bufsize))\n\n\tdef _get_", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\t# TODO: Add support for other sequence types.\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\tif not bufsize:\n\t\t\treturn None, None\n\tseq = seq[:bufsize]\n\tfor (type, func) in _SEQ_TYPES:\n\t\tif isinstance(seq, type):\n\t\t\treturn (seq, type), func\n\t\t\treturn (seq, type)\n\treturn None, None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is not None and bufsize < 1:\n\t\t\traise ValueError('bufsize must be at least 1')\n\t\t# Get the type of the sequence.\n\t\tseq_type = type(seq)\n\t\tif issubclass(seq_type, (tuple, list)):\n\t\t\t# The sequence is already a sequence type.\n\t\t\treturn seq, seq_type\n\t\t# Get the type of the elements in the sequence.\n\t\tseq_iter = iter(seq)\n\t\ttry:\n\t\t\tfirst_element = next(seq_iter)\n\t\texcept StopIteration:\n\t\t\traise ValueError('sequence cannot be empty')\n\t\telem_type = type(first_element)\n\t\tbuf = []\n\t\t# Fill in the buffer with elements of the same type.\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tbuf.append(next(seq_iter))\n\t\t\texcept StopIteration:\n\t\t\t\tbreak\n\t\t\tif elem_type is not type(buf[-1]):\n\t\t\t\traise TypeError('sequence element type is inconsistent')\n\t\tif bufsize is not None and len(buf) > bufsize:\n\t\t\traise ValueError('sequence is too long')\n\t\t# Return the buffer as the sequence and its type.\n\t\treturn buf, seq_type\n\n\tdef _get_buffer_from_seq(seq, bufsize=None):\n\t\t\"\"\"\n\t\tReturn a buffer of elements", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.scale_val is None:\n\t\t\t\tself._compute_scale()\n\t\t\treturn self.scale_val\n\t\tif not self.scale_computed:\n\t\t\traise LenaValueError(\"Can't rescale histogram with scale equal to zero.\")\n\t\telif self.scale_val == 0:\n\t\t\traise LenaValueError(\"Can't rescale histogram with scale equal to zero.\")\n\t\tif recompute:\n\t\t\tnew_scale = other / self.scale_val\n\t\t\tself.scale_val = other\n\t\telse:\n\t\t\tnew_scale = other / self.scale_val\n\t\t\tif new_scale!= self.scale_val:\n\t\t\t\tself.scale_computed = False\n\t\t\t\tself.scale_val = new_scale\n\t\tself.scale_computed = True\n\t\tself.scale_val = new_scale\n\t\tself.bins /= new_scale\n\t\tself.hist /= new_scale\n\t\tself.hist_err /= new_scale\n\t\tself.hist_err_sq /= new_scale\n\t\tself.scale_computed = True\n\t\treturn self.scale_val\n\n\tdef _get_pdf(self, other=None):\n\t\t\"\"\"\nCompute or set probability density function (normalized histogram).\n\nIf *other* is ``None``, return probability density function of this histogram.\nIf its pdf was not", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self._scale == 0:\n\t\t\t\traise LenaValueError(\"Histogram has zero scale.\")\n\t\t\tif self._scale is None or recompute:\n\t\t\t\tself._scale = self._compute_scale()\n\t\t\treturn self._scale\n\t\tif other == 0:\n\t\t\traise LenaValueError(\"Can't scale histogram to zero.\")\n\t\tif self._scale is None:\n\t\t\tself.scale = self._compute_scale()\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"Histogram has zero scale.\")\n\t\tself._scale = other\n\t\tself.rescale()\n\t\treturn self._scale\n\n\tdef rescale(self):\n\t\"\"\"\nRescale histogram.\n\nRescale histogram to have the same integral\nas the previous one.\n\nThis method is used by :meth:`.scale`.\n\t\"\"\"\n\t\tif self._scale is None:\n\t\t\tself._scale = self._compute_scale()\n\t\tself.array *= self._scale / self.scale\n\n\tdef _compute_scale(self):\n\t\"\"\"Compute scale of the histogram.\"\"\"\n\t\tif self.array.size == 0:\n\t\t\treturn 0\n\t\treturn self.array.sum()\n\n\tdef _compute_peak(self):\n\t\"\"\"Compute peak of the histogram.\"\"\"\n\t\tif self.array.size == 0", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self._scale == 0:\n\t\t\t\traise LenaValueError('Scale is zero')\n\t\t\tif self._scale_computed or recompute:\n\t\t\t\tself._scale = self._hist.sum()\n\t\t\t\tself._scale_computed = True\n\t\t\treturn self._scale\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError('Scale is zero')\n\t\tif self._scale_computed:\n\t\t\t# TODO: Should we raise an exception here?\n\t\t\tself._scale_computed = False\n\t\tself._hist *= other / self._scale\n\t\tself._scale = other\n\t\treturn self._scale\n\n\t# TODO: Add a function to get the raw histogram,\n\t# i.e., one that does not include the scale (for speed).\n\tdef to_hist(self):\n\t\t\"\"\"Return the raw histogram.\"\"\"\n\t\treturn self._hist\n\n\tdef _to_hist_and_scale(self):\n\t\t\"\"\"Return a tuple of the raw histogram and the scale.\"\"\"\n\t\treturn self._hist, self._scale\n\n\tdef _set_hist(self, hist):\n\t\t\"\"\"Set the raw histogram and the scale to zero.\n\nUsed to re-use the same object for different histograms.\n\t\"\"\"\n\t\tself._hist = hist\n\t\tself._scale = 0\n\t\tself._scale_computed =", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif not self.is_full():\n\t\t\traise LenaValueError(\"Cannot scale empty histogram.\")\n\t\tif other is None:\n\t\t\tif self.scale is None:\n\t\t\t\tself._compute_scale()\n\t\t\treturn self.scale\n\t\telif isinstance(other, float):\n\t\t\tif self.scale is None or recompute:\n\t\t\t\tself._compute_scale()\n\t\t\tif other == 0:\n\t\t\t\traise LenaValueError(\"Cannot scale to zero.\")\n\t\t\tself.scale *= other / self.scale\n\t\t\treturn self\n\t\telse:\n\t\t\traise TypeError(\"Invalid argument: expected float, got %s.\" % type(other))\n\n\tdef center_of_mass(self):\n\t\"\"\"\nReturn center of mass (weighted average of the bin centers).\n\nIf the histogram was created with zero offset, return the same as ``len(self)``.\n\t\"\"\"\n\t\tif self.offset == 0:\n\t\t\treturn len(self)\n\t\tif self.scale is None:\n\t\t\tself._compute_scale()\n\t\treturn self.scale * self.sum_of_bin_centers() / self.scale\n\n\tdef moment(self, order):\n\t\"\"\"\nCompute moment of order *order* (weighted sum of bin centers to the power *order*).\n\nThe moment is computed using the method of moments\n(the first *order* moments of", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale_\n\t\tif not self.has_scale():\n\t\t\tself.compute_scale()\n\t\tif other == 0:\n\t\t\traise LenaValueError(\"can't rescale histogram to zero\")\n\t\tif other < 0:\n\t\t\tself.scale_ = -self.scale_ / other\n\t\t\tself.scale_changed_ = True\n\t\telse:\n\t\t\tself.scale_ = self.scale_ * other\n\t\t\tself.scale_changed_ = True\n\n\tdef normalize(self, other=None, recompute=False):\n\t\"\"\"\n\tNormalize histogram by setting scale to one.\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, normalize self to *other*.\n\nHistograms with scale equal to zero can't be normalized.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale_\n\t\tif not self.has_scale():\n\t\t\tself.compute_scale()\n\t\tif", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tself.scale = self.scale or 0.\n\t\tif other is None:\n\t\t\tif not self.scale:\n\t\t\t\treturn 0\n\t\t\tif not recompute and self.scale_computed:\n\t\t\t\treturn self.scale\n\t\t\tself.scale_computed = True\n\t\t\tself.scale = sum(self.data)\n\t\t\treturn self.scale\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError('Can\\'t rescale histogram with scale=0')\n\t\tself.scale = other\n\t\tself.scale_computed = True\n\t\treturn self.scale\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tif not recompute:\n\t\t\t\treturn self.scale\n\t\t\tself._scale = self.integral()\n\t\telse:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tif other == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tself._scale = other\n\t\treturn self\n\n\tdef normalize(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set normalization factor (scale of this histogram divided by another one).\n\nIf *other* is ``None``, return normalization factor of this histogram.\nIf its normalization factor was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the normalization factor\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be normalized.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tif not", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif isinstance(other, int):\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"Can't rescale histograms with scale equal to 0\")\n\t\t\tself.scale = other\n\t\t\tif recompute:\n\t\t\t\tself._recompute_scale()\n\t\t\treturn\n\t\tif other is None:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"Can't get scale of histograms with scale equal to 0\")\n\t\t\treturn self.scale\n\t\tif (self.scale == 0) and (not other.is_zero()):\n\t\t\traise LenaValueError(\"Can't rescale histograms with scale equal to 0\")\n\t\tif recompute:\n\t\t\tif self.scale == 0:\n\t\t\t\tself.scale = other.scale\n\t\t\telse:\n\t\t\t\tself.scale = self.scale * other.scale\n\t\t\tself.data = self.data * other.scale\n\t\t\tself.errors = self.errors * other.scale\n\t\telse:\n\t\t\tself.scale = self.scale * other.scale\n\t\t\tself.data = self.data * other.scale\n\n\tdef __iadd__(self, other):\n\t\t\"\"\"\n\tIn-place add-assign.\n\nIn-place add-assign is used to add histograms\nto each other (or to add them to a histogram).\n\t\"\"\"\n\t\t", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is not None:\n\t\t\tif other <= 0.0: raise LenaValueError('Scale must be positive.')\n\t\t\tif self.scale == 0.0: raise LenaValueError('Scale of this histogram can\\'t be rescaled.')\n\t\t\tself._scale = other\n\t\t\tif recompute: self.recompute_scale()\n\t\t\treturn self\n\t\tif self.scale == 0.0: raise LenaValueError('Scale of this histogram can\\'t be computed.')\n\t\treturn self._scale\n\n\tdef recompute_scale(self):\n\t\"\"\"\n\tRecompute scale (integral of the histogram).\n\nThis method is called automatically\nif scale was computed before\nand some data was changed.\n\t\"\"\"\n\t\tself._scale = numpy.sum(self.data)\n\t\treturn self\n\n\tdef __add__(self, other):\n\t\"\"\"\n\tAdd two histograms.\n\nTwo histograms must have the same number of bins\nand the same binning.\n\t\"\"\"\n\t\tif not isinstance(other, Histogram): raise LenaTypeError('Can\\'t add a histogram to an object of type %s.' % other.__class__.__name__)\n\t\tif self.nbins!= other.nbins: raise LenaValueError('Can\\'t add histograms with different number of bins.')\n\t\tif self.binning!= other.binning: raise L", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other == None:\n\t\t\tif not self.computed:\n\t\t\t\tself.compute()\n\t\t\treturn self.scale\n\t\telif not self.computed:\n\t\t\tself.compute(recompute=recompute)\n\t\tif other == 0:\n\t\t\traise lena.LenaValueError(\"Can't rescale histogram with scale=0\")\n\t\tself.scale = other\n\t\tself.recompute_integral = False\n\t\tself.recompute_norm = False\n\treturn\n\n\tdef norm(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set norm (integral of the histogram divided by scale).\n\nIf *other* is ``None``, return norm of this histogram.\nIf its norm was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the norm\nif it was computed before.\n\nIf a float *other* is provided, renormalize self to *other*.\n\nHistograms with scale equal to zero can't be renormalized.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other == None:\n\t\t\tif not self.computed:\n\t\t\t\tself.compute()\n\t\t\treturn self.norm\n\t\telif not self.computed:\n\t\t\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.scale_val is None:\n\t\t\t\tself._compute_scale()\n\t\t\treturn self.scale_val\n\t\tif not self.scale_computed:\n\t\t\traise LenaValueError(\"Can't rescale histogram with scale equal to zero.\")\n\t\telif self.scale_val == 0:\n\t\t\traise LenaValueError(\"Can't rescale histogram with scale equal to zero.\")\n\t\tif recompute:\n\t\t\tnew_scale = other / self.scale_val\n\t\t\tself.scale_val = other\n\t\telse:\n\t\t\tnew_scale = other / self.scale_val\n\t\t\tif new_scale!= self.scale_val:\n\t\t\t\tself.scale_computed = False\n\t\t\t\tself.scale_val = new_scale\n\t\tself.scale_computed = True\n\t\tself.scale_val = new_scale\n\t\tself.bins /= new_scale\n\t\tself.hist /= new_scale\n\t\tself.hist_err /= new_scale\n\t\tself.hist_err_sq /= new_scale\n\t\tself.scale_computed = True\n\t\treturn self.scale_val\n\n\tdef _get_pdf(self, other=None):\n\t\t\"\"\"\nCompute or set probability density function (normalized histogram).\n\nIf *other* is ``None``, return probability density function of this histogram.\nIf its pdf was not", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self._scale == 0:\n\t\t\t\traise LenaValueError(\"Histogram has zero scale.\")\n\t\t\tif self._scale is None or recompute:\n\t\t\t\tself._scale = self._compute_scale()\n\t\t\treturn self._scale\n\t\tif other == 0:\n\t\t\traise LenaValueError(\"Can't scale histogram to zero.\")\n\t\tif self._scale is None:\n\t\t\tself.scale = self._compute_scale()\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"Histogram has zero scale.\")\n\t\tself._scale = other\n\t\tself.rescale()\n\t\treturn self._scale\n\n\tdef rescale(self):\n\t\"\"\"\nRescale histogram.\n\nRescale histogram to have the same integral\nas the previous one.\n\nThis method is used by :meth:`.scale`.\n\t\"\"\"\n\t\tif self._scale is None:\n\t\t\tself._scale = self._compute_scale()\n\t\tself.array *= self._scale / self.scale\n\n\tdef _compute_scale(self):\n\t\"\"\"Compute scale of the histogram.\"\"\"\n\t\tif self.array.size == 0:\n\t\t\treturn 0\n\t\treturn self.array.sum()\n\n\tdef _compute_peak(self):\n\t\"\"\"Compute peak of the histogram.\"\"\"\n\t\tif self.array.size == 0", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self._scale == 0:\n\t\t\t\traise LenaValueError('Scale is zero')\n\t\t\tif self._scale_computed or recompute:\n\t\t\t\tself._scale = self._hist.sum()\n\t\t\t\tself._scale_computed = True\n\t\t\treturn self._scale\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError('Scale is zero')\n\t\tif self._scale_computed:\n\t\t\t# TODO: Should we raise an exception here?\n\t\t\tself._scale_computed = False\n\t\tself._hist *= other / self._scale\n\t\tself._scale = other\n\t\treturn self._scale\n\n\t# TODO: Add a function to get the raw histogram,\n\t# i.e., one that does not include the scale (for speed).\n\tdef to_hist(self):\n\t\t\"\"\"Return the raw histogram.\"\"\"\n\t\treturn self._hist\n\n\tdef _to_hist_and_scale(self):\n\t\t\"\"\"Return a tuple of the raw histogram and the scale.\"\"\"\n\t\treturn self._hist, self._scale\n\n\tdef _set_hist(self, hist):\n\t\t\"\"\"Set the raw histogram and the scale to zero.\n\nUsed to re-use the same object for different histograms.\n\t\"\"\"\n\t\tself._hist = hist\n\t\tself._scale = 0\n\t\tself._scale_computed =", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif not self.is_full():\n\t\t\traise LenaValueError(\"Cannot scale empty histogram.\")\n\t\tif other is None:\n\t\t\tif self.scale is None:\n\t\t\t\tself._compute_scale()\n\t\t\treturn self.scale\n\t\telif isinstance(other, float):\n\t\t\tif self.scale is None or recompute:\n\t\t\t\tself._compute_scale()\n\t\t\tif other == 0:\n\t\t\t\traise LenaValueError(\"Cannot scale to zero.\")\n\t\t\tself.scale *= other / self.scale\n\t\t\treturn self\n\t\telse:\n\t\t\traise TypeError(\"Invalid argument: expected float, got %s.\" % type(other))\n\n\tdef center_of_mass(self):\n\t\"\"\"\nReturn center of mass (weighted average of the bin centers).\n\nIf the histogram was created with zero offset, return the same as ``len(self)``.\n\t\"\"\"\n\t\tif self.offset == 0:\n\t\t\treturn len(self)\n\t\tif self.scale is None:\n\t\t\tself._compute_scale()\n\t\treturn self.scale * self.sum_of_bin_centers() / self.scale\n\n\tdef moment(self, order):\n\t\"\"\"\nCompute moment of order *order* (weighted sum of bin centers to the power *order*).\n\nThe moment is computed using the method of moments\n(the first *order* moments of", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale_\n\t\tif not self.has_scale():\n\t\t\tself.compute_scale()\n\t\tif other == 0:\n\t\t\traise LenaValueError(\"can't rescale histogram to zero\")\n\t\tif other < 0:\n\t\t\tself.scale_ = -self.scale_ / other\n\t\t\tself.scale_changed_ = True\n\t\telse:\n\t\t\tself.scale_ = self.scale_ * other\n\t\t\tself.scale_changed_ = True\n\n\tdef normalize(self, other=None, recompute=False):\n\t\"\"\"\n\tNormalize histogram by setting scale to one.\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, normalize self to *other*.\n\nHistograms with scale equal to zero can't be normalized.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale_\n\t\tif not self.has_scale():\n\t\t\tself.compute_scale()\n\t\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tself.scale = self.scale or 0.\n\t\tif other is None:\n\t\t\tif not self.scale:\n\t\t\t\treturn 0\n\t\t\tif not recompute and self.scale_computed:\n\t\t\t\treturn self.scale\n\t\t\tself.scale_computed = True\n\t\t\tself.scale = sum(self.data)\n\t\t\treturn self.scale\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError('Can\\'t rescale histogram with scale=0')\n\t\tself.scale = other\n\t\tself.scale_computed = True\n\t\treturn self.scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tif not recompute:\n\t\t\t\treturn self.scale\n\t\t\tself._scale = self.integral()\n\t\telse:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tif other == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tself._scale = other\n\t\treturn self\n\n\tdef normalize(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set normalization factor (scale of this histogram divided by another one).\n\nIf *other* is ``None``, return normalization factor of this histogram.\nIf its normalization factor was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the normalization factor\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be normalized.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"scale is zero\")\n\t\t\tif not", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif isinstance(other, int):\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"Can't rescale histograms with scale equal to 0\")\n\t\t\tself.scale = other\n\t\t\tif recompute:\n\t\t\t\tself._recompute_scale()\n\t\t\treturn\n\t\tif other is None:\n\t\t\tif self.scale == 0:\n\t\t\t\traise LenaValueError(\"Can't get scale of histograms with scale equal to 0\")\n\t\t\treturn self.scale\n\t\tif (self.scale == 0) and (not other.is_zero()):\n\t\t\traise LenaValueError(\"Can't rescale histograms with scale equal to 0\")\n\t\tif recompute:\n\t\t\tif self.scale == 0:\n\t\t\t\tself.scale = other.scale\n\t\t\telse:\n\t\t\t\tself.scale = self.scale * other.scale\n\t\t\tself.data = self.data * other.scale\n\t\t\tself.errors = self.errors * other.scale\n\t\telse:\n\t\t\tself.scale = self.scale * other.scale\n\t\t\tself.data = self.data * other.scale\n\n\tdef __iadd__(self, other):\n\t\t\"\"\"\n\tIn-place add-assign.\n\nIn-place add-assign is used to add histograms\nto each other (or to add them to a histogram).\n\t\"\"\"\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is not None:\n\t\t\tif other <= 0.0: raise LenaValueError('Scale must be positive.')\n\t\t\tif self.scale == 0.0: raise LenaValueError('Scale of this histogram can\\'t be rescaled.')\n\t\t\tself._scale = other\n\t\t\tif recompute: self.recompute_scale()\n\t\t\treturn self\n\t\tif self.scale == 0.0: raise LenaValueError('Scale of this histogram can\\'t be computed.')\n\t\treturn self._scale\n\n\tdef recompute_scale(self):\n\t\"\"\"\n\tRecompute scale (integral of the histogram).\n\nThis method is called automatically\nif scale was computed before\nand some data was changed.\n\t\"\"\"\n\t\tself._scale = numpy.sum(self.data)\n\t\treturn self\n\n\tdef __add__(self, other):\n\t\"\"\"\n\tAdd two histograms.\n\nTwo histograms must have the same number of bins\nand the same binning.\n\t\"\"\"\n\t\tif not isinstance(other, Histogram): raise LenaTypeError('Can\\'t add a histogram to an object of type %s.' % other.__class__.__name__)\n\t\tif self.nbins!= other.nbins: raise LenaValueError('Can\\'t add histograms with different number of bins.')\n\t\tif self.binning!= other.binning: raise L", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other == None:\n\t\t\tif not self.computed:\n\t\t\t\tself.compute()\n\t\t\treturn self.scale\n\t\telif not self.computed:\n\t\t\tself.compute(recompute=recompute)\n\t\tif other == 0:\n\t\t\traise lena.LenaValueError(\"Can't rescale histogram with scale=0\")\n\t\tself.scale = other\n\t\tself.recompute_integral = False\n\t\tself.recompute_norm = False\n\treturn\n\n\tdef norm(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set norm (integral of the histogram divided by scale).\n\nIf *other* is ``None``, return norm of this histogram.\nIf its norm was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the norm\nif it was computed before.\n\nIf a float *other* is provided, renormalize self to *other*.\n\nHistograms with scale equal to zero can't be renormalized.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other == None:\n\t\t\tif not self.computed:\n\t\t\t\tself.compute()\n\t\t\treturn self.norm\n\t\telif not self.computed:\n\t\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale\n\t\telse:\n\t\t\tother = float(other)\n\t\t\tif self.scale == 0 or self.scale is None:\n\t\t\t\traise LenaValueError(\"graph is not scaled\")\n\t\t\tnew = other / self.scale\n\t\t\tfor field in self.fields:\n\t\t\t\tif self.fields[field] is None:\n\t\t\t\t\tcontinue\n\t\t\t\tfor coord in self.fields[field]:\n\t\t\t\t\tself.fields[field][coord] *= new\n\tscale = property(scale, scale)\n\n\tdef __iter__(self):\n\t\treturn self.fields.__iter__()\n\n\tdef __len__(self):\n\t\treturn len(self.fields)\n\n\tdef __eq__(self, other):\n\t\treturn self.fields == other.fields\n\n\tdef __ne__(self, other):\n\t\treturn self.fields!= other.fields\n\n\tdef __setitem__(self, field, value):\n\t\tself.fields[field] = value\n\n\tdef __getitem__(self, field):\n\t\treturn self.fields[field]\n\n\tdef __delitem__(self, field):\n\t\tdel self.fields[field]\n\n\tdef __repr__(self):\n\t\treturn \"<graph of {} fields>\".format(len(self.fields))\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tself._scale = other\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif not isinstance(other, numbers.Number):\n\t\t\traise TypeError(\"scale must be a number\")\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"graph has zero scale\")\n\t\tif other == self._scale:\n\t\t\treturn\n\t\tself._scale *= other / self._scale\n\t\tfor error in self.errors:\n\t\t\terror.scale *= other / self._scale\n\t\tself._min *= other / self._scale\n\t\tself._max *= other / self._scale\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"scale is zero\")\n\t\tif other == 1:\n\t\t\treturn\n\t\tself._scale = other\n\t\tself._scale_errors()\n\n\n\tdef scale_errors(self):\n\t\"\"\"\n\tRescale the error of the last coordinate.\n\nThe error is rescaled by the scale of the last coordinate.\n\t\"\"\"\n\t\tself._scale_errors()\n\n\n\tdef _scale_errors(self):\n\t\tif self._scale is None:\n\t\t\treturn\n\t\tfor error in self._errors:\n\t\t\tfor i in range(len(error) - 1, -1, -1):\n\t\t\t\tif error[i] is not None:\n\t\t\t\t\terror[i] *= self._scale\n\t\t\t\t\tbreak\n\n\n\tdef get_bounds(self):\n\t\"\"\"\n\tGet the bounds of the graph.\n\nThe bounds are ``(None, None)`` if the graph has\nno coordinates.\nOtherwise they are ``(min, max)`` of the coordinates.\n\t\"\"\"\n\t\tif self._coordinates is None:\n\t\t\treturn (None, None)\n\t\tbounds = []\n\t\tfor coordinate in self._coordinates:\n\t\t\tbounds.append(coordinate.min)\n\t\t\tbounds.append(coordinate.max)\n\t\treturn (min", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif not isinstance(other, (int, float)):\n\t\t\traise TypeError('numeric value expected')\n\t\tif self._scale is None:\n\t\t\traise LenaValueError('graph has zero scale')\n\t\tif other == 0:\n\t\t\traise LenaValueError('graph has zero scale')\n\t\tfor field in self._fields:\n\t\t\tif getattr(self, field) is None:\n\t\t\t\tcontinue\n\t\t\tif field == 'x':\n\t\t\t\tgetattr(self, field).scale(self._scale, other)\n\t\t\telse:\n\t\t\t\tgetattr(self, field).scale(self._scale)\n\t\tself._scale = other\n\t\treturn self\n\n    @property\n    def offset(self):\n        \"\"\"\n        Get or set the offset of the graph.\n\nIf *other* is ``None``, return the offset of this graph.\n\nIf a numeric *other* is provided, reoffset to that value.\nIf the graph has unknown or zero offset,\nreoffsetting that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is reoffset.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be reoffset, and for a 3-dimensional graph\n*z* will be reoffset.\nAll", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\n\t\tother = float(other)\n\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"scale is zero\")\n\n\t\tif self._scale == other:\n\t\t\treturn\n\t\telif self._scale < other:\n\t\t\tself._scale *= other / self._scale\n\t\telse:\n\t\t\tself._scale /= other / self._scale\n\n\t\t# rescale all fields\n\t\tfor field in self.fields:\n\t\t\tfield.scale(other)\n\n\t\t# rescale all errors\n\t\tfor error in self.errors:\n\t\t\tfor coord in error.coords:\n\t\t\t\terror.scale_coord(coord, other)\n\n\t\t# rescale all coordinates\n\t\tfor coord in self.coords:\n\t\t\tcoord.scale(other)\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t self.scale = other or self.scale\n\tif other is not None:\n\tif self.scale is None:\n\traise LenaValueError('scale is not defined')\n\tself.scale = float(other)\n\tself.rescale_errors()\n\n\n def rescale_errors(self):\n \"\"\"\nRescale all errors in the graph.\n\nErrors are rescaled using the last coordinate.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\nThis function is called by :meth:`~.scale` and :meth:`~.rescale`.\n\t\"\"\"\n\tif self.scale is None:\n\treturn\n\tfor index in range(len(self)):\n\tif self.errors:\n\tself.errors[index] *= self.scale\n\telse:\n\tself.errors = [None for _ in range(len(self))]\n\n\n def rescale(self):\n \"\"\"\nRescale all fields in the graph.\n\nFields are rescaled using the last coordinate.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale\n\t\tif other <= 0:\n\t\t\traise LenaValueError(\"scale must be positive\")\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\"can't rescale unknown graph\")\n\t\tif self.scale == other:\n\t\t\treturn\n\t\tself.scale = other\n\t\tfor i in range(len(self.fields)):\n\t\t\tself.fields[i][:] *= other\n\t\tfor err in self.errors:\n\t\t\terr.scale(other)\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif (other is None):\n\t\t\treturn self._scale\n\t\tif (other is None):\n\t\t\traise TypeError(\"scale must be a number, not None\")\n\t\tif (other == 0):\n\t\t\traise LenaValueError(\"scale can't be zero\")\n\t\tif (other < 0):\n\t\t\traise LenaValueError(\"scale can't be negative\")\n\t\tif (self._scale is None):\n\t\t\traise LenaValueError(\"graph has no scale\")\n\t\tif (self._scale == other):\n\t\t\treturn self\n\t\tself._scale = other\n\t\tfor field in self._x, self._y, self._z:\n\t\t\tif (field is None):\n\t\t\t\tcontinue\n\t\t\tfor error in field:\n\t\t\t\terror.scale(self._scale)\n\t\treturn self\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"No scale.\")\n\t\tself._scale = other\n\tscale = property(fget=scale, fset=scale)\n\n\tdef __repr__(self):\n\t\treturn \"<{0} at 0x{1:x}: {2}>\".format(\n\t\t\ttype(self).__name__, id(self), self.repr_str())\n\n\tdef repr_str(self):\n\t\t\"\"\"\nReturn a string representation of the graph.\n\t\"\"\"\n\t\treturn \"name={0}, scale={1}, {2}\".format(\n\t\t\tself.name, self.scale, self._fields)\n\n\tdef add_fields(self, x=None, y=None, z=None, t=None):\n\t\t\"\"\"\nAdd *x*, *y*, *z* and *t* fields to the graph.\n\nIf *x* is ``None``, do nothing.\nIf *x* is not ``None``, append *x* to the list of fields.\n\nIf *y* is not ``None``, append *y* to the list of fields.\n\nIf *z* is not ``None``, append *z* to the list of fields.\n\nIf *t* is not ``None``, append *t* to the list of"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale\n\t\telse:\n\t\t\tother = float(other)\n\t\t\tif self.scale == 0 or self.scale is None:\n\t\t\t\traise LenaValueError(\"graph is not scaled\")\n\t\t\tnew = other / self.scale\n\t\t\tfor field in self.fields:\n\t\t\t\tif self.fields[field] is None:\n\t\t\t\t\tcontinue\n\t\t\t\tfor coord in self.fields[field]:\n\t\t\t\t\tself.fields[field][coord] *= new\n\tscale = property(scale, scale)\n\n\tdef __iter__(self):\n\t\treturn self.fields.__iter__()\n\n\tdef __len__(self):\n\t\treturn len(self.fields)\n\n\tdef __eq__(self, other):\n\t\treturn self.fields == other.fields\n\n\tdef __ne__(self, other):\n\t\treturn self.fields!= other.fields\n\n\tdef __setitem__(self, field, value):\n\t\tself.fields[field] = value\n\n\tdef __getitem__(self, field):\n\t\treturn self.fields[field]\n\n\tdef __delitem__(self, field):\n\t\tdel self.fields[field]\n\n\tdef __repr__(self):\n\t\treturn \"<graph of {} fields>\".format(len(self.fields))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tself._scale = other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif not isinstance(other, numbers.Number):\n\t\t\traise TypeError(\"scale must be a number\")\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"graph has zero scale\")\n\t\tif other == self._scale:\n\t\t\treturn\n\t\tself._scale *= other / self._scale\n\t\tfor error in self.errors:\n\t\t\terror.scale *= other / self._scale\n\t\tself._min *= other / self._scale\n\t\tself._max *= other / self._scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"scale is zero\")\n\t\tif other == 1:\n\t\t\treturn\n\t\tself._scale = other\n\t\tself._scale_errors()\n\n\n\tdef scale_errors(self):\n\t\"\"\"\n\tRescale the error of the last coordinate.\n\nThe error is rescaled by the scale of the last coordinate.\n\t\"\"\"\n\t\tself._scale_errors()\n\n\n\tdef _scale_errors(self):\n\t\tif self._scale is None:\n\t\t\treturn\n\t\tfor error in self._errors:\n\t\t\tfor i in range(len(error) - 1, -1, -1):\n\t\t\t\tif error[i] is not None:\n\t\t\t\t\terror[i] *= self._scale\n\t\t\t\t\tbreak\n\n\n\tdef get_bounds(self):\n\t\"\"\"\n\tGet the bounds of the graph.\n\nThe bounds are ``(None, None)`` if the graph has\nno coordinates.\nOtherwise they are ``(min, max)`` of the coordinates.\n\t\"\"\"\n\t\tif self._coordinates is None:\n\t\t\treturn (None, None)\n\t\tbounds = []\n\t\tfor coordinate in self._coordinates:\n\t\t\tbounds.append(coordinate.min)\n\t\t\tbounds.append(coordinate.max)\n\t\treturn (min", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif not isinstance(other, (int, float)):\n\t\t\traise TypeError('numeric value expected')\n\t\tif self._scale is None:\n\t\t\traise LenaValueError('graph has zero scale')\n\t\tif other == 0:\n\t\t\traise LenaValueError('graph has zero scale')\n\t\tfor field in self._fields:\n\t\t\tif getattr(self, field) is None:\n\t\t\t\tcontinue\n\t\t\tif field == 'x':\n\t\t\t\tgetattr(self, field).scale(self._scale, other)\n\t\t\telse:\n\t\t\t\tgetattr(self, field).scale(self._scale)\n\t\tself._scale = other\n\t\treturn self\n\n    @property\n    def offset(self):\n        \"\"\"\n        Get or set the offset of the graph.\n\nIf *other* is ``None``, return the offset of this graph.\n\nIf a numeric *other* is provided, reoffset to that value.\nIf the graph has unknown or zero offset,\nreoffsetting that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is reoffset.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be reoffset, and for a 3-dimensional graph\n*z* will be reoffset.\nAll", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\n\t\tother = float(other)\n\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"scale is zero\")\n\n\t\tif self._scale == other:\n\t\t\treturn\n\t\telif self._scale < other:\n\t\t\tself._scale *= other / self._scale\n\t\telse:\n\t\t\tself._scale /= other / self._scale\n\n\t\t# rescale all fields\n\t\tfor field in self.fields:\n\t\t\tfield.scale(other)\n\n\t\t# rescale all errors\n\t\tfor error in self.errors:\n\t\t\tfor coord in error.coords:\n\t\t\t\terror.scale_coord(coord, other)\n\n\t\t# rescale all coordinates\n\t\tfor coord in self.coords:\n\t\t\tcoord.scale(other)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t self.scale = other or self.scale\n\tif other is not None:\n\tif self.scale is None:\n\traise LenaValueError('scale is not defined')\n\tself.scale = float(other)\n\tself.rescale_errors()\n\n\n def rescale_errors(self):\n \"\"\"\nRescale all errors in the graph.\n\nErrors are rescaled using the last coordinate.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\nThis function is called by :meth:`~.scale` and :meth:`~.rescale`.\n\t\"\"\"\n\tif self.scale is None:\n\treturn\n\tfor index in range(len(self)):\n\tif self.errors:\n\tself.errors[index] *= self.scale\n\telse:\n\tself.errors = [None for _ in range(len(self))]\n\n\n def rescale(self):\n \"\"\"\nRescale all fields in the graph.\n\nFields are rescaled using the last coordinate.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self.scale\n\t\tif other <= 0:\n\t\t\traise LenaValueError(\"scale must be positive\")\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\"can't rescale unknown graph\")\n\t\tif self.scale == other:\n\t\t\treturn\n\t\tself.scale = other\n\t\tfor i in range(len(self.fields)):\n\t\t\tself.fields[i][:] *= other\n\t\tfor err in self.errors:\n\t\t\terr.scale(other)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif (other is None):\n\t\t\treturn self._scale\n\t\tif (other is None):\n\t\t\traise TypeError(\"scale must be a number, not None\")\n\t\tif (other == 0):\n\t\t\traise LenaValueError(\"scale can't be zero\")\n\t\tif (other < 0):\n\t\t\traise LenaValueError(\"scale can't be negative\")\n\t\tif (self._scale is None):\n\t\t\traise LenaValueError(\"graph has no scale\")\n\t\tif (self._scale == other):\n\t\t\treturn self\n\t\tself._scale = other\n\t\tfor field in self._x, self._y, self._z:\n\t\t\tif (field is None):\n\t\t\t\tcontinue\n\t\t\tfor error in field:\n\t\t\t\terror.scale(self._scale)\n\t\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"No scale.\")\n\t\tself._scale = other\n\tscale = property(fget=scale, fset=scale)\n\n\tdef __repr__(self):\n\t\treturn \"<{0} at 0x{1:x}: {2}>\".format(\n\t\t\ttype(self).__name__, id(self), self.repr_str())\n\n\tdef repr_str(self):\n\t\t\"\"\"\nReturn a string representation of the graph.\n\t\"\"\"\n\t\treturn \"name={0}, scale={1}, {2}\".format(\n\t\t\tself.name, self.scale, self._fields)\n\n\tdef add_fields(self, x=None, y=None, z=None, t=None):\n\t\t\"\"\"\nAdd *x*, *y*, *z* and *t* fields to the graph.\n\nIf *x* is ``None``, do nothing.\nIf *x* is not ``None``, append *x* to the list of fields.\n\nIf *y* is not ``None``, append *y* to the list of fields.\n\nIf *z* is not ``None``, append *z* to the list of fields.\n\nIf *t* is not ``None``, append *t* to the list of", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\telif scale is None:\n\t\t\tscale = Scale()\n\t\telif scale is False:\n\t\t\tscale = None\n\t\telse:\n\t\t\tscale = Scale(scale)\n\t\tif not hist.dimensions:\n\t\t\traise ValueError(\"Histogram has no dimensions\")\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.content\n\t\telse:\n\t\t\tmake_value = partial(make_value, hist)\n\t\tif get_coordinate == \"left\":\n\t\t\tget_coordinate = lambda bin_: bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tget_coordinate = lambda bin_: bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tget_coordinate = lambda bin_: bin_.middle\n\t\telse:\n\t\t\traise ValueError(\"Invalid get_coordinate value: %r\" % get_coordinate)\n\t\tresult = Graph()\n\t\tresult.scale = scale\n\t\tresult.point_dimension = len(field_names)\n\t\tresult.dimensions = list(field_names)\n\t\tpoint_number = 0\n\t\tfor bin_ in hist.bins:\n\t\t\tif isinstance(bin_, tuple):\n\t\t\t\tx = get_coordinate(bin_[0])\n\t\t\t\ty = make_value(bin_[1])\n\t\t\telse:\n\t\t\t\tx = get_coordinate(bin_)", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tfield_names = _check_field_names(field_names, hist.dim)\n\tif scale is True:\n\t\tscale = hist.scale\n\telif scale is None:\n\t\tscale = hist.scale\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"unknown *get_coordinate* %s\" % get_coordinate)\n\tpoints = []\n\tfor bin_ in hist.bins():\n\t\tvalue = make_value(bin_)\n\t\tif is_numeric(value) and not isinstance(value, tuple):\n\t\t\tvalue = (value,)\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinate = bin_.rleft\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinate = bin_.rright\n\t\telse:\n\t\t\tcoordinate = bin_.middle\n\t\tpoints.append((coordinate,) + value)\n\treturn Graph(tuple(field_names), points, scale)\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\t# TODO: get_coordinate = \"middle\"\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Invalid get_coordinate: {}\".format(get_coordinate))\n\n\t# Convert histogram to a graph\n\tgraph = graph.graph()\n\n\t# Add fields\n\tgraph.add_fields(field_names)\n\n\t# Add points\n\tfor bin_ in hist.bins:\n\t\t# Make a value\n\t\tvalue = make_value(bin_) if make_value else bin_\n\n\t\t# Get coordinate\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinate = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinate = bin_.right\n\t\telse:\n\t\t\tcoordinate = bin_.center\n\n\t\t# Add a point\n\t\tpoint = graph.point(coordinate)\n\t\tfor field_name, field_value in zip(field_names, value):\n\t\t\tpoint.set_field(field_name, field_value)\n\n\t# Set scale\n\tif scale is not None:\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\tgraph.set_scale(scale)\n\n\treturn graph\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif get_coordinate == \"left\":\n\t\t\tget_coordinate = lambda bin_: bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tget_coordinate = lambda bin_: bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\t\telse:\n\t\t\traise ValueError(\"Unknown coordinate %s\" % get_coordinate)\n\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.value\n\n\t\tmake_point = lambda bin_: (get_coordinate(bin_), make_value(bin_))\n\n\t\tif hist.is_weighted and not hist.is_reduced:\n\t\t\traise ValueError(\"Weighted histogram must be reduced\")\n\n\t\tif hist.is_reduced:\n\t\t\tpoints = [make_point(i) for i in hist.bins]\n\t\telse:\n\t\t\tpoints = [make_point(i) for i in hist.bins if i.value is not None]\n\n\t\tif scale is None:\n\t\t\tscale = hist.scale\n\n\t\treturn Graph(points, scale=scale, field_names=field_names)\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif not hist:\n\t\t\treturn None\n\t\tif scale is None:\n\t\t\tscale = hist.scale\n\t\telif scale is True:\n\t\t\tscale = hist.scale\n\t\tif scale is False:\n\t\t\tscale = None\n\t\tif not hist.is_numeric():\n\t\t\tif make_value is None:\n\t\t\t\tmake_value = lambda bin_: bin_\n\t\t\telse:\n\t\t\t\tmake_value = lambda bin_: make_value(bin_.value)\n\t\tif not hist.is_numeric():\n\t\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\t\traise ValueError(\"get_coordinate must be 'left', 'right' or'middle'\")\n\t\t\tif make_value is None:\n\t\t\t\tdef make_value(bin_):\n\t\t\t\t\tif get_coordinate == \"left\":\n\t\t\t\t\t\treturn bin_.value[0]\n\t\t\t\t\telif get_coordinate == \"right\":\n\t\t\t\t\t\treturn bin_.value[1]\n\t\t\t\t\telif get_coordinate == \"middle\":\n\t\t\t\t\t\treturn bin_.value[0] + 0.5 * (bin_.value[1] - bin_.value[0])\n\t\t\telse:\n\t\t\t\tdef make_value(bin_):\n\t\t\t\t\treturn make_value(bin_.value)\n\t\tif not hist.is_numeric():\n\t\t\traise ValueError(\"hist must contain only numeric bins or make_value must remove context when creating a numeric", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\t# TODO: make_value should be able to create a graph with errors\n\tif scale is True:\n\t\tscale = hist.scale\n\treturn Graph(hist.bin_centers, hist.bin_contents,\n\t\tget_coordinate=get_coordinate, field_names=field_names, scale=scale)\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\tif scale is None:\n\t\t\tscale = Scale(hist.dimension)\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\"Unknown get_coordinate {0!r}\".format(get_coordinate))\n\t\tif not scale.is_compatible(hist.scale):\n\t\t\traise IncompatibleScales(scale, hist.scale)\n\t\tif not hist.scale.is_compatible(scale):\n\t\t\traise IncompatibleScales(hist.scale, scale)\n\t\tif len(field_names)!= hist.dimension:\n\t\t\traise ValueError(\"len(field_names)!= hist.dimension\")\n\t\tif hist.dimension!= 2:\n\t\t\traise ValueError(\"hist.dimension!= 2\")\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_\n\t\tif hist.dimension == 1:\n\t\t\treturn Graph(\t(\tGraphPoint(\tscale.get_coordinate(get_coordinate, bin.left), make_value(bin.left), bin.left.context)\n\t\t\t\t\t\t\t\tfor bin in hist.iter_bins()\n\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tfield_names=field_names,\n\t\t\t\t\t\t\tscale=scale\n\t\t\t\t\t\t)\n\t\telif hist.dimension == 2:\n\t\t\treturn Graph(\t(\tGraphPoint(\tscale", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\t#\n\t\tresult = graph(scale=scale)\n\t\t#\n\t\tif not make_value:\n\t\t\tmake_value = lambda bin_: bin_\n\t\t#\n\t\tif get_coordinate == \"left\":\n\t\t\tget_coordinate = lambda bin_: bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tget_coordinate = lambda bin_: bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\t\telse:\n\t\t\traise ValueError(f\"Unknown get_coordinate: {get_coordinate}\")\n\t\t#\n\t\tfor i, value in enumerate(hist.bins):\n\t\t\tresult.add_point(get_coordinate(value), make_value(value), field_names[i])\n\t\t#\n\t\treturn result\n\n#", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\"invalid value for 'get_coordinate'\")\n\n\t\tif scale is None:\n\t\t\tscale = hist.scale\n\t\telif scale is True:\n\t\t\tscale = hist.scale\n\n\t\tfield_names = tuple(field_names)\n\t\tif len(field_names)!= hist.dimension:\n\t\t\traise ValueError(\"invalid number of field names\")\n\n\t\tdef make_point(bin_):\n\t\t\t# bin_ is a bin with context\n\t\t\t# point will be a graph point without context\n\t\t\tpoint = bin_.to_tuple()\n\t\t\tif make_value is None:\n\t\t\t\tvalue = bin_.content\n\t\t\t\tpoint = point + (value,)\n\t\t\telse:\n\t\t\t\tpoint = point + (make_value(bin_.content),)\n\t\t\treturn point\n\n\t\tif get_coordinate == \"left\":\n\t\t\treturn graph.from_sequence(\n\t\t\t\tscale, hist.bins.map(make_point),\n\t\t\t\tfield_names=field_names)\n\t\telif get_coordinate == \"right\":\n\t\t\treturn graph.from_sequence(\n\t\t\t\tscale, hist.bins.map(\n\t\t\t\t\tlambda bin_: (bin_.right_value,) + make_point(bin_)),\n\t\t\t\tfield_names=field_names)\n\t\telif get_coordinate == \"middle\":\n\t\t\treturn graph", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif not isinstance(hist, histogram):\n\t\t\traise TypeError(\"hist must be a histogram\")\n\t\tif not isinstance(get_coordinate, str):\n\t\t\traise TypeError(\"get_coordinate must be a string\")\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\n\t\t\t\t\"get_coordinate must be one of 'left', 'right' or'middle'\")\n\t\tif not isinstance(field_names, (tuple, list)):\n\t\t\traise TypeError(\"field_names must be a tuple or list\")\n\t\tif len(field_names)!= hist.dimension():\n\t\t\traise ValueError(\"wrong number of field names\")\n\t\tif scale is not None:\n\t\t\tif not isinstance(scale, bin_scale):\n\t\t\t\traise TypeError(\"scale must be a bin scale\")\n\t\tresult = graph(scale=scale or hist.scale())\n\t\tresult.set_field_names(field_names)\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.value\n\t\tfor bin_ in hist.bins():\n\t\t\tif not isinstance(bin_, number_bin):\n\t\t\t\traise TypeError(\"hist must contain only numeric bins\")\n\t\t\tif get_coordinate == \"left\":\n\t\t\t\tresult.append_point(\n\t\t\t\t\tmake_value(bin_),\n\t\t\t\t\t(bin_.left(),) * result.dimension())\n\t\t\telif get"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\telif scale is None:\n\t\t\tscale = Scale()\n\t\telif scale is False:\n\t\t\tscale = None\n\t\telse:\n\t\t\tscale = Scale(scale)\n\t\tif not hist.dimensions:\n\t\t\traise ValueError(\"Histogram has no dimensions\")\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.content\n\t\telse:\n\t\t\tmake_value = partial(make_value, hist)\n\t\tif get_coordinate == \"left\":\n\t\t\tget_coordinate = lambda bin_: bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tget_coordinate = lambda bin_: bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tget_coordinate = lambda bin_: bin_.middle\n\t\telse:\n\t\t\traise ValueError(\"Invalid get_coordinate value: %r\" % get_coordinate)\n\t\tresult = Graph()\n\t\tresult.scale = scale\n\t\tresult.point_dimension = len(field_names)\n\t\tresult.dimensions = list(field_names)\n\t\tpoint_number = 0\n\t\tfor bin_ in hist.bins:\n\t\t\tif isinstance(bin_, tuple):\n\t\t\t\tx = get_coordinate(bin_[0])\n\t\t\t\ty = make_value(bin_[1])\n\t\t\telse:\n\t\t\t\tx = get_coordinate(bin_)", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tfield_names = _check_field_names(field_names, hist.dim)\n\tif scale is True:\n\t\tscale = hist.scale\n\telif scale is None:\n\t\tscale = hist.scale\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"unknown *get_coordinate* %s\" % get_coordinate)\n\tpoints = []\n\tfor bin_ in hist.bins():\n\t\tvalue = make_value(bin_)\n\t\tif is_numeric(value) and not isinstance(value, tuple):\n\t\t\tvalue = (value,)\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinate = bin_.rleft\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinate = bin_.rright\n\t\telse:\n\t\t\tcoordinate = bin_.middle\n\t\tpoints.append((coordinate,) + value)\n\treturn Graph(tuple(field_names), points, scale)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\t# TODO: get_coordinate = \"middle\"\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Invalid get_coordinate: {}\".format(get_coordinate))\n\n\t# Convert histogram to a graph\n\tgraph = graph.graph()\n\n\t# Add fields\n\tgraph.add_fields(field_names)\n\n\t# Add points\n\tfor bin_ in hist.bins:\n\t\t# Make a value\n\t\tvalue = make_value(bin_) if make_value else bin_\n\n\t\t# Get coordinate\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinate = bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinate = bin_.right\n\t\telse:\n\t\t\tcoordinate = bin_.center\n\n\t\t# Add a point\n\t\tpoint = graph.point(coordinate)\n\t\tfor field_name, field_value in zip(field_names, value):\n\t\t\tpoint.set_field(field_name, field_value)\n\n\t# Set scale\n\tif scale is not None:\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\tgraph.set_scale(scale)\n\n\treturn graph\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif get_coordinate == \"left\":\n\t\t\tget_coordinate = lambda bin_: bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tget_coordinate = lambda bin_: bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\t\telse:\n\t\t\traise ValueError(\"Unknown coordinate %s\" % get_coordinate)\n\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.value\n\n\t\tmake_point = lambda bin_: (get_coordinate(bin_), make_value(bin_))\n\n\t\tif hist.is_weighted and not hist.is_reduced:\n\t\t\traise ValueError(\"Weighted histogram must be reduced\")\n\n\t\tif hist.is_reduced:\n\t\t\tpoints = [make_point(i) for i in hist.bins]\n\t\telse:\n\t\t\tpoints = [make_point(i) for i in hist.bins if i.value is not None]\n\n\t\tif scale is None:\n\t\t\tscale = hist.scale\n\n\t\treturn Graph(points, scale=scale, field_names=field_names)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif not hist:\n\t\t\treturn None\n\t\tif scale is None:\n\t\t\tscale = hist.scale\n\t\telif scale is True:\n\t\t\tscale = hist.scale\n\t\tif scale is False:\n\t\t\tscale = None\n\t\tif not hist.is_numeric():\n\t\t\tif make_value is None:\n\t\t\t\tmake_value = lambda bin_: bin_\n\t\t\telse:\n\t\t\t\tmake_value = lambda bin_: make_value(bin_.value)\n\t\tif not hist.is_numeric():\n\t\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\t\traise ValueError(\"get_coordinate must be 'left', 'right' or'middle'\")\n\t\t\tif make_value is None:\n\t\t\t\tdef make_value(bin_):\n\t\t\t\t\tif get_coordinate == \"left\":\n\t\t\t\t\t\treturn bin_.value[0]\n\t\t\t\t\telif get_coordinate == \"right\":\n\t\t\t\t\t\treturn bin_.value[1]\n\t\t\t\t\telif get_coordinate == \"middle\":\n\t\t\t\t\t\treturn bin_.value[0] + 0.5 * (bin_.value[1] - bin_.value[0])\n\t\t\telse:\n\t\t\t\tdef make_value(bin_):\n\t\t\t\t\treturn make_value(bin_.value)\n\t\tif not hist.is_numeric():\n\t\t\traise ValueError(\"hist must contain only numeric bins or make_value must remove context when creating a numeric", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\t# TODO: make_value should be able to create a graph with errors\n\tif scale is True:\n\t\tscale = hist.scale\n\treturn Graph(hist.bin_centers, hist.bin_contents,\n\t\tget_coordinate=get_coordinate, field_names=field_names, scale=scale)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\tif scale is None:\n\t\t\tscale = Scale(hist.dimension)\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\"Unknown get_coordinate {0!r}\".format(get_coordinate))\n\t\tif not scale.is_compatible(hist.scale):\n\t\t\traise IncompatibleScales(scale, hist.scale)\n\t\tif not hist.scale.is_compatible(scale):\n\t\t\traise IncompatibleScales(hist.scale, scale)\n\t\tif len(field_names)!= hist.dimension:\n\t\t\traise ValueError(\"len(field_names)!= hist.dimension\")\n\t\tif hist.dimension!= 2:\n\t\t\traise ValueError(\"hist.dimension!= 2\")\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_\n\t\tif hist.dimension == 1:\n\t\t\treturn Graph(\t(\tGraphPoint(\tscale.get_coordinate(get_coordinate, bin.left), make_value(bin.left), bin.left.context)\n\t\t\t\t\t\t\t\tfor bin in hist.iter_bins()\n\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tfield_names=field_names,\n\t\t\t\t\t\t\tscale=scale\n\t\t\t\t\t\t)\n\t\telif hist.dimension == 2:\n\t\t\treturn Graph(\t(\tGraphPoint(\tscale", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\t#\n\t\tresult = graph(scale=scale)\n\t\t#\n\t\tif not make_value:\n\t\t\tmake_value = lambda bin_: bin_\n\t\t#\n\t\tif get_coordinate == \"left\":\n\t\t\tget_coordinate = lambda bin_: bin_.left\n\t\telif get_coordinate == \"right\":\n\t\t\tget_coordinate = lambda bin_: bin_.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\t\telse:\n\t\t\traise ValueError(f\"Unknown get_coordinate: {get_coordinate}\")\n\t\t#\n\t\tfor i, value in enumerate(hist.bins):\n\t\t\tresult.add_point(get_coordinate(value), make_value(value), field_names[i])\n\t\t#\n\t\treturn result\n\n#", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\"invalid value for 'get_coordinate'\")\n\n\t\tif scale is None:\n\t\t\tscale = hist.scale\n\t\telif scale is True:\n\t\t\tscale = hist.scale\n\n\t\tfield_names = tuple(field_names)\n\t\tif len(field_names)!= hist.dimension:\n\t\t\traise ValueError(\"invalid number of field names\")\n\n\t\tdef make_point(bin_):\n\t\t\t# bin_ is a bin with context\n\t\t\t# point will be a graph point without context\n\t\t\tpoint = bin_.to_tuple()\n\t\t\tif make_value is None:\n\t\t\t\tvalue = bin_.content\n\t\t\t\tpoint = point + (value,)\n\t\t\telse:\n\t\t\t\tpoint = point + (make_value(bin_.content),)\n\t\t\treturn point\n\n\t\tif get_coordinate == \"left\":\n\t\t\treturn graph.from_sequence(\n\t\t\t\tscale, hist.bins.map(make_point),\n\t\t\t\tfield_names=field_names)\n\t\telif get_coordinate == \"right\":\n\t\t\treturn graph.from_sequence(\n\t\t\t\tscale, hist.bins.map(\n\t\t\t\t\tlambda bin_: (bin_.right_value,) + make_point(bin_)),\n\t\t\t\tfield_names=field_names)\n\t\telif get_coordinate == \"middle\":\n\t\t\treturn graph", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\t\tif not isinstance(hist, histogram):\n\t\t\traise TypeError(\"hist must be a histogram\")\n\t\tif not isinstance(get_coordinate, str):\n\t\t\traise TypeError(\"get_coordinate must be a string\")\n\t\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError(\n\t\t\t\t\"get_coordinate must be one of 'left', 'right' or'middle'\")\n\t\tif not isinstance(field_names, (tuple, list)):\n\t\t\traise TypeError(\"field_names must be a tuple or list\")\n\t\tif len(field_names)!= hist.dimension():\n\t\t\traise ValueError(\"wrong number of field names\")\n\t\tif scale is not None:\n\t\t\tif not isinstance(scale, bin_scale):\n\t\t\t\traise TypeError(\"scale must be a bin scale\")\n\t\tresult = graph(scale=scale or hist.scale())\n\t\tresult.set_field_names(field_names)\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: bin_.value\n\t\tfor bin_ in hist.bins():\n\t\t\tif not isinstance(bin_, number_bin):\n\t\t\t\traise TypeError(\"hist must contain only numeric bins\")\n\t\t\tif get_coordinate == \"left\":\n\t\t\t\tresult.append_point(\n\t\t\t\t\tmake_value(bin_),\n\t\t\t\t\t(bin_.left(),) * result.dimension())\n\t\t\telif get", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Make sure the candidate claims to provide the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Interface.Invalid(\n\t\t\t\t'Provided object does not support %s.' % iface)\n\n\t# Make sure the candidate has all the methods\n\tmissing = []\n\tfor name, spec in iface.namesAndDescriptions(all=True):\n\t\tif name in candidate.__dict__:\n\t\t\tcontinue\n\t\tif name in ('__cmp__', '__hash__'):\n\t\t\tcontinue\n\t\tmissing.append(name)\n\n\tif missing:\n\t\traise Interface.Invalid(\n\t\t\t'Provided object does not provide all methods required by %s.'\n\t\t\t% iface)\n\n\t# Make sure the candidate has all the attributes\n\tmissing = []\n\tfor name in iface.namesAndDescriptions(all=False):\n\t\tif name in candidate.__dict__:\n\t\t\tcontinue\n\t\tmissing.append(name)\n\n\tif missing:\n\t\traise Interface.Invalid(\n\t\t\t'Provided object does not provide all attributes required by %s.'\n\t\t\t% iface)\n\n\t# Make sure the candidate has the correct signatures\n\terrors = []\n\tfor name, spec in iface.namesAndDescriptions(all=True):\n\t\tsig = inspect.signature(getattr(candidate, name))\n\t\tmin_, max_ = spec.minargs, spec.maxargs\n\t\t", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# check if the candidate 'provides' the interface using\n\t\t# providedBy. This is done only if the candidate is\n\t\t# not tentative\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t'The candidate does not provide the required interface'\n\t\t\t)\n\n\t\t# check if the candidate defines all the necessary methods\n\t\t# and attributes.\n\t\t# Note that the interface's methods and attributes are\n\t\t# obtained from the class, not from the interface.\n\t\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\t\tif name in candidate.__dict__:\n\t\t\t\tcontinue\n\t\t\tif not method.get('inherited', False):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate does not define the required '\n\t\t\t\t\t'attribute or method %s' % name\n\t\t\t\t)\n\n\t\t# check if the candidate defines all the necessary methods\n\t\t# and attributes.\n\t\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\t\tif name in candidate.__dict__:\n\t\t\t\tcontinue\n\t\t\tif not method.get('inherited', False):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate does not define the required '\n\t\t\t\t\t'attribute or method %s' % name\n\t\t\t\t)\n\n\t\t# check if the candidate defines all the necessary methods\n\t\t# and attributes.\n\t\tfor name, method in iface.", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tassert iface is not None\n\t\tif vtype is None:\n\t\t\tvtype = _default_verify_type\n\t\tif vtype == VerifyType.only_if_implemented:\n\t\t\tverify_type = VerifyType.implemented\n\t\telse:\n\t\t\tverify_type = vtype\n\n\t\tif verify_type == VerifyType.implemented:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"Interface %s not implemented by %s\"\n\t\t\t\t\t% (iface, candidate)\n\t\t\t\t)\n\n\t\t# Verify methods\n\t\t# --------------\n\n\t\tif verify_type == VerifyType.implemented:\n\t\t\t# If the candidate implements the interface, we want to\n\t\t\t# verify that it defines all the methods in the interface\n\t\t\t# (or their default implementations).\n\t\t\t#\n\t\t\t# The only exception here is that if it is a class, we\n\t\t\t# don't require it to provide a ``__init__`` method.\n\t\t\tif inspect.isclass(candidate):\n\t\t\t\tdef _init_method_defined(iface):\n\t\t\t\t\treturn '__init__' not in iface.names()\n\n\t\t\t\tverify_func = _init_method_defined\n\t\t\telse:\n\t\t\t\tverify_func = lambda iface: True\n\t\t\tfail = 0\n\t\t\tfor name in iface.names():\n\t\t\t\tif not verify_func(iface) or name == \"__name__\":\n\t\t\t\t\tcontinue", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Use the interface's signature and the candidate's signature to\n\t\t# determine the type of each attribute and method.\n\tif vtype is None:\n\t\tvtype = type(iface)\n\tvtype = vtype.__name__\n\ttry:\n\t\tcandidate_sig = inspect.getcallargs(candidate)\n\texcept TypeError:\n\t\traise Interface.Invalid(\n\t\t\t\"Invalid type for %s: %s\"\n\t\t\t% (vtype, type(candidate).__name__)\n\t\t)\n\n\t# Now we need to check that all attributes and methods are\n\t# present.\n\tfor name, sig in iface.namesAndSignatures():\n\t\tif name not in candidate_sig:\n\t\t\traise Interface.Invalid(\n\t\t\t\t\"%s missing attribute %s.%s\" % (vtype, name, sig)\n\t\t\t)\n\t\tif name in candidate_sig:\n\t\t\t# We need to check if the signature is correct.\n\t\t\t# If the candidate provides the interface, the\n\t\t\t# signature is correct if it matches the\n\t\t\t# interface's signature.\n\t\t\tif tentative:\n\t\t\t\tcontinue\n\t\t\tif sig!= candidate_sig[name]:\n\t\t\t\traise Interface.Invalid(\n\t\t\t\t\t\"%s.%s: signature mismatch: expected %s, got %s\"\n\t\t\t\t\t% (vtype, name, sig, candidate_sig[name])\n\t\t\t\t)\n\n\treturn", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tif not isInterface(iface):\n\t\t\traise Invalid(\n\t\t\t\t\"verify is only for interfaces, not %s\" % iface)\n\t\tif not hasattr(iface, \"implementedBy\"):\n\t\t\traise Invalid(\n\t\t\t\t\"verify is only for classes that implement zope.interface.Interface, not %s\"\n\t\t\t\t% iface)\n\t\tif vtype is None:\n\t\t\tvtype = _verify_type\n\t\telif vtype not in _verify_types:\n\t\t\traise ValueError(\"invalid verify_type %r\" % vtype)\n\t\tif vtype == _verify_type.TENTATIVE:\n\t\t\tif not tentative:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"verify_type is TENTATIVE, but tentative is False\")\n\t\t# Make sure that the candidate implements the interface\n\t\tif vtype in (_verify_type.TENTATIVE, _verify_type.ALL):\n\t\t\ttry:\n\t\t\t\tiface.implementedBy(candidate)\n\t\t\texcept TypeError:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"verify_type is TENTATIVE or ALL, but %r does not implement %r\"\n\t\t\t\t\t% (candidate, iface))\n\t\t# Make sure that the candidate provides the interface\n\t\tif vtype in (_verify_type.TENTATIVE, _verify_type.ALL):\n\t\t\ttry:\n\t\t\t\tiface.providedBy(candidate)\n\t\t\texcept TypeError:\n\t\t\t\t", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Determine the type of the candidate\n\tif vtype is None:\n\t\ttry:\n\t\t\tvtype = type(candidate)\n\t\texcept TypeError:\n\t\t\t# The candidate is a class (but not an interface)\n\t\t\t# We don't support this\n\t\t\traise TypeError(\n\t\t\t\t\"Cannot verify a class for an interface\"\n\t\t\t)\n\n\t\t# The candidate is a class, not an interface\n\t\t# We don't support this\n\t\tif not isclass(vtype):\n\t\t\traise TypeError(\n\t\t\t\t\"Cannot verify a class for an interface\"\n\t\t\t)\n\n\t# Make sure the candidate provides the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%s does not provide %s\"\n\t\t\t\t% (candidate, iface)\n\t\t\t)\n\n\t# Make sure the candidate defines all the methods\n\terrors = []\n\tfor name, method in iface.namesAndDescriptions():\n\t\t# Check that the method exists\n\t\tif name not in vtype.__dict__:\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Method %s is required but not defined in %s\"\n\t\t\t\t\t% (name, vtype)\n\t\t\t\t)\n\t\t\t)\n\t\telse:\n\t\t\t# Make sure the method has the right signature\n\t\t\ttry:\n\t\t\t\tverifyObject(iface[name], method)\n\t\t\texcept Invalid", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# first, make sure the candidate provides the interface\n\t\t# using the providedBy() mechanism\n\t\t#\n\t\t# Note that the tentative=True flag is used to make sure\n\t\t# that the test works even if the class does not\n\t\t# properly mark itself as providing the interface.\n\t\t#\n\t\t# We need to do this test even if the candidate\n\t\t# is marked as providing the interface, because\n\t\t# the class may have used a different name\n\t\t# for the interface.\n\t\t#\n\t\t# See zope.interface.tests.test_verify.test_verify_provides\n\t\t# for an example of why this is necessary.\n\t\t#\n\t\t# Note that if the interface is not a class-level\n\t\t# interface, then the above test will fail,\n\t\t# but that's ok - we can't verify that it\n\t\t# provides the interface in that case.\n\t\t#\n\t\tif not isinstance(iface, InterfaceClass):\n\t\t\treturn True\n\n\t\tif not iface.providedBy(candidate, tentative=tentative):\n\t\t\traise Invalid(\"%s does not provide %s\" % (candidate, iface))\n\n\t\t# now, make sure the candidate defines all the methods\n\t\t#\n\t\terrors = []\n\t\tfor name, method in iface.namesAndDescriptions(1):\n\t\t\ttry:\n\t\t\t\tmethod.get(candidate)\n\t\t\t\t", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# The type of the interface\n\tif vtype is None:\n\t\tvtype = interface.Interface\n\t# Check that the candidate implements the interface\n\tif not isinstance(candidate, vtype):\n\t\traise interface.Invalid(\n\t\t\t\"The given candidate does not implement the interface\")\n\t# Check that the candidate provides at least the methods that are\n\t# implemented in the interface\n\tfor method in iface.names():\n\t\tif (method in candidate.__dict__ and\n\t\t\t not callable(candidate.__dict__[method])):\n\t\t\traise interface.Invalid(\n\t\t\t\t\"The given candidate does not implement the method %s\" %\n\t\t\t\tmethod)\n\t# Check that the candidate provides at least the attributes that are\n\t# implemented in the interface\n\tfor attr in iface.names(all=True):\n\t\tif attr not in candidate.__dict__:\n\t\t\traise interface.Invalid(\n\t\t\t\t\"The given candidate does not implement the attribute %s\" %\n\t\t\t\tattr)\n\treturn True\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tif not isinstance(candidate, _abc.InterfaceClass):\n\t\t\traise TypeError(\n\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\tf\"that are instances of InterfaceClass objects, \"\n\t\t\t\tf\"but {candidate} is not an InterfaceClass object.\"\n\t\t\t)\n\t\tif vtype is not None:\n\t\t\tif vtype is not Interface:\n\t\t\t\traise TypeError(\n\t\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\t\tf\"that are instances of InterfaceClass objects, \"\n\t\t\t\t\tf\"but {candidate} is an {vtype.__name__} object.\"\n\t\t\t\t)\n\t\tif not candidate.providedBy(iface):\n\t\t\traise Invalid(\n\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\tf\"that declare that they provide it, but {candidate} \"\n\t\t\t\tf\"does not declare that it provides {iface}.\"\n\t\t\t)\n\t\tif not tentative:\n\t\t\tif not iface.isOrExtends(candidate):\n\t\t\t\traise Invalid(\n\t\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\t\tf\"that are subclasses of {candidate}, \"\n\t\t\t\t\tf\"but {candidate} is not a subclass of {iface}.\"\n\t\t\t\t)\n\t\tif not candidate.names():\n\t\t\traise Invalid(\n\t\t\t\tf\"Interface {iface} is implemented by objects that \"\n\t\t\t\tf\"", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tif tentative:\n\t\t\treturn True\n\t\tif not iface.providedBy(candidate):\n\t\t\traise InvalidImplementation(candidate, iface)\n\t\tif vtype is not None:\n\t\t\tverifyObject(candidate, vtype)\n\t\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Make sure the candidate claims to provide the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Interface.Invalid(\n\t\t\t\t'Provided object does not support %s.' % iface)\n\n\t# Make sure the candidate has all the methods\n\tmissing = []\n\tfor name, spec in iface.namesAndDescriptions(all=True):\n\t\tif name in candidate.__dict__:\n\t\t\tcontinue\n\t\tif name in ('__cmp__', '__hash__'):\n\t\t\tcontinue\n\t\tmissing.append(name)\n\n\tif missing:\n\t\traise Interface.Invalid(\n\t\t\t'Provided object does not provide all methods required by %s.'\n\t\t\t% iface)\n\n\t# Make sure the candidate has all the attributes\n\tmissing = []\n\tfor name in iface.namesAndDescriptions(all=False):\n\t\tif name in candidate.__dict__:\n\t\t\tcontinue\n\t\tmissing.append(name)\n\n\tif missing:\n\t\traise Interface.Invalid(\n\t\t\t'Provided object does not provide all attributes required by %s.'\n\t\t\t% iface)\n\n\t# Make sure the candidate has the correct signatures\n\terrors = []\n\tfor name, spec in iface.namesAndDescriptions(all=True):\n\t\tsig = inspect.signature(getattr(candidate, name))\n\t\tmin_, max_ = spec.minargs, spec.maxargs\n\t\t", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# check if the candidate 'provides' the interface using\n\t\t# providedBy. This is done only if the candidate is\n\t\t# not tentative\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t'The candidate does not provide the required interface'\n\t\t\t)\n\n\t\t# check if the candidate defines all the necessary methods\n\t\t# and attributes.\n\t\t# Note that the interface's methods and attributes are\n\t\t# obtained from the class, not from the interface.\n\t\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\t\tif name in candidate.__dict__:\n\t\t\t\tcontinue\n\t\t\tif not method.get('inherited', False):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate does not define the required '\n\t\t\t\t\t'attribute or method %s' % name\n\t\t\t\t)\n\n\t\t# check if the candidate defines all the necessary methods\n\t\t# and attributes.\n\t\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\t\tif name in candidate.__dict__:\n\t\t\t\tcontinue\n\t\t\tif not method.get('inherited', False):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate does not define the required '\n\t\t\t\t\t'attribute or method %s' % name\n\t\t\t\t)\n\n\t\t# check if the candidate defines all the necessary methods\n\t\t# and attributes.\n\t\tfor name, method in iface.", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tassert iface is not None\n\t\tif vtype is None:\n\t\t\tvtype = _default_verify_type\n\t\tif vtype == VerifyType.only_if_implemented:\n\t\t\tverify_type = VerifyType.implemented\n\t\telse:\n\t\t\tverify_type = vtype\n\n\t\tif verify_type == VerifyType.implemented:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"Interface %s not implemented by %s\"\n\t\t\t\t\t% (iface, candidate)\n\t\t\t\t)\n\n\t\t# Verify methods\n\t\t# --------------\n\n\t\tif verify_type == VerifyType.implemented:\n\t\t\t# If the candidate implements the interface, we want to\n\t\t\t# verify that it defines all the methods in the interface\n\t\t\t# (or their default implementations).\n\t\t\t#\n\t\t\t# The only exception here is that if it is a class, we\n\t\t\t# don't require it to provide a ``__init__`` method.\n\t\t\tif inspect.isclass(candidate):\n\t\t\t\tdef _init_method_defined(iface):\n\t\t\t\t\treturn '__init__' not in iface.names()\n\n\t\t\t\tverify_func = _init_method_defined\n\t\t\telse:\n\t\t\t\tverify_func = lambda iface: True\n\t\t\tfail = 0\n\t\t\tfor name in iface.names():\n\t\t\t\tif not verify_func(iface) or name == \"__name__\":\n\t\t\t\t\tcontinue", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Use the interface's signature and the candidate's signature to\n\t\t# determine the type of each attribute and method.\n\tif vtype is None:\n\t\tvtype = type(iface)\n\tvtype = vtype.__name__\n\ttry:\n\t\tcandidate_sig = inspect.getcallargs(candidate)\n\texcept TypeError:\n\t\traise Interface.Invalid(\n\t\t\t\"Invalid type for %s: %s\"\n\t\t\t% (vtype, type(candidate).__name__)\n\t\t)\n\n\t# Now we need to check that all attributes and methods are\n\t# present.\n\tfor name, sig in iface.namesAndSignatures():\n\t\tif name not in candidate_sig:\n\t\t\traise Interface.Invalid(\n\t\t\t\t\"%s missing attribute %s.%s\" % (vtype, name, sig)\n\t\t\t)\n\t\tif name in candidate_sig:\n\t\t\t# We need to check if the signature is correct.\n\t\t\t# If the candidate provides the interface, the\n\t\t\t# signature is correct if it matches the\n\t\t\t# interface's signature.\n\t\t\tif tentative:\n\t\t\t\tcontinue\n\t\t\tif sig!= candidate_sig[name]:\n\t\t\t\traise Interface.Invalid(\n\t\t\t\t\t\"%s.%s: signature mismatch: expected %s, got %s\"\n\t\t\t\t\t% (vtype, name, sig, candidate_sig[name])\n\t\t\t\t)\n\n\treturn", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tif not isInterface(iface):\n\t\t\traise Invalid(\n\t\t\t\t\"verify is only for interfaces, not %s\" % iface)\n\t\tif not hasattr(iface, \"implementedBy\"):\n\t\t\traise Invalid(\n\t\t\t\t\"verify is only for classes that implement zope.interface.Interface, not %s\"\n\t\t\t\t% iface)\n\t\tif vtype is None:\n\t\t\tvtype = _verify_type\n\t\telif vtype not in _verify_types:\n\t\t\traise ValueError(\"invalid verify_type %r\" % vtype)\n\t\tif vtype == _verify_type.TENTATIVE:\n\t\t\tif not tentative:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"verify_type is TENTATIVE, but tentative is False\")\n\t\t# Make sure that the candidate implements the interface\n\t\tif vtype in (_verify_type.TENTATIVE, _verify_type.ALL):\n\t\t\ttry:\n\t\t\t\tiface.implementedBy(candidate)\n\t\t\texcept TypeError:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"verify_type is TENTATIVE or ALL, but %r does not implement %r\"\n\t\t\t\t\t% (candidate, iface))\n\t\t# Make sure that the candidate provides the interface\n\t\tif vtype in (_verify_type.TENTATIVE, _verify_type.ALL):\n\t\t\ttry:\n\t\t\t\tiface.providedBy(candidate)\n\t\t\texcept TypeError:\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Determine the type of the candidate\n\tif vtype is None:\n\t\ttry:\n\t\t\tvtype = type(candidate)\n\t\texcept TypeError:\n\t\t\t# The candidate is a class (but not an interface)\n\t\t\t# We don't support this\n\t\t\traise TypeError(\n\t\t\t\t\"Cannot verify a class for an interface\"\n\t\t\t)\n\n\t\t# The candidate is a class, not an interface\n\t\t# We don't support this\n\t\tif not isclass(vtype):\n\t\t\traise TypeError(\n\t\t\t\t\"Cannot verify a class for an interface\"\n\t\t\t)\n\n\t# Make sure the candidate provides the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%s does not provide %s\"\n\t\t\t\t% (candidate, iface)\n\t\t\t)\n\n\t# Make sure the candidate defines all the methods\n\terrors = []\n\tfor name, method in iface.namesAndDescriptions():\n\t\t# Check that the method exists\n\t\tif name not in vtype.__dict__:\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Method %s is required but not defined in %s\"\n\t\t\t\t\t% (name, vtype)\n\t\t\t\t)\n\t\t\t)\n\t\telse:\n\t\t\t# Make sure the method has the right signature\n\t\t\ttry:\n\t\t\t\tverifyObject(iface[name], method)\n\t\t\texcept Invalid", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# first, make sure the candidate provides the interface\n\t\t# using the providedBy() mechanism\n\t\t#\n\t\t# Note that the tentative=True flag is used to make sure\n\t\t# that the test works even if the class does not\n\t\t# properly mark itself as providing the interface.\n\t\t#\n\t\t# We need to do this test even if the candidate\n\t\t# is marked as providing the interface, because\n\t\t# the class may have used a different name\n\t\t# for the interface.\n\t\t#\n\t\t# See zope.interface.tests.test_verify.test_verify_provides\n\t\t# for an example of why this is necessary.\n\t\t#\n\t\t# Note that if the interface is not a class-level\n\t\t# interface, then the above test will fail,\n\t\t# but that's ok - we can't verify that it\n\t\t# provides the interface in that case.\n\t\t#\n\t\tif not isinstance(iface, InterfaceClass):\n\t\t\treturn True\n\n\t\tif not iface.providedBy(candidate, tentative=tentative):\n\t\t\traise Invalid(\"%s does not provide %s\" % (candidate, iface))\n\n\t\t# now, make sure the candidate defines all the methods\n\t\t#\n\t\terrors = []\n\t\tfor name, method in iface.namesAndDescriptions(1):\n\t\t\ttry:\n\t\t\t\tmethod.get(candidate)\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# The type of the interface\n\tif vtype is None:\n\t\tvtype = interface.Interface\n\t# Check that the candidate implements the interface\n\tif not isinstance(candidate, vtype):\n\t\traise interface.Invalid(\n\t\t\t\"The given candidate does not implement the interface\")\n\t# Check that the candidate provides at least the methods that are\n\t# implemented in the interface\n\tfor method in iface.names():\n\t\tif (method in candidate.__dict__ and\n\t\t\t not callable(candidate.__dict__[method])):\n\t\t\traise interface.Invalid(\n\t\t\t\t\"The given candidate does not implement the method %s\" %\n\t\t\t\tmethod)\n\t# Check that the candidate provides at least the attributes that are\n\t# implemented in the interface\n\tfor attr in iface.names(all=True):\n\t\tif attr not in candidate.__dict__:\n\t\t\traise interface.Invalid(\n\t\t\t\t\"The given candidate does not implement the attribute %s\" %\n\t\t\t\tattr)\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tif not isinstance(candidate, _abc.InterfaceClass):\n\t\t\traise TypeError(\n\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\tf\"that are instances of InterfaceClass objects, \"\n\t\t\t\tf\"but {candidate} is not an InterfaceClass object.\"\n\t\t\t)\n\t\tif vtype is not None:\n\t\t\tif vtype is not Interface:\n\t\t\t\traise TypeError(\n\t\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\t\tf\"that are instances of InterfaceClass objects, \"\n\t\t\t\t\tf\"but {candidate} is an {vtype.__name__} object.\"\n\t\t\t\t)\n\t\tif not candidate.providedBy(iface):\n\t\t\traise Invalid(\n\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\tf\"that declare that they provide it, but {candidate} \"\n\t\t\t\tf\"does not declare that it provides {iface}.\"\n\t\t\t)\n\t\tif not tentative:\n\t\t\tif not iface.isOrExtends(candidate):\n\t\t\t\traise Invalid(\n\t\t\t\t\tf\"Interface {iface} is implemented only by objects \"\n\t\t\t\t\tf\"that are subclasses of {candidate}, \"\n\t\t\t\t\tf\"but {candidate} is not a subclass of {iface}.\"\n\t\t\t\t)\n\t\tif not candidate.names():\n\t\t\traise Invalid(\n\t\t\t\tf\"Interface {iface} is implemented by objects that \"\n\t\t\t\tf\"", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\tif tentative:\n\t\t\treturn True\n\t\tif not iface.providedBy(candidate):\n\t\t\traise InvalidImplementation(candidate, iface)\n\t\tif vtype is not None:\n\t\t\tverifyObject(candidate, vtype)\n\t\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# check that the candidate provides the interface\n\t\tif not tentative:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate %r does not provide the interface %r'\n\t\t\t\t\t% (candidate, iface))\n\n\t\t# check that the candidate defines all the methods\n\t\tmissing = []\n\t\tfor name, method in iface.namesAndDescriptions():\n\t\t\ttry:\n\t\t\t\t# use getattr so that we get a Method object,\n\t\t\t\t# which we can then use to check its signature\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\t# the candidate does not define this method\n\t\t\t\tmissing.append(name)\n\t\t\t\tcontinue\n\n\t\t\tif not isMethod(method):\n\t\t\t\t# the candidate defines this attribute as something\n\t\t\t\t# other than a method\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate %r defines the attribute %r as '\n\t\t\t\t\t'a %r, which is not a method'\n\t\t\t\t\t% (candidate, name, method.__class__.__name__))\n\n\t\t\t# check the method's signature\n\t\t\tverifyMethodSignature(method, iface[name])\n\n\t\tif missing:\n\t\t\traise Invalid(\n\t\t\t\t'The candidate %r does not provide all the required '\n\t\t\t\t'methods: %s'\n\t\t\t\t% (candidate, ', '.join(missing)))\n\n\t\t# check that the candidate defines all the attributes\n\t\t", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# The interface is provided by the candidate, so we're good.\n\t\tif not tentative:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid('No interface provided by %r' % candidate)\n\n\t\t# Make sure the candidate has all the methods\n\t\tmissing = []\n\t\tfor name, method in iface.namesAndDescriptions(True):\n\t\t\ttry:\n\t\t\t\tmethod = iface[name]\n\t\t\texcept KeyError:\n\t\t\t\tmissing.append(name)\n\n\t\tif missing:\n\t\t\traise Invalid(\n\t\t\t\t'Missing methods: %s' % ', '.join(missing))\n\n\t\t# Make sure the candidate has all the attributes\n\t\tmissing = []\n\t\tfor name in iface.names(True):\n\t\t\ttry:\n\t\t\t\tgetattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\tmissing.append(name)\n\n\t\tif missing:\n\t\t\traise Invalid(\n\t\t\t\t'Missing attributes: %s' % ', '.join(missing))\n\n\t\t# Make sure the candidate provides the interface\n\t\tif not tentative and not iface.providedBy(candidate):\n\t\t\traise Invalid('No interface provided by %r' % candidate)\n\n\t\treturn True\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport unittest\n\nfrom.. import Interface, Invalid, Attribute\nfrom.. import implement", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\n\n\tif not candidate.providedBy(iface):\n\t\traise zope.interface.Invalid(\n\t\t\t\"Object %s does not provide %s\" % (\n\t\t\t\tcandidate, iface\n\t\t\t)\n\t\t)\n\n\t# Check the methods.\n\tfor name, method in iface.namesAndMethods():\n\t\tif not hasattr(candidate, name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not have method %s\" % (\n\t\t\t\t\tcandidate, name\n\t\t\t\t)\n\t\t\t)\n\t\tverifyMethod(iface, candidate, name)\n\n\t# Check the attributes.\n\tfor name, attr in iface.namesAndAttributes():\n\t\tif not hasattr(candidate, name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not have attribute %s\" % (\n\t\t\t\t\tcandidate, name\n\t\t\t\t)\n\t\t\t)\n\t\tverifyAttribute(iface, candidate, name)\n\n\treturn True\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Verify that the candidate provides the interface.\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(candidate, 'does not provide', iface)\n\n\t\t# Verify that the candidate provides all the methods.\n\t\tfor name, method in iface.namesAndDescriptions():\n\t\t\tif name in ('__cmp__', '__conform__'):\n\t\t\t\tcontinue\n\n\t\t\tif name in ('__provides__', '__get__', '__set__', '__delete__'):\n\t\t\t\tif not hasattr(candidate, name):\n\t\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\t\t\t\tcontinue\n\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\n\t\t\t# Verify the method signature\n\t\t\tmethod = getMethod(candidate, name)\n\t\t\tif method.im_self is not None:\n\t\t\t\t# Verify the method has a'self' parameter\n\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\t\t\tsig = getSignature(method)\n\t\t\tprovided = getProvidedSignature(iface, name)\n\t\t\tif provided is not None and provided!= sig:\n\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\n\t\t# Verify that the candidate provides all the attributes.\n\t\tfor name, description in iface.namesAndDescriptions():\n\t\t\tif name in ('", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# In general, the verification is based on introspection.\n\t\t# First, we verify that the object provides the interface.\n\t\t# This is done by checking the __provides__ attribute.\n\t\t# Note that this is only done for the candidate if\n\t\t# *tentative* is false.\n\t\tif not tentative and hasattr(candidate, '__provides__'):\n\t\t\t# First, check that the object provides the interface.\n\t\t\t# This is done by checking the __provides__ attribute.\n\t\t\tprovs = getattr(candidate, '__provides__', None)\n\t\t\tif not IInterface.providedBy(provs):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'Object does not provide interface %s' % iface)\n\t\t\t# Also verify that the candidate is a provider\n\t\t\t# for the interface.\n\t\t\tif not iface.isOrExtends(provs):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'Object %s does not provide %s' % (\n\t\t\t\t\t\tcandidate, iface))\n\n\t\t# Now, verify that the candidate provides the correct\n\t\t# methods.\n\t\tfor name in iface:\n\t\t\t# If the interface has no method, we do nothing.\n\t\t\tif name not in candidate:\n\t\t\t\traise Invalid(\n\t\t\t\t\t'Missing method %s for interface %s' % (\n\t\t\t\t\t\tname, iface))\n\t\t\t# Otherwise, we check that the method is correct.\n\t\t\tmethod = candidate", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# 1) Make sure the candidate claims that it provides the interface\n\t\t#    using ``iface.providedBy`` (unless *tentative* is `True`,\n\t\t#    in which case this step is skipped). This means that the\n\t\t#    candidate's class declares that it `implements\n\t\t#    <zope.interface.implementer>` the interface, or the\n\t\t#    candidate itself declares that it `provides\n\t\t#    <zope.interface.provider>` the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"The candidate must provide the interface (or declare that it implements it).\")\n\n\t\t# 2) Make sure the candidate defines all the necessary methods\n\t\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\t\tif name in candidate.__dict__:\n\t\t\t\tcontinue\n\t\t\t# Make a specific error message for methods that don't exist\n\t\t\tif name not in candidate.__class__.__dict__:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The candidate must have a method named %r.\" % name)\n\t\t\t# Make a specific error message for methods that exist\n\t\t\t# but are not callable\n\t\t\tif not callable(method):\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The candidate's method named %r must be callable.\" % name)\n\n\t\t# 3) Make sure the", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# First, verify that the candidate is a provider of the\n\t\t# interface (unless we are tentative):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not provide %s\" % (\n\t\t\t\t\tcandidate.__name__, iface.__name__))\n\n\t\t# Then, verify that the candidate provides all the required\n\t\t# methods:\n\t\trequired = iface.namesAndDescriptions()\n\t\tprovided = iface.namesAndDescriptions(True)\n\t\tmissing = set(required) - set(provided)\n\t\tif missing:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not provide all required methods: %s\"\n\t\t\t\t% (candidate.__name__, \", \".join(missing)))\n\n\t\t# And that all methods have the correct signatures:\n\t\tfor name, description in required:\n\t\t\tmeth = getattr(candidate, name)\n\t\t\tif not verifyMethod(iface, name, meth, description):\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"Method %s.%s is invalid\" % (\n\t\t\t\t\t\tcandidate.__name__, name))\n\n\t\t# And that all attributes are defined:\n\t\tmissing = set(iface.namesAndDescriptions()) - set(provided)\n\t\tif missing:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# If the candidate claims that it provides the interface, we can skip\n\t\t# the next steps.\n\t\tif not (tentative or iface.providedBy(candidate)):\n\t\t\traise interfaces.Invalid(\n\t\t\t\t\"The candidate does not provide the interface.\",\n\t\t\t\tiface,\n\t\t\t\tcandidate,\n\t\t\t)\n\n\t\terrors = []\n\t\t# Make sure the candidate defines all the necessary methods\n\t\tfor name, method in iface.namesAndDescriptions(True):\n\t\t\tif not name in candidate.names():\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t\"The candidate does not define the necessary method.\",\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tname,\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\t# Get the method out of the candidate\n\t\t\tmeth = getattr(candidate, name)\n\t\t\t# Make sure it is callable\n\t\t\tif not inspect.ismethod(meth):\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t\"The candidate's method is not callable.\",\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tname,\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\t# Try to get a signature for the method\n\t\t\ttry:\n\t\t\t\tsig = zope.interface.interface.Specification(meth)\n\t\t\texcept TypeError:\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t\"The candidate's method cannot provide a signature.\",\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tname,\n\t\t\t\t", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# First of all, make sure the candidate claims to implement the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate does not provide %s' % iface)\n\t\tif not iface.implementedBy(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate does not implement %s' % iface)\n\n\t# Make sure the candidate has all the required methods\n\tfor method in iface.namesAndDescriptions(True):\n\t\tname, description = method\n\t\tif not hasattr(candidate, name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate does not have %s' % name)\n\t\tmethod = getattr(candidate, name)\n\t\tif not inspect.ismethod(method) and not inspect.isfunction(method):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate %r is not callable' % method)\n\t\tsig = inspect.signature(method)\n\t\tif description:\n\t\t\tif not sig.return_annotation is description.return_annotation:\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t'candidate %r does not match return annotation '\n\t\t\t\t\t'of %s' % (method, description))\n\t\t\tfor param_name, param_annotation in zip(\n\t\t\t\tdescription.parameters.keys(),\n\t\t\t\tdescription.parameters", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# make sure the candidate might provide the interface\n\t\tif not tentative and not iface.providedBy(candidate):\n\t\t\traise Invalid('Object does not provide interface')\n\n\t\t# make sure the candidate defines all the methods\n\t\terrors = []\n\t\tfor name in iface.names():\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append('Object has no attribute %s' % name)\n\n\t\t# make sure each method has the correct signature\n\t\tfor name in iface.names():\n\t\t\tif hasattr(candidate, name):\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\t\tif not verifyMethod(name, iface[name], method):\n\t\t\t\t\terrors.append('Object has wrong signature for method %s' % name)\n\n\t\t# make sure the candidate defines all the attributes\n\t\tfor name in iface.namesInOrder():\n\t\t\tif name not in iface.names() and not hasattr(candidate, name):\n\t\t\t\terrors.append('Object has no attribute %s' % name)\n\n\t\t# if we have any errors, raise them\n\t\tif errors:\n\t\t\tif len(errors) == 1:\n\t\t\t\traise Invalid(errors[0])\n\t\t\traise Invalid('Object has error(s): %s' % ', '.join(errors))\n\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# check that the candidate provides the interface\n\t\tif not tentative:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate %r does not provide the interface %r'\n\t\t\t\t\t% (candidate, iface))\n\n\t\t# check that the candidate defines all the methods\n\t\tmissing = []\n\t\tfor name, method in iface.namesAndDescriptions():\n\t\t\ttry:\n\t\t\t\t# use getattr so that we get a Method object,\n\t\t\t\t# which we can then use to check its signature\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\t# the candidate does not define this method\n\t\t\t\tmissing.append(name)\n\t\t\t\tcontinue\n\n\t\t\tif not isMethod(method):\n\t\t\t\t# the candidate defines this attribute as something\n\t\t\t\t# other than a method\n\t\t\t\traise Invalid(\n\t\t\t\t\t'The candidate %r defines the attribute %r as '\n\t\t\t\t\t'a %r, which is not a method'\n\t\t\t\t\t% (candidate, name, method.__class__.__name__))\n\n\t\t\t# check the method's signature\n\t\t\tverifyMethodSignature(method, iface[name])\n\n\t\tif missing:\n\t\t\traise Invalid(\n\t\t\t\t'The candidate %r does not provide all the required '\n\t\t\t\t'methods: %s'\n\t\t\t\t% (candidate, ', '.join(missing)))\n\n\t\t# check that the candidate defines all the attributes\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# The interface is provided by the candidate, so we're good.\n\t\tif not tentative:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid('No interface provided by %r' % candidate)\n\n\t\t# Make sure the candidate has all the methods\n\t\tmissing = []\n\t\tfor name, method in iface.namesAndDescriptions(True):\n\t\t\ttry:\n\t\t\t\tmethod = iface[name]\n\t\t\texcept KeyError:\n\t\t\t\tmissing.append(name)\n\n\t\tif missing:\n\t\t\traise Invalid(\n\t\t\t\t'Missing methods: %s' % ', '.join(missing))\n\n\t\t# Make sure the candidate has all the attributes\n\t\tmissing = []\n\t\tfor name in iface.names(True):\n\t\t\ttry:\n\t\t\t\tgetattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\tmissing.append(name)\n\n\t\tif missing:\n\t\t\traise Invalid(\n\t\t\t\t'Missing attributes: %s' % ', '.join(missing))\n\n\t\t# Make sure the candidate provides the interface\n\t\tif not tentative and not iface.providedBy(candidate):\n\t\t\traise Invalid('No interface provided by %r' % candidate)\n\n\t\treturn True\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport unittest\n\nfrom.. import Interface, Invalid, Attribute\nfrom.. import implement", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\n\n\tif not candidate.providedBy(iface):\n\t\traise zope.interface.Invalid(\n\t\t\t\"Object %s does not provide %s\" % (\n\t\t\t\tcandidate, iface\n\t\t\t)\n\t\t)\n\n\t# Check the methods.\n\tfor name, method in iface.namesAndMethods():\n\t\tif not hasattr(candidate, name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not have method %s\" % (\n\t\t\t\t\tcandidate, name\n\t\t\t\t)\n\t\t\t)\n\t\tverifyMethod(iface, candidate, name)\n\n\t# Check the attributes.\n\tfor name, attr in iface.namesAndAttributes():\n\t\tif not hasattr(candidate, name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not have attribute %s\" % (\n\t\t\t\t\tcandidate, name\n\t\t\t\t)\n\t\t\t)\n\t\tverifyAttribute(iface, candidate, name)\n\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# Verify that the candidate provides the interface.\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(candidate, 'does not provide', iface)\n\n\t\t# Verify that the candidate provides all the methods.\n\t\tfor name, method in iface.namesAndDescriptions():\n\t\t\tif name in ('__cmp__', '__conform__'):\n\t\t\t\tcontinue\n\n\t\t\tif name in ('__provides__', '__get__', '__set__', '__delete__'):\n\t\t\t\tif not hasattr(candidate, name):\n\t\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\t\t\t\tcontinue\n\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\n\t\t\t# Verify the method signature\n\t\t\tmethod = getMethod(candidate, name)\n\t\t\tif method.im_self is not None:\n\t\t\t\t# Verify the method has a'self' parameter\n\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\t\t\tsig = getSignature(method)\n\t\t\tprovided = getProvidedSignature(iface, name)\n\t\t\tif provided is not None and provided!= sig:\n\t\t\t\traise Invalid(candidate, 'does not provide', iface, name)\n\n\t\t# Verify that the candidate provides all the attributes.\n\t\tfor name, description in iface.namesAndDescriptions():\n\t\t\tif name in ('", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# In general, the verification is based on introspection.\n\t\t# First, we verify that the object provides the interface.\n\t\t# This is done by checking the __provides__ attribute.\n\t\t# Note that this is only done for the candidate if\n\t\t# *tentative* is false.\n\t\tif not tentative and hasattr(candidate, '__provides__'):\n\t\t\t# First, check that the object provides the interface.\n\t\t\t# This is done by checking the __provides__ attribute.\n\t\t\tprovs = getattr(candidate, '__provides__', None)\n\t\t\tif not IInterface.providedBy(provs):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'Object does not provide interface %s' % iface)\n\t\t\t# Also verify that the candidate is a provider\n\t\t\t# for the interface.\n\t\t\tif not iface.isOrExtends(provs):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'Object %s does not provide %s' % (\n\t\t\t\t\t\tcandidate, iface))\n\n\t\t# Now, verify that the candidate provides the correct\n\t\t# methods.\n\t\tfor name in iface:\n\t\t\t# If the interface has no method, we do nothing.\n\t\t\tif name not in candidate:\n\t\t\t\traise Invalid(\n\t\t\t\t\t'Missing method %s for interface %s' % (\n\t\t\t\t\t\tname, iface))\n\t\t\t# Otherwise, we check that the method is correct.\n\t\t\tmethod = candidate", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# 1) Make sure the candidate claims that it provides the interface\n\t\t#    using ``iface.providedBy`` (unless *tentative* is `True`,\n\t\t#    in which case this step is skipped). This means that the\n\t\t#    candidate's class declares that it `implements\n\t\t#    <zope.interface.implementer>` the interface, or the\n\t\t#    candidate itself declares that it `provides\n\t\t#    <zope.interface.provider>` the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"The candidate must provide the interface (or declare that it implements it).\")\n\n\t\t# 2) Make sure the candidate defines all the necessary methods\n\t\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\t\tif name in candidate.__dict__:\n\t\t\t\tcontinue\n\t\t\t# Make a specific error message for methods that don't exist\n\t\t\tif name not in candidate.__class__.__dict__:\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The candidate must have a method named %r.\" % name)\n\t\t\t# Make a specific error message for methods that exist\n\t\t\t# but are not callable\n\t\t\tif not callable(method):\n\t\t\t\traise Invalid(\n\t\t\t\t\t\"The candidate's method named %r must be callable.\" % name)\n\n\t\t# 3) Make sure the", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# First, verify that the candidate is a provider of the\n\t\t# interface (unless we are tentative):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not provide %s\" % (\n\t\t\t\t\tcandidate.__name__, iface.__name__))\n\n\t\t# Then, verify that the candidate provides all the required\n\t\t# methods:\n\t\trequired = iface.namesAndDescriptions()\n\t\tprovided = iface.namesAndDescriptions(True)\n\t\tmissing = set(required) - set(provided)\n\t\tif missing:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does not provide all required methods: %s\"\n\t\t\t\t% (candidate.__name__, \", \".join(missing)))\n\n\t\t# And that all methods have the correct signatures:\n\t\tfor name, description in required:\n\t\t\tmeth = getattr(candidate, name)\n\t\t\tif not verifyMethod(iface, name, meth, description):\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"Method %s.%s is invalid\" % (\n\t\t\t\t\t\tcandidate.__name__, name))\n\n\t\t# And that all attributes are defined:\n\t\tmissing = set(iface.namesAndDescriptions()) - set(provided)\n\t\tif missing:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t\"Object %s does", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# If the candidate claims that it provides the interface, we can skip\n\t\t# the next steps.\n\t\tif not (tentative or iface.providedBy(candidate)):\n\t\t\traise interfaces.Invalid(\n\t\t\t\t\"The candidate does not provide the interface.\",\n\t\t\t\tiface,\n\t\t\t\tcandidate,\n\t\t\t)\n\n\t\terrors = []\n\t\t# Make sure the candidate defines all the necessary methods\n\t\tfor name, method in iface.namesAndDescriptions(True):\n\t\t\tif not name in candidate.names():\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t\"The candidate does not define the necessary method.\",\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tname,\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\t# Get the method out of the candidate\n\t\t\tmeth = getattr(candidate, name)\n\t\t\t# Make sure it is callable\n\t\t\tif not inspect.ismethod(meth):\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t\"The candidate's method is not callable.\",\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tname,\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\t# Try to get a signature for the method\n\t\t\ttry:\n\t\t\t\tsig = zope.interface.interface.Specification(meth)\n\t\t\texcept TypeError:\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t\"The candidate's method cannot provide a signature.\",\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tname,\n\t\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# First of all, make sure the candidate claims to implement the interface\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate does not provide %s' % iface)\n\t\tif not iface.implementedBy(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate does not implement %s' % iface)\n\n\t# Make sure the candidate has all the required methods\n\tfor method in iface.namesAndDescriptions(True):\n\t\tname, description = method\n\t\tif not hasattr(candidate, name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate does not have %s' % name)\n\t\tmethod = getattr(candidate, name)\n\t\tif not inspect.ismethod(method) and not inspect.isfunction(method):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'candidate %r is not callable' % method)\n\t\tsig = inspect.signature(method)\n\t\tif description:\n\t\t\tif not sig.return_annotation is description.return_annotation:\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t'candidate %r does not match return annotation '\n\t\t\t\t\t'of %s' % (method, description))\n\t\t\tfor param_name, param_annotation in zip(\n\t\t\t\tdescription.parameters.keys(),\n\t\t\t\tdescription.parameters", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\t\t# make sure the candidate might provide the interface\n\t\tif not tentative and not iface.providedBy(candidate):\n\t\t\traise Invalid('Object does not provide interface')\n\n\t\t# make sure the candidate defines all the methods\n\t\terrors = []\n\t\tfor name in iface.names():\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append('Object has no attribute %s' % name)\n\n\t\t# make sure each method has the correct signature\n\t\tfor name in iface.names():\n\t\t\tif hasattr(candidate, name):\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\t\tif not verifyMethod(name, iface[name], method):\n\t\t\t\t\terrors.append('Object has wrong signature for method %s' % name)\n\n\t\t# make sure the candidate defines all the attributes\n\t\tfor name in iface.namesInOrder():\n\t\t\tif name not in iface.names() and not hasattr(candidate, name):\n\t\t\t\terrors.append('Object has no attribute %s' % name)\n\n\t\t# if we have any errors, raise them\n\t\tif errors:\n\t\t\tif len(errors) == 1:\n\t\t\t\traise Invalid(errors[0])\n\t\t\traise Invalid('Object has error(s): %s' % ', '.join(errors))\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t \n\t#print \"verifyClass(%s, %s, %s)\" % (iface, candidate, tentative)\n\n\tif issubclass(candidate, iface):\n\t\tprint \"verifyClass: %s is a subclass of %s\" % (candidate, iface)\n\t\treturn True\n\n\t# If the candidate is a class, we should check the parent class.\n\tif (isinstance(candidate, type) and issubclass(candidate, iface)):\n\t\tprint \"verifyClass: %s is a subclass of %s\" % (candidate, iface)\n\t\treturn True\n\n\tif tentative:\n\t\treturn False\n\n\t# If the class is an interface, we should check its parent.\n\tif issubclass(iface, IInterface):\n\t\tfor parent in iface.__bases__:\n\t\t\tif verifyClass(parent, candidate, True):\n\t\t\t\treturn True\n\n\treturn False\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\n\t# We need to resolve the interface definition\n\tiface = resolveInterface(iface)\n\n\t# Verify that the candidate provides the interface\n\tif not iface in candidate.provides:\n\t\traise Exception(\"Invalid candidate %s does not provide the interface %s\" % (candidate.name, iface))\n\n\t# Verify that the candidate has the correct methods\n\tfor method in iface.methods:\n\t\tif not method in candidate.provides:\n\t\t\traise Exception(\"Invalid candidate %s does not provide the required method %s\" % (candidate.name, method))\n\n\t# Verify that the candidate has no unnecessary methods\n\tfor method in candidate.provides:\n\t\tif not method in iface.methods:\n\t\t\traise Exception(\"Invalid candidate %s provides an unnecessary method %s\" % (candidate.name, method))\n\n\t# Check if the interface is abstract\n\tif iface.abstract:\n\t\tif not candidate.abstract:\n\t\t\traise Exception(\"Invalid candidate %s does not provide an abstract method\" % candidate.name)\n\telif not iface.abstract:\n\t\tif candidate.abstract:\n\t\t\traise Exception(\"Invalid candidate %s provides an abstract method\" % candidate.name)\n\n\t# TODO: Check for abstract classes\n\n\t# Check for super classes\n\tif iface.superClass!= None:\n\t\tverifyClass(iface.superClass, candidate, tentative=True)\n\n\t# Verify", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n#\tif not isinstance(iface, type):\n#\t\traise TypeError(\"iface must be a type\")\n#\tif not isinstance(candidate, type):\n#\t\traise TypeError(\"candidate must be a type\")\n\n\t# Verify that the candidate is a subclass of the interface\n\tif not issubclass(candidate, iface):\n\t\treturn False\n\t\n\t# Verify that the candidate does not override any methods\n\tif not tentative:\n\t\tifaceMethods = inspect.getmembers(iface, inspect.isabstract)\n\t\tfor name, method in ifaceMethods:\n\t\t\tif name!= \"__init__\":\n\t\t\t\tcandidateMethod = getattr(candidate, name)\n\t\t\t\tif inspect.ismethod(candidateMethod):\n\t\t\t\t\tif candidateMethod.__func__ is method.__func__:\n\t\t\t\t\t\treturn False\n\t\n\treturn True\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\timport unittest\n\timport doctest\n\tloader = unittest.TestLoader()\n\tsuite = unittest.TestSuite()\n\tsuite.addTests(loader.loadTestsFromTestCase(Test))\n\tsuite.addTests(doctest.DocTestSuite())\n\tunittest.TextTestRunner(verbosity=2).run(suite)\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2009  <>\n# All rights reserved.\n#\n# This software is licensed as described in the file COPYING, which\n# you should have received as part of this distribution.\n#\n# Author:  <>\n\nfrom __future__ import division\n\nimport sys\nimport os\nimport time\nimport gc\nimport math\nimport ctypes\nimport weakref\n\nfrom guppy import hpy\nfrom guppy import heapy\n\nfrom guppy.heapy.util import is_py3k\n\nfrom guppy.heapy.util import get_c_function\nfrom guppy.heapy.util import get_c_function_pointer\nfrom guppy.heapy.util import get_c_function_pointer_from_name\n\nfrom guppy.heapy.heapy_object import HeapyObject\nfrom guppy.heapy", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t \n\t#print \"verifyClass(%s, %s, %s)\" % (iface, candidate, tentative)\n\n\tif issubclass(candidate, iface):\n\t\tprint \"verifyClass: %s is a subclass of %s\" % (candidate, iface)\n\t\treturn True\n\n\t# If the candidate is a class, we should check the parent class.\n\tif (isinstance(candidate, type) and issubclass(candidate, iface)):\n\t\tprint \"verifyClass: %s is a subclass of %s\" % (candidate, iface)\n\t\treturn True\n\n\tif tentative:\n\t\treturn False\n\n\t# If the class is an interface, we should check its parent.\n\tif issubclass(iface, IInterface):\n\t\tfor parent in iface.__bases__:\n\t\t\tif verifyClass(parent, candidate, True):\n\t\t\t\treturn True\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\n\t# We need to resolve the interface definition\n\tiface = resolveInterface(iface)\n\n\t# Verify that the candidate provides the interface\n\tif not iface in candidate.provides:\n\t\traise Exception(\"Invalid candidate %s does not provide the interface %s\" % (candidate.name, iface))\n\n\t# Verify that the candidate has the correct methods\n\tfor method in iface.methods:\n\t\tif not method in candidate.provides:\n\t\t\traise Exception(\"Invalid candidate %s does not provide the required method %s\" % (candidate.name, method))\n\n\t# Verify that the candidate has no unnecessary methods\n\tfor method in candidate.provides:\n\t\tif not method in iface.methods:\n\t\t\traise Exception(\"Invalid candidate %s provides an unnecessary method %s\" % (candidate.name, method))\n\n\t# Check if the interface is abstract\n\tif iface.abstract:\n\t\tif not candidate.abstract:\n\t\t\traise Exception(\"Invalid candidate %s does not provide an abstract method\" % candidate.name)\n\telif not iface.abstract:\n\t\tif candidate.abstract:\n\t\t\traise Exception(\"Invalid candidate %s provides an abstract method\" % candidate.name)\n\n\t# TODO: Check for abstract classes\n\n\t# Check for super classes\n\tif iface.superClass!= None:\n\t\tverifyClass(iface.superClass, candidate, tentative=True)\n\n\t# Verify", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n#\tif not isinstance(iface, type):\n#\t\traise TypeError(\"iface must be a type\")\n#\tif not isinstance(candidate, type):\n#\t\traise TypeError(\"candidate must be a type\")\n\n\t# Verify that the candidate is a subclass of the interface\n\tif not issubclass(candidate, iface):\n\t\treturn False\n\t\n\t# Verify that the candidate does not override any methods\n\tif not tentative:\n\t\tifaceMethods = inspect.getmembers(iface, inspect.isabstract)\n\t\tfor name, method in ifaceMethods:\n\t\t\tif name!= \"__init__\":\n\t\t\t\tcandidateMethod = getattr(candidate, name)\n\t\t\t\tif inspect.ismethod(candidateMethod):\n\t\t\t\t\tif candidateMethod.__func__ is method.__func__:\n\t\t\t\t\t\treturn False\n\t\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\timport unittest\n\timport doctest\n\tloader = unittest.TestLoader()\n\tsuite = unittest.TestSuite()\n\tsuite.addTests(loader.loadTestsFromTestCase(Test))\n\tsuite.addTests(doctest.DocTestSuite())\n\tunittest.TextTestRunner(verbosity=2).run(suite)\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2009  <>\n# All rights reserved.\n#\n# This software is licensed as described in the file COPYING, which\n# you should have received as part of this distribution.\n#\n# Author:  <>\n\nfrom __future__ import division\n\nimport sys\nimport os\nimport time\nimport gc\nimport math\nimport ctypes\nimport weakref\n\nfrom guppy import hpy\nfrom guppy import heapy\n\nfrom guppy.heapy.util import is_py3k\n\nfrom guppy.heapy.util import get_c_function\nfrom guppy.heapy.util import get_c_function_pointer\nfrom guppy.heapy.util import get_c_function_pointer_from_name\n\nfrom guppy.heapy.heapy_object import HeapyObject\nfrom guppy.heapy", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# get metaclass from explicit metaclass, if present\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t\t\n\t\t# otherwise, if no explicit metaclass, then use type\n\t\t# for first base\n\tif len(bases) > 0:\n\t\treturn bases[0]\n\t\t\n\t\t# no explicit metaclass and no bases -> TypeError\n\traise TypeError(\"Can't determine metaclass from bases %r\" % (bases,))\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# If explicit metaclass specified, return it\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\n\t\t# If no explicit metaclass specified, determine from bases\n\tif not bases:\n\t\treturn type\n\tif len(bases) == 1:\n\t\treturn bases[0]\n\treturn MROMetaClass(bases)\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\tif not explicit_mc:\n\t\texplicit_mc = None\n\t\texplicit_bases = set()\n\n\t\t# Collect explicit metaclasses and bases\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\texplicit_mc = getattr(base, '__metaclass__', None)\n\t\t\t\tif not isclass(explicit_mc):\n\t\t\t\t\traise TypeError(\"__metaclass__ must be a class\")\n\t\t\t\texplicit_bases.add(base)\n\n\t\t# If there is only one explicit metaclass, use it\n\t\tif len(explicit_bases) == 1:\n\t\t\texplicit_mc = list(explicit_bases)[0].__metaclass__\n\n\t\t# If there are no explicit metaclasses, use the first one\n\t\tif not explicit_mc:\n\t\t\texplicit_mc = bases[0]\n\n\t\t# If we have an explicit metaclass, it must be one of the bases\n\t\tif explicit_mc not in bases:\n\t\t\traise TypeError(\"Explicit metaclass must be one of the bases\")\n\n\t\t# Now we've found our explicit metaclass, we can remove it from the\n\t\t# bases list\n\t\tbases = tuple(set(bases) - set([explicit_mc]))\n\n\treturn explicit_mc, bases\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t # Get explicit metaclass, if any\n\ttry:\n\t\texplicit_mc = bases[0].__metaclass__\n\texcept (IndexError, AttributeError):\n\t\tpass\n\t# If explicit metaclass is given, return it\n\tif explicit_mc:\n\t\treturn explicit_mc\n\t# No explicit metaclass, determine from bases\n\t# If only one base, use it\n\tif len(bases) == 1:\n\t\treturn bases[0]\n\t# If two bases, and one is module, use the other\n\tif len(bases) == 2 and module_base(bases[0]):\n\t\treturn bases[1]\n\t# Otherwise, use new-style class\n\treturn NewStyleClass\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# Determine order of search for implicit metaclasses\n\tmetaclass_bases = [explicit_mc or type] + list(bases)\n\treturn metaclass_bases[-1]\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\t# no explicit meta class\n\tif not explicit_mc:\n\t\t# no explicit meta class, so use first base class\n\t\texplicit_mc = bases[0]\n\n\t# determine the metaclass\n\tif inspect.isclass(explicit_mc):\n\t\texplicit_mc = explicit_mc.__name__\n\n\t# determine the metaclass\n\tif isinstance(explicit_mc, basestring):\n\t\texplicit_mc = resolveClass(explicit_mc, allowNone=True)\n\n\t# no explicit meta class, so use first base class\n\tif not explicit_mc:\n\t\texplicit_mc = bases[0]\n\n\t# determine the metaclass\n\tif inspect.isclass(explicit_mc):\n\t\texplicit_mc = explicit_mc.__name__\n\n\t# determine the metaclass\n\tif isinstance(explicit_mc, basestring):\n\t\texplicit_mc = resolveClass(explicit_mc, allowNone=True)\n\n\t# no explicit meta class, so use first base class\n\tif not explicit_mc:\n\t\texplicit_mc = bases[0]\n\n\t# determine the metaclass\n\tif inspect.isclass(explicit_mc):\n\t\texplicit_mc = explicit_mc.__name__\n\n\t# determine the metaclass\n\tif isinstance(explicit_mc, basestring):\n\t\texplicit_mc = resolveClass(explicit_mc, allow", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\tif explicit_mc is not None:\n\t\t\treturn explicit_mc\n\t\tfor base in bases:\n\t\t\tif isinstance(base, type):\n\t\t\t\treturn base.__class__\n\t\traise TypeError(\"No metaclass defined\")\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# get metaclass from explicit metaclass, if present\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t\t\n\t\t# otherwise, if no explicit metaclass, then use type\n\t\t# for first base\n\tif len(bases) > 0:\n\t\treturn bases[0]\n\t\t\n\t\t# no explicit metaclass and no bases -> TypeError\n\traise TypeError(\"Can't determine metaclass from bases %r\" % (bases,))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# If explicit metaclass specified, return it\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\n\t\t# If no explicit metaclass specified, determine from bases\n\tif not bases:\n\t\treturn type\n\tif len(bases) == 1:\n\t\treturn bases[0]\n\treturn MROMetaClass(bases)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\tif not explicit_mc:\n\t\texplicit_mc = None\n\t\texplicit_bases = set()\n\n\t\t# Collect explicit metaclasses and bases\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\texplicit_mc = getattr(base, '__metaclass__', None)\n\t\t\t\tif not isclass(explicit_mc):\n\t\t\t\t\traise TypeError(\"__metaclass__ must be a class\")\n\t\t\t\texplicit_bases.add(base)\n\n\t\t# If there is only one explicit metaclass, use it\n\t\tif len(explicit_bases) == 1:\n\t\t\texplicit_mc = list(explicit_bases)[0].__metaclass__\n\n\t\t# If there are no explicit metaclasses, use the first one\n\t\tif not explicit_mc:\n\t\t\texplicit_mc = bases[0]\n\n\t\t# If we have an explicit metaclass, it must be one of the bases\n\t\tif explicit_mc not in bases:\n\t\t\traise TypeError(\"Explicit metaclass must be one of the bases\")\n\n\t\t# Now we've found our explicit metaclass, we can remove it from the\n\t\t# bases list\n\t\tbases = tuple(set(bases) - set([explicit_mc]))\n\n\treturn explicit_mc, bases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t # Get explicit metaclass, if any\n\ttry:\n\t\texplicit_mc = bases[0].__metaclass__\n\texcept (IndexError, AttributeError):\n\t\tpass\n\t# If explicit metaclass is given, return it\n\tif explicit_mc:\n\t\treturn explicit_mc\n\t# No explicit metaclass, determine from bases\n\t# If only one base, use it\n\tif len(bases) == 1:\n\t\treturn bases[0]\n\t# If two bases, and one is module, use the other\n\tif len(bases) == 2 and module_base(bases[0]):\n\t\treturn bases[1]\n\t# Otherwise, use new-style class\n\treturn NewStyleClass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# Determine order of search for implicit metaclasses\n\tmetaclass_bases = [explicit_mc or type] + list(bases)\n\treturn metaclass_bases[-1]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\t# no explicit meta class\n\tif not explicit_mc:\n\t\t# no explicit meta class, so use first base class\n\t\texplicit_mc = bases[0]\n\n\t# determine the metaclass\n\tif inspect.isclass(explicit_mc):\n\t\texplicit_mc = explicit_mc.__name__\n\n\t# determine the metaclass\n\tif isinstance(explicit_mc, basestring):\n\t\texplicit_mc = resolveClass(explicit_mc, allowNone=True)\n\n\t# no explicit meta class, so use first base class\n\tif not explicit_mc:\n\t\texplicit_mc = bases[0]\n\n\t# determine the metaclass\n\tif inspect.isclass(explicit_mc):\n\t\texplicit_mc = explicit_mc.__name__\n\n\t# determine the metaclass\n\tif isinstance(explicit_mc, basestring):\n\t\texplicit_mc = resolveClass(explicit_mc, allowNone=True)\n\n\t# no explicit meta class, so use first base class\n\tif not explicit_mc:\n\t\texplicit_mc = bases[0]\n\n\t# determine the metaclass\n\tif inspect.isclass(explicit_mc):\n\t\texplicit_mc = explicit_mc.__name__\n\n\t# determine the metaclass\n\tif isinstance(explicit_mc, basestring):\n\t\texplicit_mc = resolveClass(explicit_mc, allow", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\tif explicit_mc is not None:\n\t\t\treturn explicit_mc\n\t\tfor base in bases:\n\t\t\tif isinstance(base, type):\n\t\t\t\treturn base.__class__\n\t\traise TypeError(\"No metaclass defined\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tdel self[key]\n\t\t\treturn self[key]\n\t\treturn default\n\tdef clear(self):\n\t\t\"\"\"D.clear() -> None.  Remove all items from D.\"\"\"\n\t\tself.__dict__.clear()\n\t\tself.__weakref__ = None # get rid of reference in superclass\n\tdef popitem(self):\n\t\t\"\"\"D.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t  2-tuple; but raise KeyError if D is empty.\n\t\t\"\"\"\n\t\tif not self:\n\t\t\traise KeyError('dictionary is empty')\n\t\tkey = self.keys()[-1]\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn key, value\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\tself[key] = default\n\t\treturn default\n\tdef update(self, E=None):\n\t\t\"\"\"D.update([E]) -> None.  Update D from dict/iterable E.\n\t\tIf E is a dict instance, then update D from E.\n\t\tIf E is a sequence of (", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tdel self[key]\n\t\t\treturn self.data[key]\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\telse:\n\t\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tlink = self._find(key)\n\t\tif link is not None:\n\t\t\troot = self._root\n\t\t\tprev = root.prev\n\t\t\tprev.next = link\n\t\t\tlink.prev = prev\n\t\t\troot.prev = prev.next = link\n\t\t\treturn link.value\n\t\treturn default\n\n\tdef popitem(self, last=True):\n\t\"\"\"\nD.popitem() -> (k, v), remove and return some (key, value) pair\nas a 2-tuple; but raise KeyError if D is empty.\nIf last is true or not specified, the item with highest key is removed,\nelse the item with lowest key is removed.\n\t\"\"\"\n\t\tif not self:\n\t\t\traise KeyError('dictionary is empty')\n\t\troot = self._root\n\t\tif last:\n\t\t\tlink = root.prev\n\t\t\tif root.prev is not root:\n\t\t\t\troot.prev = prev = link.prev\n\t\t\t\tprev.next = root\n\t\t\telse:\n\t\t\t\troot.prev = None\n\t\telse:\n\t\t\tlink = root.next\n\t\t\tif root.next is not root:\n\t\t\t\troot.next = prev = link.next\n\t\t\t\tprev.prev = root\n\t\t\telse:\n\t\t\t\troot.next = None\n\t\treturn link.key, link.value\n\n\tdef popitem(self, last=True):\n\t\"\"\"\n\tD", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tvalue = self[key]\n\t\t\tdel self[key]\n\t\t\treturn value\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\n\tdef setdefault(self, key, default=None):\n\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\tdef update(self, *args, **kwds):\n\t\"\"\"\n\tD.update([E, ]**F) -> None. Update D from dict/iterable E and F.\n\tIf E present and has a.keys() method, does:     for k in E: D[k] = E[k]\n\tIf E present and lacks.keys() method, does:     for (k, v) in E: D[k] = v\n\tIn either case, this is followed by: for k, v in F.items(): D[k] = v\n\t\"\"\"\n\t\tif len(args) > 1:\n\t\t\traise TypeError('update expected at most 1 arguments, got %d' % len(args))\n\t\tother = {}\n\t\tif len(args) == 1:\n\t\t\tother = args[", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tpass\n\tdef popitem(self, last=True):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n2-tuple; but raise KeyError if D is empty.\n\nIf last is true and D is an OrderedDictionnary, return the last key-value\npair; otherwise return the first.\n\t\"\"\"\n\t\tpass\n\tdef setdefault(self, key, default=None):\n\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tpass\n\tdef update(self, E=None, **F):\n\t\"\"\"\nD.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\nIf E is present and has a.keys() method, then does:     for k in E: D[k] = E[k]\nIf E is present and lacks a.keys() method, then does:     for (k, v) in E: D[k] = v\nIn either case, this is followed by: for k in F: D[k] = F[k]\n\t\"\"\"\n\t\tpass\n\tdef values(self):\n\t\"\"\"\nD.values() -> list of D", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tresult = self[key]\n\t\t\tdel self[key]\n\t\t\treturn result\n\t\treturn default\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise KeyError if D is empty.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tkey = self.keys()[-1]\n\t\texcept IndexError:\n\t\t\traise KeyError('dictionary is empty')\n\t\treturn key, self.pop(key)\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tdel self[key]\n\t\t\treturn self[key]\n\t\treturn default\n\tdef clear(self):\n\t\t\"\"\"D.clear() -> None.  Remove all items from D.\"\"\"\n\t\tself.__dict__.clear()\n\t\tself.__weakref__ = None # get rid of reference in superclass\n\tdef popitem(self):\n\t\t\"\"\"D.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t  2-tuple; but raise KeyError if D is empty.\n\t\t\"\"\"\n\t\tif not self:\n\t\t\traise KeyError('dictionary is empty')\n\t\tkey = self.keys()[-1]\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn key, value\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\tself[key] = default\n\t\treturn default\n\tdef update(self, E=None):\n\t\t\"\"\"D.update([E]) -> None.  Update D from dict/iterable E.\n\t\tIf E is a dict instance, then update D from E.\n\t\tIf E is a sequence of (", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tdel self[key]\n\t\t\treturn self.data[key]\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\telse:\n\t\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tlink = self._find(key)\n\t\tif link is not None:\n\t\t\troot = self._root\n\t\t\tprev = root.prev\n\t\t\tprev.next = link\n\t\t\tlink.prev = prev\n\t\t\troot.prev = prev.next = link\n\t\t\treturn link.value\n\t\treturn default\n\n\tdef popitem(self, last=True):\n\t\"\"\"\nD.popitem() -> (k, v), remove and return some (key, value) pair\nas a 2-tuple; but raise KeyError if D is empty.\nIf last is true or not specified, the item with highest key is removed,\nelse the item with lowest key is removed.\n\t\"\"\"\n\t\tif not self:\n\t\t\traise KeyError('dictionary is empty')\n\t\troot = self._root\n\t\tif last:\n\t\t\tlink = root.prev\n\t\t\tif root.prev is not root:\n\t\t\t\troot.prev = prev = link.prev\n\t\t\t\tprev.next = root\n\t\t\telse:\n\t\t\t\troot.prev = None\n\t\telse:\n\t\t\tlink = root.next\n\t\t\tif root.next is not root:\n\t\t\t\troot.next = prev = link.next\n\t\t\t\tprev.prev = root\n\t\t\telse:\n\t\t\t\troot.next = None\n\t\treturn link.key, link.value\n\n\tdef popitem(self, last=True):\n\t\"\"\"\n\tD", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tvalue = self[key]\n\t\t\tdel self[key]\n\t\t\treturn value\n\t\tif default is __marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\n\tdef setdefault(self, key, default=None):\n\t\"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\tdef update(self, *args, **kwds):\n\t\"\"\"\n\tD.update([E, ]**F) -> None. Update D from dict/iterable E and F.\n\tIf E present and has a.keys() method, does:     for k in E: D[k] = E[k]\n\tIf E present and lacks.keys() method, does:     for (k, v) in E: D[k] = v\n\tIn either case, this is followed by: for k, v in F.items(): D[k] = v\n\t\"\"\"\n\t\tif len(args) > 1:\n\t\t\traise TypeError('update expected at most 1 arguments, got %d' % len(args))\n\t\tother = {}\n\t\tif len(args) == 1:\n\t\t\tother = args[", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tpass\n\tdef popitem(self, last=True):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n2-tuple; but raise KeyError if D is empty.\n\nIf last is true and D is an OrderedDictionnary, return the last key-value\npair; otherwise return the first.\n\t\"\"\"\n\t\tpass\n\tdef setdefault(self, key, default=None):\n\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tpass\n\tdef update(self, E=None, **F):\n\t\"\"\"\nD.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\nIf E is present and has a.keys() method, then does:     for k in E: D[k] = E[k]\nIf E is present and lacks a.keys() method, then does:     for (k, v) in E: D[k] = v\nIn either case, this is followed by: for k in F: D[k] = F[k]\n\t\"\"\"\n\t\tpass\n\tdef values(self):\n\t\"\"\"\nD.values() -> list of D", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tresult = self[key]\n\t\t\tdel self[key]\n\t\t\treturn result\n\t\treturn default\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise KeyError if D is empty.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tkey = self.keys()[-1]\n\t\texcept IndexError:\n\t\t\traise KeyError('dictionary is empty')\n\t\treturn key, self.pop(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif not self._items:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\t(key, value) = self._items.popitem()\n\t\treturn (key, value)\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tGet the value for the specified key.\n\t\t\"\"\"\n\t\tif key not in self._items:\n\t\t\traise KeyError(str(key))\n\t\tself._items[key] = (self._items[key][0], self._items[key][1]+1)\n\t\treturn self._items[key][0]\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet the value for the specified key.\n\t\t\"\"\"\n\t\tif key not in self._items:\n\t\t\tself._items[key] = (value, 1)\n\t\telse:\n\t\t\tself._items[key] = (value, self._items[key][1]+1)\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tGet the value for the specified key.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tGet the value for the specified key. If the key is not found,\n\t\tset the key to the default value and return", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif self.curr_size == 0:\n\t\t\traise KeyError(\"popitem(): dictionary is empty\")\n\t\tself.curr_size -= 1\n\t\tleast_used = self.least_used()\n\t\tkey, value = least_used\n\t\tself.used_time[key] = self._curr_time\n\t\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\treturn self.d.popitem()\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tpass\n\n\tdef peekitem(self):\n\t\"\"\"\n\tReturn the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tpass\n\n\tdef __iter__(self):\n\t\"\"\"\n\tIterate over `self.cache_info(type, size, hits, misses, maxsize, currsize)`.\n\t\"\"\"\n\t\tpass\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of items in the cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn `True` if `key` is in the cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of this cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturn a string representation of this cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __reduce__(self):\n\t\"\"\"\n\t\"\"\"\n\t\tpass\n\n\tdef __getstate__(self):\n\t\"\"\"\n\t\"\"\"\n\t\tpass\n\n\tdef __setstate__(self, state):\n\t\"\"\"\n\t\"\"\"\n\t\tpass\n\n\tdef __eq__(self, other):\n\t\"\"\"\n\tReturn true if `self` and `other` are equal.\n\t\"\"\"\n\t\tpass\n\n\tdef __ne__(self, other):\n\t\"\"\"\n\tReturn true if `", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\t# Remove the item with the lowest frequency count and return its key and value.\n\t\t# Remove the key from the frequency map.\n\t\t# Return the key and value.\n\tif self.cache is None:\n\t\treturn None\n\tif len(self.cache) == 0:\n\t\treturn None\n\titem = min(self.cache.items(), key=lambda x: x[1])[0]\n\tdel self.cache[item]\n\treturn item\n\t# print(\"cache: \", self.cache)\n\t# print(\"cache value: \", self.cache[item])\n\t# print(\"item: \", item)\n\t# print(\"item value: \", self.cache[item])\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif not self.cache:\n\t\t\traise KeyError('Cache is empty')\n\t\tkey, value = self.cache.popitem(last=False)\n\t\tdel self.frequency[key]\n\t\treturn key, value\n\n\tdef __str__(self):\n\t\treturn str(self.frequency)\n\n\tdef __repr__(self):\n\t\treturn str(self.frequency)\n\n\nif __name__ == '__main__':\n\tcache = LFUCache(3)\n\tcache.put(1, 1)\n\tcache.put(2, 2)\n\tcache.put(3, 3)\n\tprint(cache.get(2))\n\tcache.put(4, 4)\n\tprint(cache.get(1))\n\tprint(cache.get(3))\n\tprint(cache.get(4))\n\tprint(cache)\n\n\tcache = LFUCache(2)\n\tcache.put(1, 1)\n\tcache.put(2, 2)\n\tprint(cache.get(1))\n\tcache.put(3, 3)\n\tprint(cache.get(2))\n\tprint(cache.get(3))\n\tcache.put(4, 4)\n\tprint(cache.get(1))\n\tprint(cache.get(3))\n\tprint(", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\treturn self.cache.popitem()\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\n\t# Remove key with the lowest usage count.\n\t# If there is a tie, remove the item with the lowest key.\n\t#\n\t# You can access the least used item by using statements of the form cache.min().\n\t#\n\t# It is guaranteed that a key is always within the scope of the class.\n\t#\n\t# You can access the most used item by using statements of the form cache.max().\n\t#\n\t# It is guaranteed that a key is always within the scope of the class.\n\t#\n\t# You can remove an item from the cache by using statements of the form cache.remove(key).\n\t#\n\t# It is guaranteed that a key is always within the scope of the class.\n\t#\n\t# Your code should not remove items if the cache is empty.\n\t#\n\t# The most recently used items are stored at the end of the list.\n\t# The least recently used items are stored at the beginning of the list.\n\t#\n\t# For example, you can keep track of the usage count of each key by creating a dictionary\n\t# mapping each key to the number of times it has been accessed.\n\t#\n\t# You can then store each key along with its usage count in a list.\n\t#\n\t# When you access a key, you can increment the count for that key.", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\t# use a temporary list to store the popped value, since we'll be doing\n\t\t# multiple pop operations\n\tif self.size == 0:\n\t\traise KeyError(\"popitem(): dictionary is empty\")\n\telse:\n\t\tindex = self._get_least_used()\n\t\tkey = self.keys[index]\n\t\tvalue = self.values[index]\n\t\tself.pop(key)\n\t\treturn key, value\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif not self._items:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\t(key, value) = self._items.popitem()\n\t\treturn (key, value)\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tGet the value for the specified key.\n\t\t\"\"\"\n\t\tif key not in self._items:\n\t\t\traise KeyError(str(key))\n\t\tself._items[key] = (self._items[key][0], self._items[key][1]+1)\n\t\treturn self._items[key][0]\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet the value for the specified key.\n\t\t\"\"\"\n\t\tif key not in self._items:\n\t\t\tself._items[key] = (value, 1)\n\t\telse:\n\t\t\tself._items[key] = (value, self._items[key][1]+1)\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tGet the value for the specified key.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tGet the value for the specified key. If the key is not found,\n\t\tset the key to the default value and return", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif self.curr_size == 0:\n\t\t\traise KeyError(\"popitem(): dictionary is empty\")\n\t\tself.curr_size -= 1\n\t\tleast_used = self.least_used()\n\t\tkey, value = least_used\n\t\tself.used_time[key] = self._curr_time\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\treturn self.d.popitem()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tpass\n\n\tdef peekitem(self):\n\t\"\"\"\n\tReturn the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tpass\n\n\tdef __iter__(self):\n\t\"\"\"\n\tIterate over `self.cache_info(type, size, hits, misses, maxsize, currsize)`.\n\t\"\"\"\n\t\tpass\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of items in the cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn `True` if `key` is in the cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of this cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturn a string representation of this cache.\n\t\"\"\"\n\t\tpass\n\n\tdef __reduce__(self):\n\t\"\"\"\n\t\"\"\"\n\t\tpass\n\n\tdef __getstate__(self):\n\t\"\"\"\n\t\"\"\"\n\t\tpass\n\n\tdef __setstate__(self, state):\n\t\"\"\"\n\t\"\"\"\n\t\tpass\n\n\tdef __eq__(self, other):\n\t\"\"\"\n\tReturn true if `self` and `other` are equal.\n\t\"\"\"\n\t\tpass\n\n\tdef __ne__(self, other):\n\t\"\"\"\n\tReturn true if `", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\t# Remove the item with the lowest frequency count and return its key and value.\n\t\t# Remove the key from the frequency map.\n\t\t# Return the key and value.\n\tif self.cache is None:\n\t\treturn None\n\tif len(self.cache) == 0:\n\t\treturn None\n\titem = min(self.cache.items(), key=lambda x: x[1])[0]\n\tdel self.cache[item]\n\treturn item\n\t# print(\"cache: \", self.cache)\n\t# print(\"cache value: \", self.cache[item])\n\t# print(\"item: \", item)\n\t# print(\"item value: \", self.cache[item])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif not self.cache:\n\t\t\traise KeyError('Cache is empty')\n\t\tkey, value = self.cache.popitem(last=False)\n\t\tdel self.frequency[key]\n\t\treturn key, value\n\n\tdef __str__(self):\n\t\treturn str(self.frequency)\n\n\tdef __repr__(self):\n\t\treturn str(self.frequency)\n\n\nif __name__ == '__main__':\n\tcache = LFUCache(3)\n\tcache.put(1, 1)\n\tcache.put(2, 2)\n\tcache.put(3, 3)\n\tprint(cache.get(2))\n\tcache.put(4, 4)\n\tprint(cache.get(1))\n\tprint(cache.get(3))\n\tprint(cache.get(4))\n\tprint(cache)\n\n\tcache = LFUCache(2)\n\tcache.put(1, 1)\n\tcache.put(2, 2)\n\tprint(cache.get(1))\n\tcache.put(3, 3)\n\tprint(cache.get(2))\n\tprint(cache.get(3))\n\tcache.put(4, 4)\n\tprint(cache.get(1))\n\tprint(cache.get(3))\n\tprint(", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\treturn self.cache.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\n\t# Remove key with the lowest usage count.\n\t# If there is a tie, remove the item with the lowest key.\n\t#\n\t# You can access the least used item by using statements of the form cache.min().\n\t#\n\t# It is guaranteed that a key is always within the scope of the class.\n\t#\n\t# You can access the most used item by using statements of the form cache.max().\n\t#\n\t# It is guaranteed that a key is always within the scope of the class.\n\t#\n\t# You can remove an item from the cache by using statements of the form cache.remove(key).\n\t#\n\t# It is guaranteed that a key is always within the scope of the class.\n\t#\n\t# Your code should not remove items if the cache is empty.\n\t#\n\t# The most recently used items are stored at the end of the list.\n\t# The least recently used items are stored at the beginning of the list.\n\t#\n\t# For example, you can keep track of the usage count of each key by creating a dictionary\n\t# mapping each key to the number of times it has been accessed.\n\t#\n\t# You can then store each key along with its usage count in a list.\n\t#\n\t# When you access a key, you can increment the count for that key.", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\t# use a temporary list to store the popped value, since we'll be doing\n\t\t# multiple pop operations\n\tif self.size == 0:\n\t\traise KeyError(\"popitem(): dictionary is empty\")\n\telse:\n\t\tindex = self._get_least_used()\n\t\tkey = self.keys[index]\n\t\tvalue = self.values[index]\n\t\tself.pop(key)\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\tkey = self.lru.pop(0)\n\t\tself.cache.pop(key)\n\t\treturn key, self[key]\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif self.size == 0:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\tkey, value = self.table[self.tail.prev.key]\n\t\tself.remove(key)\n\t\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif self.currsize == 0:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\titem = self._items.popitem()\n\t\tself.currsize -= 1\n\t\treturn item\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif self.items:\n\t\t\tkey, value = self.items.popitem(last=False)\n\t\t\tdel self.cache[key]\n\t\t\treturn key, value\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tRetrieve an item from the cache and move it to the end of the queue.\n\t\"\"\"\n\t\tself.cache.move_to_end(key)\n\t\treturn self.cache[key]\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSet the value for a key. If the key is already in the cache, move it to the end of the queue.\n\t\"\"\"\n\t\tself.cache[key] = value\n\t\tself.items[key] = value\nif __name__ == '__main__':\n\tcache = LRUCache(2)\n\tcache.set(1, 1)\n\tcache.set(2, 2)\n\tprint(cache.get(1))#1\n\tcache.set(3, 3)\n\tprint(cache.get(2))#-1\n\tprint(cache.items)\n\tprint(cache.cache)\n\tcache.set(4, 4)\n\tprint(cache.get(1))#-1\n\tprint(cache.cache)\n\tprint(cache.items)\n\tprint(cache.", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self._queue:\n\t\t\traise KeyError(self)\n\t\tk, v = self._queue.popleft()\n\t\tdel self._keymap[k]\n\t\treturn k, v\n\tdef popleft(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tif not self._queue:\n\t\t\traise KeyError(self)\n\t\tk, v = self._queue.pop()\n\t\tdel self._keymap[k]\n\t\treturn k, v\n\tdef move_to_end(self, key):\n\t\"\"\"\n\tMove an existing element to the end of the deque.\n\t\"\"\"\n\t\tif key not in self._keymap:\n\t\t\traise KeyError(self, key)\n\t\tself._queue.remove((key, self._keymap[key]))\n\t\tself._queue.append((key, self._keymap[key]))\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of the deque.\n\t\"\"\"\n\t\treturn 'OrderedDict(%r)' % (list(self.items()),)\n\tdef __reduce__(self):\n\t\"\"\"\n\tReturn state information for pickling.\n\t\"\"\"\n\t\treturn (\n\t\t\tOrderedDict,\n\t\t\t(\n\t\t\t\tlist(self.items()),\n\t\t\t)\n\t\t)\n\tdef __setstate__(self, state):\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\told = self.last.prev\n\t\tself._remove(old)\n\t\treturn (old.key, old.value)\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self.items:\n\t\t\traise KeyError\n\tkey, item = self.items.popitem(last=False)\n\tself.cache.pop(key)\n\treturn (key, item)\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\treturn self.list.pop()\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of (key, value) pairs.\n\t\"\"\"\n\t\treturn len(self.list)\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn `True` if `key` is a key of one of the stored\n\t`(key, value)` pairs.\n\t\"\"\"\n\t\treturn key in self.dict\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDelete the `(key, value)` pair associated with `key`.\n\t\"\"\"\n\t\tself.dict.pop(key)\n\t\tself.list.remove((key, self.dict[key]))\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of the cache.\n\t\"\"\"\n\t\treturn str(self.dict)\n\ncache = LRUCache(2)\ncache.put(1, 1)\ncache.put(2, 2)\ncache.get(1)\ncache.put(3, 3)\ncache.get(2)\ncache.put(4, 4)\ncache.get(1)\ncache.get(3)\ncache.get(4)\n\t\tif root.val > val:\n\t\t\treturn self.find(root.left, val)\n\t\telse"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\tkey = self.lru.pop(0)\n\t\tself.cache.pop(key)\n\t\treturn key, self[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif self.size == 0:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\tkey, value = self.table[self.tail.prev.key]\n\t\tself.remove(key)\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif self.currsize == 0:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\titem = self._items.popitem()\n\t\tself.currsize -= 1\n\t\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif self.items:\n\t\t\tkey, value = self.items.popitem(last=False)\n\t\t\tdel self.cache[key]\n\t\t\treturn key, value\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tRetrieve an item from the cache and move it to the end of the queue.\n\t\"\"\"\n\t\tself.cache.move_to_end(key)\n\t\treturn self.cache[key]\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSet the value for a key. If the key is already in the cache, move it to the end of the queue.\n\t\"\"\"\n\t\tself.cache[key] = value\n\t\tself.items[key] = value\nif __name__ == '__main__':\n\tcache = LRUCache(2)\n\tcache.set(1, 1)\n\tcache.set(2, 2)\n\tprint(cache.get(1))#1\n\tcache.set(3, 3)\n\tprint(cache.get(2))#-1\n\tprint(cache.items)\n\tprint(cache.cache)\n\tcache.set(4, 4)\n\tprint(cache.get(1))#-1\n\tprint(cache.cache)\n\tprint(cache.items)\n\tprint(cache.", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self._queue:\n\t\t\traise KeyError(self)\n\t\tk, v = self._queue.popleft()\n\t\tdel self._keymap[k]\n\t\treturn k, v\n\tdef popleft(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tif not self._queue:\n\t\t\traise KeyError(self)\n\t\tk, v = self._queue.pop()\n\t\tdel self._keymap[k]\n\t\treturn k, v\n\tdef move_to_end(self, key):\n\t\"\"\"\n\tMove an existing element to the end of the deque.\n\t\"\"\"\n\t\tif key not in self._keymap:\n\t\t\traise KeyError(self, key)\n\t\tself._queue.remove((key, self._keymap[key]))\n\t\tself._queue.append((key, self._keymap[key]))\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of the deque.\n\t\"\"\"\n\t\treturn 'OrderedDict(%r)' % (list(self.items()),)\n\tdef __reduce__(self):\n\t\"\"\"\n\tReturn state information for pickling.\n\t\"\"\"\n\t\treturn (\n\t\t\tOrderedDict,\n\t\t\t(\n\t\t\t\tlist(self.items()),\n\t\t\t)\n\t\t)\n\tdef __setstate__(self, state):\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\told = self.last.prev\n\t\tself._remove(old)\n\t\treturn (old.key, old.value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self.items:\n\t\t\traise KeyError\n\tkey, item = self.items.popitem(last=False)\n\tself.cache.pop(key)\n\treturn (key, item)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\treturn self.list.pop()\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of (key, value) pairs.\n\t\"\"\"\n\t\treturn len(self.list)\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn `True` if `key` is a key of one of the stored\n\t`(key, value)` pairs.\n\t\"\"\"\n\t\treturn key in self.dict\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDelete the `(key, value)` pair associated with `key`.\n\t\"\"\"\n\t\tself.dict.pop(key)\n\t\tself.list.remove((key, self.dict[key]))\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of the cache.\n\t\"\"\"\n\t\treturn str(self.dict)\n\ncache = LRUCache(2)\ncache.put(1, 1)\ncache.put(2, 2)\ncache.get(1)\ncache.put(3, 3)\ncache.get(2)\ncache.put(4, 4)\ncache.get(1)\ncache.get(3)\ncache.get(4)\n\t\tif root.val > val:\n\t\t\treturn self.find(root.left, val)\n\t\telse", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\t# if the cache is empty, return null\n\t\tif self.head is None:\n\t\t\treturn None\n\n\t\t# store the key and value of the last node\n\t\t# in the cache and remove it from the cache\n\t\tkey = self.head.key\n\t\tvalue = self.head.value\n\t\tself.remove(key)\n\n\t\t# return the key and value\n\t\treturn (key, value)\n\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\t\"Remove the key and value pair with the given key from the cache.\n\t\t\"\"\"\n\t\t# search for the node with the given key\n\t\tnode = self.head\n\t\twhile node is not None:\n\t\t\tif node.key == key:\n\t\t\t\tbreak\n\t\t\tnode = node.next\n\n\t\t# if the key is not found, return null\n\t\tif node is None:\n\t\t\treturn None\n\n\t\t# remove the node from the cache\n\t\tif self.head == node:\n\t\t\tself.head = self.head.next\n\t\telse:\n\t\t\tprev = None\n\t\t\twhile prev is not None:\n\t\t\t\tif prev.next == node:\n\t\t\t\t\tprev.next = prev.next.next\n\t\t\t\t\tbreak\n\t\t\t\tprev = prev.next\n\n\t\t# decrement the size of the cache\n\t\tself.size -= 1\n\n\t\t# return the value of the removed node\n\t\treturn node.value\n\n\t", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\treturn self.get_pair()\n\t\tself._pop_pair()\n\tdef _pop_pair(self):\n\t\tself.cache.popitem()\n\tdef __setitem__(self, k, v):\n\t\tself.cache[k] = v\n\tdef __getitem__(self, k):\n\t\treturn self.cache[k]\n\tdef __delitem__(self, k):\n\t\tself.cache.pop(k)\n\tdef __str__(self):\n\t\treturn str(self.cache)\n\tdef __repr__(self):\n\t\treturn repr(self.cache)\n\tdef __len__(self):\n\t\treturn len(self.cache)\n\tdef __contains__(self, k):\n\t\treturn k in self.cache\n\tdef __iter__(self):\n\t\treturn iter(self.cache)\n\tdef __reversed__(self):\n\t\treturn reversed(self.cache)\n\tdef clear(self):\n\t\tself.cache.clear()\n\tdef keys(self):\n\t\treturn self.cache.keys()\n\tdef values(self):\n\t\treturn self.cache.values()\n\tdef items(self):\n\t\treturn self.cache.items()\n\tdef pop(self, k, d=None):\n\t\treturn self.cache.pop(k, d)\n\tdef get(self, k, d", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\traise NotImplementedError()\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\treturn self.__dict__.popitem()\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\tself._cache = None\n\t\treturn self.cache.popitem()\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\t\"Clears the cache.\n\t\t\"\"\"\n\t\tself._cache = None\n\t\tself.cache.clear()\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\tpass\n#----------------------------------------------------------------------", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\treturn self.popitem()\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\titem = self.popitem(last=False)\n\t\treturn item\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\t\"Iterate over the keys of the dict.\n\t\t\"\"\"\n\t\treturn iter(self.keys())\n\n\tdef __reversed__(self):\n\t\t\"\"\"\n\t\t\"Return a reverse iterator over the keys of the dict.\n\t\t\"\"\"\n\t\treturn reversed(self.keys())\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\t\"Return the item of the dict with key.\n\t\t\"\"\"\n\t\treturn self.get(key)\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\t\"Set self[key] to value.\n\t\t\"\"\"\n\t\tself.set(key, value)\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\t\"Remove the item of the dict with key.\n\t\t\"\"\"\n\t\tself.remove(key)\n\n\tdef __missing__(self, key):\n\t\t\"\"\"\n\t\t\"Called if the requested key is not found.\n\t\t\"\"\"\n\t\tif self.default_factory is None:\n\t\t\traise KeyError(key)\n\t\tself[key] = value = self.default_factory()\n\t\treturn value\n\n\tdef __reduce__(self):\n\t\t\"\"\"\n\t\t\"Return state information for pickling.\n\t\t\"\"\"\n\t\titems = [[k, self[k"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\t# if the cache is empty, return null\n\t\tif self.head is None:\n\t\t\treturn None\n\n\t\t# store the key and value of the last node\n\t\t# in the cache and remove it from the cache\n\t\tkey = self.head.key\n\t\tvalue = self.head.value\n\t\tself.remove(key)\n\n\t\t# return the key and value\n\t\treturn (key, value)\n\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\t\"Remove the key and value pair with the given key from the cache.\n\t\t\"\"\"\n\t\t# search for the node with the given key\n\t\tnode = self.head\n\t\twhile node is not None:\n\t\t\tif node.key == key:\n\t\t\t\tbreak\n\t\t\tnode = node.next\n\n\t\t# if the key is not found, return null\n\t\tif node is None:\n\t\t\treturn None\n\n\t\t# remove the node from the cache\n\t\tif self.head == node:\n\t\t\tself.head = self.head.next\n\t\telse:\n\t\t\tprev = None\n\t\t\twhile prev is not None:\n\t\t\t\tif prev.next == node:\n\t\t\t\t\tprev.next = prev.next.next\n\t\t\t\t\tbreak\n\t\t\t\tprev = prev.next\n\n\t\t# decrement the size of the cache\n\t\tself.size -= 1\n\n\t\t# return the value of the removed node\n\t\treturn node.value\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\treturn self.get_pair()\n\t\tself._pop_pair()\n\tdef _pop_pair(self):\n\t\tself.cache.popitem()\n\tdef __setitem__(self, k, v):\n\t\tself.cache[k] = v\n\tdef __getitem__(self, k):\n\t\treturn self.cache[k]\n\tdef __delitem__(self, k):\n\t\tself.cache.pop(k)\n\tdef __str__(self):\n\t\treturn str(self.cache)\n\tdef __repr__(self):\n\t\treturn repr(self.cache)\n\tdef __len__(self):\n\t\treturn len(self.cache)\n\tdef __contains__(self, k):\n\t\treturn k in self.cache\n\tdef __iter__(self):\n\t\treturn iter(self.cache)\n\tdef __reversed__(self):\n\t\treturn reversed(self.cache)\n\tdef clear(self):\n\t\tself.cache.clear()\n\tdef keys(self):\n\t\treturn self.cache.keys()\n\tdef values(self):\n\t\treturn self.cache.values()\n\tdef items(self):\n\t\treturn self.cache.items()\n\tdef pop(self, k, d=None):\n\t\treturn self.cache.pop(k, d)\n\tdef get(self, k, d", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\traise NotImplementedError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\treturn self.__dict__.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\tself._cache = None\n\t\treturn self.cache.popitem()\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\t\"Clears the cache.\n\t\t\"\"\"\n\t\tself._cache = None\n\t\tself.cache.clear()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\tpass\n#----------------------------------------------------------------------", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\treturn self.popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\titem = self.popitem(last=False)\n\t\treturn item\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\t\"Iterate over the keys of the dict.\n\t\t\"\"\"\n\t\treturn iter(self.keys())\n\n\tdef __reversed__(self):\n\t\t\"\"\"\n\t\t\"Return a reverse iterator over the keys of the dict.\n\t\t\"\"\"\n\t\treturn reversed(self.keys())\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\t\"Return the item of the dict with key.\n\t\t\"\"\"\n\t\treturn self.get(key)\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\t\"Set self[key] to value.\n\t\t\"\"\"\n\t\tself.set(key, value)\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\t\"Remove the item of the dict with key.\n\t\t\"\"\"\n\t\tself.remove(key)\n\n\tdef __missing__(self, key):\n\t\t\"\"\"\n\t\t\"Called if the requested key is not found.\n\t\t\"\"\"\n\t\tif self.default_factory is None:\n\t\t\traise KeyError(key)\n\t\tself[key] = value = self.default_factory()\n\t\treturn value\n\n\tdef __reduce__(self):\n\t\t\"\"\"\n\t\t\"Return state information for pickling.\n\t\t\"\"\"\n\t\titems = [[k, self[k", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.__choice(self.__dict.items())\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.pop(self.__choice(self))\n\tdef popleft(self):\n\t\"\"\"\n\tFind, remove and return a random item in the class\n\t\"\"\"\n\t\treturn self.pop(self.__choice(self))\n\tdef remove(self, key):\n\t\"\"\"\n\tRemove the item specified by the given key\n\t\"\"\"\n\t\treturn self.pop(key)\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tFind the item with the specified key and return it\n\t\"\"\"\n\t\treturn self[key]\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tAdd the item specified by the given key\n\t\"\"\"\n\t\tself[key] = value\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tRemove the item specified by the given key\n\t\"\"\"\n\t\tdel self[key]\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn an iterator over the keys in the class\n\t\"\"\"\n\t\treturn self.keys().__iter__()\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of items in the class\n\t\"\"\"\n\t\treturn self.keys().__len__()\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn whether the specified key is in the class\n\t\"\"\"\n\t\treturn self.has_key(key)\n\tdef", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.__choice(self.data.items())\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tkey, value = choice(self._store.items())\n\t\tdel self._store[key]\n\t\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tkey, value = self.__choice()\n\t\tdel self.data[key]\n\t\treturn key, value\n#end class\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tif self.is_empty():\n\t\t\traise IndexError('popitem: dictionary is empty')\n\t\telse:\n\t\t\tkey, value = self.__choice()\n\t\t\tdel self[key]\n\t\t\treturn key, value\n\tdef clear(self):\n\t\t\"\"\"\n\t\tRemove and return all `(key, value)` pairs via __popitem in the class\n\t\t\"\"\"\n\t\twhile not self.is_empty():\n\t\t\tself.__popitem()\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tself.index = self.__choice(len(self.keys))\n\t\tkey, value = self.keys[self.index], self.values[self.index]\n\t\tself.keys.pop(self.index)\n\t\tself.values.pop(self.index)\n\t\treturn key, value\n\n\tdef remove(self, key):\n\t\"\"\"\n\tRemove the key and its corresponding value\n\t\"\"\"\n\t\tif key in self.keys:\n\t\tself.index = self.keys.index(key)\n\t\tself.keys.pop(self.index)\n\t\tself.values.pop(self.index)\n\t\treturn None\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the length of the dictionary\n\t\"\"\"\n\t\treturn len(self.keys)\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn a randomized iterator over the dictionary\n\t\"\"\"\n\t\treturn RandomDictIterator(self.keys)\n\nif __name__ == '__main__':\n\ta = RandomDict()\n\ta.add(1, 'one')\n\ta.add(2, 'two')\n\ta.add(3, 'three')\n\ta.add(4, 'four')\n\ta.add(5, 'five')\n\ta.add(6,'six')\n\ta.add(7, '", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t/Python/Pytorch/NeuralNetwork.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tif not self.has_keys():\n\t\t\traise KeyError(\"popitem(): dictionary is empty\")\n\t\tkey = self.keys.__choice()\n\t\treturn self.pop(key)\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.__class__._choice(self)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.__choice(self.__dict.items())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.pop(self.__choice(self))\n\tdef popleft(self):\n\t\"\"\"\n\tFind, remove and return a random item in the class\n\t\"\"\"\n\t\treturn self.pop(self.__choice(self))\n\tdef remove(self, key):\n\t\"\"\"\n\tRemove the item specified by the given key\n\t\"\"\"\n\t\treturn self.pop(key)\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tFind the item with the specified key and return it\n\t\"\"\"\n\t\treturn self[key]\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tAdd the item specified by the given key\n\t\"\"\"\n\t\tself[key] = value\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tRemove the item specified by the given key\n\t\"\"\"\n\t\tdel self[key]\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn an iterator over the keys in the class\n\t\"\"\"\n\t\treturn self.keys().__iter__()\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of items in the class\n\t\"\"\"\n\t\treturn self.keys().__len__()\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturn whether the specified key is in the class\n\t\"\"\"\n\t\treturn self.has_key(key)\n\tdef", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.__choice(self.data.items())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tkey, value = choice(self._store.items())\n\t\tdel self._store[key]\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tkey, value = self.__choice()\n\t\tdel self.data[key]\n\t\treturn key, value\n#end class\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tif self.is_empty():\n\t\t\traise IndexError('popitem: dictionary is empty')\n\t\telse:\n\t\t\tkey, value = self.__choice()\n\t\t\tdel self[key]\n\t\t\treturn key, value\n\tdef clear(self):\n\t\t\"\"\"\n\t\tRemove and return all `(key, value)` pairs via __popitem in the class\n\t\t\"\"\"\n\t\twhile not self.is_empty():\n\t\t\tself.__popitem()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tself.index = self.__choice(len(self.keys))\n\t\tkey, value = self.keys[self.index], self.values[self.index]\n\t\tself.keys.pop(self.index)\n\t\tself.values.pop(self.index)\n\t\treturn key, value\n\n\tdef remove(self, key):\n\t\"\"\"\n\tRemove the key and its corresponding value\n\t\"\"\"\n\t\tif key in self.keys:\n\t\tself.index = self.keys.index(key)\n\t\tself.keys.pop(self.index)\n\t\tself.values.pop(self.index)\n\t\treturn None\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the length of the dictionary\n\t\"\"\"\n\t\treturn len(self.keys)\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn a randomized iterator over the dictionary\n\t\"\"\"\n\t\treturn RandomDictIterator(self.keys)\n\nif __name__ == '__main__':\n\ta = RandomDict()\n\ta.add(1, 'one')\n\ta.add(2, 'two')\n\ta.add(3, 'three')\n\ta.add(4, 'four')\n\ta.add(5, 'five')\n\ta.add(6,'six')\n\ta.add(7, '", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t/Python/Pytorch/NeuralNetwork.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\tif not self.has_keys():\n\t\t\traise KeyError(\"popitem(): dictionary is empty\")\n\t\tkey = self.keys.__choice()\n\t\treturn self.pop(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\treturn self.__class__._choice(self)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'\\{(?P<name>[^\\}\\s]+)\\}|\\{(?P<name>[^\\}\\s]+)\\:[^\\}\\s]+\\}')\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"\"\"(?P<name>\\w+)\\s*=\\s*(?P<value>.*?)\"\"\", re.X)\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"\"\"(?P<name>\\w+)\\s*=\\s*(?P<value>.*?)\"\"\", re.X)\n\tdef _get_in_parameters(self, parameters) -> Dict[str, str]:\n\t\"\"\"\n\tGet the in-style parameters.\n\nReturns the in-style parameters (:class:`dict`).\n\t\"\"\"\n\t\treturn self._get_parameters(self._in_regex, parameters)\n\tdef _get_out_parameters(self, parameters) -> Dict[str, str]:\n\t\"\"\"\n\tGet the out-style parameters.\n\nReturns the out-style parameters (:class:`dict`).\n\t\"\"\"\n\t\treturn self._get_parameters(self._out_regex, parameters)\n\tdef _get_parameters(self, regex, parameters) -> Dict[str, str]:\n\t\"\"\"\n\tGet the parameters.\n\nReturns the parameters (:class:`dict`).\n\t\"\"\"\n\t\treturn {\n\t\t\tname: value\n\t\t\t", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(\\s+)(--)?(%s)(\\s+)(\\S+)' % self._param_name)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(\\s+)(--)?(%s)(\\s+)(.*)' % self._param_name)\n\n\tdef _parse_in_param(self, param) -> str:\n\t\"\"\"\n\tParse the in-style parameter.\n\nArgs:\n\tparam (str): The parameter to parse.\n\nReturns the parsed parameter (str).\n\t\"\"\"\n\t\treturn self._create_in_regex().sub(r'\\1\\2\\3\\4\\5', param)\n\n\tdef _parse_out_param(self, param) -> str:\n\t\"\"\"\n\tParse the out-style parameter.\n\nArgs:\n\tparam (str): The parameter to parse.\n\nReturns the parsed parameter (str).\n\t\"\"\"\n\t\treturn self._create_out_regex().sub(r'\\1\\2\\3\\4\\5', param)\n\n\tdef __call__(self, param: str) -> str:\n\t\"\"\"\n\tParse a", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"(?P<key>[A-Za-z0-9_=]+)(?P<sep>[:=])(?P<value>[A-Za-z0-9_.]+)\")\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(\n\t\t\tf'in\\\\[\\\\s*({self._type_regex})\\\\s*,\\\\s*'\n\t\t\tf'({self._type_regex})\\\\s*\\\\]'\n\t\t)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(\n\t\t\tf'out\\\\[\\\\s*({self._type_regex})\\\\s*,\\\\s*'\n\t\t\tf'({self._type_regex})\\\\s*\\\\]'\n\t\t)\n\n\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tin_regex = r'(?:'\n\t\tfor key in self.in_params:\n\t\t\tin_regex += re.escape(key) + r'(?:=([^\\s\\|]+))?'\n\t\t\tif key!= self.in_params[-1]:\n\t\t\t\tin_regex += r'\\|'\n\t\t\telse:\n\t\t\t\tin_regex += r')?'\n\t\tin_regex += r'(?:\\s+(.*))?'\n\t\treturn re.compile(in_regex, re.VERBOSE)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tout_regex = r'(?:'\n\t\tfor key in self.out_params:\n\t\t\tout_regex += re.escape(key) + r'(?:=([^\\s\\|]+))?'\n\t\t\tif key!= self.out_params[-1]:\n\t\t\t\tout_regex += r'\\|'\n\t\t\telse:\n\t\t\t\tout_regex += r')?'\n\t\tout_regex += r'(?:\\s+(.*))?'\n\t\treturn re.compile(out_regex, re.VERBOSE)\n\n\tdef _get_in_params(self, in_str: str) -> dict:\n\t\"\"\"\n\tGet the input parameters", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<key>[{0}][{1}]+)(?P<value>{2})'.format(\n\t\t\tself.key_separator,\n\t\t\tself.key_separator,\n\t\t\tself.value_separator))\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<key>{0}[{1}]+{0})(?P<value>{2})'.format(\n\t\t\tself.key_separator,\n\t\t\tself.key_separator,\n\t\t\tself.value_separator))\n\tdef _create_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the regular expression.\n\nReturns the regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<in>{0})|(?P<out>{1})'.format(\n\t\t\tself._create_in_regex(),\n\t\t\tself._create_out_regex()))\n\tdef _split_parameters(self, parameters: str) -> Dict[str, str]:\n\t\"\"\"\n\tSplit the parameters.\n\nParameters\n----------\nparameters : :obj:`str`\n\tThe parameters.\n\nReturns\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<in_param>^.*(?:\\s|$))(?P<in_val>' + self.in_val_regex + r')(?P<in_param_end>.*)$')\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn re.compile(r'(?P<out_param>^.*)(?P<out_val>' + self.out_val_regex + r')(?P<out_param_end>.*$)')\n\n\tdef _create_in_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\tCreate the in-out-style parameter regular expression.\n\nReturns the in-out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn re.compile(r'(?P<in_out_param>^.*)(?P<in_out_val>' + self.in_out_val_regex + r')(?P<in_out_param_end>.*$)')\n\n\tdef _create_val_regex(self, val_type: str) -> str:\n\t\t\"\"\"\n\tCreate the value regular expression.\n\nParameters\n----------\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tregex_str = r'(?P<name>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & InFlags.OPTIONAL:\n\t\t\tregex_str += r'\\s*\\=(?P<default>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & InFlags.MULTI:\n\t\t\tregex_str += r'\\s*\\*(?P<name2>[\\w\\-\\.\\[\\]]+)'\n\t\tregex_str += r'\\s*$'\n\t\treturn re.compile(regex_str, flags=re.I)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tregex_str = r'(?P<name>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & OutFlags.OPTIONAL:\n\t\t\tregex_str += r'\\s*\\=(?P<default>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & OutFlags.MULTI:\n\t\t\tregex_str += r'\\s*\\*(?P<name2>[\\w\\-\\.\\[\\]]+)'\n\t\tregex_str += r'\\s*$'\n\t\treturn re.compile(regex_str, flags=re.I)\n\n\t", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"^\\s*\" + re.escape(self._in_regex))\n\n\t@property\n\tdef out_regex(self) -> str:\n\t\"\"\"\n\tGet the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (string).\n\t\"\"\"\n\t\treturn self._out_regex\n\n\t@property\n\tdef out_regex_compiled(self) -> Pattern:\n\t\"\"\"\n\tGet the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn self._out_regex_compiled\n\n\t@property\n\tdef in_regex(self) -> str:\n\t\"\"\"\n\tGet the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (string).\n\t\"\"\"\n\t\treturn self._in_regex\n\n\t@property\n\tdef in_regex_compiled(self) -> Pattern:\n\t\"\"\"\n\tGet the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn self._in_regex_compiled\n\n\t@property\n\tdef in_regex_compiled_no_whitespace(self) -> Pattern:\n\t\"\"\"\n\tGet the in-style parameter regular expression without whitespace.\n\nReturns the in-style parameter"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'\\{(?P<name>[^\\}\\s]+)\\}|\\{(?P<name>[^\\}\\s]+)\\:[^\\}\\s]+\\}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"\"\"(?P<name>\\w+)\\s*=\\s*(?P<value>.*?)\"\"\", re.X)\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"\"\"(?P<name>\\w+)\\s*=\\s*(?P<value>.*?)\"\"\", re.X)\n\tdef _get_in_parameters(self, parameters) -> Dict[str, str]:\n\t\"\"\"\n\tGet the in-style parameters.\n\nReturns the in-style parameters (:class:`dict`).\n\t\"\"\"\n\t\treturn self._get_parameters(self._in_regex, parameters)\n\tdef _get_out_parameters(self, parameters) -> Dict[str, str]:\n\t\"\"\"\n\tGet the out-style parameters.\n\nReturns the out-style parameters (:class:`dict`).\n\t\"\"\"\n\t\treturn self._get_parameters(self._out_regex, parameters)\n\tdef _get_parameters(self, regex, parameters) -> Dict[str, str]:\n\t\"\"\"\n\tGet the parameters.\n\nReturns the parameters (:class:`dict`).\n\t\"\"\"\n\t\treturn {\n\t\t\tname: value\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(\\s+)(--)?(%s)(\\s+)(\\S+)' % self._param_name)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(\\s+)(--)?(%s)(\\s+)(.*)' % self._param_name)\n\n\tdef _parse_in_param(self, param) -> str:\n\t\"\"\"\n\tParse the in-style parameter.\n\nArgs:\n\tparam (str): The parameter to parse.\n\nReturns the parsed parameter (str).\n\t\"\"\"\n\t\treturn self._create_in_regex().sub(r'\\1\\2\\3\\4\\5', param)\n\n\tdef _parse_out_param(self, param) -> str:\n\t\"\"\"\n\tParse the out-style parameter.\n\nArgs:\n\tparam (str): The parameter to parse.\n\nReturns the parsed parameter (str).\n\t\"\"\"\n\t\treturn self._create_out_regex().sub(r'\\1\\2\\3\\4\\5', param)\n\n\tdef __call__(self, param: str) -> str:\n\t\"\"\"\n\tParse a", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"(?P<key>[A-Za-z0-9_=]+)(?P<sep>[:=])(?P<value>[A-Za-z0-9_.]+)\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(\n\t\t\tf'in\\\\[\\\\s*({self._type_regex})\\\\s*,\\\\s*'\n\t\t\tf'({self._type_regex})\\\\s*\\\\]'\n\t\t)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(\n\t\t\tf'out\\\\[\\\\s*({self._type_regex})\\\\s*,\\\\s*'\n\t\t\tf'({self._type_regex})\\\\s*\\\\]'\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tin_regex = r'(?:'\n\t\tfor key in self.in_params:\n\t\t\tin_regex += re.escape(key) + r'(?:=([^\\s\\|]+))?'\n\t\t\tif key!= self.in_params[-1]:\n\t\t\t\tin_regex += r'\\|'\n\t\t\telse:\n\t\t\t\tin_regex += r')?'\n\t\tin_regex += r'(?:\\s+(.*))?'\n\t\treturn re.compile(in_regex, re.VERBOSE)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tout_regex = r'(?:'\n\t\tfor key in self.out_params:\n\t\t\tout_regex += re.escape(key) + r'(?:=([^\\s\\|]+))?'\n\t\t\tif key!= self.out_params[-1]:\n\t\t\t\tout_regex += r'\\|'\n\t\t\telse:\n\t\t\t\tout_regex += r')?'\n\t\tout_regex += r'(?:\\s+(.*))?'\n\t\treturn re.compile(out_regex, re.VERBOSE)\n\n\tdef _get_in_params(self, in_str: str) -> dict:\n\t\"\"\"\n\tGet the input parameters", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<key>[{0}][{1}]+)(?P<value>{2})'.format(\n\t\t\tself.key_separator,\n\t\t\tself.key_separator,\n\t\t\tself.value_separator))\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<key>{0}[{1}]+{0})(?P<value>{2})'.format(\n\t\t\tself.key_separator,\n\t\t\tself.key_separator,\n\t\t\tself.value_separator))\n\tdef _create_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the regular expression.\n\nReturns the regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<in>{0})|(?P<out>{1})'.format(\n\t\t\tself._create_in_regex(),\n\t\t\tself._create_out_regex()))\n\tdef _split_parameters(self, parameters: str) -> Dict[str, str]:\n\t\"\"\"\n\tSplit the parameters.\n\nParameters\n----------\nparameters : :obj:`str`\n\tThe parameters.\n\nReturns\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r'(?P<in_param>^.*(?:\\s|$))(?P<in_val>' + self.in_val_regex + r')(?P<in_param_end>.*)$')\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn re.compile(r'(?P<out_param>^.*)(?P<out_val>' + self.out_val_regex + r')(?P<out_param_end>.*$)')\n\n\tdef _create_in_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\tCreate the in-out-style parameter regular expression.\n\nReturns the in-out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn re.compile(r'(?P<in_out_param>^.*)(?P<in_out_val>' + self.in_out_val_regex + r')(?P<in_out_param_end>.*$)')\n\n\tdef _create_val_regex(self, val_type: str) -> str:\n\t\t\"\"\"\n\tCreate the value regular expression.\n\nParameters\n----------\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tregex_str = r'(?P<name>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & InFlags.OPTIONAL:\n\t\t\tregex_str += r'\\s*\\=(?P<default>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & InFlags.MULTI:\n\t\t\tregex_str += r'\\s*\\*(?P<name2>[\\w\\-\\.\\[\\]]+)'\n\t\tregex_str += r'\\s*$'\n\t\treturn re.compile(regex_str, flags=re.I)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\tregex_str = r'(?P<name>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & OutFlags.OPTIONAL:\n\t\t\tregex_str += r'\\s*\\=(?P<default>[\\w\\-\\.\\[\\]]+)'\n\t\tif self.flags & OutFlags.MULTI:\n\t\t\tregex_str += r'\\s*\\*(?P<name2>[\\w\\-\\.\\[\\]]+)'\n\t\tregex_str += r'\\s*$'\n\t\treturn re.compile(regex_str, flags=re.I)\n\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn re.compile(r\"^\\s*\" + re.escape(self._in_regex))\n\n\t@property\n\tdef out_regex(self) -> str:\n\t\"\"\"\n\tGet the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (string).\n\t\"\"\"\n\t\treturn self._out_regex\n\n\t@property\n\tdef out_regex_compiled(self) -> Pattern:\n\t\"\"\"\n\tGet the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn self._out_regex_compiled\n\n\t@property\n\tdef in_regex(self) -> str:\n\t\"\"\"\n\tGet the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (string).\n\t\"\"\"\n\t\treturn self._in_regex\n\n\t@property\n\tdef in_regex_compiled(self) -> Pattern:\n\t\"\"\"\n\tGet the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn self._in_regex_compiled\n\n\t@property\n\tdef in_regex_compiled_no_whitespace(self) -> Pattern:\n\t\"\"\"\n\tGet the in-style parameter regular expression without whitespace.\n\nReturns the in-style parameter", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\tif self._converter is None:\n\t\t\tself._converter = _converting._Converter(self)\n\t\treturn self._converter\n\tdef _create_args(self) -> _converting._Args:\n\t\"\"\"\n\tCreate the parameter style argument list.\n\n\tReturns the parameter style argument list (:class:`._converting._Args`).\n\t\"\"\"\n\t\tif self._args is None:\n\t\t\tself._args = _converting._Args(self)\n\t\treturn self._args\n\tdef _create_kwargs(self) -> _converting._Kwargs:\n\t\"\"\"\n\tCreate the parameter style keyword argument list.\n\n\tReturns the parameter style keyword argument list (:class:`._converting._Kwargs`).\n\t\"\"\"\n\t\tif self._kwargs is None:\n\t\t\tself._kwargs = _converting._Kwargs(self)\n\t\treturn self._kwargs\n\tdef _create_defaults(self) -> _converting._Defaults:\n\t\"\"\"\n\tCreate the parameter style default values.\n\n\tReturns the parameter style default values (:class:`._converting._Defaults`).\n\t\"\"\"\n\t\tif self._defaults is None:\n\t\t\tself._defaults = _converting._Defaults(self)\n\t\treturn self._defaults\n\tdef _create_arg_spec(self) -> _converting._ArgSpec:\n\t\"\"\"\n\tCreate the parameter style argument specification.\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter()\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self._paramstyle)\n\n\n\tdef __call__(self, *args: Any, **kwargs: Any) -> \"Connection\":\n\t\"\"\"\n\tCreate a new connection.\n\nReturns the new connection (:class:`Connection`).\n\t\"\"\"\n\t\treturn Connection(self._create_converter(), self._host, self._port, self._database, self._user, self._password, *args, **kwargs)\n\n\n\tdef __str__(self) -> str:\n\t\"\"\"\n\tCreate a string representation of this connection.\n\nReturns the string representation.\n\t\"\"\"\n\t\treturn \"Connection(host='{}', port='{}', database='{}', user='{}', password='{}')\".format(self._host, self._port, self._database, self._user, self._password)\n\n\n\tdef __repr__(self) -> str:\n\t\"\"\"\n\tCreate a string representation of this connection.\n\nReturns the string representation.\n\t\"\"\"\n\t\treturn str(self)\n\n\n\t@property\n\tdef cursor(self) -> \"Cursor\":\n\t\"\"\"\n\tThe connection's cursor.\n\n\tReturns the connection's cursor (:class:`Cursor`).\n\t\"\"\"\n\t\treturn self\n\n\n\tdef commit(self) -> None:\n\t\"\"\"\n\tCommit the current transaction.\n\t\"\"\"\n\t\tself._connection.commit()\n\n\n\tdef rollback(", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self._parameter_style)\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn self._converter_class(self)\n\tdef _create_validator(self) -> _validating._Validator:\n\t\"\"\"\n\tCreate the parameter style validator.\n\nReturns the parameter style validator (:class:`._validating._Validator`).\n\t\"\"\"\n\t\treturn self._validator_class(self)\n\tdef _create_parser(self) -> _parsing._Parser:\n\t\"\"\"\n\tCreate the parameter style parser.\n\nReturns the parameter style parser (:class:`._parsing._Parser`).\n\t\"\"\"\n\t\treturn self._parser_class(self)\n\tdef _create_emitter(self) -> _emitting._Emitter:\n\t\"\"\"\n\tCreate the parameter style emitter.\n\nReturns the parameter style emitter (:class:`._emitting._Emitter`).\n\t\"\"\"\n\t\treturn self._emitter_class(self)\n\tdef _create_help(self) -> _help._Help:\n\t\"\"\"\n\tCreate the parameter style help.\n\nReturns the parameter style help (:class:`._help._Help`).\n\t\"\"\"\n\t\treturn self._help_class(self)\n\tdef _create_completer(self) -> _completing._Completer:\n\t\"\"\"\n\tCreate the parameter style completer.\n\nReturns the parameter style completer (:class:`._completing._Completer`).\n\t\"\"\"\n\t\treturn self._completer_class(", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\tconverter = self._get_converter()\n\t\tself._register_converter(converter)\n\t\treturn converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self._parameter_style_converter)\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self)\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\t# Create the converter.\n\t\tconverter = _converting._Converter()\n\t\treturn converter\n\n# Create the parameter style converter.\nconverter = _create_converter()\n\n# Create the parameter style converter.\n#\n#\tconverter = _create_converter()", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self.params)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\tif self._converter is None:\n\t\t\tself._converter = _converting._Converter(self)\n\t\treturn self._converter\n\tdef _create_args(self) -> _converting._Args:\n\t\"\"\"\n\tCreate the parameter style argument list.\n\n\tReturns the parameter style argument list (:class:`._converting._Args`).\n\t\"\"\"\n\t\tif self._args is None:\n\t\t\tself._args = _converting._Args(self)\n\t\treturn self._args\n\tdef _create_kwargs(self) -> _converting._Kwargs:\n\t\"\"\"\n\tCreate the parameter style keyword argument list.\n\n\tReturns the parameter style keyword argument list (:class:`._converting._Kwargs`).\n\t\"\"\"\n\t\tif self._kwargs is None:\n\t\t\tself._kwargs = _converting._Kwargs(self)\n\t\treturn self._kwargs\n\tdef _create_defaults(self) -> _converting._Defaults:\n\t\"\"\"\n\tCreate the parameter style default values.\n\n\tReturns the parameter style default values (:class:`._converting._Defaults`).\n\t\"\"\"\n\t\tif self._defaults is None:\n\t\t\tself._defaults = _converting._Defaults(self)\n\t\treturn self._defaults\n\tdef _create_arg_spec(self) -> _converting._ArgSpec:\n\t\"\"\"\n\tCreate the parameter style argument specification.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self._paramstyle)\n\n\n\tdef __call__(self, *args: Any, **kwargs: Any) -> \"Connection\":\n\t\"\"\"\n\tCreate a new connection.\n\nReturns the new connection (:class:`Connection`).\n\t\"\"\"\n\t\treturn Connection(self._create_converter(), self._host, self._port, self._database, self._user, self._password, *args, **kwargs)\n\n\n\tdef __str__(self) -> str:\n\t\"\"\"\n\tCreate a string representation of this connection.\n\nReturns the string representation.\n\t\"\"\"\n\t\treturn \"Connection(host='{}', port='{}', database='{}', user='{}', password='{}')\".format(self._host, self._port, self._database, self._user, self._password)\n\n\n\tdef __repr__(self) -> str:\n\t\"\"\"\n\tCreate a string representation of this connection.\n\nReturns the string representation.\n\t\"\"\"\n\t\treturn str(self)\n\n\n\t@property\n\tdef cursor(self) -> \"Cursor\":\n\t\"\"\"\n\tThe connection's cursor.\n\n\tReturns the connection's cursor (:class:`Cursor`).\n\t\"\"\"\n\t\treturn self\n\n\n\tdef commit(self) -> None:\n\t\"\"\"\n\tCommit the current transaction.\n\t\"\"\"\n\t\tself._connection.commit()\n\n\n\tdef rollback(", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self._parameter_style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn self._converter_class(self)\n\tdef _create_validator(self) -> _validating._Validator:\n\t\"\"\"\n\tCreate the parameter style validator.\n\nReturns the parameter style validator (:class:`._validating._Validator`).\n\t\"\"\"\n\t\treturn self._validator_class(self)\n\tdef _create_parser(self) -> _parsing._Parser:\n\t\"\"\"\n\tCreate the parameter style parser.\n\nReturns the parameter style parser (:class:`._parsing._Parser`).\n\t\"\"\"\n\t\treturn self._parser_class(self)\n\tdef _create_emitter(self) -> _emitting._Emitter:\n\t\"\"\"\n\tCreate the parameter style emitter.\n\nReturns the parameter style emitter (:class:`._emitting._Emitter`).\n\t\"\"\"\n\t\treturn self._emitter_class(self)\n\tdef _create_help(self) -> _help._Help:\n\t\"\"\"\n\tCreate the parameter style help.\n\nReturns the parameter style help (:class:`._help._Help`).\n\t\"\"\"\n\t\treturn self._help_class(self)\n\tdef _create_completer(self) -> _completing._Completer:\n\t\"\"\"\n\tCreate the parameter style completer.\n\nReturns the parameter style completer (:class:`._completing._Completer`).\n\t\"\"\"\n\t\treturn self._completer_class(", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\tconverter = self._get_converter()\n\t\tself._register_converter(converter)\n\t\treturn converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self._parameter_style_converter)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\t# Create the converter.\n\t\tconverter = _converting._Converter()\n\t\treturn converter\n\n# Create the parameter style converter.\nconverter = _create_converter()\n\n# Create the parameter style converter.\n#\n#\tconverter = _create_converter()", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter(self.params)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\treturn self._parse_iso(dt_str)\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\traise NotImplementedError\n\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# This is a bit of an ugly hack, but it should be good enough.\n\t\t# We don't want to use strptime at all; we want to validate\n\t\t# the input as much as possible before doing anything. We\n\t\t# also don't want to use a full parser; that's overkill.\n\t\t# This is a simple state machine that handles the most common\n\t\t# cases for an ISO-8601 date string, which is a date\n\t\t# followed by a time, the time being optional.\n\t\t#\n\t\t# This is a bit of a hack, but it should be good enough.\n\t\t# We don't want to use strptime at all; we want to validate\n\t\t# the input as much as possible before doing anything. We\n\t\t# also don't want to use a full parser; that's overkill.\n\t\t# This is a simple state machine that handles the most common\n\t\t# cases for an ISO-8601 date string, which is a date\n\t\t# followed by a time, the time being optional.\n\t\t#\n\t\t# This is a bit of a hack, but it should be good enough.\n\t\t# We don't want to use strptime at all; we want to validate\n\t\t# the input as much as possible before doing anything. We\n\t\t# also don't want to use a full parser; that's over", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# TODO: Handle fractional seconds.\n\t\t# TODO: Handle timezones.\n\t\t# TODO: Handle 'Z' notation.\n\n\tc = self.get_next_char()\n\tif c.isdigit():\n\t\tyear = self.parse_number(4)\n\t\tif self.get_next_char() == '-':\n\t\t\tc = self.get_next_char()\n\t\t\tif c.isdigit():\n\t\t\t\tmonth = self.parse_number(2)\n\t\t\t\tif self.get_next_char() == '-':\n\t\t\t\t\tc = self.get_next_char()\n\t\t\t\t\tif c.isdigit():\n\t\t\t\t\t\tday = self.parse_number(2)\n\t\t\t\t\t\tif self.get_next_char() == 'T':\n\t\t\t\t\t\t\treturn self.parse_iso_datetime(year, month, day)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.unget_char(c)\n\t\t\t\t\t\t\treturn self.parse_iso_date(year, month, day)\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.unget_char(c)\n\t\t\t\t\t\treturn self.parse_iso_date(year, month)\n\t\t\t\telse:\n\t\t\t\t\tself.unget_char(c)\n\t\t\t\t\treturn self.parse_iso_year(year)\n\t\t\telse:\n\t\t\t\tself.unget_char(c)\n\t\t\t\treturn self.parse_iso_year(year)", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\tpass\n\tdef isoparse_tz(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time zone offset, followed optionally by a time\nportion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time zone offset.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# This method was automatically generated by API Extractor\n\tpass\n\n\n\tdef isoparse_tz(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`\n\twith a time zone.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion and a time zone offset - the date and time\nportions are separated by a single character separator, which is ``T``\nin the official standard. Incomplete date formats (such as ``YYYY-MM``)\nmay *not* be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\traise NotImplementedError\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\tpass\n\n\tdef parse(self, timestr):\n\t\t\"\"\"\n\t\tParse a string into a :class:`datetime.datetime`.\n\nThe string may be in any of the formats supported by the\n:func:`parse` function.\n\n:param timestr:\n    A string or stream containing a datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\tpass\n\n\tdef parse_date(self, datestr):\n\t\t\"\"\"\n\t\tParse a string into a :class:`datetime.date`.\n\nThe string may be in any of the formats supported by the\n:func:`parse_datetime` function.\n\n:param datestr:\n    A string or stream containing a date string\n\n:return:\n    Returns a :class:`datetime.date` representing the string.\n\t\"\"\"\n\t\tpass\n\n\tdef parse_time(self, timestr):\n\t\t\"\"\"\n\t\tParse a string into a :class:`datetime.time`.\n\nThe string may be in any of the formats supported by the\n:func:`parse_datetime` function.\n\n:param timestr:\n    A string or stream containing a time string\n\n:return:\n    Returns a :class:`datetime.time` representing the string.\n\t\"\"\"\n\t", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\treturn self.__class__(dt_str)\n\n#\n#\tdef isoparse_tz(self, dt_str):\n#\t\treturn self.__class__(dt_str)\n\n\tdef isoparse_tz(self, dt_str):\n\t\t\"\"\"\n\t\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion and a time zone offset - the date, time\nand time zone offset portions are separated by a single character\nseparator, which is ``T`` in the official standard. Incomplete date\nformats (such as ``YYYY-MM``) may *not* be combined with a time\nportion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# TODO: Parse fractional components (microseconds, etc.)\n\t\t# TODO: Parse time zone offset formats\n\t\t# TODO: Parse timezone names\n\t\t# TODO: Parse weekday names\n\t\t# TODO: Parse day of week names\n\t\t# TODO: Parse month names\n\t\t# TODO: Parse date formats for incomplete dates (e.g. 'YYYY-M' or 'YYYY-M-D')\n\t\t# TODO: Parse date formats for incomplete times (e.g. 'HH' or 'HH:MM' or 'HH:MM:SS')\n\t\t# TODO: Parse date formats for incomplete datetimes (e.g. 'YYYY-M-DTHH' or 'YYYY-M-DTHH:MM' or 'YYYY-M-DTHH:MM:SS')\n\t\t# TODO: Parse date formats for incomplete datetimes with time zone offsets (e.g. 'YYYY-M-DTHH+HH:MM' or 'YYYY-M-DTHH+HHMM' or 'YYYY-M-DTHH+HH')\n\t\t# TODO: Parse date formats for incomplete datetimes with time zone names (e.g. 'YYYY-M-DTHH+HH:MM' or 'YYYY-M-DTHH+HHMM' or 'YYYY-M-DTHH+HH')\n\n\t\t# Parse datetime\n\t\t# Parse"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\treturn self._parse_iso(dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# This is a bit of an ugly hack, but it should be good enough.\n\t\t# We don't want to use strptime at all; we want to validate\n\t\t# the input as much as possible before doing anything. We\n\t\t# also don't want to use a full parser; that's overkill.\n\t\t# This is a simple state machine that handles the most common\n\t\t# cases for an ISO-8601 date string, which is a date\n\t\t# followed by a time, the time being optional.\n\t\t#\n\t\t# This is a bit of a hack, but it should be good enough.\n\t\t# We don't want to use strptime at all; we want to validate\n\t\t# the input as much as possible before doing anything. We\n\t\t# also don't want to use a full parser; that's overkill.\n\t\t# This is a simple state machine that handles the most common\n\t\t# cases for an ISO-8601 date string, which is a date\n\t\t# followed by a time, the time being optional.\n\t\t#\n\t\t# This is a bit of a hack, but it should be good enough.\n\t\t# We don't want to use strptime at all; we want to validate\n\t\t# the input as much as possible before doing anything. We\n\t\t# also don't want to use a full parser; that's over", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# TODO: Handle fractional seconds.\n\t\t# TODO: Handle timezones.\n\t\t# TODO: Handle 'Z' notation.\n\n\tc = self.get_next_char()\n\tif c.isdigit():\n\t\tyear = self.parse_number(4)\n\t\tif self.get_next_char() == '-':\n\t\t\tc = self.get_next_char()\n\t\t\tif c.isdigit():\n\t\t\t\tmonth = self.parse_number(2)\n\t\t\t\tif self.get_next_char() == '-':\n\t\t\t\t\tc = self.get_next_char()\n\t\t\t\t\tif c.isdigit():\n\t\t\t\t\t\tday = self.parse_number(2)\n\t\t\t\t\t\tif self.get_next_char() == 'T':\n\t\t\t\t\t\t\treturn self.parse_iso_datetime(year, month, day)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.unget_char(c)\n\t\t\t\t\t\t\treturn self.parse_iso_date(year, month, day)\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.unget_char(c)\n\t\t\t\t\t\treturn self.parse_iso_date(year, month)\n\t\t\t\telse:\n\t\t\t\t\tself.unget_char(c)\n\t\t\t\t\treturn self.parse_iso_year(year)\n\t\t\telse:\n\t\t\t\tself.unget_char(c)\n\t\t\t\treturn self.parse_iso_year(year)", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\tpass\n\tdef isoparse_tz(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time zone offset, followed optionally by a time\nportion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time zone offset.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# This method was automatically generated by API Extractor\n\tpass\n\n\n\tdef isoparse_tz(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`\n\twith a time zone.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion and a time zone offset - the date and time\nportions are separated by a single character separator, which is ``T``\nin the official standard. Incomplete date formats (such as ``YYYY-MM``)\nmay *not* be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\traise NotImplementedError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\tpass\n\n\tdef parse(self, timestr):\n\t\t\"\"\"\n\t\tParse a string into a :class:`datetime.datetime`.\n\nThe string may be in any of the formats supported by the\n:func:`parse` function.\n\n:param timestr:\n    A string or stream containing a datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\tpass\n\n\tdef parse_date(self, datestr):\n\t\t\"\"\"\n\t\tParse a string into a :class:`datetime.date`.\n\nThe string may be in any of the formats supported by the\n:func:`parse_datetime` function.\n\n:param datestr:\n    A string or stream containing a date string\n\n:return:\n    Returns a :class:`datetime.date` representing the string.\n\t\"\"\"\n\t\tpass\n\n\tdef parse_time(self, timestr):\n\t\t\"\"\"\n\t\tParse a string into a :class:`datetime.time`.\n\nThe string may be in any of the formats supported by the\n:func:`parse_datetime` function.\n\n:param timestr:\n    A string or stream containing a time string\n\n:return:\n    Returns a :class:`datetime.time` representing the string.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\treturn self.__class__(dt_str)\n\n#\n#\tdef isoparse_tz(self, dt_str):\n#\t\treturn self.__class__(dt_str)\n\n\tdef isoparse_tz(self, dt_str):\n\t\t\"\"\"\n\t\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion and a time zone offset - the date, time\nand time zone offset portions are separated by a single character\nseparator, which is ``T`` in the official standard. Incomplete date\nformats (such as ``YYYY-MM``) may *not* be combined with a time\nportion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\t\t# TODO: Parse fractional components (microseconds, etc.)\n\t\t# TODO: Parse time zone offset formats\n\t\t# TODO: Parse timezone names\n\t\t# TODO: Parse weekday names\n\t\t# TODO: Parse day of week names\n\t\t# TODO: Parse month names\n\t\t# TODO: Parse date formats for incomplete dates (e.g. 'YYYY-M' or 'YYYY-M-D')\n\t\t# TODO: Parse date formats for incomplete times (e.g. 'HH' or 'HH:MM' or 'HH:MM:SS')\n\t\t# TODO: Parse date formats for incomplete datetimes (e.g. 'YYYY-M-DTHH' or 'YYYY-M-DTHH:MM' or 'YYYY-M-DTHH:MM:SS')\n\t\t# TODO: Parse date formats for incomplete datetimes with time zone offsets (e.g. 'YYYY-M-DTHH+HH:MM' or 'YYYY-M-DTHH+HHMM' or 'YYYY-M-DTHH+HH')\n\t\t# TODO: Parse date formats for incomplete datetimes with time zone names (e.g. 'YYYY-M-DTHH+HH:MM' or 'YYYY-M-DTHH+HHMM' or 'YYYY-M-DTHH+HH')\n\n\t\t# Parse datetime\n\t\t# Parse", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\ttimestr = _coerce_to_unicode(timestr)\n\t\tif default is not None:\n\t\t\tdefault = self._to_datetime(default)\n\t\tif not hasattr(default, 'utcoffset'):\n\t\t\tdefault = None\n\t\tif ignoretz:\n\t\t\tkwargs['tzinfos'] = None\n\t\tif tzinfos is not None:\n\t\t\tkwargs['tzinfos'] = tzinfos\n\t\tif self._strict:\n\t\t\tkwargs['strict'] = True\n\t\tif self._dayfirst:\n\t\t\tkwargs['dayfirst'] = True\n\t\tif self._date_patterns or self._datetime_formats:\n\t\t\tkwargs['_timestr'] = timestr\n\t\t\tkwargs['_formats'] = self._datetime_formats\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_formats_strict'] = self._datetime_formats\n\t\t\tif self._dayfirst:\n\t\t\t\tkwargs['_formats_dayfirst'] = self._datetime_formats\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_formats_strict_dayfirst'] = self._datetime_formats\n\t\t\tkwargs['_date_patterns'] = self._date_patterns\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_date_patterns_strict'] = self._date_patterns\n\t\t\tif self._dayfirst:\n\t\t\t\tkwargs['_date_patterns_dayfirst'] = self._date_patterns\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_date_patterns_", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\texcept ValueError as err:\n\t\t\traise ParserError(err)\n\n\tdef _parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   ..", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif self._parser is None:\n\t\t\tself._parser = Parser(self._config, **kwargs)\n\t\treturn self._parser.parse(timestr, default=default,\n\t\t\tignoretz=ignoretz, tzinfos=tzinfos)\n/dateutil/tz/tz.py\nfrom __future__ import absolute_import\n\nfrom datetime import tzinfo, timedelta, time, tzinfo\nfrom.tzfile import DSTTzInfo, STDTzInfo\n\ntry:\n\tfrom dateutil.tz._common import gettzfile, gettzname, gettzinfo, gettzinfofromutc\nexcept ImportError:\n\tfrom dateutil.tz.tzfile import gettzfile, gettzname, gettzinfo, gettzinfofromutc\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif not isinstance(timestr, basestring):\n\t\t\traise TypeError(\"expected string or character stream, not %s\" % type(timestr))\n\n\t\tif kwargs.get('fuzzy_with_tokens', False):\n\t\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\telse:\n\t\t\treturn self._parse(timestr, default, ignoretz, tzinfos)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif isinstance(timestr, datetime):\n\t\t\treturn timestr\n\t\tif default is not None and not isinstance(default, datetime):\n\t\t\traise TypeError(\"default must be a datetime object\")\n\t\tif ignoretz and tzinfos is not None:\n\t\t\traise ValueError(\"cannot specify tzinfos if ignoretz is True\")\n\t\tif ignoretz and default is not None:\n\t\t\treturn default.replace(tzinfo=None)\n\t\tif tzinfos is not None:\n\t\t\tkwargs[\"tzinfos\"] = tzinfos\n\n\t\tif self.fuzzy_with_tokens:\n\t\t\treturn self._parse(timestr, **kwargs)\n\t\telse:\n\t\t\treturn self._parse(timestr, default, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif not isinstance(timestr, basestring):\n\t\t\traise TypeError('Argument must be a string')\n\t\tif isinstance(timestr, six.text_type):\n\t\t\ttimestr = timestr.encode('UTF-8')\n\t\ttry:\n\t\t\treturn _parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\texcept ParserError:\n\t\t\ttry:\n\t\t\t\treturn _parse(timestr, default, ignoretz, tzinfos, fuzzy_with_tokens=True, **kwargs)\n\t\t\texcept ParserError as e:\n\t\t\t\traise ParserError('ParserError: %s' % e)\nimport unittest\nfrom dateutil.parser import parse\nfrom dateutil.parser import parserinfo\nfrom dateutil.parser import ParserError\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\t...\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\treturn _parse_dateutiltz(self, timestr, default, ignoretz, tzinfos, **kwargs)\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\treturn _parse(timestr, default, ignoretz, tzinfos, **kwargs)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\ttimestr = _coerce_to_unicode(timestr)\n\t\tif default is not None:\n\t\t\tdefault = self._to_datetime(default)\n\t\tif not hasattr(default, 'utcoffset'):\n\t\t\tdefault = None\n\t\tif ignoretz:\n\t\t\tkwargs['tzinfos'] = None\n\t\tif tzinfos is not None:\n\t\t\tkwargs['tzinfos'] = tzinfos\n\t\tif self._strict:\n\t\t\tkwargs['strict'] = True\n\t\tif self._dayfirst:\n\t\t\tkwargs['dayfirst'] = True\n\t\tif self._date_patterns or self._datetime_formats:\n\t\t\tkwargs['_timestr'] = timestr\n\t\t\tkwargs['_formats'] = self._datetime_formats\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_formats_strict'] = self._datetime_formats\n\t\t\tif self._dayfirst:\n\t\t\t\tkwargs['_formats_dayfirst'] = self._datetime_formats\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_formats_strict_dayfirst'] = self._datetime_formats\n\t\t\tkwargs['_date_patterns'] = self._date_patterns\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_date_patterns_strict'] = self._date_patterns\n\t\t\tif self._dayfirst:\n\t\t\t\tkwargs['_date_patterns_dayfirst'] = self._date_patterns\n\t\t\tif self._strict:\n\t\t\t\tkwargs['_date_patterns_", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\texcept ValueError as err:\n\t\t\traise ParserError(err)\n\n\tdef _parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   ..", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif self._parser is None:\n\t\t\tself._parser = Parser(self._config, **kwargs)\n\t\treturn self._parser.parse(timestr, default=default,\n\t\t\tignoretz=ignoretz, tzinfos=tzinfos)\n/dateutil/tz/tz.py\nfrom __future__ import absolute_import\n\nfrom datetime import tzinfo, timedelta, time, tzinfo\nfrom.tzfile import DSTTzInfo, STDTzInfo\n\ntry:\n\tfrom dateutil.tz._common import gettzfile, gettzname, gettzinfo, gettzinfofromutc\nexcept ImportError:\n\tfrom dateutil.tz.tzfile import gettzfile, gettzname, gettzinfo, gettzinfofromutc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif not isinstance(timestr, basestring):\n\t\t\traise TypeError(\"expected string or character stream, not %s\" % type(timestr))\n\n\t\tif kwargs.get('fuzzy_with_tokens', False):\n\t\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\telse:\n\t\t\treturn self._parse(timestr, default, ignoretz, tzinfos)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif isinstance(timestr, datetime):\n\t\t\treturn timestr\n\t\tif default is not None and not isinstance(default, datetime):\n\t\t\traise TypeError(\"default must be a datetime object\")\n\t\tif ignoretz and tzinfos is not None:\n\t\t\traise ValueError(\"cannot specify tzinfos if ignoretz is True\")\n\t\tif ignoretz and default is not None:\n\t\t\treturn default.replace(tzinfo=None)\n\t\tif tzinfos is not None:\n\t\t\tkwargs[\"tzinfos\"] = tzinfos\n\n\t\tif self.fuzzy_with_tokens:\n\t\t\treturn self._parse(timestr, **kwargs)\n\t\telse:\n\t\t\treturn self._parse(timestr, default, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\tif not isinstance(timestr, basestring):\n\t\t\traise TypeError('Argument must be a string')\n\t\tif isinstance(timestr, six.text_type):\n\t\t\ttimestr = timestr.encode('UTF-8')\n\t\ttry:\n\t\t\treturn _parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\texcept ParserError:\n\t\t\ttry:\n\t\t\t\treturn _parse(timestr, default, ignoretz, tzinfos, fuzzy_with_tokens=True, **kwargs)\n\t\t\texcept ParserError as e:\n\t\t\t\traise ParserError('ParserError: %s' % e)\nimport unittest\nfrom dateutil.parser import parse\nfrom dateutil.parser import parserinfo\nfrom dateutil.parser import ParserError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\treturn _parse_dateutiltz(self, timestr, default, ignoretz, tzinfos, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n   .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\treturn _parse(timestr, default, ignoretz, tzinfos, **kwargs)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# if the datetime is in a fold state, then we need to calculate\n\t\t# the right date in the new timezone\n\t\tif dt.fold:\n\t\t\t# we need to calculate the right date in the new timezone\n\t\t\t# by using the UTC offset of the new timezone\n\t\t\t# (which is the difference between the new timezone and UTC)\n\t\t\t# and adding this offset to the UTC-timezone date of the\n\t\t\t# original datetime\n\t\t\tdt_utc = dt - datetime.timedelta(seconds=dt.tzinfo.utcoffset(dt).seconds)\n\t\t\treturn dt_utc.replace(tzinfo=self)\n\n\t\t# if the datetime is not in a fold state, then we can just\n\t\t# replace the timezone with the new timezone and return it\n\t\treturn dt.replace(tzinfo=self)\n\npytz.timezone.fromutc = fromutc\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif not dt.tzinfo:\n\t\t\traise ValueError('A timezone-aware datetime is required.')\n\t\tif dt.tzinfo.utcoffset(dt) is not None:\n\t\t\treturn dt\n\t\tif dt.tzinfo.dst(dt) is not None:\n\t\t\traise ValueError('Ambiguous datetime.')\n\n\t\t# The datetime is ambiguous, and is in a \"fold\" state.\n\t\t# This means that it's the first occurrence, chronologically,\n\t\t# of the ambiguous datetime.\n\t\t#\n\t\t# We need to ensure that we're not folding an ambiguous datetime\n\t\t# that's already in a \"fold\" state.\n\t\t#\n\t\t# If the datetime has a fold state, then the fold state is\n\t\t# determined by whether the datetime is *after* the folding\n\t\t# date.\n\t\tif self._fold:\n\t\t\tif dt > self._fold:\n\t\t\t\treturn dt\n\t\treturn dt.replace(tzinfo=self)\n\n#\n# The *other* way to change the timezone of a datetime is to use the\n# tz_convert() method.  This is the same as the fromutc() function, except\n# that it works on a timezone-naive datetime, rather than a\n# timezone-aware one.\n#", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\telse:\n\t\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\t\treturn dt\n\t\t\telse:\n\t\t\t\treturn dt.astimezone(self)\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# If we already have a naive datetime, we're done.\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\n\t\t# If we have a fold-aware datetime, we can't do anything\n\t\t# with it, so we just return it as-is.\n\t\tif dt.fold is not None:\n\t\t\treturn dt\n\n\t\t# If we have a naive datetime, we know that it's ambiguous,\n\t\t# so we need to determine if it's the first occurrence of\n\t\t# this datetime.\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\t# If the current timezone was UTC, then we know that\n\t\t\t# this is the first occurrence of the datetime.\n\t\t\tif self.name == 'UTC':\n\t\t\t\tfold = 0\n\t\t\telse:\n\t\t\t\tfold = 1\n\t\telse:\n\t\t\tfold = None\n\n\t\t# Convert the datetime to UTC, and then convert it back\n\t\t# to this timezone.\n\t\treturn self.toutc(self.fromutc(dt)).replace(fold=fold)\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# If we have a naive datetime, we can't do timezone conversions\n        if dt.tzinfo is None:\n            raise ValueError('not a timezone-aware datetime')\n\n        # Convert to UTC\n        dt = dt.astimezone(pytz.utc)\n\n        # Convert to the new timezone\n        dt = dt.astimezone(self)\n\n        # If we're \"folding\" the datetime, make sure the folding\n        # state is set.\n        if dt.fold:\n            dt = dt.replace(fold=1)\n\n        # Convert back to a datetime\n        return datetime.datetime.combine(dt.date(), dt.time())\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.utcoffset() is None:\n\t\t\traise ValueError(\"Cannot convert a naive datetime to a timezone-aware datetime\")\n\t\t# Determine the fold state here, since we know we have a\n\t\t# timezone-aware datetime\n\t\tfold = 0\n\t\tif dt.fold!= 0:\n\t\t\tfold = 1\n\t\treturn self.localize(dt).astimezone(self)\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# if we have an unambiguous datetime, we can simply use\n\t\t# the fold state of the original datetime to determine the\n\t\t# fold state of this new one.\n\t\t#\n\t\t# XXX: this doesn't work for datetime-naive datetime objects\n\t\t# in a timezone that is not UTC\n\t\tif dt.tzinfo is not None:\n\t\t\treturn dt.replace(tzinfo=TimezoneInfo(self.tzname, self.fold))\n\t\t# if we don't have a timezone-aware datetime, we can't\n\t\t# determine the fold state and must assume it's a fold state.\n\t\t# XXX: this is a hack (see above)\n\t\treturn dt.replace(tzinfo=TimezoneInfo(self.tzname, True))\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"Timezone-aware datetime required\")\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError(\"Timezone-aware datetime required\")\n\t\tif dt.tzinfo.dst(dt) is None:\n\t\t\traise ValueError(\"Timezone-aware datetime required\")\n\t\treturn dt.replace(tzinfo=self)\n\n# end class Timezone\n\n# Utility functions\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError(\"Cannot convert naive datetime to tzinfo\")\n\t\telse:\n\t\t\tif dt.tzinfo.utcoffset(dt) == self.utcoffset(dt):\n\t\t\t\treturn dt\n\t\t\telse:\n\t\t\t\treturn self.localize(dt).astimezone(self)\n# END def fromutc\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tassert dt.tzinfo is not None\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError(\"Not a valid timezone-aware datetime object.\")\n\t\tif dt.tzinfo.dst(dt) is None:\n\t\t\t# non-folding, non-ambiguous\n\t\t\treturn dt.replace(tzinfo=self)\n\t\t# folding, ambiguous\n\t\tif dt.tzinfo.dst(dt) == self._dst:\n\t\t\t# non-folding, ambiguous\n\t\t\treturn dt.replace(tzinfo=self)\n\t\t# folding, non-ambiguous\n\t\treturn dt.replace(tzinfo=self, fold=1-dt.fold)\n\n\n#\n#\t\tclass UTC(tzinfo):\n#\n#\t\t\tdef __init__(self):\n#\t\t\t\tsuper(UTC, self).__init__()\n#\t\t\t\tself.offset = timedelta(0)\n#\t\t\t\tself.dst = timedelta(0)\n#\n#\t\t\tdef utcoffset(self, dt):\n#\t\t\t\treturn self.offset\n#\n#\t\t\tdef dst(self, dt):\n#\t\t\t\treturn self.dst\n#\n#\t\t\tdef tzname(self, dt):\n#\t\t\t\treturn \"UTC\"\n#\n#\t\tclass UCT(tzinfo):\n#\n#\t\t\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# if the datetime is in a fold state, then we need to calculate\n\t\t# the right date in the new timezone\n\t\tif dt.fold:\n\t\t\t# we need to calculate the right date in the new timezone\n\t\t\t# by using the UTC offset of the new timezone\n\t\t\t# (which is the difference between the new timezone and UTC)\n\t\t\t# and adding this offset to the UTC-timezone date of the\n\t\t\t# original datetime\n\t\t\tdt_utc = dt - datetime.timedelta(seconds=dt.tzinfo.utcoffset(dt).seconds)\n\t\t\treturn dt_utc.replace(tzinfo=self)\n\n\t\t# if the datetime is not in a fold state, then we can just\n\t\t# replace the timezone with the new timezone and return it\n\t\treturn dt.replace(tzinfo=self)\n\npytz.timezone.fromutc = fromutc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif not dt.tzinfo:\n\t\t\traise ValueError('A timezone-aware datetime is required.')\n\t\tif dt.tzinfo.utcoffset(dt) is not None:\n\t\t\treturn dt\n\t\tif dt.tzinfo.dst(dt) is not None:\n\t\t\traise ValueError('Ambiguous datetime.')\n\n\t\t# The datetime is ambiguous, and is in a \"fold\" state.\n\t\t# This means that it's the first occurrence, chronologically,\n\t\t# of the ambiguous datetime.\n\t\t#\n\t\t# We need to ensure that we're not folding an ambiguous datetime\n\t\t# that's already in a \"fold\" state.\n\t\t#\n\t\t# If the datetime has a fold state, then the fold state is\n\t\t# determined by whether the datetime is *after* the folding\n\t\t# date.\n\t\tif self._fold:\n\t\t\tif dt > self._fold:\n\t\t\t\treturn dt\n\t\treturn dt.replace(tzinfo=self)\n\n#\n# The *other* way to change the timezone of a datetime is to use the\n# tz_convert() method.  This is the same as the fromutc() function, except\n# that it works on a timezone-naive datetime, rather than a\n# timezone-aware one.\n#", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\telse:\n\t\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\t\treturn dt\n\t\t\telse:\n\t\t\t\treturn dt.astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# If we already have a naive datetime, we're done.\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\n\t\t# If we have a fold-aware datetime, we can't do anything\n\t\t# with it, so we just return it as-is.\n\t\tif dt.fold is not None:\n\t\t\treturn dt\n\n\t\t# If we have a naive datetime, we know that it's ambiguous,\n\t\t# so we need to determine if it's the first occurrence of\n\t\t# this datetime.\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\t# If the current timezone was UTC, then we know that\n\t\t\t# this is the first occurrence of the datetime.\n\t\t\tif self.name == 'UTC':\n\t\t\t\tfold = 0\n\t\t\telse:\n\t\t\t\tfold = 1\n\t\telse:\n\t\t\tfold = None\n\n\t\t# Convert the datetime to UTC, and then convert it back\n\t\t# to this timezone.\n\t\treturn self.toutc(self.fromutc(dt)).replace(fold=fold)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# If we have a naive datetime, we can't do timezone conversions\n        if dt.tzinfo is None:\n            raise ValueError('not a timezone-aware datetime')\n\n        # Convert to UTC\n        dt = dt.astimezone(pytz.utc)\n\n        # Convert to the new timezone\n        dt = dt.astimezone(self)\n\n        # If we're \"folding\" the datetime, make sure the folding\n        # state is set.\n        if dt.fold:\n            dt = dt.replace(fold=1)\n\n        # Convert back to a datetime\n        return datetime.datetime.combine(dt.date(), dt.time())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.utcoffset() is None:\n\t\t\traise ValueError(\"Cannot convert a naive datetime to a timezone-aware datetime\")\n\t\t# Determine the fold state here, since we know we have a\n\t\t# timezone-aware datetime\n\t\tfold = 0\n\t\tif dt.fold!= 0:\n\t\t\tfold = 1\n\t\treturn self.localize(dt).astimezone(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\t# if we have an unambiguous datetime, we can simply use\n\t\t# the fold state of the original datetime to determine the\n\t\t# fold state of this new one.\n\t\t#\n\t\t# XXX: this doesn't work for datetime-naive datetime objects\n\t\t# in a timezone that is not UTC\n\t\tif dt.tzinfo is not None:\n\t\t\treturn dt.replace(tzinfo=TimezoneInfo(self.tzname, self.fold))\n\t\t# if we don't have a timezone-aware datetime, we can't\n\t\t# determine the fold state and must assume it's a fold state.\n\t\t# XXX: this is a hack (see above)\n\t\treturn dt.replace(tzinfo=TimezoneInfo(self.tzname, True))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"Timezone-aware datetime required\")\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError(\"Timezone-aware datetime required\")\n\t\tif dt.tzinfo.dst(dt) is None:\n\t\t\traise ValueError(\"Timezone-aware datetime required\")\n\t\treturn dt.replace(tzinfo=self)\n\n# end class Timezone\n\n# Utility functions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError(\"Cannot convert naive datetime to tzinfo\")\n\t\telse:\n\t\t\tif dt.tzinfo.utcoffset(dt) == self.utcoffset(dt):\n\t\t\t\treturn dt\n\t\t\telse:\n\t\t\t\treturn self.localize(dt).astimezone(self)\n# END def fromutc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tassert dt.tzinfo is not None\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError(\"Not a valid timezone-aware datetime object.\")\n\t\tif dt.tzinfo.dst(dt) is None:\n\t\t\t# non-folding, non-ambiguous\n\t\t\treturn dt.replace(tzinfo=self)\n\t\t# folding, ambiguous\n\t\tif dt.tzinfo.dst(dt) == self._dst:\n\t\t\t# non-folding, ambiguous\n\t\t\treturn dt.replace(tzinfo=self)\n\t\t# folding, non-ambiguous\n\t\treturn dt.replace(tzinfo=self, fold=1-dt.fold)\n\n\n#\n#\t\tclass UTC(tzinfo):\n#\n#\t\t\tdef __init__(self):\n#\t\t\t\tsuper(UTC, self).__init__()\n#\t\t\t\tself.offset = timedelta(0)\n#\t\t\t\tself.dst = timedelta(0)\n#\n#\t\t\tdef utcoffset(self, dt):\n#\t\t\t\treturn self.offset\n#\n#\t\t\tdef dst(self, dt):\n#\t\t\t\treturn self.dst\n#\n#\t\t\tdef tzname(self, dt):\n#\t\t\t\treturn \"UTC\"\n#\n#\t\tclass UCT(tzinfo):\n#\n#\t\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\n\treturn dt\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\tdt = dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\timport os\nimport sys\nimport unittest\n\nfrom dateutil import parser\nfrom dateutil.tz import tzlocal\nfrom dateutil.parser._parser import ParserInfo\nfrom dateutil.parser._parser import ParserError\nfrom dateutil.parser._parser import _timelex\nfrom dateutil.parser._parser import _tzlex\nfrom dateutil.parser._parser import _tzoffset\nfrom dateutil.parser._parser import _tzutc\nfrom dateutil.parser._parser import _tzutcoffset\nfrom dateutil.parser._parser import _parse\nfrom dateutil.parser._parser import _result\nfrom dateutil.parser._parser import _check_parser_warning\nfrom dateutil.parser._parser import _check_parser_warnings\nfrom dateutil.parser._parser import _get_parser_info\nfrom dateutil.parser._parser import _check_parser_error\nfrom dateutil.parser._parser import _parse_tzinfo\nfrom dateutil.parser._parser import _parse_tzname\nfrom dateutil.parser._parser import _parse_tzoffset\nfrom dateutil.parser._parser import _parse_tzutc\nfrom dateutil.parser._parser import _parse_tzutcoffset\nfrom dateutil.parser._parser import _parse_rfc2822\nfrom dateutil.parser._parser import _parse_iso8601\nfrom date", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\telse:\n\t\t\treturn dt\n\n/repoData/dcramer-django-templatetag-sugar/allPythonContent.py\n__FILENAME__ = conf\r\n# -*- coding: utf-8 -*-\r\n#\r\n# django-templatetag-sugar documentation build configuration file, created by\r\n# sphinx-quickstart on Tue Dec 19 13:21:08 2012.\r\n#\r\n# This file is execfile()d with the current directory set to its containing dir.\r\n#\r\n# Note that not all possible configuration values are present in this\r\n# autogenerated file.\r\n#\r\n# All configuration values have a default; values that are commented out\r\n# serve to show the default.\r\n\r\nimport sys, os\r\n\r\n# If extensions (or modules to document with autodoc) are in another directory,\r\n# add these directories to sys.path here. If the directory is relative to the\r\n# documentation root, use os.path.abspath to make it absolute, like shown here.\r\n#sys.path.insert(0, os.path.abspath('.'))\r\n\r\n# -- General configuration -----------------------------------------------------\r\n\r\n# If", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif tzinfo is None:\n\t\t\traise ValueError(\"tzinfo must be set\")\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t \n\tif dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n/dateutil/parser/parser.py\nfrom __future__ import absolute_import\n\"\"\"\nThis module supplies a generic date/time parsing function ``parse()``.\n\nThis function parses a string representing a date and time and returns a\n:py:class:`datetime.datetime` instance. It can parse many kinds of strings\nand can be customized with custom parsers.\n\nThe function ``parse()`` can be used in the following ways.\n\n.. doctest::\n\n    >>> from dateutil.parser import parse\n    >>> parse('Jun 1 2005 10:33PM')\n    datetime.datetime(2005, 6, 1, 22, 33)\n\n.. doctest::\n\n    >>> parse('2015-05-21 17:23:45')\n    datetime.datetime(2015, 5, 21, 17, 23, 45)\n\n.. doctest::\n\n    >>> parse('2015-05-21 17:23:45 -0500')\n    datetime.datetime(2", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\n\treturn dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\tdt = dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\timport os\nimport sys\nimport unittest\n\nfrom dateutil import parser\nfrom dateutil.tz import tzlocal\nfrom dateutil.parser._parser import ParserInfo\nfrom dateutil.parser._parser import ParserError\nfrom dateutil.parser._parser import _timelex\nfrom dateutil.parser._parser import _tzlex\nfrom dateutil.parser._parser import _tzoffset\nfrom dateutil.parser._parser import _tzutc\nfrom dateutil.parser._parser import _tzutcoffset\nfrom dateutil.parser._parser import _parse\nfrom dateutil.parser._parser import _result\nfrom dateutil.parser._parser import _check_parser_warning\nfrom dateutil.parser._parser import _check_parser_warnings\nfrom dateutil.parser._parser import _get_parser_info\nfrom dateutil.parser._parser import _check_parser_error\nfrom dateutil.parser._parser import _parse_tzinfo\nfrom dateutil.parser._parser import _parse_tzname\nfrom dateutil.parser._parser import _parse_tzoffset\nfrom dateutil.parser._parser import _parse_tzutc\nfrom dateutil.parser._parser import _parse_tzutcoffset\nfrom dateutil.parser._parser import _parse_rfc2822\nfrom dateutil.parser._parser import _parse_iso8601\nfrom date", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\telse:\n\t\t\treturn dt\n\n/repoData/dcramer-django-templatetag-sugar/allPythonContent.py\n__FILENAME__ = conf\r\n# -*- coding: utf-8 -*-\r\n#\r\n# django-templatetag-sugar documentation build configuration file, created by\r\n# sphinx-quickstart on Tue Dec 19 13:21:08 2012.\r\n#\r\n# This file is execfile()d with the current directory set to its containing dir.\r\n#\r\n# Note that not all possible configuration values are present in this\r\n# autogenerated file.\r\n#\r\n# All configuration values have a default; values that are commented out\r\n# serve to show the default.\r\n\r\nimport sys, os\r\n\r\n# If extensions (or modules to document with autodoc) are in another directory,\r\n# add these directories to sys.path here. If the directory is relative to the\r\n# documentation root, use os.path.abspath to make it absolute, like shown here.\r\n#sys.path.insert(0, os.path.abspath('.'))\r\n\r\n# -- General configuration -----------------------------------------------------\r\n\r\n# If", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif tzinfo is None:\n\t\t\traise ValueError(\"tzinfo must be set\")\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t \n\tif dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n/dateutil/parser/parser.py\nfrom __future__ import absolute_import\n\"\"\"\nThis module supplies a generic date/time parsing function ``parse()``.\n\nThis function parses a string representing a date and time and returns a\n:py:class:`datetime.datetime` instance. It can parse many kinds of strings\nand can be customized with custom parsers.\n\nThe function ``parse()`` can be used in the following ways.\n\n.. doctest::\n\n    >>> from dateutil.parser import parse\n    >>> parse('Jun 1 2005 10:33PM')\n    datetime.datetime(2005, 6, 1, 22, 33)\n\n.. doctest::\n\n    >>> parse('2015-05-21 17:23:45')\n    datetime.datetime(2015, 5, 21, 17, 23, 45)\n\n.. doctest::\n\n    >>> parse('2015-05-21 17:23:45 -0500')\n    datetime.datetime(2", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n\tdef set_start_cut_char(self, char: bytes) -> None:\n\t\"\"\"\n\tSet the slice start delimiter.\n\nArgs:\n    char: Slice start delimiter.\n\t\"\"\"\n\t\tself._before = char\n\n\tdef set_end_cut_char(self, char: bytes) -> None:\n\t\"\"\"\n\tSet the slice end delimiter.\n\nArgs:\n    char: Slice end delimiter.\n\t\"\"\"\n\t\tself._after = char\n\n\tdef get_start_cut_char(self) -> bytes:\n\t\"\"\"\n\tGet the slice start delimiter.\n\n\tReturns:\n\t\tSlice start delimiter.\n\t\"\"\"\n\t\treturn self._before\n\n\tdef get_end_cut_char(self) -> bytes:\n\t\"\"\"\n\tGet the slice end delimiter.\n\n\tReturns:\n\t\tSlice end delimiter.\n\t\"\"\"\n\t\treturn self._after\n\n\tdef __str__(self) -> str:\n\t\"\"\"\n\tGet the slice parameters.\n\n\tReturns:\n\t\tSlice parameters.\n\t\"\"\"\n\t\treturn f\"Cut chars: {self._before} {self._after}\"\n/splitfile/splitfile.py\n# -*- coding: utf-8 -*-\n\n\"\"\"\nSplitfile library.\n\n@author: \n@license: MIT", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before\n\t\tself.after = after\n\n\tdef split(self, filename: str) -> typing.List[str]:\n\t\t\"\"\"\n\t\tSplit a file into many smaller files.\n\nArgs:\n    filename: The file to split.\n\nReturns:\n    A list of filenames, in the order they should be joined.\n\t\"\"\"\n\t\twith open(filename, \"rb\") as fin:\n\t\t\tcontent = fin.read()\n\n\t\tparts = content.split(self.before)\n\t\tparts = [part.split(self.after)[0] for part in parts]\n\n\t\treturn parts\n\n\tdef join(self, parts: typing.List[str], filename: str) -> None:\n\t\t\"\"\"\n\t\tJoin many smaller files into one.\n\nArgs:\n    parts: A list of filenames, in the order they should be joined.\n    filename: The file to write to.\n\t\"\"\"\n\t\twith open(filename, \"wb\") as fout:\n\t\t\tfout.write(self.before)\n\t\t\tfor part in parts:\n\t\t\t\twith open(part, \"rb\") as fin:\n\t\t\t\t\tfout.write(fin.read())\n\t\t\tfout.write(self.after)\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.cut_chars = (before, after)\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._cut_before = before\n\t\tself._cut_after = after\n\n\tdef _split_file_by_line(self, line_count: int, lines: LineBuffer) -> None:\n\t\t\"\"\"\n\t\tSplit the file by line count.\n\nArgs:\n    line_count: The number of lines in the file.\n    lines: The contents of the file.\n\t\"\"\"\n\t\ttotal_lines = len(lines)\n\t\tif line_count == 0:\n\t\t\treturn\n\n\t\tif line_count == total_lines:\n\t\t\tself.write_file(lines)\n\t\t\treturn\n\n\t\tline_count = min(line_count, total_lines)\n\t\tcount = 0\n\t\tlast_line = None\n\t\tfor index, line in enumerate(lines):\n\t\t\tif self._cut_before in line:\n\t\t\t\tlast_line = index\n\t\t\telif self._cut_after in line:\n\t\t\t\tcount += 1\n\t\t\tif count == line_count:\n\t\t\t\tself.write_file(lines[0:last_line + 1])\n\t\t\t\tself.write_file(lines[last_line + 1:])\n\t\t\t\treturn\n\n\tdef _split_file_by_bytes(self, bytes_count: int, bytes: BytesBuffer) -> None:\n\t\t\"\"\"\n\t\tSplit the file by byte count.\n\nArgs:", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n\tdef _get_before(self) -> bytes:\n\t\t\"\"\"Return the bytes used to delimit slice points.\"\"\"\n\t\treturn self._before\n\n\tdef _get_after(self) -> bytes:\n\t\t\"\"\"Return the bytes used to delimit slice points.\"\"\"\n\t\treturn self._after\n\n\tdef _set_before(self, new_before: bytes) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tself._before = new_before\n\n\tdef _set_after(self, new_after: bytes) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tself._after = new_after\n\n\tdef _del_before(self) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tdel self._before\n\n\tdef _del_after(self) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tdel self._after\n\n\tbefore = property(_get_before, _set_before, _del_before, 'bytes used to delimit slice points.')\n\tafter = property(_get_after, _set_after, _del_after, 'bytes used to delimit slice points.')\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before\n\t\tself.after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n\t@property\n\tdef before(self) -> bytes:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\nReturns:\n    The cut characters.\n\t\t\"\"\"\n\t\treturn self._before\n\n\t@property\n\tdef after(self) -> bytes:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\nReturns:\n    The cut characters.\n\t\t\"\"\"\n\t\treturn self._after\n\n\t@property\n\tdef cut_chars(self) -> bytes:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\nReturns:\n    The cut characters.\n\t\t\"\"\"\n\t\tif len(self._before)!= len(self._after):\n\t\t\traise ValueError(\"Before and after cut chars must be the same length.\")\n\t\treturn self._before + self._after\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock\n\nfrom fs.base import FS\nfrom fs.file_like import FileLike\n\nfrom fs_s3fs.s3fs import S3FS\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before\n\t\tself.after = after\n\n\tdef split(self, in_filename: str, out_dir: str) -> None:\n\t\t\"\"\"\n\t\tSplit a file into multiple files.\n\nArgs:\n    in_filename: The input file to split.\n    out_dir: The directory to write the split files to.\n\t\"\"\"\n\t\tif not self.before or not self.after:\n\t\t\traise Exception(\"Cut chars not set\")\n\n\t\twith open(in_filename, \"rb\") as in_file:\n\t\t\twhile True:\n\t\t\t\tdata = in_file.read(1024)\n\t\t\t\tif not data:\n\t\t\t\t\tbreak\n\n\t\t\t\tbefore = self.before\n\t\t\t\tif before in data:\n\t\t\t\t\tbefore_index = data.index(before)\n\t\t\t\t\tdata = data[before_index + len(before):]\n\n\t\t\t\tafter = self.after\n\t\t\t\tif after in data:\n\t\t\t\t\tafter_index = data.index(after)\n\t\t\t\t\tdata = data[:after_index]\n\t\t\t\t\twith open(os.path.join(out_dir, f\"{self.count}.mp3\"), \"wb\") as out_file:\n\t\t\t\t\t\tout_file.write(data)\n\t\t\t\t\tself.count += 1\n\n\tdef join(self, in_dir: str, out_filename: str) -> None:\n\t\t\"\"\"", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.cut_chars = (before, after)\n\tdef set_use_cut_chars(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to use the bytes used to delimit slice points.\n\nArgs:\n    value: True to use the bytes used to delimit slice points, False otherwise.\n\t\"\"\"\n\t\tself.use_cut_chars = value\n\tdef set_use_file_name_prefix(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to include the file name as part of the slice file name.\n\nArgs:\n    value: True to include the file name as part of the slice file name, False otherwise.\n\t\"\"\"\n\t\tself.use_file_name_prefix = value\n\tdef set_use_file_name_suffix(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to include the file name as part of the slice file name.\n\nArgs:\n    value: True to include the file name as part of the slice file name, False otherwise.\n\t\"\"\"\n\t\tself.use_file_name_suffix = value\n\tdef set_use_slice_number(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to include the slice number as part of the slice file name.\n\t\"\"\"\n\t\tself.use_slice"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n\tdef set_start_cut_char(self, char: bytes) -> None:\n\t\"\"\"\n\tSet the slice start delimiter.\n\nArgs:\n    char: Slice start delimiter.\n\t\"\"\"\n\t\tself._before = char\n\n\tdef set_end_cut_char(self, char: bytes) -> None:\n\t\"\"\"\n\tSet the slice end delimiter.\n\nArgs:\n    char: Slice end delimiter.\n\t\"\"\"\n\t\tself._after = char\n\n\tdef get_start_cut_char(self) -> bytes:\n\t\"\"\"\n\tGet the slice start delimiter.\n\n\tReturns:\n\t\tSlice start delimiter.\n\t\"\"\"\n\t\treturn self._before\n\n\tdef get_end_cut_char(self) -> bytes:\n\t\"\"\"\n\tGet the slice end delimiter.\n\n\tReturns:\n\t\tSlice end delimiter.\n\t\"\"\"\n\t\treturn self._after\n\n\tdef __str__(self) -> str:\n\t\"\"\"\n\tGet the slice parameters.\n\n\tReturns:\n\t\tSlice parameters.\n\t\"\"\"\n\t\treturn f\"Cut chars: {self._before} {self._after}\"\n/splitfile/splitfile.py\n# -*- coding: utf-8 -*-\n\n\"\"\"\nSplitfile library.\n\n@author: \n@license: MIT", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before\n\t\tself.after = after\n\n\tdef split(self, filename: str) -> typing.List[str]:\n\t\t\"\"\"\n\t\tSplit a file into many smaller files.\n\nArgs:\n    filename: The file to split.\n\nReturns:\n    A list of filenames, in the order they should be joined.\n\t\"\"\"\n\t\twith open(filename, \"rb\") as fin:\n\t\t\tcontent = fin.read()\n\n\t\tparts = content.split(self.before)\n\t\tparts = [part.split(self.after)[0] for part in parts]\n\n\t\treturn parts\n\n\tdef join(self, parts: typing.List[str], filename: str) -> None:\n\t\t\"\"\"\n\t\tJoin many smaller files into one.\n\nArgs:\n    parts: A list of filenames, in the order they should be joined.\n    filename: The file to write to.\n\t\"\"\"\n\t\twith open(filename, \"wb\") as fout:\n\t\t\tfout.write(self.before)\n\t\t\tfor part in parts:\n\t\t\t\twith open(part, \"rb\") as fin:\n\t\t\t\t\tfout.write(fin.read())\n\t\t\tfout.write(self.after)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.cut_chars = (before, after)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._cut_before = before\n\t\tself._cut_after = after\n\n\tdef _split_file_by_line(self, line_count: int, lines: LineBuffer) -> None:\n\t\t\"\"\"\n\t\tSplit the file by line count.\n\nArgs:\n    line_count: The number of lines in the file.\n    lines: The contents of the file.\n\t\"\"\"\n\t\ttotal_lines = len(lines)\n\t\tif line_count == 0:\n\t\t\treturn\n\n\t\tif line_count == total_lines:\n\t\t\tself.write_file(lines)\n\t\t\treturn\n\n\t\tline_count = min(line_count, total_lines)\n\t\tcount = 0\n\t\tlast_line = None\n\t\tfor index, line in enumerate(lines):\n\t\t\tif self._cut_before in line:\n\t\t\t\tlast_line = index\n\t\t\telif self._cut_after in line:\n\t\t\t\tcount += 1\n\t\t\tif count == line_count:\n\t\t\t\tself.write_file(lines[0:last_line + 1])\n\t\t\t\tself.write_file(lines[last_line + 1:])\n\t\t\t\treturn\n\n\tdef _split_file_by_bytes(self, bytes_count: int, bytes: BytesBuffer) -> None:\n\t\t\"\"\"\n\t\tSplit the file by byte count.\n\nArgs:", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n\tdef _get_before(self) -> bytes:\n\t\t\"\"\"Return the bytes used to delimit slice points.\"\"\"\n\t\treturn self._before\n\n\tdef _get_after(self) -> bytes:\n\t\t\"\"\"Return the bytes used to delimit slice points.\"\"\"\n\t\treturn self._after\n\n\tdef _set_before(self, new_before: bytes) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tself._before = new_before\n\n\tdef _set_after(self, new_after: bytes) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tself._after = new_after\n\n\tdef _del_before(self) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tdel self._before\n\n\tdef _del_after(self) -> None:\n\t\t\"\"\"Set the bytes used to delimit slice points.\"\"\"\n\t\tdel self._after\n\n\tbefore = property(_get_before, _set_before, _del_before, 'bytes used to delimit slice points.')\n\tafter = property(_get_after, _set_after, _del_after, 'bytes used to delimit slice points.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before\n\t\tself.after = after\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself._before = before\n\t\tself._after = after\n\n\t@property\n\tdef before(self) -> bytes:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\nReturns:\n    The cut characters.\n\t\t\"\"\"\n\t\treturn self._before\n\n\t@property\n\tdef after(self) -> bytes:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\nReturns:\n    The cut characters.\n\t\t\"\"\"\n\t\treturn self._after\n\n\t@property\n\tdef cut_chars(self) -> bytes:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\nReturns:\n    The cut characters.\n\t\t\"\"\"\n\t\tif len(self._before)!= len(self._after):\n\t\t\traise ValueError(\"Before and after cut chars must be the same length.\")\n\t\treturn self._before + self._after\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock\n\nfrom fs.base import FS\nfrom fs.file_like import FileLike\n\nfrom fs_s3fs.s3fs import S3FS\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before\n\t\tself.after = after\n\n\tdef split(self, in_filename: str, out_dir: str) -> None:\n\t\t\"\"\"\n\t\tSplit a file into multiple files.\n\nArgs:\n    in_filename: The input file to split.\n    out_dir: The directory to write the split files to.\n\t\"\"\"\n\t\tif not self.before or not self.after:\n\t\t\traise Exception(\"Cut chars not set\")\n\n\t\twith open(in_filename, \"rb\") as in_file:\n\t\t\twhile True:\n\t\t\t\tdata = in_file.read(1024)\n\t\t\t\tif not data:\n\t\t\t\t\tbreak\n\n\t\t\t\tbefore = self.before\n\t\t\t\tif before in data:\n\t\t\t\t\tbefore_index = data.index(before)\n\t\t\t\t\tdata = data[before_index + len(before):]\n\n\t\t\t\tafter = self.after\n\t\t\t\tif after in data:\n\t\t\t\t\tafter_index = data.index(after)\n\t\t\t\t\tdata = data[:after_index]\n\t\t\t\t\twith open(os.path.join(out_dir, f\"{self.count}.mp3\"), \"wb\") as out_file:\n\t\t\t\t\t\tout_file.write(data)\n\t\t\t\t\tself.count += 1\n\n\tdef join(self, in_dir: str, out_filename: str) -> None:\n\t\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.cut_chars = (before, after)\n\tdef set_use_cut_chars(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to use the bytes used to delimit slice points.\n\nArgs:\n    value: True to use the bytes used to delimit slice points, False otherwise.\n\t\"\"\"\n\t\tself.use_cut_chars = value\n\tdef set_use_file_name_prefix(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to include the file name as part of the slice file name.\n\nArgs:\n    value: True to include the file name as part of the slice file name, False otherwise.\n\t\"\"\"\n\t\tself.use_file_name_prefix = value\n\tdef set_use_file_name_suffix(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to include the file name as part of the slice file name.\n\nArgs:\n    value: True to include the file name as part of the slice file name, False otherwise.\n\t\"\"\"\n\t\tself.use_file_name_suffix = value\n\tdef set_use_slice_number(self, value: bool) -> None:\n\t\"\"\"\n\tSet whether to include the slice number as part of the slice file name.\n\t\"\"\"\n\t\tself.use_slice", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\tif request.get(\"events\", default=None) is not None:\n\t\t\treturn True\n\t\telif request.get(\"tag\", default=None) == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\t# check whether the request body loaded using JSON contains events\n\t# if yes, return True, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG\n\t# if yes, return True, otherwise, return False\n\ttry:\n\t\t# try to convert the request body loaded using JSON into a dict\n\t\trequest_body_dict = json.loads(request.body)\n\texcept Exception as e:\n\t\t# if the request body loaded using JSON cannot be converted into a dict, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG\n\t\tif request.headers[\"Content-Type\"].split(\";\")[0] == \"application/xml\":\n\t\t\trequest_body_etree = ET.fromstringlist(request.body)\n\t\t\tif request_body_etree.tag == Magic_ENV_TAG:\n\t\t\t\treturn True\n\telse:\n\t\t# if the request body loaded using JSON can be converted into a dict, check whether the request body contains events\n\t\tif \"events\" in request_body_dict:\n\t\t\treturn True\n\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif not isinstance(request, RequestType):\n\t\treturn False\n\n\ttry:\n\t\tif isinstance(request, RequestType):\n\t\t\trequest_data = json.loads(request.body)\n\t\t\tif 'events' in request_data:\n\t\t\t\treturn True\n\t\t\telif request.headers.get('Content-Type') == 'application/json' and request_data.get('tag') == Magic_ENV_TAG:\n\t\t\t\treturn True\n\texcept Exception as e:\n\t\tlogger.error('Failed to identify_request:'+ str(request.body))\n\t\tlogger.error(str(e))\n\t\treturn False\n\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif \"events\" in request.json:\n\t\treturn True\n\telif \"xml\" in request.json and request.json['xml'][:len(Magic_ENV_TAG)] == Magic_ENV_TAG:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif not request.json:\n\t\treturn False\n\n\treturn True\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\tprint(request)\n\tif request.json is not None:\n\t\tif len(request.json['events']) > 0:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif request.xml is not None:\n\t\tif request.xml.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\tif isinstance(request, dict):\n\t\t\tevents = request.get('events')\n\t\t\tif events:\n\t\t\t\treturn True\n\n\t\t\tif request.get('tag') == Magic_ENV_TAG:\n\t\t\t\treturn True\n\n\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tif not request.json:\n\t\t\treturn False\n\t\telse:\n\t\t\tif not isinstance(request.json, dict):\n\t\t\t\traise InvalidEventException(\n\t\t\t\t\t\"The request body is not a JSON.\")\n\n\t\t\tif not request.json.get(\"events\"):\n\t\t\t\traise InvalidEventException(\n\t\t\t\t\t\"The event field is not found.\")\n\n\t\t\tif isinstance(request.json.get(\"events\"), list):\n\t\t\t\traise InvalidEventException(\n\t\t\t\t\t\"The event field is an array.\")\n\n\t\t\treturn True\n\texcept InvalidEventException as e:\n\t\treturn False\n\n\texcept Exception as e:\n\t\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t \n\tif isinstance(request, dict):\n\t\tif 'events' in request:\n\t\t\treturn True\n\t\telif 'event' in request:\n\t\t\tfor event in request['event']:\n\t\t\t\tif event['tag'] == Magic_ENV_TAG:\n\t\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\troot = request.getroot()\n\t\tif root.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telif root.tag == 'events':\n\t\t\treturn True\n\t\telif root.tag == 'event':\n\t\t\tfor event in root:\n\t\t\t\tif event.tag == Magic_ENV_TAG:\n\t\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif request.json:\n\t\tif request.json.get(\"events\"):\n\t\t\treturn True\n\tif request.data:\n\t\tif request.data.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\treturn False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\tif request.get(\"events\", default=None) is not None:\n\t\t\treturn True\n\t\telif request.get(\"tag\", default=None) == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\t# check whether the request body loaded using JSON contains events\n\t# if yes, return True, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG\n\t# if yes, return True, otherwise, return False\n\ttry:\n\t\t# try to convert the request body loaded using JSON into a dict\n\t\trequest_body_dict = json.loads(request.body)\n\texcept Exception as e:\n\t\t# if the request body loaded using JSON cannot be converted into a dict, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG\n\t\tif request.headers[\"Content-Type\"].split(\";\")[0] == \"application/xml\":\n\t\t\trequest_body_etree = ET.fromstringlist(request.body)\n\t\t\tif request_body_etree.tag == Magic_ENV_TAG:\n\t\t\t\treturn True\n\telse:\n\t\t# if the request body loaded using JSON can be converted into a dict, check whether the request body contains events\n\t\tif \"events\" in request_body_dict:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif not isinstance(request, RequestType):\n\t\treturn False\n\n\ttry:\n\t\tif isinstance(request, RequestType):\n\t\t\trequest_data = json.loads(request.body)\n\t\t\tif 'events' in request_data:\n\t\t\t\treturn True\n\t\t\telif request.headers.get('Content-Type') == 'application/json' and request_data.get('tag') == Magic_ENV_TAG:\n\t\t\t\treturn True\n\texcept Exception as e:\n\t\tlogger.error('Failed to identify_request:'+ str(request.body))\n\t\tlogger.error(str(e))\n\t\treturn False\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif \"events\" in request.json:\n\t\treturn True\n\telif \"xml\" in request.json and request.json['xml'][:len(Magic_ENV_TAG)] == Magic_ENV_TAG:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif not request.json:\n\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\tprint(request)\n\tif request.json is not None:\n\t\tif len(request.json['events']) > 0:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif request.xml is not None:\n\t\tif request.xml.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\tif isinstance(request, dict):\n\t\t\tevents = request.get('events')\n\t\t\tif events:\n\t\t\t\treturn True\n\n\t\t\tif request.get('tag') == Magic_ENV_TAG:\n\t\t\t\treturn True\n\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tif not request.json:\n\t\t\treturn False\n\t\telse:\n\t\t\tif not isinstance(request.json, dict):\n\t\t\t\traise InvalidEventException(\n\t\t\t\t\t\"The request body is not a JSON.\")\n\n\t\t\tif not request.json.get(\"events\"):\n\t\t\t\traise InvalidEventException(\n\t\t\t\t\t\"The event field is not found.\")\n\n\t\t\tif isinstance(request.json.get(\"events\"), list):\n\t\t\t\traise InvalidEventException(\n\t\t\t\t\t\"The event field is an array.\")\n\n\t\t\treturn True\n\texcept InvalidEventException as e:\n\t\treturn False\n\n\texcept Exception as e:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t \n\tif isinstance(request, dict):\n\t\tif 'events' in request:\n\t\t\treturn True\n\t\telif 'event' in request:\n\t\t\tfor event in request['event']:\n\t\t\t\tif event['tag'] == Magic_ENV_TAG:\n\t\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\troot = request.getroot()\n\t\tif root.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\t\telif root.tag == 'events':\n\t\t\treturn True\n\t\telif root.tag == 'event':\n\t\t\tfor event in root:\n\t\t\t\tif event.tag == Magic_ENV_TAG:\n\t\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\n\tif request.json:\n\t\tif request.json.get(\"events\"):\n\t\t\treturn True\n\tif request.data:\n\t\tif request.data.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\treturn 'events' in request.json\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\treturn 'events' in request.get('body')\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tif len(request.json) == 0:\n\t\t\treturn False\n\t\telse:\n\t\t\t# check whether the events are in the list\n\t\t\tif \"events\" in request.json:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\texcept:\n\t\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\treturn 'events' in json.loads(request.body)\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\ttry:\n\t\t\tif request.body[\"events\"]:\n\t\t\t\treturn True\n\t\texcept KeyError:\n\t\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\n\n# TODO: Implement the event handler for the message received.", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\tif 'events' not in request.json:\n\t\treturn False\n\telse:\n\t\treturn True\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\tif 'events' in request:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tevents = request.json[\"events\"]\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n/pyproject.toml\n[tool.poetry]\nname = \"line-bot-sdk-with-flask\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\" <>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\nline-bot-sdk = \"^1.19.0\"\nFlask = \"^2.0.1\"\ngunicorn = \"^20.1.0\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n/src/line_bot_sdk_with_flask/bot.py\nfrom linebot import LineBotApi\nfrom linebot.models import TextSendMessage\n\nfrom src.line_bot_sdk_with_flask.config import ACCESS_TOKEN\n\nline_bot_api = LineBotApi(ACCESS_TOKEN)\n\nTEXT_MESSAGE_TEMPLATE = TextSendMessage(text=\"Hello, world.\")\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\t# Check whether the request body contains 'events'\n\tif 'events' in request:\n\t\treturn True\n\treturn False\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\treturn 'events' in request.json\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\treturn 'events' in request.get('body')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tif len(request.json) == 0:\n\t\t\treturn False\n\t\telse:\n\t\t\t# check whether the events are in the list\n\t\t\tif \"events\" in request.json:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\treturn 'events' in json.loads(request.body)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\ttry:\n\t\t\tif request.body[\"events\"]:\n\t\t\t\treturn True\n\t\texcept KeyError:\n\t\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\n\n# TODO: Implement the event handler for the message received.", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\tif 'events' not in request.json:\n\t\treturn False\n\telse:\n\t\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t \n\tif 'events' in request:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tevents = request.json[\"events\"]\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n/pyproject.toml\n[tool.poetry]\nname = \"line-bot-sdk-with-flask\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\" <>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\nline-bot-sdk = \"^1.19.0\"\nFlask = \"^2.0.1\"\ngunicorn = \"^20.1.0\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n/src/line_bot_sdk_with_flask/bot.py\nfrom linebot import LineBotApi\nfrom linebot.models import TextSendMessage\n\nfrom src.line_bot_sdk_with_flask.config import ACCESS_TOKEN\n\nline_bot_api = LineBotApi(ACCESS_TOKEN)\n\nTEXT_MESSAGE_TEMPLATE = TextSendMessage(text=\"Hello, world.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\t\t# Check whether the request body contains 'events'\n\tif 'events' in request:\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt, dt.tzinfo)\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n        if dt.tzinfo is None:\n                dt = ensure_timezone(dt)\n\treturn dt.time()\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt).time()\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t \n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%H:%M')\n\t", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\tdt = ensure_timezone(dt)\n\treturn dt.time()\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\treturn dt.strftime(\"%I:%M %p\").lstrip(\"0\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt).strftime(\"%I:%M %p\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t \n\tdt = ensure_timezone(dt)\n\treturn dt.time()\n\t", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt, dt.tzinfo)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n        if dt.tzinfo is None:\n                dt = ensure_timezone(dt)\n\treturn dt.time()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt).time()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t \n\tdt = ensure_timezone(dt)\n\treturn dt.strftime('%H:%M')\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\tdt = ensure_timezone(dt)\n\treturn dt.time()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\treturn dt.strftime(\"%I:%M %p\").lstrip(\"0\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt).strftime(\"%I:%M %p\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\n\treturn ensure_timezone(dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t \n\tdt = ensure_timezone(dt)\n\treturn dt.time()\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\treturn ensure_timezone(dt).strftime(\"%H:%M\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\ttags = set()\n\t\ttext = text.replace('\\\\', '\\\\\\\\')\n\t\tif replacer:\n\t\t\t# use a copy of the replacer so the original isn't mutated\n\t\t\treplacer = copy.copy(replacer)\n\n\t\t# replace HTML tags\n\t\twhile True:\n\t\t\ttag = re.search(r'<(.*?)>', text)\n\t\t\tif tag:\n\t\t\t\tstart, end = tag.span()\n\t\t\t\ttag_text = tag[0]\n\t\t\t\ttag_name = tag[1]\n\t\t\t\ttext = text[:start] + text[end:]\n\t\t\t\tif not tag_name.startswith('code'):\n\t\t\t\t\ttags.add(tag_name)\n\t\t\t\t\tif replacer:\n\t\t\t\t\t\ttext = replacer(tag_name) + text\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\t# replace BBCode tags\n\t\twhile True:\n\t\t\ttag = re.search(r'\\[(.*?)\\]', text)\n\t\t\tif tag:\n\t\t\t\tstart, end = tag.span()\n\t\t\t\ttag_text = tag[0]\n\t\t\t\ttag_name = tag[1]\n\t\t\t\ttext = text[:start] + text[end:]\n\t\t\t\tif tag_name == 'code':\n\t\t\t\t\ttag_text = tag_text.replace('\\\\', '\\\\\\\\')\n\t\t\t\t\ttags.add(tag_name)\n\t\t\t\t\ttext = '`' + tag_text + '`' +", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# The code below is a bit hacky, but it's the only way I could\n\t\t# make it work.\n\t\t#\n\t\t# The idea is that we want to be able to find tags while ignoring\n\t\t# code blocks, but we also want to make sure that we don't accidentally\n\t\t# replace tags inside a code block.\n\t\t#\n\t\t# So we use a regex to find tag words that are not inside a code block,\n\t\t# then we use another regex to find all tags inside code blocks, and\n\t\t# we remove matched tag words from the text.\n\t\t#\n\t\t# This is obviously not ideal, but I couldn't find a better way.\n\t#\n\t# The first regex finds all tag words that are not inside code blocks.\n\t# We also don't match inside comment blocks.\n\t#\n\t# This regex is a bit hacky, but it works.\n\t#\n\t# The idea is that we want to match any tag word that is not inside a code\n\t# block, but we also want to make sure that we don't match inside a comment\n\t# block.\n\t#\n\t# So we match any tag word if it is not inside a code block, but we also\n\t# make sure that we don't match inside comment blocks.\n\t#\n\t# This is a bit hacky, but it works.", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\tcode_blocks = []\n\t\ttags = set()\n\t\ttext = text.split(\"\\n\")\n\t\tfor line in text:\n\t\t\tif line.startswith(\"```\"):\n\t\t\t\tcode_blocks.append(not code_blocks[-1])\n\t\t\telse:\n\t\t\t\tcode_blocks.append(code_blocks[-1])\n\n\t\tfor i in range(len(text)):\n\t\t\tif not code_blocks[i]:\n\t\t\t\tfor tag in re.findall(r\"([^ \\n]+)\", text[i]):\n\t\t\t\t\tif tag.startswith(\"#\"):\n\t\t\t\t\t\ttags.add(tag)\n\t\t\t\t\t\tif replacer is not None:\n\t\t\t\t\t\t\ttext[i] = text[i].replace(tag, replacer(tag))\n\t\t\t\t\t\tbreak\n\n\t\treturn tags, \"\\n\".join(text)\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\ttags: Set[str] = set()\n\t\treplace_text = text\n\t\t# Check for tags\n\t\tfor tag in re.finditer(r\"\\[([^\\]]+)\\]\", text):\n\t\t\ttags.add(tag.group(1))\n\t\t\tif replacer:\n\t\t\t\treplace_text = replace_text.replace(tag.group(0), replacer(tag.group(1)))\n\t\treturn tags, replace_text\nimport re\nfrom typing import Set\n\nfrom. import utils\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# TODO: handle escaped tags?\n\t\t# TODO: handle escaped backslashes?\n\t\t# TODO: handle escaped asterisks?\n\n\ttags = set()\n\ttext = text\n\twhile True:\n\t\tstart = text.find('`')\n\t\tif start == -1:\n\t\t\tbreak\n\t\ttext = text[:start] + text[start + 1:]\n\t\tend = text.find('`')\n\t\tif end == -1:\n\t\t\tbreak\n\t\ttag = text[start + 1:end]\n\t\tif tag[0] == '*' and tag[-1] == '*':\n\t\t\ttag = tag[1:-1]\n\t\t\ttags.add(tag)\n\t\t\ttext = text[:start] + text[start + 1:end] + text[end + 1:]\n\t\telse:\n\t\t\ttext = text[:end + 1]\n\tif replacer is not None:\n\t\tfor tag in tags:\n\t\t\ttext = text.replace(f'`{tag}`', replacer(tag))\n\treturn tags, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n# TODO\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# Find and ignore code blocks\n\t\tcode_blocks = find_code_blocks(text)\n\t\tfor block in code_blocks:\n\t\t\ttext = text.replace(block, \"CODE_BLOCK_PLACEHOLDER\")\n\n\t\t# Find tags\n\t\ttags = set()\n\t\tfor word in text.split():\n\t\t\tif word[0] == \"#\":\n\t\t\t\ttags.add(word)\n\t\t\t\tif replacer:\n\t\t\t\t\ttext = text.replace(word, replacer(word))\n\n\t\treturn tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\tre_pattern = r'\\{\\{(.+?)\\}\\}'\n\t\ttag_set = set()\n\t\tnew_text = text\n\t\tfor match in re.finditer(re_pattern, text):\n\t\t\ttag = match.group(1).lower()\n\t\t\tif tag not in tag_set:\n\t\t\t\ttag_set.add(tag)\n\t\t\t\tif replacer is not None:\n\t\t\t\t\tnew_text = replacer(new_text, tag)\n\t\treturn tag_set, new_text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# Find tags\n\t\ttags = set(re.findall(r'\\{(\\w+?)\\}', text))\n\t\t# Find code blocks\n\t\tcode = re.findall(r'```(?:\\s*\\n)?([^\\n]*?)(?:\\n\\s*```)', text, flags=re.DOTALL)\n\t\t# Remove code blocks\n\t\tfor c in code:\n\t\t\ttext = text.replace('```' + c + '```', '')\n\t\t# If the replacer was passed\n\t\tif replacer:\n\t\t\t# Replace tags\n\t\t\tfor t in tags:\n\t\t\t\ttext = text.replace('{' + t + '}', replacer(t))\n\t\treturn tags, text\n\n\n# Regexs for checking if a string is a valid tag\n# These are all the same regex, but with different flags, so they can be called with \"re.fullmatch\"\n# (see https://docs.python.org/3/library/re.html#re.fullmatch)\nis_valid_tag_regex = [\n\tre.compile(r'^[a-zA-Z\\d\\-\\_\\s]+$', flags=re.IGNORECASE),\n\tre.compile(r'^[a-zA-Z\\d\\-\\_\\s]+$', flags=re.IGNORECASE | re.MULTILINE),\n\tre.compile", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\n\t# TODO: Make this work for nested blocks\n\t# TODO: Find a way to only replace tags in codeblocks\n\t# TODO: Create a function that finds tags in text\n\n\t# TODO: Check whether the tag is a code block tag\n\n\t# TODO: Create a function that finds the start and end of a code block\n\t# TODO: Make a \"find_tags_in_code_block\" function that only works on code blocks\n\n\t# TODO: Create a \"find_tags_in_code_block\" function that only works on code blocks\n\n\t# TODO: Create a \"find_tags_in_text\" function that only works on text\n\n\t# TODO: Create a \"find_tags_in_code_block\" function that only works on code blocks\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag searching into a function\n\n\t# TODO: Move the tag replacement into a function\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag searching into a function\n\n\t# TODO: Move the tag replacement into a function\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag searching into a function\n\n\t# TODO: Move the tag replacement into a function\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\ttags = set()\n\t\ttext = text.replace('\\\\', '\\\\\\\\')\n\t\tif replacer:\n\t\t\t# use a copy of the replacer so the original isn't mutated\n\t\t\treplacer = copy.copy(replacer)\n\n\t\t# replace HTML tags\n\t\twhile True:\n\t\t\ttag = re.search(r'<(.*?)>', text)\n\t\t\tif tag:\n\t\t\t\tstart, end = tag.span()\n\t\t\t\ttag_text = tag[0]\n\t\t\t\ttag_name = tag[1]\n\t\t\t\ttext = text[:start] + text[end:]\n\t\t\t\tif not tag_name.startswith('code'):\n\t\t\t\t\ttags.add(tag_name)\n\t\t\t\t\tif replacer:\n\t\t\t\t\t\ttext = replacer(tag_name) + text\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\t# replace BBCode tags\n\t\twhile True:\n\t\t\ttag = re.search(r'\\[(.*?)\\]', text)\n\t\t\tif tag:\n\t\t\t\tstart, end = tag.span()\n\t\t\t\ttag_text = tag[0]\n\t\t\t\ttag_name = tag[1]\n\t\t\t\ttext = text[:start] + text[end:]\n\t\t\t\tif tag_name == 'code':\n\t\t\t\t\ttag_text = tag_text.replace('\\\\', '\\\\\\\\')\n\t\t\t\t\ttags.add(tag_name)\n\t\t\t\t\ttext = '`' + tag_text + '`' +", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# The code below is a bit hacky, but it's the only way I could\n\t\t# make it work.\n\t\t#\n\t\t# The idea is that we want to be able to find tags while ignoring\n\t\t# code blocks, but we also want to make sure that we don't accidentally\n\t\t# replace tags inside a code block.\n\t\t#\n\t\t# So we use a regex to find tag words that are not inside a code block,\n\t\t# then we use another regex to find all tags inside code blocks, and\n\t\t# we remove matched tag words from the text.\n\t\t#\n\t\t# This is obviously not ideal, but I couldn't find a better way.\n\t#\n\t# The first regex finds all tag words that are not inside code blocks.\n\t# We also don't match inside comment blocks.\n\t#\n\t# This regex is a bit hacky, but it works.\n\t#\n\t# The idea is that we want to match any tag word that is not inside a code\n\t# block, but we also want to make sure that we don't match inside a comment\n\t# block.\n\t#\n\t# So we match any tag word if it is not inside a code block, but we also\n\t# make sure that we don't match inside comment blocks.\n\t#\n\t# This is a bit hacky, but it works.", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\tcode_blocks = []\n\t\ttags = set()\n\t\ttext = text.split(\"\\n\")\n\t\tfor line in text:\n\t\t\tif line.startswith(\"```\"):\n\t\t\t\tcode_blocks.append(not code_blocks[-1])\n\t\t\telse:\n\t\t\t\tcode_blocks.append(code_blocks[-1])\n\n\t\tfor i in range(len(text)):\n\t\t\tif not code_blocks[i]:\n\t\t\t\tfor tag in re.findall(r\"([^ \\n]+)\", text[i]):\n\t\t\t\t\tif tag.startswith(\"#\"):\n\t\t\t\t\t\ttags.add(tag)\n\t\t\t\t\t\tif replacer is not None:\n\t\t\t\t\t\t\ttext[i] = text[i].replace(tag, replacer(tag))\n\t\t\t\t\t\tbreak\n\n\t\treturn tags, \"\\n\".join(text)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\ttags: Set[str] = set()\n\t\treplace_text = text\n\t\t# Check for tags\n\t\tfor tag in re.finditer(r\"\\[([^\\]]+)\\]\", text):\n\t\t\ttags.add(tag.group(1))\n\t\t\tif replacer:\n\t\t\t\treplace_text = replace_text.replace(tag.group(0), replacer(tag.group(1)))\n\t\treturn tags, replace_text\nimport re\nfrom typing import Set\n\nfrom. import utils\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# TODO: handle escaped tags?\n\t\t# TODO: handle escaped backslashes?\n\t\t# TODO: handle escaped asterisks?\n\n\ttags = set()\n\ttext = text\n\twhile True:\n\t\tstart = text.find('`')\n\t\tif start == -1:\n\t\t\tbreak\n\t\ttext = text[:start] + text[start + 1:]\n\t\tend = text.find('`')\n\t\tif end == -1:\n\t\t\tbreak\n\t\ttag = text[start + 1:end]\n\t\tif tag[0] == '*' and tag[-1] == '*':\n\t\t\ttag = tag[1:-1]\n\t\t\ttags.add(tag)\n\t\t\ttext = text[:start] + text[start + 1:end] + text[end + 1:]\n\t\telse:\n\t\t\ttext = text[:end + 1]\n\tif replacer is not None:\n\t\tfor tag in tags:\n\t\t\ttext = text.replace(f'`{tag}`', replacer(tag))\n\treturn tags, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n# TODO\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# Find and ignore code blocks\n\t\tcode_blocks = find_code_blocks(text)\n\t\tfor block in code_blocks:\n\t\t\ttext = text.replace(block, \"CODE_BLOCK_PLACEHOLDER\")\n\n\t\t# Find tags\n\t\ttags = set()\n\t\tfor word in text.split():\n\t\t\tif word[0] == \"#\":\n\t\t\t\ttags.add(word)\n\t\t\t\tif replacer:\n\t\t\t\t\ttext = text.replace(word, replacer(word))\n\n\t\treturn tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\tre_pattern = r'\\{\\{(.+?)\\}\\}'\n\t\ttag_set = set()\n\t\tnew_text = text\n\t\tfor match in re.finditer(re_pattern, text):\n\t\t\ttag = match.group(1).lower()\n\t\t\tif tag not in tag_set:\n\t\t\t\ttag_set.add(tag)\n\t\t\t\tif replacer is not None:\n\t\t\t\t\tnew_text = replacer(new_text, tag)\n\t\treturn tag_set, new_text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\t# Find tags\n\t\ttags = set(re.findall(r'\\{(\\w+?)\\}', text))\n\t\t# Find code blocks\n\t\tcode = re.findall(r'```(?:\\s*\\n)?([^\\n]*?)(?:\\n\\s*```)', text, flags=re.DOTALL)\n\t\t# Remove code blocks\n\t\tfor c in code:\n\t\t\ttext = text.replace('```' + c + '```', '')\n\t\t# If the replacer was passed\n\t\tif replacer:\n\t\t\t# Replace tags\n\t\t\tfor t in tags:\n\t\t\t\ttext = text.replace('{' + t + '}', replacer(t))\n\t\treturn tags, text\n\n\n# Regexs for checking if a string is a valid tag\n# These are all the same regex, but with different flags, so they can be called with \"re.fullmatch\"\n# (see https://docs.python.org/3/library/re.html#re.fullmatch)\nis_valid_tag_regex = [\n\tre.compile(r'^[a-zA-Z\\d\\-\\_\\s]+$', flags=re.IGNORECASE),\n\tre.compile(r'^[a-zA-Z\\d\\-\\_\\s]+$', flags=re.IGNORECASE | re.MULTILINE),\n\tre.compile", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\n\t# TODO: Make this work for nested blocks\n\t# TODO: Find a way to only replace tags in codeblocks\n\t# TODO: Create a function that finds tags in text\n\n\t# TODO: Check whether the tag is a code block tag\n\n\t# TODO: Create a function that finds the start and end of a code block\n\t# TODO: Make a \"find_tags_in_code_block\" function that only works on code blocks\n\n\t# TODO: Create a \"find_tags_in_code_block\" function that only works on code blocks\n\n\t# TODO: Create a \"find_tags_in_text\" function that only works on text\n\n\t# TODO: Create a \"find_tags_in_code_block\" function that only works on code blocks\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag searching into a function\n\n\t# TODO: Move the tag replacement into a function\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag searching into a function\n\n\t# TODO: Move the tag replacement into a function\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag searching into a function\n\n\t# TODO: Move the tag replacement into a function\n\n\t# TODO: Move the code block searching into a function\n\n\t# TODO: Move the tag", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# find all email addresses\n\tfor email_addr in re.findall(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,3})', text):\n\t\ttext = text.replace(email_addr, '<a href=\"mailto:%s\">%s</a>' % (email_addr, email_addr))\n\n\t# find all URLs\n\tfor url in re.findall(r'((?:http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)', text):\n\t\ttext = text.replace(url, '<a href=\"%s\">%s</a>' % (url, url))\n\n\treturn text\n\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# process links\n\tp = re.compile(r'\\[([^\\]]+)\\]\\(([^\\)]+)\\)')\n\tdef repl(m):\n\t\ttext = m.group(1)\n\t\tlink = m.group(2)\n\t\treturn '<a class=\"link\" href=\"%s\">%s</a>' % (link, text)\n\ttext = p.sub(repl, text)\n\n\t# add attribute to links\n\tp = re.compile(r'<a\\s+[^>]+>')\n\tdef repl(m):\n\t\treturn m.group(0) +'target=\"_blank\"'\n\ttext = p.sub(repl, text)\n\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n#\ttext = re.sub(r'\\((\\w+\\.txt)\\)', r'(\\1)', text)\n\ttext = re.sub(r'\\[(%s)\\]' % re.escape(text_link_regex), r'[\\1](\\1)', text)\n#\ttext = re.sub(r'\\[(%s)\\]' % re.escape(text_link_regex), r'[\\1](\\1)', text)\n\ttext = re.sub(r'\\[(%s)\\]' % re.escape(image_link_regex), r'[\\1](\\1)', text)\n\ttext = re.sub(r'\\[([^\\]]+)\\]', r'<a href=\"\\1\">\\1</a>', text)\n\treturn text\n\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\ttext = text.replace('\\n', '<br>')\n\t\tfor link in re.findall(r'(?P<link>https?://[^\\s]+)', text):\n\t\t\tlink_text = link.replace('https://', '').replace('http://', '')\n\t\t\tif link_text.startswith('www.'):\n\t\t\t\tlink_text = link_text[4:]\n\t\t\ttext = text.replace(link, '<a target=\"_blank\" href=\"' + link + '\">' + link_text + '</a>')\n\t\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Add attributes to the links in the text\n\ttext = re.sub(r'\\[(\\w+)\\]\\((https?://[^\\s\\[\\]\\(\\)]+)\\)', r'<a href=\"\\2\" class=\"\\1\">\\2</a>', text)\n\n\t# Linkify textual links\n\ttext = re.sub(r'\\[(\\w+)\\]\\((https?://[^\\s\\[\\]\\(\\)]+)\\)', r'<a href=\"\\2\" class=\"\\1\">\\2</a>', text)\n\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t \n\t# Add target=\"_blank\" to links\n\ttext = re.sub(r'<a(.*?)>', r'<a \\1 target=\"_blank\">', text)\n\n\t# Linkify text links\n\ttext = re.sub(r'http://([\\w\\-\\.]+)', r'<a href=\"http://\\1\">http://\\1</a>', text)\n\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n#\ttext = \"http://www.google.com\"\n#\ttext = \"http://www.google.com/search?q=test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Add class=internal to internal links\n\ttext = re.sub(r'(<a\\s+href=\")(?P<link>.*?)(\")', r'\\g<1>internal \\g<2>\\g<3>', text)\n\n\t# Linkify textual links\n\ttext = re.sub(r'(\\w+):\\/\\/([^ <>]+)', r'<a href=\"http://\\2\" class=\"internal external\">\\1://\\2</a>', text)\n\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\t# Add some attributes to links\n\t\tfor m in re.finditer(r'<a href=\"([^\"]+)\">([^<]+)</a>', text):\n\t\t\t# Add attributes\n\t\t\ttext = text.replace(m.group(0), '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (m.group(1), m.group(2)))\n\t\t# Linkify textual links\n\t\tfor m in re.finditer(r'(?<!<a href=\"[^\"]+\">)(https?://[^<]+)', text):\n\t\t\t# Add attributes\n\t\t\ttext = text.replace(m.group(0), '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (m.group(0), m.group(0)))\n\treturn text\n\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Add attributes to links\n\ttext = re.sub(r'\\[(https?://[^\\s]+)\\]', r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n\ttext = re.sub(r'\\[(https?://[^\\s]+)\\s+([^\\s]+)\\]', r'<a href=\"\\1\" target=\"_blank\">\\2</a>', text)\n\n\t# Add attributes to textual links\n\ttext = re.sub(r'([^\\s])\\s+([^\\s]+)\\s+([^\\s])', r'\\1<a href=\"\\2\" target=\"_blank\">\\2</a>\\3', text)\n\n\treturn text\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# find all email addresses\n\tfor email_addr in re.findall(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,3})', text):\n\t\ttext = text.replace(email_addr, '<a href=\"mailto:%s\">%s</a>' % (email_addr, email_addr))\n\n\t# find all URLs\n\tfor url in re.findall(r'((?:http|ftp)s?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)', text):\n\t\ttext = text.replace(url, '<a href=\"%s\">%s</a>' % (url, url))\n\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# process links\n\tp = re.compile(r'\\[([^\\]]+)\\]\\(([^\\)]+)\\)')\n\tdef repl(m):\n\t\ttext = m.group(1)\n\t\tlink = m.group(2)\n\t\treturn '<a class=\"link\" href=\"%s\">%s</a>' % (link, text)\n\ttext = p.sub(repl, text)\n\n\t# add attribute to links\n\tp = re.compile(r'<a\\s+[^>]+>')\n\tdef repl(m):\n\t\treturn m.group(0) +'target=\"_blank\"'\n\ttext = p.sub(repl, text)\n\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n#\ttext = re.sub(r'\\((\\w+\\.txt)\\)', r'(\\1)', text)\n\ttext = re.sub(r'\\[(%s)\\]' % re.escape(text_link_regex), r'[\\1](\\1)', text)\n#\ttext = re.sub(r'\\[(%s)\\]' % re.escape(text_link_regex), r'[\\1](\\1)', text)\n\ttext = re.sub(r'\\[(%s)\\]' % re.escape(image_link_regex), r'[\\1](\\1)', text)\n\ttext = re.sub(r'\\[([^\\]]+)\\]', r'<a href=\"\\1\">\\1</a>', text)\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\ttext = text.replace('\\n', '<br>')\n\t\tfor link in re.findall(r'(?P<link>https?://[^\\s]+)', text):\n\t\t\tlink_text = link.replace('https://', '').replace('http://', '')\n\t\t\tif link_text.startswith('www.'):\n\t\t\t\tlink_text = link_text[4:]\n\t\t\ttext = text.replace(link, '<a target=\"_blank\" href=\"' + link + '\">' + link_text + '</a>')\n\t\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Add attributes to the links in the text\n\ttext = re.sub(r'\\[(\\w+)\\]\\((https?://[^\\s\\[\\]\\(\\)]+)\\)', r'<a href=\"\\2\" class=\"\\1\">\\2</a>', text)\n\n\t# Linkify textual links\n\ttext = re.sub(r'\\[(\\w+)\\]\\((https?://[^\\s\\[\\]\\(\\)]+)\\)', r'<a href=\"\\2\" class=\"\\1\">\\2</a>', text)\n\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t \n\t# Add target=\"_blank\" to links\n\ttext = re.sub(r'<a(.*?)>', r'<a \\1 target=\"_blank\">', text)\n\n\t# Linkify text links\n\ttext = re.sub(r'http://([\\w\\-\\.]+)', r'<a href=\"http://\\1\">http://\\1</a>', text)\n\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n#\ttext = \"http://www.google.com\"\n#\ttext = \"http://www.google.com/search?q=test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=test&test=test#test#test#test#test#test#test#test\"\n#\ttext = \"http://www.google.com/search?q=", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Add class=internal to internal links\n\ttext = re.sub(r'(<a\\s+href=\")(?P<link>.*?)(\")', r'\\g<1>internal \\g<2>\\g<3>', text)\n\n\t# Linkify textual links\n\ttext = re.sub(r'(\\w+):\\/\\/([^ <>]+)', r'<a href=\"http://\\2\" class=\"internal external\">\\1://\\2</a>', text)\n\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\t# Add some attributes to links\n\t\tfor m in re.finditer(r'<a href=\"([^\"]+)\">([^<]+)</a>', text):\n\t\t\t# Add attributes\n\t\t\ttext = text.replace(m.group(0), '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (m.group(1), m.group(2)))\n\t\t# Linkify textual links\n\t\tfor m in re.finditer(r'(?<!<a href=\"[^\"]+\">)(https?://[^<]+)', text):\n\t\t\t# Add attributes\n\t\t\ttext = text.replace(m.group(0), '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (m.group(0), m.group(0)))\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Add attributes to links\n\ttext = re.sub(r'\\[(https?://[^\\s]+)\\]', r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n\ttext = re.sub(r'\\[(https?://[^\\s]+)\\s+([^\\s]+)\\]', r'<a href=\"\\1\" target=\"_blank\">\\2</a>', text)\n\n\t# Add attributes to textual links\n\ttext = re.sub(r'([^\\s])\\s+([^\\s]+)\\s+([^\\s])', r'\\1<a href=\"\\2\" target=\"_blank\">\\2</a>\\3', text)\n\n\treturn text\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\ttry:\n\t\t\tif url.startswith(r\"http://\") or url.startswith(r\"https://\"):\n\t\t\t\thead = {\n\t\t\t\t\t\"User-Agent\": USER_AGENT,\n\t\t\t\t\t\"Accept\": \"text/html\",\n\t\t\t\t\t\"Accept-Charset\": \"utf-8\",\n\t\t\t\t\t\"Connection\": \"keep-alive\",\n\t\t\t\t}\n\t\t\telse:\n\t\t\t\thead = {\"User-Agent\": USER_AGENT}\n\t\t\tr = requests.get(url, headers=head)\n\t\t\tif r.status_code == 200:\n\t\t\t\treturn r.headers['Content-Type']\n\t\texcept:\n\t\t\treturn None\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\ttry:\n\t\t\thtml = requests.get(url, headers=headers)\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\t_LOGGER.error(e)\n\t\t\treturn None\n\t\telse:\n\t\t\treturn html.headers.get(\"Content-Type\")\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\twith requests.Session() as s:\n\t\ts.headers = HEADERS\n\t\tr = s.get(url)\n\t\treturn r.headers.get('content-type')\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\theaders = {\n\t\t'User-Agent': USER_AGENT,\n\t}\n\n\twith requests.Session() as session:\n\t\ttry:\n\t\t\tresponse = session.head(url, timeout=10, headers=headers)\n\t\t\tresponse.raise_for_status()\n\t\t\tcontent_type = response.headers['content-type']\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tprint(e)\n\t\t\tcontent_type = None\n\treturn content_type\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\treturn content_type\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\trequest = requests.get(url, headers={\n\t\t\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"})\n\t\treturn request.headers.get('Content-Type')\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\theaders = {\n\t\t\t'User-Agent': USER_AGENT,\n\t\t\t'referer': url,\n\t\t}\n\t\ttry:\n\t\t\twith closing(requests.get(url, headers=headers, stream=True, timeout=5)) as response:\n\t\t\t\tif not response.ok:\n\t\t\t\t\treturn None\n\t\t\t\tcontent_type = response.headers.get('content-type')\n\t\t\t\tif not content_type:\n\t\t\t\t\treturn None\n\t\t\t\treturn content_type\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tprint(e)\n\t\t\treturn None\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\tr = requests.head(url, headers={'User-Agent': USER_AGENT})\n\tif 'Content-Type' in r.headers:\n\t\treturn r.headers['Content-Type']\n\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\tresponse = requests.get(url, headers={\"User-agent\": USER_AGENT}, allow_redirects=True)\n\t\treturn response.headers.get(\"Content-Type\")\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\ttry:\n\t\t\tif url.startswith(r\"http://\") or url.startswith(r\"https://\"):\n\t\t\t\thead = {\n\t\t\t\t\t\"User-Agent\": USER_AGENT,\n\t\t\t\t\t\"Accept\": \"text/html\",\n\t\t\t\t\t\"Accept-Charset\": \"utf-8\",\n\t\t\t\t\t\"Connection\": \"keep-alive\",\n\t\t\t\t}\n\t\t\telse:\n\t\t\t\thead = {\"User-Agent\": USER_AGENT}\n\t\t\tr = requests.get(url, headers=head)\n\t\t\tif r.status_code == 200:\n\t\t\t\treturn r.headers['Content-Type']\n\t\texcept:\n\t\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\ttry:\n\t\t\thtml = requests.get(url, headers=headers)\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\t_LOGGER.error(e)\n\t\t\treturn None\n\t\telse:\n\t\t\treturn html.headers.get(\"Content-Type\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\twith requests.Session() as s:\n\t\ts.headers = HEADERS\n\t\tr = s.get(url)\n\t\treturn r.headers.get('content-type')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\theaders = {\n\t\t'User-Agent': USER_AGENT,\n\t}\n\n\twith requests.Session() as session:\n\t\ttry:\n\t\t\tresponse = session.head(url, timeout=10, headers=headers)\n\t\t\tresponse.raise_for_status()\n\t\t\tcontent_type = response.headers['content-type']\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tprint(e)\n\t\t\tcontent_type = None\n\treturn content_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\treturn content_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\trequest = requests.get(url, headers={\n\t\t\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"})\n\t\treturn request.headers.get('Content-Type')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\theaders = {\n\t\t\t'User-Agent': USER_AGENT,\n\t\t\t'referer': url,\n\t\t}\n\t\ttry:\n\t\t\twith closing(requests.get(url, headers=headers, stream=True, timeout=5)) as response:\n\t\t\t\tif not response.ok:\n\t\t\t\t\treturn None\n\t\t\t\tcontent_type = response.headers.get('content-type')\n\t\t\t\tif not content_type:\n\t\t\t\t\treturn None\n\t\t\t\treturn content_type\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tprint(e)\n\t\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\n\n\tr = requests.head(url, headers={'User-Agent': USER_AGENT})\n\tif 'Content-Type' in r.headers:\n\t\treturn r.headers['Content-Type']\n\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\t\tresponse = requests.get(url, headers={\"User-agent\": USER_AGENT}, allow_redirects=True)\n\t\treturn response.headers.get(\"Content-Type\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\tfor char in tag:\n\t\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\t\treturn False\n\n\t\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t1 + 1\n\treturn all(c in LEEGAL_TAG_CHARS for c in tag)\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\treturn all(c in LEEGAL_TAG_CHARS for c in tag)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\tfor char in tag:\n\t\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\t\treturn False\n\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t \n\tfor char in tag:\n\t\tif char not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t1 + 1\n\treturn all(c in LEEGAL_TAG_CHARS for c in tag)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\treturn all(c in LEEGAL_TAG_CHARS for c in tag)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t \n\trv = {}\n\tfor child in node.children:\n\t\tif child.tag in rv:\n\t\t\tif not isinstance(rv[child.tag], list):\n\t\t\t\trv[child.tag] = [rv[child.tag]]\n\t\t\trv[child.tag].append(child)\n\t\telse:\n\t\t\trv[child.tag] = child\n\treturn rv\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\td = {}\n\t\tfor child in node.childNodes:\n\t\t\tif child.nodeType == 1:\n\t\t\t\td[child.tagName] = xml_children_as_dict(child)\n\t\treturn d\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\treturn {child.tag: xml_children_as_dict(child) for child in node}\n\treturn {node.tag: [xml_children_as_dict(child) for child in node]}\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\n\n\td = {}\n\tfor n in node.childNodes:\n\t\tif n.nodeType == n.ELEMENT_NODE:\n\t\t\td[n.tagName] = xml_children_as_dict(n)\n\t\telse:\n\t\t\td[n.tagName] = n.nodeValue\n\treturn d\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t \n\tres = {}\n\tfor child in node:\n\t\tres[child.tag] = child.text\n\treturn res\n\n\t", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t \n\tchildren = node.getchildren()\n\tif not children:\n\t\treturn node.text\n\telse:\n\t\treturn dict_from_xml(children)\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\n#\tif node.getchildren() == []:\n#\t\treturn None\n#\telse:\n#\t\treturn dict((child.tag, child.text) for child in node.getchildren())\n\treturn dict((child.tag, child.text) for child in node.getchildren())\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\treturn dict(\n\t\t\t(child.tag, xml_children_as_dict(child))\n\t\t\tfor child in node.getchildren()\n\t\t)\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t # We'll use a dict as the accumulator, keyed by tag name\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\t# If this is a leaf element, just add it to the dict\n\t\tif child.getchildren() == []:\n\t\t\tchildren[child.tag] = child.text\n\t\t# If it's a complex child, recurse into it\n\t\telse:\n\t\t\tchildren[child.tag] = xml_children_as_dict(child)\n\treturn children\n\n# Read up on the xml.etree.ElementTree (https://docs.python.org/3/library/xml.etree.elementtree.html) module to see how this works", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\treturn {child.tag: xml_children_as_dict(child) for child in node}\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t \n\trv = {}\n\tfor child in node.children:\n\t\tif child.tag in rv:\n\t\t\tif not isinstance(rv[child.tag], list):\n\t\t\t\trv[child.tag] = [rv[child.tag]]\n\t\t\trv[child.tag].append(child)\n\t\telse:\n\t\t\trv[child.tag] = child\n\treturn rv\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\td = {}\n\t\tfor child in node.childNodes:\n\t\t\tif child.nodeType == 1:\n\t\t\t\td[child.tagName] = xml_children_as_dict(child)\n\t\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\treturn {child.tag: xml_children_as_dict(child) for child in node}\n\treturn {node.tag: [xml_children_as_dict(child) for child in node]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\n\n\td = {}\n\tfor n in node.childNodes:\n\t\tif n.nodeType == n.ELEMENT_NODE:\n\t\t\td[n.tagName] = xml_children_as_dict(n)\n\t\telse:\n\t\t\td[n.tagName] = n.nodeValue\n\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t \n\tres = {}\n\tfor child in node:\n\t\tres[child.tag] = child.text\n\treturn res\n\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t \n\tchildren = node.getchildren()\n\tif not children:\n\t\treturn node.text\n\telse:\n\t\treturn dict_from_xml(children)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\n#\tif node.getchildren() == []:\n#\t\treturn None\n#\telse:\n#\t\treturn dict((child.tag, child.text) for child in node.getchildren())\n\treturn dict((child.tag, child.text) for child in node.getchildren())\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\treturn dict(\n\t\t\t(child.tag, xml_children_as_dict(child))\n\t\t\tfor child in node.getchildren()\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t # We'll use a dict as the accumulator, keyed by tag name\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\t# If this is a leaf element, just add it to the dict\n\t\tif child.getchildren() == []:\n\t\t\tchildren[child.tag] = child.text\n\t\t# If it's a complex child, recurse into it\n\t\telse:\n\t\t\tchildren[child.tag] = xml_children_as_dict(child)\n\treturn children\n\n# Read up on the xml.etree.ElementTree (https://docs.python.org/3/library/xml.etree.elementtree.html) module to see how this works", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\t\treturn {child.tag: xml_children_as_dict(child) for child in node}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t/README.md\n# MARL-Game\n\nThis repository is the code for the paper:\n\n** and  and  and  and  and  and  and. (2020, September). Multi-Agent Reinforcement Learning for Generalized Markov Games. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 9106-9113). IEEE.**\n\nThe code is modified from https://github.com/google-research/google-research/tree/master/marl.\n\n## Installation\n\nTo install the dependencies, run the following command:\n\n```\npip install -r requirements.txt\npip install -e.\n```\n\n## Usage\n\nTo train the model, run the following command:\n\n```\npython -m marl.trpo.run_atari \\\n    --root=~/data/marl_atari_data \\\n    --game=PongNoFrameskip-v4 \\\n    --num_iterations=1000000 \\\n    --num_processes=10 \\\n    --batch_size=1024 \\\n    --num_epochs=3 \\\n    --num_units=512 \\\n    --num_agent_train_steps_per_iter", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\treturn True if sender_handle == entity_handle else False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t \n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\tif sender_handle == entity_handle:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t \n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\treturn sender_handle == entity_handle\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t \n\tif sender_handle == entity_handle:\n\t\treturn True\n\telif sender_handle!= entity_handle:\n\t\treturn False\n\telse:\n\t\tprint(\"check_sender_and_entity_handle_match: ERROR\")\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t/README.md\n# MARL-Game\n\nThis repository is the code for the paper:\n\n** and  and  and  and  and  and  and. (2020, September). Multi-Agent Reinforcement Learning for Generalized Markov Games. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 9106-9113). IEEE.**\n\nThe code is modified from https://github.com/google-research/google-research/tree/master/marl.\n\n## Installation\n\nTo install the dependencies, run the following command:\n\n```\npip install -r requirements.txt\npip install -e.\n```\n\n## Usage\n\nTo train the model, run the following command:\n\n```\npython -m marl.trpo.run_atari \\\n    --root=~/data/marl_atari_data \\\n    --game=PongNoFrameskip-v4 \\\n    --num_iterations=1000000 \\\n    --num_processes=10 \\\n    --batch_size=1024 \\\n    --num_epochs=3 \\\n    --num_units=512 \\\n    --num_agent_train_steps_per_iter", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\treturn True if sender_handle == entity_handle else False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t \n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\tif sender_handle == entity_handle:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t \n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\treturn sender_handle == entity_handle\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t \n\tif sender_handle == entity_handle:\n\t\treturn True\n\telif sender_handle!= entity_handle:\n\t\treturn False\n\telse:\n\t\tprint(\"check_sender_and_entity_handle_match: ERROR\")\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif not document_path:\n\t\tdocument_path = url.rsplit('/', 1)[-1]\n\n\treturn { \"url\": url, \"document_path\": document_path }\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif document_path is None:\n\t\tdocument_path = get_nodeinfo_well_known_document_path(url)\n\tdocument_url = url + document_path\n\t\n\ttry:\n\t\tresponse = requests.get(document_url)\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path,\n\t\t\t\"document_url\": document_url,\n\t\t\t\"response\": response\n\t\t}\n\texcept requests.exceptions.RequestException as e:\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path,\n\t\t\t\"document_url\": document_url,\n\t\t\t\"response\": {\n\t\t\t\t\"error\": e\n\t\t\t}\n\t\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\n\tnodeinfo = dict()\n\tnodeinfo['url'] = url\n\tnodeinfo['document_path'] = document_path\n\n\tif document_path is not None:\n\t\tnodeinfo['document_path'] = document_path\n\n\treturn nodeinfo\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\ty\n\"\"\"\n\nfrom. import web_common\n\n__all__ = [\"web_common\"]\"\"\"\nAll functions/classes for the web interface will be defined in this director\"\"\"\nA module which contains all functions/classes for the web interface.\n\"\"\"\n\nfrom. import web_common\n\n__all__ = [\"web_common\"]\t\treturn \"unknown\"\n\telse:\n\t\treturn \"unknown\"\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = get_document_path(url)\n\t\turl_info = get_url_info(url)\n\t\turl_info[\"document_path\"] = document_path\n\t\treturn url_info\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif document_path is None:\n\t\tdocument_path = \"\"\n\t\n\tnodeinfo = {}\n\tnodeinfo[\"url\"] = url\n\tnodeinfo[\"document_path\"] = document_path\n\tnodeinfo[\"status\"] = \"Success\"\n\t\n\treturn nodeinfo\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\n\tif not document_path:\n\t\tdocument_path = urlparse(url).path\n\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\n\tif document_path is None:\n\t\t#document_path = get_nodeinfo_well_known_path(url)\n\t\tdocument_path = get_nodeinfo_well_known_document(url, document_path)\n\n\tdocument_path = resolve_url(url, document_path)\n\treturn get_document(document_path)\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\t# get nodeinfo well-known document\n\tnodeinfo_url = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(nodeinfo_url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\tnodeinfo_url = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(nodeinfo_url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif document_path is None:\n\t\tdocument_path = _get_nodeinfo_document_path(url)\n\tif document_path is None:\n\t\treturn None\n\treturn {'url': url, 'document_path': document_path}\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif not document_path:\n\t\tdocument_path = url.rsplit('/', 1)[-1]\n\n\treturn { \"url\": url, \"document_path\": document_path }\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif document_path is None:\n\t\tdocument_path = get_nodeinfo_well_known_document_path(url)\n\tdocument_url = url + document_path\n\t\n\ttry:\n\t\tresponse = requests.get(document_url)\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path,\n\t\t\t\"document_url\": document_url,\n\t\t\t\"response\": response\n\t\t}\n\texcept requests.exceptions.RequestException as e:\n\t\treturn {\n\t\t\t\"url\": url,\n\t\t\t\"document_path\": document_path,\n\t\t\t\"document_url\": document_url,\n\t\t\t\"response\": {\n\t\t\t\t\"error\": e\n\t\t\t}\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\n\tnodeinfo = dict()\n\tnodeinfo['url'] = url\n\tnodeinfo['document_path'] = document_path\n\n\tif document_path is not None:\n\t\tnodeinfo['document_path'] = document_path\n\n\treturn nodeinfo\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\ty\n\"\"\"\n\nfrom. import web_common\n\n__all__ = [\"web_common\"]\"\"\"\nAll functions/classes for the web interface will be defined in this director\"\"\"\nA module which contains all functions/classes for the web interface.\n\"\"\"\n\nfrom. import web_common\n\n__all__ = [\"web_common\"]\t\treturn \"unknown\"\n\telse:\n\t\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = get_document_path(url)\n\t\turl_info = get_url_info(url)\n\t\turl_info[\"document_path\"] = document_path\n\t\treturn url_info\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif document_path is None:\n\t\tdocument_path = \"\"\n\t\n\tnodeinfo = {}\n\tnodeinfo[\"url\"] = url\n\tnodeinfo[\"document_path\"] = document_path\n\tnodeinfo[\"status\"] = \"Success\"\n\t\n\treturn nodeinfo\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\n\tif not document_path:\n\t\tdocument_path = urlparse(url).path\n\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path,\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\n\tif document_path is None:\n\t\t#document_path = get_nodeinfo_well_known_path(url)\n\t\tdocument_path = get_nodeinfo_well_known_document(url, document_path)\n\n\tdocument_path = resolve_url(url, document_path)\n\treturn get_document(document_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\t# get nodeinfo well-known document\n\tnodeinfo_url = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(nodeinfo_url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\tnodeinfo_url = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(nodeinfo_url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo_resp.json()\n\n\t# get nodeinfo well-know document\n\turl = url + 'nodeinfo/1'\n\tnodeinfo_resp = requests.get(url)\n\tnodeinfo_json = nodeinfo", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t \n\tif document_path is None:\n\t\tdocument_path = _get_nodeinfo_document_path(url)\n\tif document_path is None:\n\t\treturn None\n\treturn {'url': url, 'document_path': document_path}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\n\n\t# Verify the signature\n\t# public_key = RSA.importKey(open(public_key_file).read())\n\tverifier = PKCS1_v1_5.new(public_key)\n\tsignature = base64.b64decode(signature)\n\t# print repr(doc.toxml())\n\treturn verifier.verify(doc, signature)\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t # TODO: Implement this function\n\treturn False\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\tpubkey = _load_public_key(public_key)\n\t\tsigner = ds.DSIGNature(None, pubkey, signature)\n\t\treturn signer.verify(doc)\n\n\tdef _load_public_key(public_key):\n\t\tif isinstance(public_key, basestring):\n\t\t\tpublic_key = public_key.encode('utf-8')\n\t\treturn load_public_key(public_key)\n\n\tdef _get_element_names(doc, element):\n\t\t\"\"\"\n\t\tGet a list of all the element names from the given element\n\t\tand its descendants.\n\t\t\"\"\"\n\t\telements = []\n\t\tfor e in doc.iter(element):\n\t\t\tif e.tag not in elements:\n\t\t\t\telements.append(e.tag)\n\t\treturn elements\n\n##\n# The following is a copy of the code in the django.contrib.auth.tokens\n# module. I don't know why it is there, but it has to be there for\n# django to work correctly.\n#\n\nfrom hashlib import sha1\nfrom django.utils.http import int_to_base36\nfrom django.utils.translation import ugettext_lazy as _\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\t# make sure the signature is valid\n\trsakey = RSA.importKey(public_key)\n\trsakey = PKCS1_v1_5.new(rsakey)\n\tdigest = SHA.new()\n\tfor element in doc.getroot().iter():\n\t\tdigest.update(element.tag)\n\t\tdigest.update(element.text)\n\tif not rsakey.verify(digest, base64.b64decode(signature)):\n\t\traise Exception('Invalid signature')\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\tmain()#!/usr/bin/env python\n\nimport re, os, sys, logging, logging.handlers, time, threading, Queue\nfrom xml.dom import minidom\nfrom xml.parsers import expat\nfrom xml.sax import make_parser\nfrom xml.sax.handler import ContentHandler\nfrom xml.sax.saxutils import escape\nfrom xml.etree import ElementTree as ET\nfrom xml.etree.ElementTree import XML, Element, SubElement, Comment\nfrom xml.etree.ElementTree import tostring, dump\n\nimport relayable_common\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\n\n\treturn True\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\n\n\t# Verify the signature\n\t# public_key = RSA.importKey(open(public_key_file).read())\n\tverifier = PKCS1_v1_5.new(public_key)\n\tsignature = base64.b64decode(signature)\n\t# print repr(doc.toxml())\n\treturn verifier.verify(doc, signature)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t # TODO: Implement this function\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\tpubkey = _load_public_key(public_key)\n\t\tsigner = ds.DSIGNature(None, pubkey, signature)\n\t\treturn signer.verify(doc)\n\n\tdef _load_public_key(public_key):\n\t\tif isinstance(public_key, basestring):\n\t\t\tpublic_key = public_key.encode('utf-8')\n\t\treturn load_public_key(public_key)\n\n\tdef _get_element_names(doc, element):\n\t\t\"\"\"\n\t\tGet a list of all the element names from the given element\n\t\tand its descendants.\n\t\t\"\"\"\n\t\telements = []\n\t\tfor e in doc.iter(element):\n\t\t\tif e.tag not in elements:\n\t\t\t\telements.append(e.tag)\n\t\treturn elements\n\n##\n# The following is a copy of the code in the django.contrib.auth.tokens\n# module. I don't know why it is there, but it has to be there for\n# django to work correctly.\n#\n\nfrom hashlib import sha1\nfrom django.utils.http import int_to_base36\nfrom django.utils.translation import ugettext_lazy as _\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\t# make sure the signature is valid\n\trsakey = RSA.importKey(public_key)\n\trsakey = PKCS1_v1_5.new(rsakey)\n\tdigest = SHA.new()\n\tfor element in doc.getroot().iter():\n\t\tdigest.update(element.tag)\n\t\tdigest.update(element.text)\n\tif not rsakey.verify(digest, base64.b64decode(signature)):\n\t\traise Exception('Invalid signature')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\tmain()#!/usr/bin/env python\n\nimport re, os, sys, logging, logging.handlers, time, threading, Queue\nfrom xml.dom import minidom\nfrom xml.parsers import expat\nfrom xml.sax import make_parser\nfrom xml.sax.handler import ContentHandler\nfrom xml.sax.saxutils import escape\nfrom xml.etree import ElementTree as ET\nfrom xml.etree.ElementTree import XML, Element, SubElement, Comment\nfrom xml.etree.ElementTree import tostring, dump\n\nimport relayable_common\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\n\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tdiaspora_webfinger = {}\n\n\t# Find the links in the document (the format is the same as the links in the document)\n\t# <link rel=\"http://webfinger.net/rel/profile-page\" href=\"https://diaspora.example.com/people/123456789\">\n\tlinks = re.findall(\"<link rel=\\\"http://webfinger.net/rel/profile-page\\\" href=\\\"(.*?)\\\"\", document)\n\n\t# Select the first link, the format is https://diaspora.example.com/people/123456789\n\tdiaspora_webfinger['hcard_url'] = links[0]\n\n\t# Find the links in the document (the format is the same as the links in the document)\n\t# <link rel=\"http://webfinger.net/rel/profile-page\" href=\"https://diaspora.example.com/people/123456789\">\n\tlinks = re.findall(\"<link rel=\\\"http://webfinger.net/rel/profile-page\\\" href=\\\"(.*?)\\\"\", document)\n\n\t# Select the first link, the format is https://diaspora.example.com/people/123456789\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\treturn {\n\t\t'hcard_url': get_hcard_url(document)\n\t}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tdef get_hcard_url(links: list[dict]) -> str:\n\t\tfor link_dict in links:\n\t\t\tif link_dict[\"rel\"] == \"http://webfinger.net/rel/profile-page\":\n\t\t\t\treturn link_dict[\"href\"]\n\t\treturn \"\"\n\n\ttry:\n\t\tparsed_json = json.loads(document)\n\t\tif parsed_json.get(\"links\"):\n\t\t\treturn {\"hcard_url\": get_hcard_url(parsed_json.get(\"links\"))}\n\t\telse:\n\t\t\treturn {}\n\texcept:\n\t\treturn {}\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\t# parse the document to a dict\n\ttry:\n\t\tdocument = json.loads(document)\n\texcept:\n\t\traise ValueError(\"The document is not in JSON format.\")\n\n\t# parse the hcard_url\n\ttry:\n\t\thcard_url = document[\"links\"][0][\"href\"]\n\texcept KeyError:\n\t\traise ValueError(\"The document does not contain the hcard_url.\")\n\n\t# return the hcard_url\n\treturn {\n\t\t\"hcard_url\": hcard_url\n\t}\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tjson_obj = json.loads(document)\n\thcard_url = json_obj['links'][0]['href']\n\n\treturn {\n\t\t'hcard_url': hcard_url,\n\t}\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t/src/lib/utils.py\nimport os\nimport re\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t \n\ttry:\n\t\tdata = json.loads(document)\n\t\tdata = data['links']\n\t\tdata = list(filter(lambda x: x['rel'] == 'http://webfinger.net/rel/profile-page', data))\n\t\tdata = data[0]\n\t\tdata = data['href']\n\t\treturn {\n\t\t\t'hcard_url': data,\n\t\t}\n\texcept Exception as e:\n\t\treturn {}\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\t# get links\n\tlinks = []\n\ttry:\n\t\tdocument = json.loads(document)\n\t\tlinks = document.get(\"links\")\n\texcept:\n\t\traise Exception(\"Not a json document\")\n\n\tif not links:\n\t\traise Exception(\"No links in the document\")\n\n\t# get the value of hcard_url in links\n\thcard_url = \"\"\n\tfor link in links:\n\t\tif link.get(\"rel\") == \"hcard_url\":\n\t\t\thcard_url = link.get(\"href\")\n\n\tif not hcard_url:\n\t\traise Exception(\"No hcard_url in the document\")\n\n\t# get the value of the link\n\tdata = {\"id\": hcard_url}\n\n\treturn data\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\twebfinger = {}\n\tif document:\n\t\ttry:\n\t\t\tdocument = json.loads(document)\n\t\texcept ValueError:\n\t\t\treturn webfinger\n\n\t\tlinks = document.get('links', [])\n\n\t\tfor link in links:\n\t\t\tif link.get('rel') == 'http://webfinger.net/rel/profile-page':\n\t\t\t\twebfinger['hcard_url'] = link.get('href')\n\t\t\t\tbreak\n\n\t\twebfinger['hcard_url'] = webfinger.get('hcard_url') or ''\n\n\treturn webfinger\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tif \"hcard_url\" in document:\n\t\thcard_url = document[\"hcard_url\"]\n\t\tif \"href\" in hcard_url:\n\t\t\treturn {\n\t\t\t\t\"url\": hcard_url[\"href\"]\n\t\t\t}\n\n\treturn {}\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tdiaspora_webfinger = {}\n\n\t# Find the links in the document (the format is the same as the links in the document)\n\t# <link rel=\"http://webfinger.net/rel/profile-page\" href=\"https://diaspora.example.com/people/123456789\">\n\tlinks = re.findall(\"<link rel=\\\"http://webfinger.net/rel/profile-page\\\" href=\\\"(.*?)\\\"\", document)\n\n\t# Select the first link, the format is https://diaspora.example.com/people/123456789\n\tdiaspora_webfinger['hcard_url'] = links[0]\n\n\t# Find the links in the document (the format is the same as the links in the document)\n\t# <link rel=\"http://webfinger.net/rel/profile-page\" href=\"https://diaspora.example.com/people/123456789\">\n\tlinks = re.findall(\"<link rel=\\\"http://webfinger.net/rel/profile-page\\\" href=\\\"(.*?)\\\"\", document)\n\n\t# Select the first link, the format is https://diaspora.example.com/people/123456789\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\treturn {\n\t\t'hcard_url': get_hcard_url(document)\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tdef get_hcard_url(links: list[dict]) -> str:\n\t\tfor link_dict in links:\n\t\t\tif link_dict[\"rel\"] == \"http://webfinger.net/rel/profile-page\":\n\t\t\t\treturn link_dict[\"href\"]\n\t\treturn \"\"\n\n\ttry:\n\t\tparsed_json = json.loads(document)\n\t\tif parsed_json.get(\"links\"):\n\t\t\treturn {\"hcard_url\": get_hcard_url(parsed_json.get(\"links\"))}\n\t\telse:\n\t\t\treturn {}\n\texcept:\n\t\treturn {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\t# parse the document to a dict\n\ttry:\n\t\tdocument = json.loads(document)\n\texcept:\n\t\traise ValueError(\"The document is not in JSON format.\")\n\n\t# parse the hcard_url\n\ttry:\n\t\thcard_url = document[\"links\"][0][\"href\"]\n\texcept KeyError:\n\t\traise ValueError(\"The document does not contain the hcard_url.\")\n\n\t# return the hcard_url\n\treturn {\n\t\t\"hcard_url\": hcard_url\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tjson_obj = json.loads(document)\n\thcard_url = json_obj['links'][0]['href']\n\n\treturn {\n\t\t'hcard_url': hcard_url,\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t/src/lib/utils.py\nimport os\nimport re\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t \n\ttry:\n\t\tdata = json.loads(document)\n\t\tdata = data['links']\n\t\tdata = list(filter(lambda x: x['rel'] == 'http://webfinger.net/rel/profile-page', data))\n\t\tdata = data[0]\n\t\tdata = data['href']\n\t\treturn {\n\t\t\t'hcard_url': data,\n\t\t}\n\texcept Exception as e:\n\t\treturn {}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\t# get links\n\tlinks = []\n\ttry:\n\t\tdocument = json.loads(document)\n\t\tlinks = document.get(\"links\")\n\texcept:\n\t\traise Exception(\"Not a json document\")\n\n\tif not links:\n\t\traise Exception(\"No links in the document\")\n\n\t# get the value of hcard_url in links\n\thcard_url = \"\"\n\tfor link in links:\n\t\tif link.get(\"rel\") == \"hcard_url\":\n\t\t\thcard_url = link.get(\"href\")\n\n\tif not hcard_url:\n\t\traise Exception(\"No hcard_url in the document\")\n\n\t# get the value of the link\n\tdata = {\"id\": hcard_url}\n\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\twebfinger = {}\n\tif document:\n\t\ttry:\n\t\t\tdocument = json.loads(document)\n\t\texcept ValueError:\n\t\t\treturn webfinger\n\n\t\tlinks = document.get('links', [])\n\n\t\tfor link in links:\n\t\t\tif link.get('rel') == 'http://webfinger.net/rel/profile-page':\n\t\t\t\twebfinger['hcard_url'] = link.get('href')\n\t\t\t\tbreak\n\n\t\twebfinger['hcard_url'] = webfinger.get('hcard_url') or ''\n\n\treturn webfinger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\t\n\n\tif \"hcard_url\" in document:\n\t\thcard_url = document[\"hcard_url\"]\n\t\tif \"href\" in hcard_url:\n\t\t\treturn {\n\t\t\t\t\"url\": hcard_url[\"href\"]\n\t\t\t}\n\n\treturn {}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/frictionless/steps/resource.py@@ -1,12 +1,13 @@\n from __future__ import annotations\n \n from typing import TYPE_CHECKING, Dict, List, Optional, Union\n+import warnings\n \n from frictionless import errors\n from frictionless.helpers.exceptions import FrictionlessException\n from frictionless.helpers.debug import DebugError\n from frictionless.resources import Resource\n from frictionless.schema import Schema\n from frictionless.steps import Step\n from frictionless.steps.validate import StepValidate\n @@ -34,20 +35,20 @@\n \tdef __init__(\n \t\tself,\n \t\tresource: Resource,\n \t\tschema: Union[Schema, bool, str] = None,\n \t\tschema_infer: bool = False,\n \t\tschema_force: bool = False,\n \t\tschema_missing_fields: bool = False,\n \t\tschema_missing_foreign_keys: bool = False,\n \t\tschema_missing_constraints: bool = False,\n-\t\tschema_table_name: str = \"table\",\n-\t\tschema_infer_foreign_keys: bool = False,\n+\t\tschema_table_name: str = \"table\",  # type: ignore\n+\t\tschema_infer_foreign_keys: bool = False,  # type:", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/tests/test_webfinger.py\t\"webfinger\": {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"href\": \"https://example.com/.well-known/webfinger?resource=acct:\",\n\t\t\t\t\"rel\": \"http://webfinger.net/rel/profile-page\"\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"href\": \"https://example.com/.well-known/host-meta?host=example.com\",\n\t\t\t\t\"rel\": \"http://webfinger.net/rel/host-meta\"\n\t\t\t}\n\t\t]\n\t}\n}\n\n@pytest.mark.asyncio\nasync def test_retrieve_webfinger_document_success():\n\tresponse_mock = MagicMock(status=200)\n\tresponse_mock.json = mock_json\n\tasync_mock.patch.object(aiohttp.ClientSession, 'get', async_mock.CoroutineMock(return_value=response_mock))\n\n\twebfinger_doc = await retrieve_webfinger_document(handle)\n\tassert webfinger_doc == mock_json\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_webfinger_document_fail():\n\tresponse_mock = MagicMock(status=404)\n\tasync_mock.patch.object(aio", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/ponyo/constants.py# constants.py\n#\n# constants for use in ponyo\n#\n# \n# \n# 2020-10-21\n\n# 0.1.0\n# 2020-10-21\n#\t- add constants for use in ponyo\n\n# 0.1.1\n# 2020-11-10\n#\t- fix typo in constants.py\n\n# 0.2.0\n# 2020-11-11\n#\t- add constants.py\n\n# 0.2.1\n# 2020-11-13\n#\t- fix typo in constants.py\n\n# 0.2.2\n# 2020-11-13\n#\t- update constants.py\n\n# 0.2.3\n# 2020-11-13\n#\t- update constants.py\n\n# 0.3.0\n# 2020-11-13\n#\t- update constants.py\n\n# 0.3.1\n# 2020-11-13\n#\t- update constants.py\n\n# ", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tif not isinstance(e, HTTPError):\n\t\t\traise e\n\t\telse:\n\t\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ttry:\n\t\t\tresp = requests.get(f\"https://webfinger.net/{handle}\")\n\t\texcept Exception as e:\n\t\t\tlogger.debug(f\"Failed to retrieve webfinger document for {handle}: {e}\")\n\t\t\treturn None\n\n\t\tif resp.status_code not in (200, 404):\n\t\t\treturn None\n\n\t\ttry:\n\t\t\treturn resp.json()\n\t\texcept json.decoder.JSONDecodeError:\n\t\t\tlogger.debug(f\"Failed to parse webfinger document for {handle}\")\n\t\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn _retrieve_webfinger_document(handle)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\turl = make_webfinger_url(handle)\n\t\ttry:\n\t\t\tres = http_get(url)\n\t\texcept Exception:\n\t\t\treturn None\n\t\tif res.status_code!= 200:\n\t\t\treturn None\n\t\treturn res.text\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t \n\ttry:\n\t\turl = 'https://' + handle.lower() + '.webfinger.org'\n\t\treturn requests.get(url, timeout=10).text\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(os.path.join(settings.BASE_DIR, \"static/webfinger-documents\", handle + \".json\"), \"r\")\n\texcept FileNotFoundError:\n\t\treturn None\n\treturn f.read()\n/src/app/views/accounts/views.py\nimport base64\nimport json\nimport os\n\nimport requests\nfrom aiohttp import web\nfrom aiohttp.web import HTTPFound, HTTPUnauthorized\nfrom pydantic import EmailStr, constr\nfrom pydantic.fields import Field\nfrom pydantic.networks import AnyHttpUrl\nfrom pydantic.schema import Schema\n\nfrom app.models import User\nfrom app.utils.constants import APP_NAME\nfrom app.utils.decorators import check_captcha, handle_async_exception\nfrom app.utils.security import generate_nonce, hash_password, verify_password\nfrom app.utils.templates import render_template\nfrom app.utils.web import get_current_user, get_request\nfrom. import accounts\nfrom.models import (\n\tAccountSettings,\n\tAccountSettingsEditSchema,\n\tAccountSettingsSchema,\n\tChangePasswordSchema,\n\tLoginSchema,\n\tRegisterSchema,\n\tRegisterSchemaSchema,\n\tRequestPasswordResetSchema,\n\tResetPasswordSchema,\n\tSettingsSchema,\n)\n\n# This", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/src/oauthlib/oauth2/rfc6749/endpoints/webfinger.py@@ -1,16 +1,16 @@\n import logging\n import re\n import ssl\n import typing\n import warnings\n from typing import Optional\n \n-from oauthlib.common import urlencode, urlparse, urlunparse, urlquote\n+from oauthlib.common import urlencode, urlparse, urlunparse, urlquote, to_unicode\n from oauthlib.oauth2.rfc6749.endpoints.base import BaseEndpoint\n from oauthlib.oauth2.rfc6749.errors import MissingTokenError\n from oauthlib.oauth2.rfc6749.parameters import (\n     parse_request_body_parameters,\n     prepare_request_uri_for_body_parameters,\n )\n \n try:@@ -19,17 +19,17 @@\n except ImportError:\n     pass\n \n try:\n     import requests\n except ImportError:\n     requests = None\n \n-__all__ = (\"WebFinger\",)\n+__all__ = (\"WebFinger\", \"WebFingerError\")\n \n logger = logging.getLogger(__name__)\n \n \n class WebFingerError(Exception):\n     def __init__(self, error: str, description: str):\n         super().__init__(error)\n         self.description = description@@ -"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/frictionless/steps/resource.py@@ -1,12 +1,13 @@\n from __future__ import annotations\n \n from typing import TYPE_CHECKING, Dict, List, Optional, Union\n+import warnings\n \n from frictionless import errors\n from frictionless.helpers.exceptions import FrictionlessException\n from frictionless.helpers.debug import DebugError\n from frictionless.resources import Resource\n from frictionless.schema import Schema\n from frictionless.steps import Step\n from frictionless.steps.validate import StepValidate\n @@ -34,20 +35,20 @@\n \tdef __init__(\n \t\tself,\n \t\tresource: Resource,\n \t\tschema: Union[Schema, bool, str] = None,\n \t\tschema_infer: bool = False,\n \t\tschema_force: bool = False,\n \t\tschema_missing_fields: bool = False,\n \t\tschema_missing_foreign_keys: bool = False,\n \t\tschema_missing_constraints: bool = False,\n-\t\tschema_table_name: str = \"table\",\n-\t\tschema_infer_foreign_keys: bool = False,\n+\t\tschema_table_name: str = \"table\",  # type: ignore\n+\t\tschema_infer_foreign_keys: bool = False,  # type:", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/tests/test_webfinger.py\t\"webfinger\": {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"href\": \"https://example.com/.well-known/webfinger?resource=acct:\",\n\t\t\t\t\"rel\": \"http://webfinger.net/rel/profile-page\"\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"href\": \"https://example.com/.well-known/host-meta?host=example.com\",\n\t\t\t\t\"rel\": \"http://webfinger.net/rel/host-meta\"\n\t\t\t}\n\t\t]\n\t}\n}\n\n@pytest.mark.asyncio\nasync def test_retrieve_webfinger_document_success():\n\tresponse_mock = MagicMock(status=200)\n\tresponse_mock.json = mock_json\n\tasync_mock.patch.object(aiohttp.ClientSession, 'get', async_mock.CoroutineMock(return_value=response_mock))\n\n\twebfinger_doc = await retrieve_webfinger_document(handle)\n\tassert webfinger_doc == mock_json\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_webfinger_document_fail():\n\tresponse_mock = MagicMock(status=404)\n\tasync_mock.patch.object(aio", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/ponyo/constants.py# constants.py\n#\n# constants for use in ponyo\n#\n# \n# \n# 2020-10-21\n\n# 0.1.0\n# 2020-10-21\n#\t- add constants for use in ponyo\n\n# 0.1.1\n# 2020-11-10\n#\t- fix typo in constants.py\n\n# 0.2.0\n# 2020-11-11\n#\t- add constants.py\n\n# 0.2.1\n# 2020-11-13\n#\t- fix typo in constants.py\n\n# 0.2.2\n# 2020-11-13\n#\t- update constants.py\n\n# 0.2.3\n# 2020-11-13\n#\t- update constants.py\n\n# 0.3.0\n# 2020-11-13\n#\t- update constants.py\n\n# 0.3.1\n# 2020-11-13\n#\t- update constants.py\n\n# ", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tif not isinstance(e, HTTPError):\n\t\t\traise e\n\t\telse:\n\t\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ttry:\n\t\t\tresp = requests.get(f\"https://webfinger.net/{handle}\")\n\t\texcept Exception as e:\n\t\t\tlogger.debug(f\"Failed to retrieve webfinger document for {handle}: {e}\")\n\t\t\treturn None\n\n\t\tif resp.status_code not in (200, 404):\n\t\t\treturn None\n\n\t\ttry:\n\t\t\treturn resp.json()\n\t\texcept json.decoder.JSONDecodeError:\n\t\t\tlogger.debug(f\"Failed to parse webfinger document for {handle}\")\n\t\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t \n\ttry:\n\t\treturn _retrieve_webfinger_document(handle)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\turl = make_webfinger_url(handle)\n\t\ttry:\n\t\t\tres = http_get(url)\n\t\texcept Exception:\n\t\t\treturn None\n\t\tif res.status_code!= 200:\n\t\t\treturn None\n\t\treturn res.text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t \n\ttry:\n\t\turl = 'https://' + handle.lower() + '.webfinger.org'\n\t\treturn requests.get(url, timeout=10).text\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(os.path.join(settings.BASE_DIR, \"static/webfinger-documents\", handle + \".json\"), \"r\")\n\texcept FileNotFoundError:\n\t\treturn None\n\treturn f.read()\n/src/app/views/accounts/views.py\nimport base64\nimport json\nimport os\n\nimport requests\nfrom aiohttp import web\nfrom aiohttp.web import HTTPFound, HTTPUnauthorized\nfrom pydantic import EmailStr, constr\nfrom pydantic.fields import Field\nfrom pydantic.networks import AnyHttpUrl\nfrom pydantic.schema import Schema\n\nfrom app.models import User\nfrom app.utils.constants import APP_NAME\nfrom app.utils.decorators import check_captcha, handle_async_exception\nfrom app.utils.security import generate_nonce, hash_password, verify_password\nfrom app.utils.templates import render_template\nfrom app.utils.web import get_current_user, get_request\nfrom. import accounts\nfrom.models import (\n\tAccountSettings,\n\tAccountSettingsEditSchema,\n\tAccountSettingsSchema,\n\tChangePasswordSchema,\n\tLoginSchema,\n\tRegisterSchema,\n\tRegisterSchemaSchema,\n\tRequestPasswordResetSchema,\n\tResetPasswordSchema,\n\tSettingsSchema,\n)\n\n# This", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t/src/oauthlib/oauth2/rfc6749/endpoints/webfinger.py@@ -1,16 +1,16 @@\n import logging\n import re\n import ssl\n import typing\n import warnings\n from typing import Optional\n \n-from oauthlib.common import urlencode, urlparse, urlunparse, urlquote\n+from oauthlib.common import urlencode, urlparse, urlunparse, urlquote, to_unicode\n from oauthlib.oauth2.rfc6749.endpoints.base import BaseEndpoint\n from oauthlib.oauth2.rfc6749.errors import MissingTokenError\n from oauthlib.oauth2.rfc6749.parameters import (\n     parse_request_body_parameters,\n     prepare_request_uri_for_body_parameters,\n )\n \n try:@@ -19,17 +19,17 @@\n except ImportError:\n     pass\n \n try:\n     import requests\n except ImportError:\n     requests = None\n \n-__all__ = (\"WebFinger\",)\n+__all__ = (\"WebFinger\", \"WebFingerError\")\n \n logger = logging.getLogger(__name__)\n \n \n class WebFingerError(Exception):\n     def __init__(self, error: str, description: str):\n         super().__init__(error)\n         self.description = description@@ -", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\n\n\ttry:\n\t\turi = \"http://%s.diaspora.net/webfinger?resource=acct:%s@diaspora.net\" % (handle, handle)\n\t\tresponse = urllib2.urlopen(uri)\n\t\tdata = response.read()\n\t\tparsed = json.loads(data)\n\texcept urllib2.HTTPError, e:\n\t\tif e.code == 404:\n\t\t\treturn False\n\t\telse:\n\t\t\traise\n\treturn parsed['links']\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\turl = handle\n\t\tparser = DiasporaWebfingerParser()\n\t\tparser.feed(url)\n\t\tparser.close()\n\t\treturn parser.webfinger\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\treq = requests.get(handle, headers=headers)\n\t\tdata = req.json()\n\t\treturn data\n\n# Retrieve the webfinger document from remote server\nhandle = \"https://diaspora.social/webfinger?resource=%s\" % (username)\ndata = retrieve_and_parse_diaspora_webfinger(handle)\n\n# Parse the webfinger document\nif 'error' in data:\n\tprint \"Error: %s\" % (data['error'])\n\tsys.exit(1)\n\n# Retrieve the XRD document and parse it\nhandle = data[\"links\"][0][\"href\"]\ndata = retrieve_and_parse_xrd(handle)\n\n# Parse the XRD document\nif 'error' in data:\n\tprint \"Error: %s\" % (data['error'])\n\tsys.exit(1)\n\n# Generate an OAuth token\ntoken = generate_diaspora_oauth_token(data)\n\n# Parse the token\nif 'error' in token:\n\tprint \"Error: %s\" % (token['error'])\n\tsys.exit(1)\n\n# Retrieve the user's Diaspora profile\ndata = retrieve_diaspora_profile(data, token)\n\n# Parse the user's Diaspora profile\nif '", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\ttry:\n\t\t\twebfinger = fetch(handle)\n\t\t\treturn parse_webfinger(webfinger)\n\t\texcept:\n\t\t\treturn {}\n/README.md\n# diaspora-python-client\nDiaspora Python Client\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\n\n\ttry:\n\t\thandle = urllib.quote(handle)\n\t\trequest = urllib2.Request('%s&resource=%s' % (DIASPORA_WEBFINGER_URL, handle))\n\t\tresponse = urllib2.urlopen(request)\n\t\tcontent = response.read()\n\t\tresponse.close()\n\n\t\treturn webfinger_parser.parse(content)\n\texcept urllib2.URLError, e:\n\t\tif e.code == 404:\n\t\t\treturn {}\n\t\telse:\n\t\t\tprint \"Error while retrieving and parsing remote Diaspora webfinger document: %s\" % e.code\n\t\t\treturn {}\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\txml = retrieve_and_parse_webfinger(handle)\n\t\treturn parse_diaspora_webfinger_xml(xml)\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\ttry:\n\t\t\tresponse = urllib2.urlopen(handle)\n\t\t\tresponse_string = response.read()\n\t\t\tresponse.close()\n\t\t\treturn parse_webfinger(response_string)\n\t\texcept urllib2.HTTPError:\n\t\t\treturn None\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\tresponse = requests.get(handle)\n\t\tresponse.raise_for_status()\n\t\treturn parse_diaspora_webfinger(response.text)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\n\n\ttry:\n\t\turi = \"http://%s.diaspora.net/webfinger?resource=acct:%s@diaspora.net\" % (handle, handle)\n\t\tresponse = urllib2.urlopen(uri)\n\t\tdata = response.read()\n\t\tparsed = json.loads(data)\n\texcept urllib2.HTTPError, e:\n\t\tif e.code == 404:\n\t\t\treturn False\n\t\telse:\n\t\t\traise\n\treturn parsed['links']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\turl = handle\n\t\tparser = DiasporaWebfingerParser()\n\t\tparser.feed(url)\n\t\tparser.close()\n\t\treturn parser.webfinger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\treq = requests.get(handle, headers=headers)\n\t\tdata = req.json()\n\t\treturn data\n\n# Retrieve the webfinger document from remote server\nhandle = \"https://diaspora.social/webfinger?resource=%s\" % (username)\ndata = retrieve_and_parse_diaspora_webfinger(handle)\n\n# Parse the webfinger document\nif 'error' in data:\n\tprint \"Error: %s\" % (data['error'])\n\tsys.exit(1)\n\n# Retrieve the XRD document and parse it\nhandle = data[\"links\"][0][\"href\"]\ndata = retrieve_and_parse_xrd(handle)\n\n# Parse the XRD document\nif 'error' in data:\n\tprint \"Error: %s\" % (data['error'])\n\tsys.exit(1)\n\n# Generate an OAuth token\ntoken = generate_diaspora_oauth_token(data)\n\n# Parse the token\nif 'error' in token:\n\tprint \"Error: %s\" % (token['error'])\n\tsys.exit(1)\n\n# Retrieve the user's Diaspora profile\ndata = retrieve_diaspora_profile(data, token)\n\n# Parse the user's Diaspora profile\nif '", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\ttry:\n\t\t\twebfinger = fetch(handle)\n\t\t\treturn parse_webfinger(webfinger)\n\t\texcept:\n\t\t\treturn {}\n/README.md\n# diaspora-python-client\nDiaspora Python Client\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\n\n\ttry:\n\t\thandle = urllib.quote(handle)\n\t\trequest = urllib2.Request('%s&resource=%s' % (DIASPORA_WEBFINGER_URL, handle))\n\t\tresponse = urllib2.urlopen(request)\n\t\tcontent = response.read()\n\t\tresponse.close()\n\n\t\treturn webfinger_parser.parse(content)\n\texcept urllib2.URLError, e:\n\t\tif e.code == 404:\n\t\t\treturn {}\n\t\telse:\n\t\t\tprint \"Error while retrieving and parsing remote Diaspora webfinger document: %s\" % e.code\n\t\t\treturn {}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\txml = retrieve_and_parse_webfinger(handle)\n\t\treturn parse_diaspora_webfinger_xml(xml)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\ttry:\n\t\t\tresponse = urllib2.urlopen(handle)\n\t\t\tresponse_string = response.read()\n\t\t\tresponse.close()\n\t\t\treturn parse_webfinger(response_string)\n\t\texcept urllib2.HTTPError:\n\t\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\t\tresponse = requests.get(handle)\n\t\tresponse.raise_for_status()\n\t\treturn parse_diaspora_webfinger(response.text)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\turl = 'http://%s/data/host-meta' % host\n\n\tres = requests.get(url, timeout=20)\n\n\treturn res.content\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tr = requests.get(host + \"/host-meta.xml\")\n\t\tif r.status_code == 200:\n\t\t\treturn XRD(r.text)\n\t\telse:\n\t\t\treturn None\n\n\n\tdef retrieve_diaspora_host_meta_link(self, host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tr = requests.get(host + \"/host-meta.xml\")\n\t\tif r.status_code == 200:\n\t\t\treturn XRD(r.text)\n\t\telse:\n\t\t\treturn None\n\n\n\tdef retrieve_diaspora_host_meta_link(self, host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tr = requests.get(host + \"/host-meta.xml\")\n\t\tif r.status_code == 200:\n\t\t\treturn XRD(r.text)\n\t\telse:\n\t\t\treturn None\n\n\n\tdef retrieve_diaspora_host_meta_link(self, host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tif not host:\n\t\t\treturn\n\t\tif isinstance(host, Host):\n\t\t\thost = host.get_url()\n\t\tif not host.startswith('http://'):\n\t\t\thost = 'http://' + host\n\t\tif not host.endswith('/'):\n\t\t\thost += '/'\n\t\ttry:\n\t\t\tr = requests.get(host + 'host-meta')\n\t\t\treturn XRD.parse(r.text)\n\t\texcept:\n\t\t\treturn\n\n\tdef retrieve_diaspora_host_meta_all(host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tif not host:\n\t\t\treturn\n\t\tif isinstance(host, Host):\n\t\t\thost = host.get_url()\n\t\tif not host.startswith('http://'):\n\t\t\thost = 'http://' + host\n\t\tif not host.endswith('/'):\n\t\t\thost += '/'\n\t\ttry:\n\t\t\tr = requests.get(host + 'host-meta', params={'all': True})\n\t\t\treturn XRD.parse(r.text)\n\t\texcept:\n\t\t\treturn\n\n\tdef retrieve_diaspora_host_meta_profile(host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\treturn XRD.parse(host.get_meta())\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\tif not isinstance(host, urlparse.ParsedURL):\n\t\thost = urlparse.urlparse(host)\n\n\tif host.scheme not in ('http', 'https'):\n\t\traise ValueError('Unsupported scheme')\n\n\tpath = urlparse.urljoin(host.geturl(), '/host-meta.xml')\n\tresp = urlopen(path)\n\n\ttry:\n\t\tdoc = XRD(resp.read())\n\tfinally:\n\t\tresp.close()\n\n\tif not doc.is_xrd:\n\t\traise ValueError('Not an XRD document')\n\n\tif not doc.is_host_meta:\n\t\traise ValueError('Not a host-meta document')\n\n\treturn doc\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tif not host:\n\t\t\traise TypeError('must provide host')\n\t\treturn _retrieve_doc(host, 'host-meta')\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\treturn retrieve_xrd(host, \"/api/v1/host-meta\")\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\t# Send the request\n\t\tresponse = requests.get(host)\n\t\t# Parse the response\n\t\treturn XRD(response.text)\n/README.md\n# PyXRD\n\nPyXRD is a Python XRD library.\n\n## Installation\n\nTo install PyXRD, simply:\n\n```sh\n$ pip install PyXRD\n```\n\n## Usage\n\nPyXRD is simple to use:\n\n```python\n>>> from PyXRD import XRD\n>>> xrd = XRD('<xrd xmlns=\"http://www.w3.org/2005/Atom\" xmlns:xrdfs=\"http://www.w3.org/2005/01/rdf-schema#\" xmlns:xrd=\"http://www.w3.org/ns/xri/xrd#\" xmlns:sioc=\"http://rdfs.org/sioc/ns#\" xmlns:skos=\"http://www.w3.org/2004/02/skos/core#\" xmlns:foaf=\"http://xmlns.com/foaf/0.1/\" xmlns:iana=\"http://www.iana.org/assignments/relation/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:dc=\"http://purl."], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\turl = 'http://%s/data/host-meta' % host\n\n\tres = requests.get(url, timeout=20)\n\n\treturn res.content\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tr = requests.get(host + \"/host-meta.xml\")\n\t\tif r.status_code == 200:\n\t\t\treturn XRD(r.text)\n\t\telse:\n\t\t\treturn None\n\n\n\tdef retrieve_diaspora_host_meta_link(self, host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tr = requests.get(host + \"/host-meta.xml\")\n\t\tif r.status_code == 200:\n\t\t\treturn XRD(r.text)\n\t\telse:\n\t\t\treturn None\n\n\n\tdef retrieve_diaspora_host_meta_link(self, host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tr = requests.get(host + \"/host-meta.xml\")\n\t\tif r.status_code == 200:\n\t\t\treturn XRD(r.text)\n\t\telse:\n\t\t\treturn None\n\n\n\tdef retrieve_diaspora_host_meta_link(self, host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tif not host:\n\t\t\treturn\n\t\tif isinstance(host, Host):\n\t\t\thost = host.get_url()\n\t\tif not host.startswith('http://'):\n\t\t\thost = 'http://' + host\n\t\tif not host.endswith('/'):\n\t\t\thost += '/'\n\t\ttry:\n\t\t\tr = requests.get(host + 'host-meta')\n\t\t\treturn XRD.parse(r.text)\n\t\texcept:\n\t\t\treturn\n\n\tdef retrieve_diaspora_host_meta_all(host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tif not host:\n\t\t\treturn\n\t\tif isinstance(host, Host):\n\t\t\thost = host.get_url()\n\t\tif not host.startswith('http://'):\n\t\t\thost = 'http://' + host\n\t\tif not host.endswith('/'):\n\t\t\thost += '/'\n\t\ttry:\n\t\t\tr = requests.get(host + 'host-meta', params={'all': True})\n\t\t\treturn XRD.parse(r.text)\n\t\texcept:\n\t\t\treturn\n\n\tdef retrieve_diaspora_host_meta_profile(host):\n\t\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\treturn XRD.parse(host.get_meta())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\tif not isinstance(host, urlparse.ParsedURL):\n\t\thost = urlparse.urlparse(host)\n\n\tif host.scheme not in ('http', 'https'):\n\t\traise ValueError('Unsupported scheme')\n\n\tpath = urlparse.urljoin(host.geturl(), '/host-meta.xml')\n\tresp = urlopen(path)\n\n\ttry:\n\t\tdoc = XRD(resp.read())\n\tfinally:\n\t\tresp.close()\n\n\tif not doc.is_xrd:\n\t\traise ValueError('Not an XRD document')\n\n\tif not doc.is_host_meta:\n\t\traise ValueError('Not a host-meta document')\n\n\treturn doc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\tif not host:\n\t\t\traise TypeError('must provide host')\n\t\treturn _retrieve_doc(host, 'host-meta')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\treturn retrieve_xrd(host, \"/api/v1/host-meta\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\t\t# Send the request\n\t\tresponse = requests.get(host)\n\t\t# Parse the response\n\t\treturn XRD(response.text)\n/README.md\n# PyXRD\n\nPyXRD is a Python XRD library.\n\n## Installation\n\nTo install PyXRD, simply:\n\n```sh\n$ pip install PyXRD\n```\n\n## Usage\n\nPyXRD is simple to use:\n\n```python\n>>> from PyXRD import XRD\n>>> xrd = XRD('<xrd xmlns=\"http://www.w3.org/2005/Atom\" xmlns:xrdfs=\"http://www.w3.org/2005/01/rdf-schema#\" xmlns:xrd=\"http://www.w3.org/ns/xri/xrd#\" xmlns:sioc=\"http://rdfs.org/sioc/ns#\" xmlns:skos=\"http://www.w3.org/2004/02/skos/core#\" xmlns:foaf=\"http://xmlns.com/foaf/0.1/\" xmlns:iana=\"http://www.iana.org/assignments/relation/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:dc=\"http://purl.", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t \n\t# Make the request\n\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\n\treturn response/README.md\n# DJANGO\n\n## Getting Started\n\n### Setup\n\n```\n$ cd django/\n$ pip install -r requirements.txt\n$ python manage.py migrate\n$ python manage.py runserver\n```\n\n### Test\n\n```\n$ python manage.py test\n```\n\n## License\n\n[MIT](https://github.com/yogendra-saini/django/blob/master/LICENSE) &copy; \n/django/api/views.py\nimport json\nimport requests\n\nfrom rest_framework.decorators import api_view, permission_classes\nfrom rest_framework.permissions import AllowAny\nfrom rest_framework.response import Response\nfrom rest_framework.status import HTTP_400_BAD_REQUEST\n\nfrom api.utils import send_document\n\n\n@api_view([\"POST\"])\n@permission_classes((AllowAny, ))", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t(self, *args, **kwargs):\n\t\treturn self.send_document(self.url, self.data, self.timeout, self.method, *args, **kwargs)\n\t\n\tdef run(self):\n\t\t\"\"\"\n\t\tRun the server.\n\t\t\"\"\"\n\t\t# Run the server.\n\t\tself.server.serve_forever()\n\t\n\tdef stop(self):\n\t\t\"\"\"\n\t\tStop the server.\n\t\t\"\"\"\n\t\t# Stop the server.\n\t\tself.server.socket.close()\n\n# Define the default server.\nserver = Server()\nserver.run()# -*- coding: utf-8 -*-\n\"\"\"\nThis module contains the Server class.\n\"\"\"\n\n# Import the modules.\nfrom BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler\nfrom SocketServer import ThreadingMixIn\nimport json\n\n# Define the HTTPRequestHandler.", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\n\tdata = json.dumps(data)\n\tdata = data.encode(\"utf-8\")\n\trequest = urllib.request.Request(url, data=data)\n\trequest.add_header('Content-Type', 'application/json')\n\trequest.get_method = lambda: method\n\tresponse = urllib.request.urlopen(request, timeout=timeout)\n\treturn response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\nsend_response(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n# -*- coding: utf-8 -*-\n\"\"\"\n\tpybot\n\t~~~~~\n\n\t:copyright: (c) 2012 by.\n\t:license: BSD, see LICENSE for more details.\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\nimport os\nimport time\nimport unittest\nimport logging\nimport threading\n\nfrom nose.tools import assert_true\n\nfrom pybot.lib.http import RequestHandler, HTTPServer\nfrom pybot.lib.utils import import_class\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n# \tprint(\"Sending data to %s\" % url)\n\t\n\tif method == \"post\":\n\t\tresponse = requests.post(url, data, timeout=timeout, *args, **kwargs)\n\telif method == \"put\":\n\t\tresponse = requests.put(url, data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\traise ValueError(\"method must be either 'post' or 'put'.\")\n\t\n\treturn response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\te)\n\t#print(response.url)\n\t#print(response.status_code)\n\t#print(response.text)\n\n\treturn response.textimport requests\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\n\tif method == \"post\":\n\t\tresponse = requests.post(url, data=data, timeout=timeout)\n\telif method == \"get\":\n\t\tresponse = requests.get(url, data=data, timeout=timeout)\n\telse:\n\t\traise ValueError(\"Unsupported method\")\n\n\tif response.status_code == 200:\n\t\treturn response.json()\n\telse:\n\t\tresponse.raise_for_status()\nimport os\nimport sys\nimport json\nimport inspect\nfrom functools import wraps\nfrom requests import Session\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import RequestException\nfrom requests.packages.urllib3.util import Retry\nfrom requests.exceptions import SSLError\nfrom requests.packages.urllib3.exceptions import InsecureRequestWarning\n\n# Disable SSL warning messages\nrequests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t \n\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\treturn response\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\tself.send_response(200)\n\t\tself.send_header('Content-type', 'text/html')\n\t\tself.end_headers()\n\t\tself.wfile.write(data)\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t \n\t# Make the request\n\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\n\treturn response/README.md\n# DJANGO\n\n## Getting Started\n\n### Setup\n\n```\n$ cd django/\n$ pip install -r requirements.txt\n$ python manage.py migrate\n$ python manage.py runserver\n```\n\n### Test\n\n```\n$ python manage.py test\n```\n\n## License\n\n[MIT](https://github.com/yogendra-saini/django/blob/master/LICENSE) &copy; \n/django/api/views.py\nimport json\nimport requests\n\nfrom rest_framework.decorators import api_view, permission_classes\nfrom rest_framework.permissions import AllowAny\nfrom rest_framework.response import Response\nfrom rest_framework.status import HTTP_400_BAD_REQUEST\n\nfrom api.utils import send_document\n\n\n@api_view([\"POST\"])\n@permission_classes((AllowAny, ))", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t(self, *args, **kwargs):\n\t\treturn self.send_document(self.url, self.data, self.timeout, self.method, *args, **kwargs)\n\t\n\tdef run(self):\n\t\t\"\"\"\n\t\tRun the server.\n\t\t\"\"\"\n\t\t# Run the server.\n\t\tself.server.serve_forever()\n\t\n\tdef stop(self):\n\t\t\"\"\"\n\t\tStop the server.\n\t\t\"\"\"\n\t\t# Stop the server.\n\t\tself.server.socket.close()\n\n# Define the default server.\nserver = Server()\nserver.run()# -*- coding: utf-8 -*-\n\"\"\"\nThis module contains the Server class.\n\"\"\"\n\n# Import the modules.\nfrom BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler\nfrom SocketServer import ThreadingMixIn\nimport json\n\n# Define the HTTPRequestHandler.", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\n\tdata = json.dumps(data)\n\tdata = data.encode(\"utf-8\")\n\trequest = urllib.request.Request(url, data=data)\n\trequest.add_header('Content-Type', 'application/json')\n\trequest.get_method = lambda: method\n\tresponse = urllib.request.urlopen(request, timeout=timeout)\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\nsend_response(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n# -*- coding: utf-8 -*-\n\"\"\"\n\tpybot\n\t~~~~~\n\n\t:copyright: (c) 2012 by.\n\t:license: BSD, see LICENSE for more details.\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\nimport os\nimport time\nimport unittest\nimport logging\nimport threading\n\nfrom nose.tools import assert_true\n\nfrom pybot.lib.http import RequestHandler, HTTPServer\nfrom pybot.lib.utils import import_class\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n# \tprint(\"Sending data to %s\" % url)\n\t\n\tif method == \"post\":\n\t\tresponse = requests.post(url, data, timeout=timeout, *args, **kwargs)\n\telif method == \"put\":\n\t\tresponse = requests.put(url, data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\traise ValueError(\"method must be either 'post' or 'put'.\")\n\t\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\te)\n\t#print(response.url)\n\t#print(response.status_code)\n\t#print(response.text)\n\n\treturn response.textimport requests\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\n\tif method == \"post\":\n\t\tresponse = requests.post(url, data=data, timeout=timeout)\n\telif method == \"get\":\n\t\tresponse = requests.get(url, data=data, timeout=timeout)\n\telse:\n\t\traise ValueError(\"Unsupported method\")\n\n\tif response.status_code == 200:\n\t\treturn response.json()\n\telse:\n\t\tresponse.raise_for_status()\nimport os\nimport sys\nimport json\nimport inspect\nfrom functools import wraps\nfrom requests import Session\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import RequestException\nfrom requests.packages.urllib3.util import Retry\nfrom requests.exceptions import SSLError\nfrom requests.packages.urllib3.exceptions import InsecureRequestWarning\n\n# Disable SSL warning messages\nrequests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t \n\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\treturn response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\tself.send_response(200)\n\t\tself.send_header('Content-type', 'text/html')\n\t\tself.end_headers()\n\t\tself.wfile.write(data)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n        if not keys:\n                dic[key] = val\n                return\n\n        if key not in dic:\n                dic[key] = {}\n                dict_insert(dic[key], val, keys[0], *keys[1:])\n\n        return dict_insert(dic[key], val, keys[0], *keys[1:])\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tdef insert(dic, val, key):\n\t\t\"\"\"\n\t\tinsert a value of a key into a dictionary\n\n\t\tthis method is used internally and should not be called directly\n\n\t\t:param dic: a dictionary object to insert the key value into\n\t\t:param val: a value to insert to the given dictionary\n\t\t:param key: a key in the given dictionary\n\t\t\"\"\"\n\n\t\tif key in dic:\n\t\t\tif isinstance(dic[key], dict):\n\t\t\t\tinsert(dic[key], val, key)\n\t\telse:\n\t\t\tdic[key] = {}\n\t\t\tinsert(dic[key], val, key)\n\n\tinsert(dic, val, key)\n\tif len(keys):\n\t\tfor key in keys:\n\t\t\tinsert(dic, val, key)\n/src/com/shaw/core/security/encryption/aes.py\n\"\"\"\nAES encryption and decryption\n\"\"\"\n\nfrom Crypto.Cipher import AES\nfrom Crypto import Random\n\nfrom com.shaw.core.security.encryption.base import BaseEncryption\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\t# insert the value to the given key\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\n\t# go to the next level of the keys' chain\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\t# insert the value to the next level of the keys' chain\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n#\tif not keys:\n#\t\tkeys = []\n#\t\tkeys.append(key)\n#\t\tkey = None\n#\t\t#key = keys.pop()\n#\t\t#print key\n#\t\t#print keys\n#\t\t#print key in dic\n#\t\tif key in dic:\n#\t\t\t#print 'key exist'\n#\t\t\t#print dic[key]\n#\t\t\t#print type(dic[key])\n#\t\t\tif isinstance(dic[key], dict):\n#\t\t\t\t#print 'dict'\n#\t\t\t\t#print dic[key]\n#\t\t\t\t#print keys\n#\t\t\t\t#print keys[0]\n#\t\t\t\t#print keys[1:]\n#\t\t\t\tdic[key] = dict_insert(dic[key], val, keys[0], *keys[1:])\n#\t\t\telse:\n#\t\t\t\t#print 'not dict'\n#\t\t\t\tdic[key] = [dic[key]]\n#\t\t\t\tdic[key].append(val)\n#\t\t\t\t#print dic[key]\n#\t\telse:\n#\t\t\t#print 'key not exist'\n#\t\t\tdic[key] = val\n#\t\t\t#print dic[key]\n#\t\t#print dic\n\tif isinstance(dic, dict):\n\t\t", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t \n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tif len(keys) == 1:\n\t\t\tdic[key] = val\n\t\telse:\n\t\t\tif key not in dic:\n\t\t\t\tdic[key] = {}\n\t\t\tdict_insert(dic[key], val, *keys)\n```\n\n\n### dict_get\n\n```python", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t \n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tif len(keys) > 0:\n\t\tsubkey = keys[0]\n\t\tif subkey in dic.keys():\n\t\t\tdict_insert(dic[subkey], val, *keys[1:])\n\t\telse:\n\t\t\tdic[subkey] = {}\n\t\t\tdict_insert(dic[subkey], val, *keys[1:])\n\telse:\n\t\tdic[key] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t \n\tif len(keys) > 0:\n\t\tif not key in dic:\n\t\t\tdic[key] = {}\n\t\treturn dict_insert(dic[key], val, keys[0], *keys[1:])\n\telse:\n\t\tdic[key] = val\n/python/web/html/table.py\n# -*- coding: utf-8 -*-\n\"\"\"\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom. import tag\nfrom.. import html\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n        if not keys:\n                dic[key] = val\n                return\n\n        if key not in dic:\n                dic[key] = {}\n                dict_insert(dic[key], val, keys[0], *keys[1:])\n\n        return dict_insert(dic[key], val, keys[0], *keys[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tdef insert(dic, val, key):\n\t\t\"\"\"\n\t\tinsert a value of a key into a dictionary\n\n\t\tthis method is used internally and should not be called directly\n\n\t\t:param dic: a dictionary object to insert the key value into\n\t\t:param val: a value to insert to the given dictionary\n\t\t:param key: a key in the given dictionary\n\t\t\"\"\"\n\n\t\tif key in dic:\n\t\t\tif isinstance(dic[key], dict):\n\t\t\t\tinsert(dic[key], val, key)\n\t\telse:\n\t\t\tdic[key] = {}\n\t\t\tinsert(dic[key], val, key)\n\n\tinsert(dic, val, key)\n\tif len(keys):\n\t\tfor key in keys:\n\t\t\tinsert(dic, val, key)\n/src/com/shaw/core/security/encryption/aes.py\n\"\"\"\nAES encryption and decryption\n\"\"\"\n\nfrom Crypto.Cipher import AES\nfrom Crypto import Random\n\nfrom com.shaw.core.security.encryption.base import BaseEncryption\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\t# insert the value to the given key\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\n\t# go to the next level of the keys' chain\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\t# insert the value to the next level of the keys' chain\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n#\tif not keys:\n#\t\tkeys = []\n#\t\tkeys.append(key)\n#\t\tkey = None\n#\t\t#key = keys.pop()\n#\t\t#print key\n#\t\t#print keys\n#\t\t#print key in dic\n#\t\tif key in dic:\n#\t\t\t#print 'key exist'\n#\t\t\t#print dic[key]\n#\t\t\t#print type(dic[key])\n#\t\t\tif isinstance(dic[key], dict):\n#\t\t\t\t#print 'dict'\n#\t\t\t\t#print dic[key]\n#\t\t\t\t#print keys\n#\t\t\t\t#print keys[0]\n#\t\t\t\t#print keys[1:]\n#\t\t\t\tdic[key] = dict_insert(dic[key], val, keys[0], *keys[1:])\n#\t\t\telse:\n#\t\t\t\t#print 'not dict'\n#\t\t\t\tdic[key] = [dic[key]]\n#\t\t\t\tdic[key].append(val)\n#\t\t\t\t#print dic[key]\n#\t\telse:\n#\t\t\t#print 'key not exist'\n#\t\t\tdic[key] = val\n#\t\t\t#print dic[key]\n#\t\t#print dic\n\tif isinstance(dic, dict):\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t \n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tif len(keys) == 1:\n\t\t\tdic[key] = val\n\t\telse:\n\t\t\tif key not in dic:\n\t\t\t\tdic[key] = {}\n\t\t\tdict_insert(dic[key], val, *keys)\n```\n\n\n### dict_get\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t \n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tif len(keys) > 0:\n\t\tsubkey = keys[0]\n\t\tif subkey in dic.keys():\n\t\t\tdict_insert(dic[subkey], val, *keys[1:])\n\t\telse:\n\t\t\tdic[subkey] = {}\n\t\t\tdict_insert(dic[subkey], val, *keys[1:])\n\telse:\n\t\tdic[key] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t \n\tif len(keys) > 0:\n\t\tif not key in dic:\n\t\t\tdic[key] = {}\n\t\treturn dict_insert(dic[key], val, keys[0], *keys[1:])\n\telse:\n\t\tdic[key] = val\n/python/web/html/table.py\n# -*- coding: utf-8 -*-\n\"\"\"\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom. import tag\nfrom.. import html\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tdict_insert(dic[key], val, keys[0], *keys[1:])\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\tlist_of_file_names = IniType.ListOfFileNames()\n\tfor settings_dir in settings_dirs:\n\t\tlist_of_file_names.files.append(settings_dir + spec_option['name'] + '.ini')\n\treturn list_of_file_names\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\tret_type = IniType()\n\tret_type.list_of_file_names = []\n\t\n\tfor settings_dir in settings_dirs:\n\t\tfor file_name in fs.list_dir(settings_dir):\n\t\t\tif file_name.startswith(spec_option) and file_name.endswith(INI_FILE_EXTENSION):\n\t\t\t\tret_type.list_of_file_names.append(file_name)\n\n\tret_type.list_of_file_names.sort()\n\treturn ret_type\n\t", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n#\tlog.debug(\"list_of_file_names()\")\n\t\n\t# Create the list\n\tlist_of_file_names = cli.ListOfFileNames()\n\t\n\t# Create the file names\n\tfor settings_dir in settings_dirs:\n\t\tfile_name = os.path.join(settings_dir, spec_option)\n\t\tif os.path.exists(file_name):\n\t\t\tlist_of_file_names.append(file_name)\n\t\n\treturn list_of_file_names\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\treturn IniType.ListOfFileNames(cli.ListOfFileNames(settings_dirs, spec_option))\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\t# FIXME: this should be done with a classmethod of IniType.\n\t\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n#\tlist_of_file_names = cli.ListOfFileNames()\n#\tlist_of_file_names.add_option(cli.FileNames(spec_option))\n#\tlist_of_file_names.add_option(cli.FileNames(settings_dirs))\n#\tlist_of_file_names.add_option(cli.FileNames(settings_dirs))\n#\tlist_of_file_names.add_option(cli.FileNames(settings_dirs))\n\n\treturn cli.ListOfFileNames(\n\t\tcli.FileNames(spec_option),\n\t\tcli.FileNames(settings_dirs),\n\t\tcli.FileNames(settings_dirs),\n\t)\n\n#----------------------------------------------------------------------------------------\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\tini_type = IniType()\n\tini_type.list_of_file_names = []\n\tini_type.spec_option = spec_option\n\tfor settings_dir in settings_dirs:\n\t\tini_type.list_of_file_names.append(os.path.join(settings_dir, \"settings.ini\"))\n\t\n\treturn ini_type\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\t# Create an IniType object \n\tlist_of_file_names = IniType()\n\tlist_of_file_names.setSpecOption(spec_option)\n\t\n\t# Add all files in the settings dir\n\tfor settings_dir in settings_dirs:\n\t\tfor settings_file in os.listdir(settings_dir):\n\t\t\tlist_of_file_names.addFile(settings_dir + '/' + settings_file)\n\t\n\t# Return the IniType object\n\treturn list_of_file_names\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\tlist_of_file_names = IniType.ListOfFileNames()\n\tfor settings_dir in settings_dirs:\n\t\tlist_of_file_names.files.append(settings_dir + spec_option['name'] + '.ini')\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\tret_type = IniType()\n\tret_type.list_of_file_names = []\n\t\n\tfor settings_dir in settings_dirs:\n\t\tfor file_name in fs.list_dir(settings_dir):\n\t\t\tif file_name.startswith(spec_option) and file_name.endswith(INI_FILE_EXTENSION):\n\t\t\t\tret_type.list_of_file_names.append(file_name)\n\n\tret_type.list_of_file_names.sort()\n\treturn ret_type\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n#\tlog.debug(\"list_of_file_names()\")\n\t\n\t# Create the list\n\tlist_of_file_names = cli.ListOfFileNames()\n\t\n\t# Create the file names\n\tfor settings_dir in settings_dirs:\n\t\tfile_name = os.path.join(settings_dir, spec_option)\n\t\tif os.path.exists(file_name):\n\t\t\tlist_of_file_names.append(file_name)\n\t\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\treturn IniType.ListOfFileNames(cli.ListOfFileNames(settings_dirs, spec_option))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\t# FIXME: this should be done with a classmethod of IniType.\n\t\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a classmethod of IniType.\n\t# But it is not.\n\t#\n\t# FIXME: this should be done with a", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n#\tlist_of_file_names = cli.ListOfFileNames()\n#\tlist_of_file_names.add_option(cli.FileNames(spec_option))\n#\tlist_of_file_names.add_option(cli.FileNames(settings_dirs))\n#\tlist_of_file_names.add_option(cli.FileNames(settings_dirs))\n#\tlist_of_file_names.add_option(cli.FileNames(settings_dirs))\n\n\treturn cli.ListOfFileNames(\n\t\tcli.FileNames(spec_option),\n\t\tcli.FileNames(settings_dirs),\n\t\tcli.FileNames(settings_dirs),\n\t)\n\n#----------------------------------------------------------------------------------------\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\tini_type = IniType()\n\tini_type.list_of_file_names = []\n\tini_type.spec_option = spec_option\n\tfor settings_dir in settings_dirs:\n\t\tini_type.list_of_file_names.append(os.path.join(settings_dir, \"settings.ini\"))\n\t\n\treturn ini_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t \n\t# Create an IniType object \n\tlist_of_file_names = IniType()\n\tlist_of_file_names.setSpecOption(spec_option)\n\t\n\t# Add all files in the settings dir\n\tfor settings_dir in settings_dirs:\n\t\tfor settings_file in os.listdir(settings_dir):\n\t\t\tlist_of_file_names.addFile(settings_dir + '/' + settings_file)\n\t\n\t# Return the IniType object\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\tansible_config_manager_service = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\t\tif ansible_config_manager_service is None:\n\t\t\traise AnsibleConfigManagerDoesNotExist()\n\t\treturn ansible_config_manager_service\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls.get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n\n\treturn cls._get_service(cls.ANSIBLE_CONFIG_MANAGER)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\t@classmethod\n\tdef ansible_runner(cls):\n\t\"\"\"\n\tGets the ansible runner via ServiceName.ANSIBLE_RUNNER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_RUNNER)\n\t@classmethod\n\tdef ansible_vault(cls):\n\t\"\"\"\n\tGets the ansible vault via ServiceName.ANSIBLE_VAULT in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_VAULT)\n\t@classmethod\n\tdef apt_cacher_ng(cls):\n\t\"\"\"\n\tGets the apt cacher ng via ServiceName.APT_CACHER_NG in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.APT_CACHER_NG)\n\t@classmethod\n\tdef apt_cacher_ng_http(cls):\n\t\"\"\"\n\tGets the apt cacher ng http via ServiceName.APT_CACHER_NG_HTTP in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.APT_CACHER_NG_HTTP)\n\t@classmethod\n\tdef apt_cacher_ng_https(cls):\n\t\"\"\"\n\tGets the apt cacher ng", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t/README.md\n# ansible-config-manager\n\n## Overview\n\nAnsibleConfigManager is a configurable manager for Ansible.  It allows you to use pre-defined playbooks, templates, and/or variables to generate dynamic Ansible configuration files.  It also allows you to use any external service to dynamically generate your Ansible configuration.  This allows you to implement dynamic configuration management without having to write a new Ansible module or plugin.\n\n## Installation\n\n1. Install using pip:\n\n```\npip install ansible-config-manager\n```\n\n2. Add the following to your Ansible config file:\n\n```\n[defaults]\n# Set the default config manager to AnsibleConfigManager\nconfig_manager = ansible_config_manager.AnsibleConfigManager\n```\n\n## Usage\n\n### Creating a Config Manager\n\nA Config Manager is an object that manages Ansible configurations.  To create a Config Manager, create a class that inherits from the AnsibleConfigManager class.  In the example below, a Config Manager is created that uses a service to generate the configuration.\n\n```python\nfrom ansible_config_manager import AnsibleConfigManager\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\tmanager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\t\tif not manager:\n\t\t\tmanager = AnsibleConfigManager()\n\t\treturn manager\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\t@classmethod\n\tdef ansible_playbook_manager(cls):\n\t\"\"\"\n\tGets the ansible playbook manager via ServiceName.ANSIBLE_PLAYBOOK_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_PLAYBOOK_MANAGER)\n\n\t@classmethod\n\tdef ansible_host_manager(cls):\n\t\"\"\"\n\tGets the ansible host manager via ServiceName.ANSIBLE_HOST_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_HOST_MANAGER)\n\n\t@classmethod\n\tdef ansible_playbook(cls):\n\t\"\"\"\n\tGets the ansible playbook via ServiceName.ANSIBLE_PLAYBOOK in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_PLAYBOOK)\n\n\t@classmethod\n\tdef ansible_playbook_runner(cls):\n\t\"\"\"\n\tGets the ansible playbook runner via ServiceName.ANSIBLE_PLAYBOOK_RUNNER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_PLAYBOOK_RUNNER)\n\n\t@classmethod\n\tdef ansible_playbook_runner_manager(cls):\n\t\"\"\"\n\tGets the ansible playbook", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n@classmethod", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\t#cls._get_service() is the ServiceName.ANSIBLE_CONFIG_MANAGER\n\treturn cls._get_service()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\tansible_config_manager_service = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\t\tif ansible_config_manager_service is None:\n\t\t\traise AnsibleConfigManagerDoesNotExist()\n\t\treturn ansible_config_manager_service\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls.get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n\n\treturn cls._get_service(cls.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\t@classmethod\n\tdef ansible_runner(cls):\n\t\"\"\"\n\tGets the ansible runner via ServiceName.ANSIBLE_RUNNER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_RUNNER)\n\t@classmethod\n\tdef ansible_vault(cls):\n\t\"\"\"\n\tGets the ansible vault via ServiceName.ANSIBLE_VAULT in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_VAULT)\n\t@classmethod\n\tdef apt_cacher_ng(cls):\n\t\"\"\"\n\tGets the apt cacher ng via ServiceName.APT_CACHER_NG in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.APT_CACHER_NG)\n\t@classmethod\n\tdef apt_cacher_ng_http(cls):\n\t\"\"\"\n\tGets the apt cacher ng http via ServiceName.APT_CACHER_NG_HTTP in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.APT_CACHER_NG_HTTP)\n\t@classmethod\n\tdef apt_cacher_ng_https(cls):\n\t\"\"\"\n\tGets the apt cacher ng", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t/README.md\n# ansible-config-manager\n\n## Overview\n\nAnsibleConfigManager is a configurable manager for Ansible.  It allows you to use pre-defined playbooks, templates, and/or variables to generate dynamic Ansible configuration files.  It also allows you to use any external service to dynamically generate your Ansible configuration.  This allows you to implement dynamic configuration management without having to write a new Ansible module or plugin.\n\n## Installation\n\n1. Install using pip:\n\n```\npip install ansible-config-manager\n```\n\n2. Add the following to your Ansible config file:\n\n```\n[defaults]\n# Set the default config manager to AnsibleConfigManager\nconfig_manager = ansible_config_manager.AnsibleConfigManager\n```\n\n## Usage\n\n### Creating a Config Manager\n\nA Config Manager is an object that manages Ansible configurations.  To create a Config Manager, create a class that inherits from the AnsibleConfigManager class.  In the example below, a Config Manager is created that uses a service to generate the configuration.\n\n```python\nfrom ansible_config_manager import AnsibleConfigManager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\tmanager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\t\tif not manager:\n\t\t\tmanager = AnsibleConfigManager()\n\t\treturn manager\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\t@classmethod\n\tdef ansible_playbook_manager(cls):\n\t\"\"\"\n\tGets the ansible playbook manager via ServiceName.ANSIBLE_PLAYBOOK_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_PLAYBOOK_MANAGER)\n\n\t@classmethod\n\tdef ansible_host_manager(cls):\n\t\"\"\"\n\tGets the ansible host manager via ServiceName.ANSIBLE_HOST_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_HOST_MANAGER)\n\n\t@classmethod\n\tdef ansible_playbook(cls):\n\t\"\"\"\n\tGets the ansible playbook via ServiceName.ANSIBLE_PLAYBOOK in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_PLAYBOOK)\n\n\t@classmethod\n\tdef ansible_playbook_runner(cls):\n\t\"\"\"\n\tGets the ansible playbook runner via ServiceName.ANSIBLE_PLAYBOOK_RUNNER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.ANSIBLE_PLAYBOOK_RUNNER)\n\n\t@classmethod\n\tdef ansible_playbook_runner_manager(cls):\n\t\"\"\"\n\tGets the ansible playbook", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\t\t#cls._get_service() is the ServiceName.ANSIBLE_CONFIG_MANAGER\n\treturn cls._get_service()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n\n\tdef __get__(self, obj, cls):\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\treturn workspace_manager\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\tmanager = cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\t\treturn manager\n\n@classmethod", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n@classmethod", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n\n\tdef __get__(self, obj, cls):\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\treturn workspace_manager\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\tmanager = cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\t\treturn manager\n\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n#\t@classmethod\n#\tdef get_service(cls, name):\n#\t\t\"\"\"\n#\t\tGets the service via ServiceName.PLUGINS_MANAGER in cls._get_service()\n#\t\t\"\"\"\n#\t\tpass\n#\t\treturn ServiceName.PLUGINS_MANAGER.get_service(name)\n\t\n\t###############################\n\t#\n\t# \tPlugin Manager Functions\n\t#\n\t###############################\n\t\n\t@classmethod\n\tdef create_plugin(cls, name, plugin_type, **kwargs):\n\t\t\"\"\"\n\t\tCreates a plugin from the given plugin type and returns it.\n\t\t\"\"\"\n\t\tplugin_class = plugin_type.get_plugin(name)\n\t\treturn plugin_class(**kwargs)\n\t\n\t@classmethod\n\tdef create_plugin_instance(cls, plugin_type, **kwargs):\n\t\t\"\"\"\n\t\tCreates a plugin instance from the given plugin type and returns it.\n\t\t\"\"\"\n\t\tplugin_class = plugin_type.get_plugin()\n\t\treturn plugin_class(**kwargs)\n\t\n\t@classmethod\n\tdef load_plugin_config(cls, plugin, config):\n\t\t\"\"\"\n\t\tLoads the given config for the given plugin.\n\t\t\"\"\"\n\t\tplugin_type = plugin.plugin_type\n\t\tplugin_config = config.get_plugin_config(plugin_type)\n\t\treturn plugin_config", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t \n\treturn cls._get_service('PLUGINS_MANAGER')\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n#\tname = cls._get_service_name()\n\tname = cls.get_service_name()\n\tmanager = ServiceName.PLUGINS_MANAGER.get_service(name)\n\t\n\treturn manager\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t/app/test/test_models.py\nfrom django.test import TestCase\nfrom django.core.urlresolvers import reverse\nfrom django.core.exceptions import ValidationError\nfrom django.db import IntegrityError\n\nfrom django.contrib.auth.models import User\n\nfrom app.models import (\n\tUser,\n\tPlugin,\n\tService,\n\tServiceUser,\n\tServicePlugin,\n\tServicePluginUser,\n\tServicePluginUserOption,\n\tServicePluginUserOptionValue\n)\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n#\t@classmethod\n#\tdef get_service(cls, name):\n#\t\t\"\"\"\n#\t\tGets the service via ServiceName.PLUGINS_MANAGER in cls._get_service()\n#\t\t\"\"\"\n#\t\tpass\n#\t\treturn ServiceName.PLUGINS_MANAGER.get_service(name)\n\t\n\t###############################\n\t#\n\t# \tPlugin Manager Functions\n\t#\n\t###############################\n\t\n\t@classmethod\n\tdef create_plugin(cls, name, plugin_type, **kwargs):\n\t\t\"\"\"\n\t\tCreates a plugin from the given plugin type and returns it.\n\t\t\"\"\"\n\t\tplugin_class = plugin_type.get_plugin(name)\n\t\treturn plugin_class(**kwargs)\n\t\n\t@classmethod\n\tdef create_plugin_instance(cls, plugin_type, **kwargs):\n\t\t\"\"\"\n\t\tCreates a plugin instance from the given plugin type and returns it.\n\t\t\"\"\"\n\t\tplugin_class = plugin_type.get_plugin()\n\t\treturn plugin_class(**kwargs)\n\t\n\t@classmethod\n\tdef load_plugin_config(cls, plugin, config):\n\t\t\"\"\"\n\t\tLoads the given config for the given plugin.\n\t\t\"\"\"\n\t\tplugin_type = plugin.plugin_type\n\t\tplugin_config = config.get_plugin_config(plugin_type)\n\t\treturn plugin_config", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t \n\treturn cls._get_service('PLUGINS_MANAGER')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t\n#\tname = cls._get_service_name()\n\tname = cls.get_service_name()\n\tmanager = ServiceName.PLUGINS_MANAGER.get_service(name)\n\t\n\treturn manager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t/app/test/test_models.py\nfrom django.test import TestCase\nfrom django.core.urlresolvers import reverse\nfrom django.core.exceptions import ValidationError\nfrom django.db import IntegrityError\n\nfrom django.contrib.auth.models import User\n\nfrom app.models import (\n\tUser,\n\tPlugin,\n\tService,\n\tServiceUser,\n\tServicePlugin,\n\tServicePluginUser,\n\tServicePluginUserOption,\n\tServicePluginUserOptionValue\n)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tif spec_content is None:\n\t\traise IRValidatorException('SPEC_CONTENT_MISSING')\n\n\ttry:\n\t\tspec_data = yaml.load(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException('INVALID_YAML', e)\n\n\t# mandatory data\n\trequired_data = ['spec_name','spec_version','spec_type','spec_description']\n\tfor field in required_data:\n\t\tif field not in spec_data:\n\t\t\traise IRValidatorException('MISSING_REQUIRED_DATA', field)\n\n\t# validate data types\n\tif not isinstance(spec_data['spec_name'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_name', 'basestring')\n\tif not isinstance(spec_data['spec_version'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_version', 'basestring')\n\tif not isinstance(spec_data['spec_type'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_type', 'basestring')\n\tif not isinstance(spec_data['spec_description'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_description', 'basestring')\n\tif not isinstance(spec_data['spec_owner", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"Spec content cannot be None\")\n\n\t\ttry:\n\t\t\tspec_data = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"Unable to parse spec content: %s\" % str(e))\n\t\tif not spec_data:\n\t\t\traise IRValidatorException(\"Spec content is empty\")\n\n\t\tif spec_data.get('Name') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Name' is missing in spec file\")\n\n\t\tif spec_data.get('Description') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Description' is missing in spec file\")\n\n\t\tif spec_data.get('Version') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Version' is missing in spec file\")\n\n\t\tif spec_data.get('Category') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Category' is missing in spec file\")\n\n\t\tif spec_data.get('Keywords') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Keywords' is missing in spec file\")\n\n\t\tif spec_data.get('License') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'License' is missing in spec file\")\n\n\t\treturn spec_data\n/ir_validator/ir_validator.py", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tif spec_content is None:\n\t\tspec_content = cls.spec_content\n\n\tif spec_content is None:\n\t\traise IRValidatorException(\n\t\t\t'Spec content is empty')\n\n\ttry:\n\t\tspec = yaml.load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\n\t\t\t'Can not parse spec content as valid YAML: %s' % exc)\n\n\t# Check mandatory fields\n\tif not cls.is_spec_valid(spec):\n\t\traise IRValidatorException(\n\t\t\t'Spec is not valid: %s' % spec)\n\n\treturn spec\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tspec_content = cls._get_spec_content(spec_content)\n\t\ttry:\n\t\t\tspec_dict = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tlogger.error(\"Error loading data from IR spec file: {}\".format(exc))\n\t\t\traise IRValidatorException(\"Error loading data from IR spec file: {}\".format(exc))\n\n\t\ttry:\n\t\t\tmandatory_data = [\n\t\t\t\t\"name\", \"version\", \"type\", \"description\", \"source\"\n\t\t\t]\n\t\t\tfor key in mandatory_data:\n\t\t\t\tif key not in spec_dict:\n\t\t\t\t\traise IRValidatorException(\"Missing mandatory data {} in IR spec file\".format(key))\n\t\texcept KeyError as exc:\n\t\t\tlogger.error(\"Error loading data from IR spec file: {}\".format(exc))\n\t\t\traise IRValidatorException(\"Error loading data from IR spec file: {}\".format(exc))\n\n\t\ttry:\n\t\t\tif not spec_dict[\"type\"] in cls.IR_TYPES:\n\t\t\t\traise IRValidatorException(\"Invalid type {} in IR spec file\".format(spec_dict[\"type\"]))\n\t\texcept KeyError as exc:\n\t\t\tlogger.error(\"Error loading data from IR spec file: {}\".format(exc))\n\t\t\traise IRValidatorException(\"Error loading data from IR spec file: {}\".format(exc))\n\n\t\treturn spec_dict\n\n\nclass", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\"No spec content provided\")\n\n\t\tyaml_data = None\n\t\ttry:\n\t\t\tyaml_data = yaml.load(spec_content)\n\t\texcept Exception, e:\n\t\t\traise IRValidatorException(\"Error in YAML format: %s\" % e)\n\t\ttry:\n\t\t\tmandatory_fields = cls.get_mandatory_fields()\n\t\t\tfor field in mandatory_fields:\n\t\t\t\tif field not in yaml_data:\n\t\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\t\"Mandatory field %s is missing in spec\" % field)\n\t\texcept Exception, e:\n\t\t\traise IRValidatorException(\"Error in spec format: %s\" % e)\n\t\treturn yaml_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Cannot validate an empty content\")\n\n\t# load spec content into a dict\n\tspec_dict = yaml.load(spec_content)\n\n\t# validate mandatory fields\n\tmandatory_fields = [\"title\", \"description\", \"version\", \"vendor\", \"release\", \"vendor_id\", \"vendor_name\", \"vendor_url\", \"vendor_contact_name\", \"vendor_contact_email\", \"vendor_contact_phone\", \"vendor_contact_url\"]\n\tfor field in mandatory_fields:\n\t\tif field not in spec_dict:\n\t\t\traise IRValidatorException(\"Spec file missing mandatory field: {0}\".format(field))\n\n\treturn spec_dict\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\n\t\t\t\t'input spec file content is None')\n\t\tspec_dict = yaml.load(spec_content)\n\t\tif spec_dict is None:\n\t\t\traise IRValidatorException(\n\t\t\t\t'input spec file content is None')\n\t\tif 'name' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"name\" field')\n\t\tif'version' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"version\" field')\n\t\tif 'description' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"description\" field')\n\t\tif 'categories' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"categories\" field')\n\t\tif'sources' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"sources\" field')\n\t\tif'requirements' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"requirements\" field')\n\t\tif 'tags' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"tags", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tspec_dict = yaml.load(spec_content)\n\t\tif not spec_dict:\n\t\t\traise IRValidatorException(\"Empty spec\")\n\t\tif spec_dict['format']!= '1.0':\n\t\t\traise IRValidatorException(\"Wrong format: \" + str(spec_dict['format']))\n\t\tif 'name' not in spec_dict:\n\t\t\traise IRValidatorException(\"No name field\")\n\t\tif'version' not in spec_dict:\n\t\t\traise IRValidatorException(\"No version field\")\n\t\tif 'description' not in spec_dict:\n\t\t\traise IRValidatorException(\"No description field\")\n\t\tif'resources' not in spec_dict:\n\t\t\traise IRValidatorException(\"No resources field\")\n\t\tif 'properties' not in spec_dict:\n\t\t\traise IRValidatorException(\"No properties field\")\n\t\treturn spec_dict\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\t# validate the content of the spec file\n\tif spec_content is None:\n\t\traise IRValidatorException('No spec content provided.')\n\n\t# load the spec data into a dictionary\n\tdata = yaml.load(spec_content)\n\n\t# validate that the required fields are present\n\tif 'name' not in data:\n\t\traise IRValidatorException('Name is missing.')\n\tif 'id' not in data:\n\t\traise IRValidatorException('ID is missing.')\n\tif'version' not in data:\n\t\traise IRValidatorException('Version is missing.')\n\tif 'description' not in data:\n\t\traise IRValidatorException('Description is missing.')\n\tif 'author' not in data:\n\t\traise IRValidatorException('Author is missing.')\n\tif 'keywords' not in data:\n\t\traise IRValidatorException('Keywords are missing.')\n\tif 'contact' not in data:\n\t\traise IRValidatorException('Contact is missing.')\n\tif 'copyright' not in data:\n\t\traise IRValidatorException('Copyright is missing.')\n\tif 'license' not in data:\n\t\traise IRValidatorException('License is missing.')\n\tif 'url' not in data:\n\t\traise IRValidatorException('URL is missing.')\n\tif'requires' not in data:\n\t\traise IRValidatorException('Requires is missing.')\n\tif 'provides' not", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\"Data can't be validated\")\n\t\ttry:\n\t\t\tspec_data = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"Can not parse spec file: {}\".format(e))\n\n\t\tif not spec_data:\n\t\t\traise IRValidatorException(\"Spec file is empty. Can not validate.\")\n\n\t\tmandatory_fields = ['name','version', 'description','maintainer', 'license', 'homepage']\n\n\t\tfor field in mandatory_fields:\n\t\t\tif field not in spec_data:\n\t\t\t\traise IRValidatorException(\"Mandatory field '{}' is missing in spec file\".format(field))\n\n\t\treturn spec_data\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tif spec_content is None:\n\t\traise IRValidatorException('SPEC_CONTENT_MISSING')\n\n\ttry:\n\t\tspec_data = yaml.load(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException('INVALID_YAML', e)\n\n\t# mandatory data\n\trequired_data = ['spec_name','spec_version','spec_type','spec_description']\n\tfor field in required_data:\n\t\tif field not in spec_data:\n\t\t\traise IRValidatorException('MISSING_REQUIRED_DATA', field)\n\n\t# validate data types\n\tif not isinstance(spec_data['spec_name'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_name', 'basestring')\n\tif not isinstance(spec_data['spec_version'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_version', 'basestring')\n\tif not isinstance(spec_data['spec_type'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_type', 'basestring')\n\tif not isinstance(spec_data['spec_description'], basestring):\n\t\traise IRValidatorException('INVALID_DATA_TYPE','spec_description', 'basestring')\n\tif not isinstance(spec_data['spec_owner", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"Spec content cannot be None\")\n\n\t\ttry:\n\t\t\tspec_data = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"Unable to parse spec content: %s\" % str(e))\n\t\tif not spec_data:\n\t\t\traise IRValidatorException(\"Spec content is empty\")\n\n\t\tif spec_data.get('Name') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Name' is missing in spec file\")\n\n\t\tif spec_data.get('Description') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Description' is missing in spec file\")\n\n\t\tif spec_data.get('Version') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Version' is missing in spec file\")\n\n\t\tif spec_data.get('Category') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Category' is missing in spec file\")\n\n\t\tif spec_data.get('Keywords') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'Keywords' is missing in spec file\")\n\n\t\tif spec_data.get('License') is None:\n\t\t\traise IRValidatorException(\"Mandatory field 'License' is missing in spec file\")\n\n\t\treturn spec_data\n/ir_validator/ir_validator.py", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tif spec_content is None:\n\t\tspec_content = cls.spec_content\n\n\tif spec_content is None:\n\t\traise IRValidatorException(\n\t\t\t'Spec content is empty')\n\n\ttry:\n\t\tspec = yaml.load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\n\t\t\t'Can not parse spec content as valid YAML: %s' % exc)\n\n\t# Check mandatory fields\n\tif not cls.is_spec_valid(spec):\n\t\traise IRValidatorException(\n\t\t\t'Spec is not valid: %s' % spec)\n\n\treturn spec\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tspec_content = cls._get_spec_content(spec_content)\n\t\ttry:\n\t\t\tspec_dict = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tlogger.error(\"Error loading data from IR spec file: {}\".format(exc))\n\t\t\traise IRValidatorException(\"Error loading data from IR spec file: {}\".format(exc))\n\n\t\ttry:\n\t\t\tmandatory_data = [\n\t\t\t\t\"name\", \"version\", \"type\", \"description\", \"source\"\n\t\t\t]\n\t\t\tfor key in mandatory_data:\n\t\t\t\tif key not in spec_dict:\n\t\t\t\t\traise IRValidatorException(\"Missing mandatory data {} in IR spec file\".format(key))\n\t\texcept KeyError as exc:\n\t\t\tlogger.error(\"Error loading data from IR spec file: {}\".format(exc))\n\t\t\traise IRValidatorException(\"Error loading data from IR spec file: {}\".format(exc))\n\n\t\ttry:\n\t\t\tif not spec_dict[\"type\"] in cls.IR_TYPES:\n\t\t\t\traise IRValidatorException(\"Invalid type {} in IR spec file\".format(spec_dict[\"type\"]))\n\t\texcept KeyError as exc:\n\t\t\tlogger.error(\"Error loading data from IR spec file: {}\".format(exc))\n\t\t\traise IRValidatorException(\"Error loading data from IR spec file: {}\".format(exc))\n\n\t\treturn spec_dict\n\n\nclass", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\"No spec content provided\")\n\n\t\tyaml_data = None\n\t\ttry:\n\t\t\tyaml_data = yaml.load(spec_content)\n\t\texcept Exception, e:\n\t\t\traise IRValidatorException(\"Error in YAML format: %s\" % e)\n\t\ttry:\n\t\t\tmandatory_fields = cls.get_mandatory_fields()\n\t\t\tfor field in mandatory_fields:\n\t\t\t\tif field not in yaml_data:\n\t\t\t\t\traise IRValidatorException(\n\t\t\t\t\t\t\"Mandatory field %s is missing in spec\" % field)\n\t\texcept Exception, e:\n\t\t\traise IRValidatorException(\"Error in spec format: %s\" % e)\n\t\treturn yaml_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Cannot validate an empty content\")\n\n\t# load spec content into a dict\n\tspec_dict = yaml.load(spec_content)\n\n\t# validate mandatory fields\n\tmandatory_fields = [\"title\", \"description\", \"version\", \"vendor\", \"release\", \"vendor_id\", \"vendor_name\", \"vendor_url\", \"vendor_contact_name\", \"vendor_contact_email\", \"vendor_contact_phone\", \"vendor_contact_url\"]\n\tfor field in mandatory_fields:\n\t\tif field not in spec_dict:\n\t\t\traise IRValidatorException(\"Spec file missing mandatory field: {0}\".format(field))\n\n\treturn spec_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\n\t\t\t\t'input spec file content is None')\n\t\tspec_dict = yaml.load(spec_content)\n\t\tif spec_dict is None:\n\t\t\traise IRValidatorException(\n\t\t\t\t'input spec file content is None')\n\t\tif 'name' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"name\" field')\n\t\tif'version' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"version\" field')\n\t\tif 'description' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"description\" field')\n\t\tif 'categories' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"categories\" field')\n\t\tif'sources' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"sources\" field')\n\t\tif'requirements' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"requirements\" field')\n\t\tif 'tags' not in spec_dict:\n\t\t\traise IRValidatorException(\n\t\t\t\t'Spec YAML file does not have mandatory \"tags", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tspec_dict = yaml.load(spec_content)\n\t\tif not spec_dict:\n\t\t\traise IRValidatorException(\"Empty spec\")\n\t\tif spec_dict['format']!= '1.0':\n\t\t\traise IRValidatorException(\"Wrong format: \" + str(spec_dict['format']))\n\t\tif 'name' not in spec_dict:\n\t\t\traise IRValidatorException(\"No name field\")\n\t\tif'version' not in spec_dict:\n\t\t\traise IRValidatorException(\"No version field\")\n\t\tif 'description' not in spec_dict:\n\t\t\traise IRValidatorException(\"No description field\")\n\t\tif'resources' not in spec_dict:\n\t\t\traise IRValidatorException(\"No resources field\")\n\t\tif 'properties' not in spec_dict:\n\t\t\traise IRValidatorException(\"No properties field\")\n\t\treturn spec_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\t# validate the content of the spec file\n\tif spec_content is None:\n\t\traise IRValidatorException('No spec content provided.')\n\n\t# load the spec data into a dictionary\n\tdata = yaml.load(spec_content)\n\n\t# validate that the required fields are present\n\tif 'name' not in data:\n\t\traise IRValidatorException('Name is missing.')\n\tif 'id' not in data:\n\t\traise IRValidatorException('ID is missing.')\n\tif'version' not in data:\n\t\traise IRValidatorException('Version is missing.')\n\tif 'description' not in data:\n\t\traise IRValidatorException('Description is missing.')\n\tif 'author' not in data:\n\t\traise IRValidatorException('Author is missing.')\n\tif 'keywords' not in data:\n\t\traise IRValidatorException('Keywords are missing.')\n\tif 'contact' not in data:\n\t\traise IRValidatorException('Contact is missing.')\n\tif 'copyright' not in data:\n\t\traise IRValidatorException('Copyright is missing.')\n\tif 'license' not in data:\n\t\traise IRValidatorException('License is missing.')\n\tif 'url' not in data:\n\t\traise IRValidatorException('URL is missing.')\n\tif'requires' not in data:\n\t\traise IRValidatorException('Requires is missing.')\n\tif 'provides' not", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\"Data can't be validated\")\n\t\ttry:\n\t\t\tspec_data = yaml.load(spec_content)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"Can not parse spec file: {}\".format(e))\n\n\t\tif not spec_data:\n\t\t\traise IRValidatorException(\"Spec file is empty. Can not validate.\")\n\n\t\tmandatory_fields = ['name','version', 'description','maintainer', 'license', 'homepage']\n\n\t\tfor field in mandatory_fields:\n\t\t\tif field not in spec_data:\n\t\t\t\traise IRValidatorException(\"Mandatory field '{}' is missing in spec file\".format(field))\n\n\t\treturn spec_data\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tdata = cls.load_from_file(yaml_file)\n\t\tcls.validate(data)\n\t\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not yaml_file:\n\t\t\traise IRValidatorException(\"Missing YAML file path\")\n\n\t\ttry:\n\t\t\tyaml_data = yaml.load(open(yaml_file))\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException(\"Invalid YAML file, %s\" % str(e))\n\n\t\treturn cls.validate_from_data(yaml_data)\n\n\t@classmethod\n\tdef validate_from_data(cls, data=None):\n\t\t\"\"\"\n\t\tValidates that a data has all mandatory fields\n\n:param data: Dictionary with data\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not data:\n\t\t\traise IRValidatorException(\"Missing data\")\n\n\t\tif not data.get(\"id\"):\n\t\t\traise IRValidatorException(\"Missing 'id' in data\")\n\t\tif not data.get(\"version\"):\n\t\t\traise IRValidatorException(\"Missing'version' in data\")\n\t\tif not data.get(\"name\"):\n\t\t\traise IRValidatorException(\"Missing 'name' in data\")\n\t\tif not data.get(\"description\"):\n\t\t\traise IRValidatorException(\"Missing 'description' in data\")\n\t\tif not data.get(\"author\"):\n\t\t\traise IRValidatorException(\"Missing 'author' in data\")\n\t\tif not data.get(\"license\"):\n\t\t\traise", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not os.path.exists(yaml_file):\n\t\t\traise IRValidatorException('File does not exist: %s' % yaml_file)\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tyaml_data = yaml.load(stream)\n\t\t\texcept yaml.YAMLError:\n\t\t\t\traise IRValidatorException('YAML file %s is not valid' % yaml_file)\n\t\treturn cls.validate_data(yaml_data)\n\n\t@classmethod\n\tdef validate_data(cls, yaml_data):\n\t\t\"\"\"\n\t\tValidates that a YAML data structure has all required fields\n\n:param yaml_data: Dictionary with YAML data loaded from a YAML file\n:raise IRValidatorException: when mandatory data is missing in data structure\n:return: Dictionary with data from YAML file\n\t\"\"\"\n\t\tif not yaml_data:\n\t\t\traise IRValidatorException('YAML data structure is empty')\n\t\tmandatory_fields = ['name','version', 'description','schema_version','schema_url','schema_hash']\n\t\tfor field in mandatory_fields:\n\t\t\tif field not in yaml_data:\n\t\t\t\traise IRValidatorException('Missing mandatory field: \"%s\"' % field)\n\t\treturn yaml_data\n\n\t@classmethod\n\tdef validate_schema(cls, yaml_data):\n\t\t\"\"\"\n\t\tValid", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\tif yaml_file is None:\n\t\tyaml_file = os.path.join(os.path.dirname(__file__), 'config.yml')\n\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException('Config file not found: %s' % yaml_file)\n\n\ttry:\n\t\tconfig = yaml.safe_load(open(yaml_file))\n\texcept yaml.YAMLError:\n\t\traise IRValidatorException('Error in config file: %s' % yaml_file)\n\n\tif not config.get('data_dir'):\n\t\traise IRValidatorException('Data directory not found in config file: %s' % yaml_file)\n\n\tif not config.get('log_dir'):\n\t\traise IRValidatorException('Log directory not found in config file: %s' % yaml_file)\n\n\tif not config.get('log_file'):\n\t\traise IRValidatorException('Log file not found in config file: %s' % yaml_file)\n\n\tif not config.get('db_file'):\n\t\traise IRValidatorException('DB file not found in config file: %s' % yaml_file)\n\n\tif not config.get('db_schema'):\n\t\traise IRValidatorException('DB schema not found in config file: %s' % yaml_file)\n\n\tif not config.get('", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\ttry:\n\t\t\twith open(yaml_file, 'r') as fp:\n\t\t\t\tdata = yaml.load(fp)\n\t\t\tcls.validate(data)\n\t\t\treturn data\n\t\texcept:\n\t\t\traise IRValidatorException('Error in file: %s' % yaml_file)\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException('No YAML file provided')\n\t\tyaml_data = None\n\t\ttry:\n\t\t\tyaml_data = yaml.load(open(yaml_file), Loader=yaml.FullLoader)\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException('YAML file cannot be loaded: %s' % e)\n\t\treturn cls.validate_from_data(yaml_data)\n\n\t@classmethod\n\tdef validate_from_data(cls, data):\n\t\t\"\"\"\n\t\tValidates that all required fields are present in a dictionary\n\n:param data: Dictionary with data\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif data is None:\n\t\t\traise IRValidatorException('No data provided')\n\t\ttry:\n\t\t\tfor key in cls.MANDATORY_FIELDS:\n\t\t\t\tif key not in data:\n\t\t\t\t\traise IRValidatorException('Mandatory field %s is missing' % key)\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException('Mandatory field %s is missing')\n\t\treturn data\n#\n# Copyright (c) 2021 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\t# Load YAML data\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_data = yaml.load(f)\n\texcept Exception:\n\t\traise IRValidatorException('Could not read file: {}'.format(yaml_file))\n\t\t# Validate data\n\tif yaml_data is None:\n\t\traise IRValidatorException('No data loaded from file: {}'.format(yaml_file))\n\treturn cls.validate(yaml_data)\n\t# Validate data\n\tif yaml_data is None:\n\t\traise IRValidatorException('No data loaded from file: {}'.format(yaml_file))\n\treturn cls.validate(yaml_data)\n/ir_validator/validator.py\n#!/usr/bin/env python\n\"\"\"\nA module for validating IR data.\n\"\"\"\nfrom __future__ import print_function\n\nimport yaml\nfrom ir_validator.exceptions import IRValidatorException\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException(\"No YAML file specified\")\n\n\t\ttry:\n\t\t\tyaml_data = yaml.load(open(yaml_file))\n\t\texcept Exception as ex:\n\t\t\traise IRValidatorException(\"Error while reading YAML file: %s\" % ex)\n\n\t\t# Validate that all fields are present in the file\n\t\ttry:\n\t\t\tfor mandatory_field in cls.get_mandatory_fields():\n\t\t\t\tif mandatory_field not in yaml_data:\n\t\t\t\t\traise IRValidatorException(\"Field '%s' is missing in the YAML file\" % mandatory_field)\n\t\texcept Exception as ex:\n\t\t\traise IRValidatorException(\"Error while validating YAML file: %s\" % ex)\n\n\t\t# Check that all fields are of the right type\n\t\ttry:\n\t\t\tfor field_name, field_type in cls.get_field_types().iteritems():\n\t\t\t\tif yaml_data.get(field_name) is not None and not isinstance(yaml_data.get(field_name), field_type):\n\t\t\t\t\traise IRValidatorException(\"Field '%s' is of wrong type (%s)\" % (field_name, field_type))\n\t\texcept Exception as ex:\n\t\t\traise IRValidatorException(\"Error while validating YAML file: %s\" % ex)\n\n\t\treturn yaml_data\nimport os\nimport re\nimport sys\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException(\"Path to the YAML file is empty\")\n\t\twith open(yaml_file, \"r\") as f:\n\t\t\tyaml_data = yaml.safe_load(f)\n\t\tcls.validate_from_dict(yaml_data)\n\t\treturn yaml_data\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\ttry:\n\t\t\twith open(yaml_file, 'r') as f:\n\t\t\t\tyaml_data = yaml.safe_load(f)\n\t\t\t\treturn cls.validate(yaml_data)\n\t\texcept (IOError, yaml.YAMLError) as e:\n\t\t\traise IRValidatorException('Could not parse YAML file: {}'.format(e))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tdata = cls.load_from_file(yaml_file)\n\t\tcls.validate(data)\n\t\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not yaml_file:\n\t\t\traise IRValidatorException(\"Missing YAML file path\")\n\n\t\ttry:\n\t\t\tyaml_data = yaml.load(open(yaml_file))\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException(\"Invalid YAML file, %s\" % str(e))\n\n\t\treturn cls.validate_from_data(yaml_data)\n\n\t@classmethod\n\tdef validate_from_data(cls, data=None):\n\t\t\"\"\"\n\t\tValidates that a data has all mandatory fields\n\n:param data: Dictionary with data\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not data:\n\t\t\traise IRValidatorException(\"Missing data\")\n\n\t\tif not data.get(\"id\"):\n\t\t\traise IRValidatorException(\"Missing 'id' in data\")\n\t\tif not data.get(\"version\"):\n\t\t\traise IRValidatorException(\"Missing'version' in data\")\n\t\tif not data.get(\"name\"):\n\t\t\traise IRValidatorException(\"Missing 'name' in data\")\n\t\tif not data.get(\"description\"):\n\t\t\traise IRValidatorException(\"Missing 'description' in data\")\n\t\tif not data.get(\"author\"):\n\t\t\traise IRValidatorException(\"Missing 'author' in data\")\n\t\tif not data.get(\"license\"):\n\t\t\traise", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not os.path.exists(yaml_file):\n\t\t\traise IRValidatorException('File does not exist: %s' % yaml_file)\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tyaml_data = yaml.load(stream)\n\t\t\texcept yaml.YAMLError:\n\t\t\t\traise IRValidatorException('YAML file %s is not valid' % yaml_file)\n\t\treturn cls.validate_data(yaml_data)\n\n\t@classmethod\n\tdef validate_data(cls, yaml_data):\n\t\t\"\"\"\n\t\tValidates that a YAML data structure has all required fields\n\n:param yaml_data: Dictionary with YAML data loaded from a YAML file\n:raise IRValidatorException: when mandatory data is missing in data structure\n:return: Dictionary with data from YAML file\n\t\"\"\"\n\t\tif not yaml_data:\n\t\t\traise IRValidatorException('YAML data structure is empty')\n\t\tmandatory_fields = ['name','version', 'description','schema_version','schema_url','schema_hash']\n\t\tfor field in mandatory_fields:\n\t\t\tif field not in yaml_data:\n\t\t\t\traise IRValidatorException('Missing mandatory field: \"%s\"' % field)\n\t\treturn yaml_data\n\n\t@classmethod\n\tdef validate_schema(cls, yaml_data):\n\t\t\"\"\"\n\t\tValid", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\tif yaml_file is None:\n\t\tyaml_file = os.path.join(os.path.dirname(__file__), 'config.yml')\n\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException('Config file not found: %s' % yaml_file)\n\n\ttry:\n\t\tconfig = yaml.safe_load(open(yaml_file))\n\texcept yaml.YAMLError:\n\t\traise IRValidatorException('Error in config file: %s' % yaml_file)\n\n\tif not config.get('data_dir'):\n\t\traise IRValidatorException('Data directory not found in config file: %s' % yaml_file)\n\n\tif not config.get('log_dir'):\n\t\traise IRValidatorException('Log directory not found in config file: %s' % yaml_file)\n\n\tif not config.get('log_file'):\n\t\traise IRValidatorException('Log file not found in config file: %s' % yaml_file)\n\n\tif not config.get('db_file'):\n\t\traise IRValidatorException('DB file not found in config file: %s' % yaml_file)\n\n\tif not config.get('db_schema'):\n\t\traise IRValidatorException('DB schema not found in config file: %s' % yaml_file)\n\n\tif not config.get('", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\ttry:\n\t\t\twith open(yaml_file, 'r') as fp:\n\t\t\t\tdata = yaml.load(fp)\n\t\t\tcls.validate(data)\n\t\t\treturn data\n\t\texcept:\n\t\t\traise IRValidatorException('Error in file: %s' % yaml_file)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException('No YAML file provided')\n\t\tyaml_data = None\n\t\ttry:\n\t\t\tyaml_data = yaml.load(open(yaml_file), Loader=yaml.FullLoader)\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException('YAML file cannot be loaded: %s' % e)\n\t\treturn cls.validate_from_data(yaml_data)\n\n\t@classmethod\n\tdef validate_from_data(cls, data):\n\t\t\"\"\"\n\t\tValidates that all required fields are present in a dictionary\n\n:param data: Dictionary with data\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif data is None:\n\t\t\traise IRValidatorException('No data provided')\n\t\ttry:\n\t\t\tfor key in cls.MANDATORY_FIELDS:\n\t\t\t\tif key not in data:\n\t\t\t\t\traise IRValidatorException('Mandatory field %s is missing' % key)\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException('Mandatory field %s is missing')\n\t\treturn data\n#\n# Copyright (c) 2021 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\t# Load YAML data\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_data = yaml.load(f)\n\texcept Exception:\n\t\traise IRValidatorException('Could not read file: {}'.format(yaml_file))\n\t\t# Validate data\n\tif yaml_data is None:\n\t\traise IRValidatorException('No data loaded from file: {}'.format(yaml_file))\n\treturn cls.validate(yaml_data)\n\t# Validate data\n\tif yaml_data is None:\n\t\traise IRValidatorException('No data loaded from file: {}'.format(yaml_file))\n\treturn cls.validate(yaml_data)\n/ir_validator/validator.py\n#!/usr/bin/env python\n\"\"\"\nA module for validating IR data.\n\"\"\"\nfrom __future__ import print_function\n\nimport yaml\nfrom ir_validator.exceptions import IRValidatorException\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException(\"No YAML file specified\")\n\n\t\ttry:\n\t\t\tyaml_data = yaml.load(open(yaml_file))\n\t\texcept Exception as ex:\n\t\t\traise IRValidatorException(\"Error while reading YAML file: %s\" % ex)\n\n\t\t# Validate that all fields are present in the file\n\t\ttry:\n\t\t\tfor mandatory_field in cls.get_mandatory_fields():\n\t\t\t\tif mandatory_field not in yaml_data:\n\t\t\t\t\traise IRValidatorException(\"Field '%s' is missing in the YAML file\" % mandatory_field)\n\t\texcept Exception as ex:\n\t\t\traise IRValidatorException(\"Error while validating YAML file: %s\" % ex)\n\n\t\t# Check that all fields are of the right type\n\t\ttry:\n\t\t\tfor field_name, field_type in cls.get_field_types().iteritems():\n\t\t\t\tif yaml_data.get(field_name) is not None and not isinstance(yaml_data.get(field_name), field_type):\n\t\t\t\t\traise IRValidatorException(\"Field '%s' is of wrong type (%s)\" % (field_name, field_type))\n\t\texcept Exception as ex:\n\t\t\traise IRValidatorException(\"Error while validating YAML file: %s\" % ex)\n\n\t\treturn yaml_data\nimport os\nimport re\nimport sys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException(\"Path to the YAML file is empty\")\n\t\twith open(yaml_file, \"r\") as f:\n\t\t\tyaml_data = yaml.safe_load(f)\n\t\tcls.validate_from_dict(yaml_data)\n\t\treturn yaml_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\ttry:\n\t\t\twith open(yaml_file, 'r') as f:\n\t\t\t\tyaml_data = yaml.safe_load(f)\n\t\t\t\treturn cls.validate(yaml_data)\n\t\texcept (IOError, yaml.YAMLError) as e:\n\t\t\traise IRValidatorException('Could not parse YAML file: {}'.format(e))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# include directive\n\tif 'include' in parser_dict:\n\t\tincludes = parser_dict['include']\n\t\tif isinstance(includes, list):\n\t\t\tfor include in includes:\n\t\t\t\tself._include_group(include)\n\t\telse:\n\t\t\tself._include_group(includes)\n\n\tdef _include_group(self, include):\n\t\t\"\"\"\n\t\tResolves the include directive in the spec files.\n\t\t\"\"\"\n\t\tif isinstance(include, string):\n\t\t\tinclude_path = include\n\t\telif isinstance(include, dict):\n\t\t\tinclude_path = include['path']\n\t\t\tif 'group' in include:\n\t\t\t\tgroup = include['group']\n\t\telse:\n\t\t\traise ValueError(\"Unexpected include value: %s\" % include)\n\n\t\tif group:\n\t\t\tinclude_group = group\n\t\telse:\n\t\t\tinclude_group = os.path.basename(os.path.splitext(include_path)[0])\n\n\t\twith open(include_path, 'r') as f:\n\t\t\tinclude_spec = f.read()\n\t\tif not include_spec:\n\t\t\traise ValueError(\"Include file is empty: %s\" % include_path)\n\n\t\t# Include group\n\t\tif include_group not in self._groups:\n\t\t\tself._groups[include_group] = {}\n\t\tself._groups[include_group].update(self._", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# 1. Check if the include directive is set.\n\tif 'include' not in parser_dict:\n\t\treturn\n\n\t# 2. Add groups to the include dict.\n\tfor group in parser_dict['include']:\n\t\t# 2.1 Check if the group is valid.\n\t\tif not os.path.exists(group):\n\t\t\traise Exception('Invalid group: %s' % group)\n\n\t\t# 2.2 Check if group has already been included.\n\t\tif group in self._included_groups:\n\t\t\tcontinue\n\n\t\t# 2.3 Include the group.\n\t\tself._included_groups[group] = True\n\n\t\t# 2.4 Read the group.\n\t\twith open(group, 'r') as f:\n\t\t\tfor line in f:\n\t\t\t\t# 2.4.1 Check if the line is a comment.\n\t\t\t\tif line.strip()[0] == '#':\n\t\t\t\t\tcontinue\n\n\t\t\t\t# 2.4.2 Parse the line.\n\t\t\t\tparser_dict['include'].append(line.strip())\n\n\t# 3. Remove the include directive.\n\tparser_dict.pop('include')\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor item in parser_dict['include']:\n\t\t\tif item not in self._groups:\n\t\t\t\tmsg = (\"The group '%s' does not exist.\" % item)\n\t\t\t\traise NagiosPluginError(msg)\n\t\t\tgroup = self._groups[item]\n\t\t\tfor key, value in group.items():\n\t\t\t\tif key not in parser_dict:\n\t\t\t\t\tparser_dict[key] = value\n\t\t\t\t\tif type(value) is list:\n\t\t\t\t\t\tparser_dict[key] = value[:]\n\t\t\t\telif type(value) is list:\n\t\t\t\t\tparser_dict[key].extend(value)\n\t\t\t\telse:\n\t\t\t\t\tparser_dict[key] = [parser_dict[key], value]\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tgroups = []\n\t\tfor group in parser_dict.get('include', []):\n\t\t\tinclude_path = os.path.join(os.path.dirname(parser_dict['spec_file']), group)\n\t\t\tif os.path.exists(include_path):\n\t\t\t\tgroups.append(include_path)\n\t\t\telse:\n\t\t\t\traise Exception('Failed to locate include file: {}'.format(group))\n\t\treturn groups\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t \n\t# get the current file info\n\tfile_name = self.file_name\n\tfile_path = self.file_path\n\tfile_dir = os.path.dirname(file_path)\n\tfile_stem = os.path.splitext(file_name)[0]\n\n\t# parse the includes\n\tfor group, include in parser_dict.get('include', {}).items():\n\n\t\tinclude_path = include.get('file', None)\n\t\tinclude_file = include.get('file', None)\n\t\tinclude_dir = include.get('dir', None)\n\t\tinclude_stem = include.get('stem', None)\n\t\tinclude_ext = include.get('ext', None)\n\t\tinclude_ext_list = include.get('ext_list', None)\n\n\t\t# validate\n\t\tif (include_path is None) and ((include_file is None) or (include_dir is None) or (include_stem is None)):\n\t\t\traise ConfigError('Invalid include directive: %s' % group)\n\n\t\t# resolve the path\n\t\tif include_path is None:\n\n\t\t\t# resolve the file name\n\t\t\tif include_file is not None:\n\t\t\t\tinclude_file_path = os.path.join(file_dir, include_file)\n\t\t\telse:\n\t\t\t\tinclude_file_path = os.path.", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor group in parser_dict:\n\t\t\tif \"include\" in parser_dict[group]:\n\t\t\t\tif isinstance(parser_dict[group][\"include\"], str):\n\t\t\t\t\tparser_dict[group][\"include\"] = parser_dict[group][\"include\"].split(\",\")\n\t\t\t\tfor incl in parser_dict[group][\"include\"]:\n\t\t\t\t\tif not incl in parser_dict:\n\t\t\t\t\t\tparser_dict[group][\"include\"].remove(incl)\n\t\t\t\t\t\tparser_dict[group][\"include\"].append(incl)\n\tdef _resolve_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor group in parser_dict:\n\t\t\tif \"include\" in parser_dict[group]:\n\t\t\t\tfor incl in parser_dict[group][\"include\"]:\n\t\t\t\t\tif group in parser_dict[incl]:\n\t\t\t\t\t\tparser_dict[incl][\"include\"].extend(parser_dict[group][\"include\"])\n\t\t\t\t\t\tparser_dict[incl][\"include\"].extend(parser_dict[group][\"include\"])\n\t\t\t\t\telse:\n\t\t\t\t\t\tparser_dict[incl][\"include\"] = parser_dict[group][\"include\"]\n\t\t\t\t\t\tparser_dict[incl][\"include\"].extend(parser_dict[group][\"include\"])\n\tdef _resolve_include(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor group in parser_dict['include']:\n\t\t\t_group_file = os.path.join(self._path, group.replace('.', '/')) + '.spec'\n\t\t\t_group_dict = self._load_spec(_group_file)\n\t\t\tparser_dict.update(_group_dict)\n\t\treturn parser_dict\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# check if groups are included:\n\t\tif \"groups\" in self._spec.get(\"include\", {}):\n\t\t\tgroup_dict = self._spec.get(\"include\", {}).get(\"groups\", {})\n\t\t\tfor group in group_dict.get(\"include\", []):\n\t\t\t\tgroups = self._api.search_groups({\"_id\": group})\n\t\t\t\tif groups:\n\t\t\t\t\tself._include_groups(groups[0])\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Group '{}' not found. Skipping...\".format(group))\n\n\tdef _include_tags(self, parser_dict):\n\t\t\"\"\"\n\t\tResolves the include dict directive in the spec files.\n\t\t\"\"\"\n\t\t\t# check if tags are included:\n\t\t\tif \"tags\" in self._spec.get(\"include\", {}):\n\t\t\t\ttag_dict = self._spec.get(\"include\", {}).get(\"tags\", {})\n\t\t\t\tfor tag in tag_dict.get(\"include\", []):\n\t\t\t\t\ttags = self._api.search_tags({\"name\": tag})\n\t\t\t\t\tif tags:\n\t\t\t\t\t\tself._include_tags(tags[0])\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint(\"Tag '{}' not found. Skipping...\".format(tag))\n\n\tdef _include_users(self, parser_dict):\n\t\t\"\"\"\n\t\tResolves the include dict directive in the spec files.\n\t\t\"\"\"\n\t\t\t# check", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n \tfor include in parser_dict.keys():\n\t\tif include == 'include':\n\t\t\tself.include_groups(parser_dict[include])\n\n#\n# This function is called by the main function in the main.py module.\n#", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor key, value in parser_dict.iteritems():\n\t\t\tif key.startswith('include'):\n\t\t\t\t#print key\n\t\t\t\tif key.endswith('.spec'):\n\t\t\t\t\tif key in self.spec_files_list:\n\t\t\t\t\t\tparser_dict.pop(key)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\t#print \"Adding %s to list of spec files...\" % key\n\t\t\t\t\t\tself.spec_files_list.append(key)\n\t\t\t\t\t\t#print \"Include %s\" % key\n\t\t\t\t\t\tinclude_spec_file = SpecFile()\n\t\t\t\t\t\tinclude_spec_file.spec_file_name = key\n\t\t\t\t\t\tinclude_spec_file.parse_file(self.spec_file_name)\n\t\t\t\t\t\tparser_dict.update(include_spec_file.parser_dict)\n\t\t\t\t\t\tparser_dict.pop(key)\n\t\t\t\t\t\tself.spec_files_list.remove(key)\n\t\t\t\t\t#else:\n\t\t\t\t\t#\tprint \"Error: %s is not a spec file.\" % key\n\t\t\t\telse:\n\t\t\t\t\tparser_dict.pop(key)\n\treturn parser_dict\n```\n\nWe'll make a few modifications to the class. First, we'll add a list to hold the spec files that are being read.\n\n```python"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# include directive\n\tif 'include' in parser_dict:\n\t\tincludes = parser_dict['include']\n\t\tif isinstance(includes, list):\n\t\t\tfor include in includes:\n\t\t\t\tself._include_group(include)\n\t\telse:\n\t\t\tself._include_group(includes)\n\n\tdef _include_group(self, include):\n\t\t\"\"\"\n\t\tResolves the include directive in the spec files.\n\t\t\"\"\"\n\t\tif isinstance(include, string):\n\t\t\tinclude_path = include\n\t\telif isinstance(include, dict):\n\t\t\tinclude_path = include['path']\n\t\t\tif 'group' in include:\n\t\t\t\tgroup = include['group']\n\t\telse:\n\t\t\traise ValueError(\"Unexpected include value: %s\" % include)\n\n\t\tif group:\n\t\t\tinclude_group = group\n\t\telse:\n\t\t\tinclude_group = os.path.basename(os.path.splitext(include_path)[0])\n\n\t\twith open(include_path, 'r') as f:\n\t\t\tinclude_spec = f.read()\n\t\tif not include_spec:\n\t\t\traise ValueError(\"Include file is empty: %s\" % include_path)\n\n\t\t# Include group\n\t\tif include_group not in self._groups:\n\t\t\tself._groups[include_group] = {}\n\t\tself._groups[include_group].update(self._", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# 1. Check if the include directive is set.\n\tif 'include' not in parser_dict:\n\t\treturn\n\n\t# 2. Add groups to the include dict.\n\tfor group in parser_dict['include']:\n\t\t# 2.1 Check if the group is valid.\n\t\tif not os.path.exists(group):\n\t\t\traise Exception('Invalid group: %s' % group)\n\n\t\t# 2.2 Check if group has already been included.\n\t\tif group in self._included_groups:\n\t\t\tcontinue\n\n\t\t# 2.3 Include the group.\n\t\tself._included_groups[group] = True\n\n\t\t# 2.4 Read the group.\n\t\twith open(group, 'r') as f:\n\t\t\tfor line in f:\n\t\t\t\t# 2.4.1 Check if the line is a comment.\n\t\t\t\tif line.strip()[0] == '#':\n\t\t\t\t\tcontinue\n\n\t\t\t\t# 2.4.2 Parse the line.\n\t\t\t\tparser_dict['include'].append(line.strip())\n\n\t# 3. Remove the include directive.\n\tparser_dict.pop('include')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor item in parser_dict['include']:\n\t\t\tif item not in self._groups:\n\t\t\t\tmsg = (\"The group '%s' does not exist.\" % item)\n\t\t\t\traise NagiosPluginError(msg)\n\t\t\tgroup = self._groups[item]\n\t\t\tfor key, value in group.items():\n\t\t\t\tif key not in parser_dict:\n\t\t\t\t\tparser_dict[key] = value\n\t\t\t\t\tif type(value) is list:\n\t\t\t\t\t\tparser_dict[key] = value[:]\n\t\t\t\telif type(value) is list:\n\t\t\t\t\tparser_dict[key].extend(value)\n\t\t\t\telse:\n\t\t\t\t\tparser_dict[key] = [parser_dict[key], value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tgroups = []\n\t\tfor group in parser_dict.get('include', []):\n\t\t\tinclude_path = os.path.join(os.path.dirname(parser_dict['spec_file']), group)\n\t\t\tif os.path.exists(include_path):\n\t\t\t\tgroups.append(include_path)\n\t\t\telse:\n\t\t\t\traise Exception('Failed to locate include file: {}'.format(group))\n\t\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t \n\t# get the current file info\n\tfile_name = self.file_name\n\tfile_path = self.file_path\n\tfile_dir = os.path.dirname(file_path)\n\tfile_stem = os.path.splitext(file_name)[0]\n\n\t# parse the includes\n\tfor group, include in parser_dict.get('include', {}).items():\n\n\t\tinclude_path = include.get('file', None)\n\t\tinclude_file = include.get('file', None)\n\t\tinclude_dir = include.get('dir', None)\n\t\tinclude_stem = include.get('stem', None)\n\t\tinclude_ext = include.get('ext', None)\n\t\tinclude_ext_list = include.get('ext_list', None)\n\n\t\t# validate\n\t\tif (include_path is None) and ((include_file is None) or (include_dir is None) or (include_stem is None)):\n\t\t\traise ConfigError('Invalid include directive: %s' % group)\n\n\t\t# resolve the path\n\t\tif include_path is None:\n\n\t\t\t# resolve the file name\n\t\t\tif include_file is not None:\n\t\t\t\tinclude_file_path = os.path.join(file_dir, include_file)\n\t\t\telse:\n\t\t\t\tinclude_file_path = os.path.", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor group in parser_dict:\n\t\t\tif \"include\" in parser_dict[group]:\n\t\t\t\tif isinstance(parser_dict[group][\"include\"], str):\n\t\t\t\t\tparser_dict[group][\"include\"] = parser_dict[group][\"include\"].split(\",\")\n\t\t\t\tfor incl in parser_dict[group][\"include\"]:\n\t\t\t\t\tif not incl in parser_dict:\n\t\t\t\t\t\tparser_dict[group][\"include\"].remove(incl)\n\t\t\t\t\t\tparser_dict[group][\"include\"].append(incl)\n\tdef _resolve_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor group in parser_dict:\n\t\t\tif \"include\" in parser_dict[group]:\n\t\t\t\tfor incl in parser_dict[group][\"include\"]:\n\t\t\t\t\tif group in parser_dict[incl]:\n\t\t\t\t\t\tparser_dict[incl][\"include\"].extend(parser_dict[group][\"include\"])\n\t\t\t\t\t\tparser_dict[incl][\"include\"].extend(parser_dict[group][\"include\"])\n\t\t\t\t\telse:\n\t\t\t\t\t\tparser_dict[incl][\"include\"] = parser_dict[group][\"include\"]\n\t\t\t\t\t\tparser_dict[incl][\"include\"].extend(parser_dict[group][\"include\"])\n\tdef _resolve_include(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor group in parser_dict['include']:\n\t\t\t_group_file = os.path.join(self._path, group.replace('.', '/')) + '.spec'\n\t\t\t_group_dict = self._load_spec(_group_file)\n\t\t\tparser_dict.update(_group_dict)\n\t\treturn parser_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# check if groups are included:\n\t\tif \"groups\" in self._spec.get(\"include\", {}):\n\t\t\tgroup_dict = self._spec.get(\"include\", {}).get(\"groups\", {})\n\t\t\tfor group in group_dict.get(\"include\", []):\n\t\t\t\tgroups = self._api.search_groups({\"_id\": group})\n\t\t\t\tif groups:\n\t\t\t\t\tself._include_groups(groups[0])\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Group '{}' not found. Skipping...\".format(group))\n\n\tdef _include_tags(self, parser_dict):\n\t\t\"\"\"\n\t\tResolves the include dict directive in the spec files.\n\t\t\"\"\"\n\t\t\t# check if tags are included:\n\t\t\tif \"tags\" in self._spec.get(\"include\", {}):\n\t\t\t\ttag_dict = self._spec.get(\"include\", {}).get(\"tags\", {})\n\t\t\t\tfor tag in tag_dict.get(\"include\", []):\n\t\t\t\t\ttags = self._api.search_tags({\"name\": tag})\n\t\t\t\t\tif tags:\n\t\t\t\t\t\tself._include_tags(tags[0])\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint(\"Tag '{}' not found. Skipping...\".format(tag))\n\n\tdef _include_users(self, parser_dict):\n\t\t\"\"\"\n\t\tResolves the include dict directive in the spec files.\n\t\t\"\"\"\n\t\t\t# check", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n \tfor include in parser_dict.keys():\n\t\tif include == 'include':\n\t\t\tself.include_groups(parser_dict[include])\n\n#\n# This function is called by the main function in the main.py module.\n#", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor key, value in parser_dict.iteritems():\n\t\t\tif key.startswith('include'):\n\t\t\t\t#print key\n\t\t\t\tif key.endswith('.spec'):\n\t\t\t\t\tif key in self.spec_files_list:\n\t\t\t\t\t\tparser_dict.pop(key)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\t#print \"Adding %s to list of spec files...\" % key\n\t\t\t\t\t\tself.spec_files_list.append(key)\n\t\t\t\t\t\t#print \"Include %s\" % key\n\t\t\t\t\t\tinclude_spec_file = SpecFile()\n\t\t\t\t\t\tinclude_spec_file.spec_file_name = key\n\t\t\t\t\t\tinclude_spec_file.parse_file(self.spec_file_name)\n\t\t\t\t\t\tparser_dict.update(include_spec_file.parser_dict)\n\t\t\t\t\t\tparser_dict.pop(key)\n\t\t\t\t\t\tself.spec_files_list.remove(key)\n\t\t\t\t\t#else:\n\t\t\t\t\t#\tprint \"Error: %s is not a spec file.\" % key\n\t\t\t\telse:\n\t\t\t\t\tparser_dict.pop(key)\n\treturn parser_dict\n```\n\nWe'll make a few modifications to the class. First, we'll add a list to hold the spec files that are being read.\n\n```python", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\n        # Get default values from spec\n        spec_defaults = self.spec.get_default_value()\n\n        # Get default values from other sources e.g. config file\n        defaults = self._get_defaults()\n\n        # Merge defaults from spec and other sources\n        defaults.update(spec_defaults)\n\n        return defaults\n\n    def _get_defaults(self):\n        \"\"\"\n        Get default values from other sources e.g. config file\n        \"\"\"\n        return {}\n\n    def _set_default_value(self, key, value):\n        \"\"\"\n        Set default value for this argument.\n        \"\"\"\n        if not key in self.spec:\n            return False\n\n        self.spec[key].default_value = value\n        return True\n\n    def _set_default_values(self, defaults):\n        \"\"\"\n        Set default values for this argument.\n        \"\"\"\n        for key, value in defaults.iteritems():\n            self._set_default_value(key, value)\n\n    def _set_value(self, key, value):\n        \"\"\"\n        Set argument value.\n        \"\"\"\n        if not key in self.spec:\n            return False\n\n        self.spec[key].value = value\n        return True\n\n    def _set_values(self, values):\n        \"\"\"\n        Set argument values.\n        \"\"\"\n        for key, value in values.iteritems():\n            self._set_value(key, value)\n\n    def _unset_value(self, key):\n        \"\"\"\n        Un", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tdef _get_defaults(self, key, value):\n\t\t\"\"\"\n\t\tGet default value for the argument\n\t\t\"\"\"\n\t\tif key in self.spec:\n\t\t\treturn self.spec[key]\n\t\telse:\n\t\t\treturn value\n\t\n# +\n# # %load src/pydantic/dataclasses.py\nfrom __future__ import annotations\nfrom typing import (\n    Any,\n    Callable,\n    Default,\n    Dict,\n    Generic,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom pydantic_core import (\n    BaseConfig,\n    BaseModel,\n    BaseModelMetaclass,\n    FieldInfo,\n    Schema,\n    SchemaGenerator,\n    SchemaGenerationError,\n    SchemaGenerationMode,\n    SchemaOpts,\n    SchemaOptsDefault,\n    Undefined,\n    config_container,\n    core_schema,\n)\nfrom pydantic_core.types import StrBytes\nfrom pydantic_core.typing import is_generic_type\nfrom pydantic_core.typing import is_literal_type\nfrom pydantic_core.typing import is_new_type\nfrom pydantic_core.typing import is_optional_type\n\n__all__ = (\"dataclasses\", \"dataclass\")\n\n# Note: this module is a compatibility layer for", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tif self.spec:\n\t\t\tfor key, value in self.spec.items():\n\t\t\t\tif key in self.argument_spec:\n\t\t\t\t\tself.argument_spec[key]['default'] = value\n\n\tdef get_current_spec(self):\n\t\t\"\"\"\n\t\tGet the current spec.\n\t\t\"\"\"\n\t\treturn self._get_spec()\n\n\tdef get_module_spec(self, module_name):\n\t\t\"\"\"\n\t\tGet the module spec.\n\t\t\"\"\"\n\t\treturn self._get_module_spec(module_name)\n\n\tdef get_module_spec_from_module(self, module_name):\n\t\t\"\"\"\n\t\tGet the module spec from the module.\n\t\t\"\"\"\n\t\treturn self._get_module_spec_from_module(module_name)\n\n\tdef get_module_spec_from_spec(self, module_name):\n\t\t\"\"\"\n\t\tGet the module spec from the spec.\n\t\t\"\"\"\n\t\treturn self._get_module_spec_from_spec(module_name)\n\n\tdef set_module_spec(self, module_name, spec):\n\t\t\"\"\"\n\t\tSet the module spec.\n\t\t\"\"\"\n\t\treturn self._set_module_spec(module_name, spec)\n\n\tdef get_module_spec_from_task(self, module_name):\n\t\t\"\"\"\n\t\tGet", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\n        # Get the default values from the spec\n        spec = self.get_spec()\n\n        self._defaults = self._get_defaults(spec)\n\n        # Get the value for the argument\n        if self.name in self._defaults.keys():\n            self.value = self._defaults[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the config file\n        if self.name in self._config.keys():\n            self.value = self._config[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the command line\n        if self.name in self._command_line_options.keys():\n            self.value = self._command_line_options[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the environment\n        if self.name in self._environ.keys():\n            self.value = self._environ[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the config file\n        if self.name in self._config.keys():\n            self.value = self._config[self.name]\n        else:\n            self.value = None\n\n        if self.name in self._config.keys():\n            self.value = self._config[self.name]\n        else:\n            self.value = None\n\n        #", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tfor arg_name in self.get_arg_names():\n\t\tvalue = self._get_defaults(arg_name)\n\t\tif value is not None:\n\t\t\tself.set_value(arg_name, value)\n\t\n\tdef get_spec_arg(self, arg_name):\n\t\t\"\"\"\n\t\tGet argument value from spec\n\t\t\"\"\"\n\t\treturn self.get_spec().get(arg_name, None)\n\t\n\tdef get_spec_args(self):\n\t\t\"\"\"\n\t\tGet arguments values from spec\n\t\t\"\"\"\n\t\treturn self.get_spec()\n\t\n\tdef get_spec_arg_names(self):\n\t\t\"\"\"\n\t\tGet names of arguments from spec\n\t\t\"\"\"\n\t\treturn self.get_spec().keys()\n\t\n\tdef set_spec_arg(self, arg_name, value):\n\t\t\"\"\"\n\t\tSet argument value in spec\n\t\t\"\"\"\n\t\tself.get_spec()[arg_name] = value\n\t\n\tdef set_spec_args(self, args):\n\t\t\"\"\"\n\t\tSet arguments values in spec\n\t\t\"\"\"\n\t\tself.get_spec().update(args)\n\t\n\tdef get_spec(self):\n\t\t\"\"\"\n\t\tGet spec\n\t\t\"\"\"\n\t\treturn self._spec\n\t\n\tdef set_spec(self, spec):\n\t\t\"\"\"\n\t\tSet spec\n\t\t\"\"\"\n\t\tself._spec =", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tspec_args = []\n\t\tspec_args.extend(self.spec.arguments)\n\t\tspec_args.extend(self.spec.options)\n\n\t\tfor arg in spec_args:\n\t\t\targ_name = arg.name\n\n\t\t\tif arg_name in self._args and arg_name in self._defaults:\n\t\t\t\tself._args[arg_name].value = self._defaults[arg_name]\n\t\t\telif arg_name in self._args and arg_name not in self._defaults:\n\t\t\t\tself._args[arg_name].value = self._args[arg_name].value\n\t\t\telif arg_name not in self._args:\n\t\t\t\tself._args[arg_name] = arg\n\n\t\t\t\tif arg.default is not None:\n\t\t\t\t\tself._args[arg_name].value = arg.default\n\t\t\t\telse:\n\t\t\t\t\tself._args[arg_name].value = None\n\n\t\t\t\tif arg.value is not None:\n\t\t\t\t\tself._args[arg_name].value = arg.value\n\n\tdef _resolve_defaults(self):\n\t\t\"\"\"\n\t\tResolve argument defaults\n\t\t\"\"\"\n\t\tself._get_defaults()\n\t\tself.get_spec_defaults()\n\n\t\tself._defaults.update(self._args)\n\n\tdef _get_defaults(self):\n\t\t\"\"\"\n\t\tGet default values from other sources\n\t\t\"\"\"\n\t\t", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tspec_defaults = {}\n\t\tif self.spec is not None:\n\t\t\tfor arg in self.arguments:\n\t\t\t\tif arg.name in self.spec:\n\t\t\t\t\tspec_defaults[arg.name] = self.spec[arg.name]\n\t\treturn spec_defaults\n\n\tdef _get_defaults(self, source=None, args=None):\n\t\t\"\"\"\n\t\tGet default values of arguments from source.\n\n\t\tArgs:\n\t\t\tsource (str):\n\t\t\t\tDefaults for arguments will be resolved from this source.\n\t\t\t\tIf source is None, default values will be resolved from command line if any.\n\t\t\targs (dict):\n\t\t\t\tSpecific arguments to resolve defaults from.\n\t\t\t\tIf args is None, the defaults will be resolved for all arguments.\n\t\t\"\"\"\n\t\tdefaults = {}\n\t\tif source is None:\n\t\t\tsource = 'command line'\n\t\tif source == 'command line':\n\t\t\tif args is None:\n\t\t\t\targs = self.arguments\n\t\t\tfor arg in args:\n\t\t\t\tif arg.default is not None:\n\t\t\t\t\tdefaults[arg.name] = arg.default\n\t\telif source =='spec':\n\t\t\tif args is None:\n\t\t\t\targs = self.arguments\n\t\t\tfor arg in args:\n\t\t\t\tif arg.name in self.spec:\n\t\t\t\t\tdefaults[arg.name] = self.spec[arg.name]\n\t\telif source ==", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tself.spec.resolve_defaults(self._get_defaults())\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\n        # Get default values from spec\n        spec_defaults = self.spec.get_default_value()\n\n        # Get default values from other sources e.g. config file\n        defaults = self._get_defaults()\n\n        # Merge defaults from spec and other sources\n        defaults.update(spec_defaults)\n\n        return defaults\n\n    def _get_defaults(self):\n        \"\"\"\n        Get default values from other sources e.g. config file\n        \"\"\"\n        return {}\n\n    def _set_default_value(self, key, value):\n        \"\"\"\n        Set default value for this argument.\n        \"\"\"\n        if not key in self.spec:\n            return False\n\n        self.spec[key].default_value = value\n        return True\n\n    def _set_default_values(self, defaults):\n        \"\"\"\n        Set default values for this argument.\n        \"\"\"\n        for key, value in defaults.iteritems():\n            self._set_default_value(key, value)\n\n    def _set_value(self, key, value):\n        \"\"\"\n        Set argument value.\n        \"\"\"\n        if not key in self.spec:\n            return False\n\n        self.spec[key].value = value\n        return True\n\n    def _set_values(self, values):\n        \"\"\"\n        Set argument values.\n        \"\"\"\n        for key, value in values.iteritems():\n            self._set_value(key, value)\n\n    def _unset_value(self, key):\n        \"\"\"\n        Un", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tdef _get_defaults(self, key, value):\n\t\t\"\"\"\n\t\tGet default value for the argument\n\t\t\"\"\"\n\t\tif key in self.spec:\n\t\t\treturn self.spec[key]\n\t\telse:\n\t\t\treturn value\n\t\n# +\n# # %load src/pydantic/dataclasses.py\nfrom __future__ import annotations\nfrom typing import (\n    Any,\n    Callable,\n    Default,\n    Dict,\n    Generic,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom pydantic_core import (\n    BaseConfig,\n    BaseModel,\n    BaseModelMetaclass,\n    FieldInfo,\n    Schema,\n    SchemaGenerator,\n    SchemaGenerationError,\n    SchemaGenerationMode,\n    SchemaOpts,\n    SchemaOptsDefault,\n    Undefined,\n    config_container,\n    core_schema,\n)\nfrom pydantic_core.types import StrBytes\nfrom pydantic_core.typing import is_generic_type\nfrom pydantic_core.typing import is_literal_type\nfrom pydantic_core.typing import is_new_type\nfrom pydantic_core.typing import is_optional_type\n\n__all__ = (\"dataclasses\", \"dataclass\")\n\n# Note: this module is a compatibility layer for", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tif self.spec:\n\t\t\tfor key, value in self.spec.items():\n\t\t\t\tif key in self.argument_spec:\n\t\t\t\t\tself.argument_spec[key]['default'] = value\n\n\tdef get_current_spec(self):\n\t\t\"\"\"\n\t\tGet the current spec.\n\t\t\"\"\"\n\t\treturn self._get_spec()\n\n\tdef get_module_spec(self, module_name):\n\t\t\"\"\"\n\t\tGet the module spec.\n\t\t\"\"\"\n\t\treturn self._get_module_spec(module_name)\n\n\tdef get_module_spec_from_module(self, module_name):\n\t\t\"\"\"\n\t\tGet the module spec from the module.\n\t\t\"\"\"\n\t\treturn self._get_module_spec_from_module(module_name)\n\n\tdef get_module_spec_from_spec(self, module_name):\n\t\t\"\"\"\n\t\tGet the module spec from the spec.\n\t\t\"\"\"\n\t\treturn self._get_module_spec_from_spec(module_name)\n\n\tdef set_module_spec(self, module_name, spec):\n\t\t\"\"\"\n\t\tSet the module spec.\n\t\t\"\"\"\n\t\treturn self._set_module_spec(module_name, spec)\n\n\tdef get_module_spec_from_task(self, module_name):\n\t\t\"\"\"\n\t\tGet", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\n        # Get the default values from the spec\n        spec = self.get_spec()\n\n        self._defaults = self._get_defaults(spec)\n\n        # Get the value for the argument\n        if self.name in self._defaults.keys():\n            self.value = self._defaults[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the config file\n        if self.name in self._config.keys():\n            self.value = self._config[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the command line\n        if self.name in self._command_line_options.keys():\n            self.value = self._command_line_options[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the environment\n        if self.name in self._environ.keys():\n            self.value = self._environ[self.name]\n        else:\n            self.value = None\n\n        # Get the value for the argument from the config file\n        if self.name in self._config.keys():\n            self.value = self._config[self.name]\n        else:\n            self.value = None\n\n        if self.name in self._config.keys():\n            self.value = self._config[self.name]\n        else:\n            self.value = None\n\n        #", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tfor arg_name in self.get_arg_names():\n\t\tvalue = self._get_defaults(arg_name)\n\t\tif value is not None:\n\t\t\tself.set_value(arg_name, value)\n\t\n\tdef get_spec_arg(self, arg_name):\n\t\t\"\"\"\n\t\tGet argument value from spec\n\t\t\"\"\"\n\t\treturn self.get_spec().get(arg_name, None)\n\t\n\tdef get_spec_args(self):\n\t\t\"\"\"\n\t\tGet arguments values from spec\n\t\t\"\"\"\n\t\treturn self.get_spec()\n\t\n\tdef get_spec_arg_names(self):\n\t\t\"\"\"\n\t\tGet names of arguments from spec\n\t\t\"\"\"\n\t\treturn self.get_spec().keys()\n\t\n\tdef set_spec_arg(self, arg_name, value):\n\t\t\"\"\"\n\t\tSet argument value in spec\n\t\t\"\"\"\n\t\tself.get_spec()[arg_name] = value\n\t\n\tdef set_spec_args(self, args):\n\t\t\"\"\"\n\t\tSet arguments values in spec\n\t\t\"\"\"\n\t\tself.get_spec().update(args)\n\t\n\tdef get_spec(self):\n\t\t\"\"\"\n\t\tGet spec\n\t\t\"\"\"\n\t\treturn self._spec\n\t\n\tdef set_spec(self, spec):\n\t\t\"\"\"\n\t\tSet spec\n\t\t\"\"\"\n\t\tself._spec =", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tspec_args = []\n\t\tspec_args.extend(self.spec.arguments)\n\t\tspec_args.extend(self.spec.options)\n\n\t\tfor arg in spec_args:\n\t\t\targ_name = arg.name\n\n\t\t\tif arg_name in self._args and arg_name in self._defaults:\n\t\t\t\tself._args[arg_name].value = self._defaults[arg_name]\n\t\t\telif arg_name in self._args and arg_name not in self._defaults:\n\t\t\t\tself._args[arg_name].value = self._args[arg_name].value\n\t\t\telif arg_name not in self._args:\n\t\t\t\tself._args[arg_name] = arg\n\n\t\t\t\tif arg.default is not None:\n\t\t\t\t\tself._args[arg_name].value = arg.default\n\t\t\t\telse:\n\t\t\t\t\tself._args[arg_name].value = None\n\n\t\t\t\tif arg.value is not None:\n\t\t\t\t\tself._args[arg_name].value = arg.value\n\n\tdef _resolve_defaults(self):\n\t\t\"\"\"\n\t\tResolve argument defaults\n\t\t\"\"\"\n\t\tself._get_defaults()\n\t\tself.get_spec_defaults()\n\n\t\tself._defaults.update(self._args)\n\n\tdef _get_defaults(self):\n\t\t\"\"\"\n\t\tGet default values from other sources\n\t\t\"\"\"\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tspec_defaults = {}\n\t\tif self.spec is not None:\n\t\t\tfor arg in self.arguments:\n\t\t\t\tif arg.name in self.spec:\n\t\t\t\t\tspec_defaults[arg.name] = self.spec[arg.name]\n\t\treturn spec_defaults\n\n\tdef _get_defaults(self, source=None, args=None):\n\t\t\"\"\"\n\t\tGet default values of arguments from source.\n\n\t\tArgs:\n\t\t\tsource (str):\n\t\t\t\tDefaults for arguments will be resolved from this source.\n\t\t\t\tIf source is None, default values will be resolved from command line if any.\n\t\t\targs (dict):\n\t\t\t\tSpecific arguments to resolve defaults from.\n\t\t\t\tIf args is None, the defaults will be resolved for all arguments.\n\t\t\"\"\"\n\t\tdefaults = {}\n\t\tif source is None:\n\t\t\tsource = 'command line'\n\t\tif source == 'command line':\n\t\t\tif args is None:\n\t\t\t\targs = self.arguments\n\t\t\tfor arg in args:\n\t\t\t\tif arg.default is not None:\n\t\t\t\t\tdefaults[arg.name] = arg.default\n\t\telif source =='spec':\n\t\t\tif args is None:\n\t\t\t\targs = self.arguments\n\t\t\tfor arg in args:\n\t\t\t\tif arg.name in self.spec:\n\t\t\t\t\tdefaults[arg.name] = self.spec[arg.name]\n\t\telif source ==", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t\tself.spec.resolve_defaults(self._get_defaults())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\treturn self.get_deprecated_args_from_dict(self.spec_helper.iterate_option_specs())\n\tdef get_deprecated_args_from_dict(self, option_specs):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from given dict\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.deprecate:\n\t\t\t\tdeprecated_args[option_spec.name] = option_spec.deprecate\n\t\treturn deprecated_args\n\tdef get_default_args_from_dict(self, option_specs):\n\t\"\"\"\n\tReturning dict with default options. options from given dict\n\t\"\"\"\n\t\tdefault_args = {}\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.default:\n\t\t\t\tdefault_args[option_spec.name] = option_spec.default\n\t\treturn default_args\n\tdef get_required_args_from_dict(self, option_specs):\n\t\"\"\"\n\tReturning dict with required options. options from given dict\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.required:\n\t\t\t\trequired_args.append(option_spec.name)\n\t\treturn required_args\n\tdef get", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_options = dict()\n\t\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\t\tif \"deprecated\" in option_spec:\n\t\t\t\tkey = option_spec[\"deprecated\"]\n\t\t\t\tdeprecated_options[key] = option_spec\n\t\treturn deprecated_options\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor spec in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated_options' in spec:\n\t\t\t\tfor deprecated_option in spec['deprecated_options']:\n\t\t\t\t\tdeprecated_args[deprecated_option] = {'option_name': spec['option_name'], 'description': spec['description']}\n\t\treturn deprecated_args\n\n\tdef get_deprecated_options(self):\n\t\t\"\"\"\n\t\tReturning dict with deprecated options. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tdeprecated_options = {}\n\t\tfor spec in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated_options' in spec:\n\t\t\t\tfor deprecated in spec['deprecated_options']:\n\t\t\t\t\tdeprecated_options[deprecated] = spec['description']\n\t\treturn deprecated_options\n\n\tdef get_deprecated_option(self, option):\n\t\t\"\"\"\n\t\tReturning option description in case if option is deprecated.\n\t\t\"\"\"\n\t\tfor spec in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated_options' in spec:\n\t\t\t\tif option in spec['deprecated_options']:\n\t\t\t\t\treturn spec['description']\n\t\treturn None\n\n\tdef get_deprecated_options_list(self):\n\t\t\"\"\"\n\t\tReturning list with deprecated options.\n\t\t\"\"\"\n\t\tdeprecated", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\t# this is where the deprecated options go\n\t\treturn self.deprecated_args\n\n\tdef get_deprecated_args_for_option(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option.get(option, {})\n\n\tdef get_deprecated_args_for_option_short(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option_short.get(option, {})\n\n\tdef get_deprecated_args_for_option_long(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option_long.get(option, {})\n\n\tdef get_deprecated_args_for_option_long_short(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option_long_short.get(option, {})\n\n\tdef get_args_for_option(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which are passed with the one passed in.\n\t\t\"\"\"\n\t\treturn self.args", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in option:\n\t\t\t\tdeprecated_args[option['name']] = option['deprecated']\n\t\treturn deprecated_args\n\n\tdef is_deprecated(self):\n\t\t\"\"\"\n\t\tReturns true if any of the options in self.spec_helper.iterate_option_specs() is deprecated.\n\t\t\"\"\"\n\t\tdeprecated_args = self.get_deprecated_args()\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['name'] in deprecated_args:\n\t\t\t\treturn True\n\t\treturn False\n\n\tdef get_deprecated_message(self):\n\t\t\"\"\"\n\t\tReturns a string with the names of the deprecated options, and the replacement option.\n\t\t\"\"\"\n\t\tdeprecated_args = self.get_deprecated_args()\n\t\tdeprecated_options = []\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['name'] in deprecated_args:\n\t\t\t\tdeprecated_options.append(option['name'] +'option')\n\t\treturn 'Deprecated options:'+ ', '.join(deprecated_options) + '. Use'+ deprecated_args[option['name']] +'instead.'\n\n\tdef get_option_spec(self, option_name):\n\t\t\"\"\"\n\t\tReturns", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor (key, value) in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in value:\n\t\t\t\tdeprecated_args[key] = value\n\t\treturn deprecated_args\n\n\tdef get_options_from_spec(self, key):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\targs = {}\n\t\tfor (key, value) in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in value:\n\t\t\t\targs[key] = value\n\t\treturn args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor (key, value) in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in value:\n\t\t\t\tdeprecated_args[key] = value\n\t\treturn deprecated_args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor (key, value)", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\treturn {\n\t\t\t'search_type':'search_type',\n\t\t\t'search_route':'search_route',\n\t\t\t'search_field':'search_field'\n\t\t}\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\treturn self.spec_helper.iterate_option_specs()\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\treturn self.get_deprecated_args_from_dict(self.spec_helper.iterate_option_specs())\n\tdef get_deprecated_args_from_dict(self, option_specs):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from given dict\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.deprecate:\n\t\t\t\tdeprecated_args[option_spec.name] = option_spec.deprecate\n\t\treturn deprecated_args\n\tdef get_default_args_from_dict(self, option_specs):\n\t\"\"\"\n\tReturning dict with default options. options from given dict\n\t\"\"\"\n\t\tdefault_args = {}\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.default:\n\t\t\t\tdefault_args[option_spec.name] = option_spec.default\n\t\treturn default_args\n\tdef get_required_args_from_dict(self, option_specs):\n\t\"\"\"\n\tReturning dict with required options. options from given dict\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.required:\n\t\t\t\trequired_args.append(option_spec.name)\n\t\treturn required_args\n\tdef get", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_options = dict()\n\t\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\t\tif \"deprecated\" in option_spec:\n\t\t\t\tkey = option_spec[\"deprecated\"]\n\t\t\t\tdeprecated_options[key] = option_spec\n\t\treturn deprecated_options\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor spec in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated_options' in spec:\n\t\t\t\tfor deprecated_option in spec['deprecated_options']:\n\t\t\t\t\tdeprecated_args[deprecated_option] = {'option_name': spec['option_name'], 'description': spec['description']}\n\t\treturn deprecated_args\n\n\tdef get_deprecated_options(self):\n\t\t\"\"\"\n\t\tReturning dict with deprecated options. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tdeprecated_options = {}\n\t\tfor spec in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated_options' in spec:\n\t\t\t\tfor deprecated in spec['deprecated_options']:\n\t\t\t\t\tdeprecated_options[deprecated] = spec['description']\n\t\treturn deprecated_options\n\n\tdef get_deprecated_option(self, option):\n\t\t\"\"\"\n\t\tReturning option description in case if option is deprecated.\n\t\t\"\"\"\n\t\tfor spec in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated_options' in spec:\n\t\t\t\tif option in spec['deprecated_options']:\n\t\t\t\t\treturn spec['description']\n\t\treturn None\n\n\tdef get_deprecated_options_list(self):\n\t\t\"\"\"\n\t\tReturning list with deprecated options.\n\t\t\"\"\"\n\t\tdeprecated", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\t# this is where the deprecated options go\n\t\treturn self.deprecated_args\n\n\tdef get_deprecated_args_for_option(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option.get(option, {})\n\n\tdef get_deprecated_args_for_option_short(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option_short.get(option, {})\n\n\tdef get_deprecated_args_for_option_long(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option_long.get(option, {})\n\n\tdef get_deprecated_args_for_option_long_short(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate the one passed in.\n\t\t\"\"\"\n\t\treturn self.deprecated_args_for_option_long_short.get(option, {})\n\n\tdef get_args_for_option(self, option):\n\t\t\"\"\"\n\t\tReturning dict with options which are passed with the one passed in.\n\t\t\"\"\"\n\t\treturn self.args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in option:\n\t\t\t\tdeprecated_args[option['name']] = option['deprecated']\n\t\treturn deprecated_args\n\n\tdef is_deprecated(self):\n\t\t\"\"\"\n\t\tReturns true if any of the options in self.spec_helper.iterate_option_specs() is deprecated.\n\t\t\"\"\"\n\t\tdeprecated_args = self.get_deprecated_args()\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['name'] in deprecated_args:\n\t\t\t\treturn True\n\t\treturn False\n\n\tdef get_deprecated_message(self):\n\t\t\"\"\"\n\t\tReturns a string with the names of the deprecated options, and the replacement option.\n\t\t\"\"\"\n\t\tdeprecated_args = self.get_deprecated_args()\n\t\tdeprecated_options = []\n\t\tfor option in self.spec_helper.iterate_option_specs():\n\t\t\tif option['name'] in deprecated_args:\n\t\t\t\tdeprecated_options.append(option['name'] +'option')\n\t\treturn 'Deprecated options:'+ ', '.join(deprecated_options) + '. Use'+ deprecated_args[option['name']] +'instead.'\n\n\tdef get_option_spec(self, option_name):\n\t\t\"\"\"\n\t\tReturns", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor (key, value) in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in value:\n\t\t\t\tdeprecated_args[key] = value\n\t\treturn deprecated_args\n\n\tdef get_options_from_spec(self, key):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\targs = {}\n\t\tfor (key, value) in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in value:\n\t\t\t\targs[key] = value\n\t\treturn args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor (key, value) in self.spec_helper.iterate_option_specs():\n\t\t\tif 'deprecated' in value:\n\t\t\t\tdeprecated_args[key] = value\n\t\treturn deprecated_args\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor (key, value)", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\treturn {\n\t\t\t'search_type':'search_type',\n\t\t\t'search_route':'search_route',\n\t\t\t'search_field':'search_field'\n\t\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\t\treturn self.spec_helper.iterate_option_specs()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\t# Print deprecation warning\n\t\tif cli_args.get(\"verbose\") is not None or cli_args.get(\"version\") is not None:\n\t\t\tprint(\"WARNING: The options --verbose and --version are deprecated. Please use --help to show the available options.\")\n\n\t\t# Validate deprecated arguments\n\t\tif cli_args.get(\"verbose\") is not None or cli_args.get(\"version\") is not None:\n\t\t\tsys.exit(1)\n\n\t\t# Validate deprecated arguments\n\t\tfor arg in [\"verbose\", \"version\"]:\n\t\t\tif arg in answer_file_args:\n\t\t\t\tprint(\"WARNING: The option %s is deprecated. Please use --help to show the available options.\" % arg)\n\t\t\t\tsys.exit(1)\n/python/tests/test_help.py\nimport os\nimport sys\nimport pytest\n\nfrom unittest.mock import patch, call\nfrom io import StringIO\n\nfrom click.testing import CliRunner\n\nfrom tiktools import help\n\n@pytest.fixture", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n    \tdeprecated = []\n    \tfor file_arg in answer_file_args:\n        \tif file_arg not in cli_args:\n            \t\tdeprecated.append(file_arg)\n\n\t\n    \tif deprecated:\n        \tprint \"Warning: the following arguments are deprecated:\"\n        \tfor arg in deprecated:\n            \t\tprint \"\\t\", arg\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\t# validate answer_file_args\n\tif answer_file_args:\n\t\tfor key in answer_file_args:\n\t\t\tif key not in self.defaults:\n\t\t\t\tself.print_error(\n\t\t\t\t\t\"WARNING: the argument '{}' is deprecated, please use the new argument '{}' instead\".format(\n\t\t\t\t\t\tkey, self.defaults[key].cli_name))\n\n\t\t# validate cli_args\n\tif cli_args:\n\t\tfor key in cli_args:\n\t\t\tif key not in self.defaults:\n\t\t\t\tself.print_error(\n\t\t\t\t\t\"WARNING: the argument '{}' is deprecated, please use the new argument '{}' instead\".format(\n\t\t\t\t\t\tkey, self.defaults[key].cli_name))\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tdeprecated_args = ['--config', '--config_file', '--config_file_path']\n\t\tfor arg in deprecated_args:\n\n\t\t\tif arg in cli_args.keys():\n\t\t\t\tself.logger.warning(f\"{arg} is deprecated and will be removed soon\")\n\t\t\t\tif arg in answer_file_args.keys():\n\t\t\t\t\tself.logger.warning(f\"{arg} is deprecated and will be removed soon\")\n\t\t\t\t\tself.logger.warning(f\"Answer file {answer_file_args[arg]} is deprecated and will be removed soon\")\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t # Get the deprecated arguments\n\tdeprecated_args = self.deprecated_args()\n\n\t# Get the deprecated arguments from cli\n\tcli_deprecated_args = self.deprecated_args_from_cli(cli_args, deprecated_args)\n\n\t# Get the deprecated arguments from files\n\tfile_deprecated_args = self.deprecated_args_from_files(answer_file_args, deprecated_args)\n\n\t# Get the deprecated arguments from both cli and files\n\tboth_deprecated_args = self.deprecated_args_from_both(cli_deprecated_args, file_deprecated_args)\n\n\t# Print the deprecated arguments\n\tself.print_deprecated_args(both_deprecated_args)\n\n\t# If there are deprecated arguments, exit\n\tif both_deprecated_args:\n\t\tself.exit()\n\n\t# Exit if there are deprecated arguments\n\tself.exit_if_deprecated_args(cli_args, answer_file_args)\n\n\t# Get the deprecated arguments that are not deprecated\n\tnot_deprecated_args = self.deprecated_args_not_deprecated(cli_args, answer_file_args)\n\n\t# If there are not deprecated arguments, exit\n\tif not_deprecated_args:\n\t\tself.exit_if_not_deprecated_args(not_deprecated_args)\n\n\t# Exit if there are deprecated arguments that are", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tif answer_file_args is not None:\n\t\t\tfor key, value in answer_file_args.items():\n\t\t\t\tif key in cli_args:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in both cli and answer file. Command line argument will be used\".format(key))\n\t\t\t\telse:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in answer file. Use this argument in the cli\".format(key))\n\t\telse:\n\t\t\tfor key, value in cli_args.items():\n\t\t\t\tif key in answer_file_args:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in both cli and answer file. Answer file argument will be used\".format(key))\n\t\t\t\telse:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in cli. Use this argument in the answer file\".format(key))\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tvalid_args = {\n\t    'config_file': ['--config-file'],\n\t    'config_file_name': ['--config-file-name'],\n\t    'config_file_path': ['--config-file-path'],\n\t    'default_config_file': ['--default-config-file'],\n\t    'default_config_file_name': ['--default-config-file-name'],\n\t    'default_config_file_path': ['--default-config-file-path'],\n\t    'output_file_name': ['--output-file-name'],\n\t    'output_file_path': ['--output-file-path'],\n\t    'answer_file_name': ['--answer-file-name'],\n\t    'answer_file_path': ['--answer-file-path'],\n\t    'output_file': ['--output-file'],\n\t    'answer_file': ['--answer-file'],\n\t    'default_output_file_name': ['--default-output-file-name'],\n\t    'default_output_file_path': ['--default-output-file-path'],\n\t    'default_answer_file_name': ['--default-answer-file-name'],\n\t    'default_answer_file_path': ['--default", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tfor arg in self.deprecated_args:\n\t\t\tif arg in cli_args:\n\t\t\t\tprint(\"WARNING: -%s option is deprecated.\" % cli_args[arg])\n\t\t\t\tif arg in answer_file_args:\n\t\t\t\t\tprint(\"WARNING: %s option is deprecated.\" % arg)\n\t\t\t\t\tprint(\"WARNING: The new option is -%s.\" % answer_file_args[arg])\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\t# Deprecated arguments\n\tprint(\"This tool is deprecated, please use 'qtest --help' for the latest command line options\")\n\tdeprecated_args = ['testcase', 'testrun', 'testcase_file', 'testrun_file', 'qtest_type', 'testcase_type', 'testrun_type', 'qtest_type', 'testcase_type', 'testrun_type', 'testrun', 'testrun_file', 'tc_id', 'tc_name', 'tc_type', 'tc_file', 'tc_file_type', 'tc_file_path', 'tc_file_name', 'testrun_name', 'testrun_type', 'plan_file', 'plan_file_type', 'plan_file_path', 'plan_file_name', 'plan_file_type', 'test_plan', 'test_plan_type', 'test_plan_path', 'test_plan_name', 'test_plan_file_type', 'test_plan_file_path', 'test_plan_file_name', 'test_plan_file_type', 'test_plan_file_path', 'test_plan_file_name', 'test_plan_file_type', 'test_plan_file_path', 'test_plan_file_name', 'test_plan_file_type']\n\t", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tdeprecated_args = []\n\n\t\tdeprecated_args += self.check_deprecated_args(cli_args, answer_file_args)\n\t\tdeprecated_args += self.check_deprecated_args(answer_file_args, cli_args)\n\n\t\tif len(deprecated_args) > 0:\n\t\t\tself.logger.warn(\"The following arguments are deprecated and will be removed in the next release: \" + \",\".join(deprecated_args))\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\t# Print deprecation warning\n\t\tif cli_args.get(\"verbose\") is not None or cli_args.get(\"version\") is not None:\n\t\t\tprint(\"WARNING: The options --verbose and --version are deprecated. Please use --help to show the available options.\")\n\n\t\t# Validate deprecated arguments\n\t\tif cli_args.get(\"verbose\") is not None or cli_args.get(\"version\") is not None:\n\t\t\tsys.exit(1)\n\n\t\t# Validate deprecated arguments\n\t\tfor arg in [\"verbose\", \"version\"]:\n\t\t\tif arg in answer_file_args:\n\t\t\t\tprint(\"WARNING: The option %s is deprecated. Please use --help to show the available options.\" % arg)\n\t\t\t\tsys.exit(1)\n/python/tests/test_help.py\nimport os\nimport sys\nimport pytest\n\nfrom unittest.mock import patch, call\nfrom io import StringIO\n\nfrom click.testing import CliRunner\n\nfrom tiktools import help\n\n@pytest.fixture", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n    \tdeprecated = []\n    \tfor file_arg in answer_file_args:\n        \tif file_arg not in cli_args:\n            \t\tdeprecated.append(file_arg)\n\n\t\n    \tif deprecated:\n        \tprint \"Warning: the following arguments are deprecated:\"\n        \tfor arg in deprecated:\n            \t\tprint \"\\t\", arg\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\t# validate answer_file_args\n\tif answer_file_args:\n\t\tfor key in answer_file_args:\n\t\t\tif key not in self.defaults:\n\t\t\t\tself.print_error(\n\t\t\t\t\t\"WARNING: the argument '{}' is deprecated, please use the new argument '{}' instead\".format(\n\t\t\t\t\t\tkey, self.defaults[key].cli_name))\n\n\t\t# validate cli_args\n\tif cli_args:\n\t\tfor key in cli_args:\n\t\t\tif key not in self.defaults:\n\t\t\t\tself.print_error(\n\t\t\t\t\t\"WARNING: the argument '{}' is deprecated, please use the new argument '{}' instead\".format(\n\t\t\t\t\t\tkey, self.defaults[key].cli_name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tdeprecated_args = ['--config', '--config_file', '--config_file_path']\n\t\tfor arg in deprecated_args:\n\n\t\t\tif arg in cli_args.keys():\n\t\t\t\tself.logger.warning(f\"{arg} is deprecated and will be removed soon\")\n\t\t\t\tif arg in answer_file_args.keys():\n\t\t\t\t\tself.logger.warning(f\"{arg} is deprecated and will be removed soon\")\n\t\t\t\t\tself.logger.warning(f\"Answer file {answer_file_args[arg]} is deprecated and will be removed soon\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t # Get the deprecated arguments\n\tdeprecated_args = self.deprecated_args()\n\n\t# Get the deprecated arguments from cli\n\tcli_deprecated_args = self.deprecated_args_from_cli(cli_args, deprecated_args)\n\n\t# Get the deprecated arguments from files\n\tfile_deprecated_args = self.deprecated_args_from_files(answer_file_args, deprecated_args)\n\n\t# Get the deprecated arguments from both cli and files\n\tboth_deprecated_args = self.deprecated_args_from_both(cli_deprecated_args, file_deprecated_args)\n\n\t# Print the deprecated arguments\n\tself.print_deprecated_args(both_deprecated_args)\n\n\t# If there are deprecated arguments, exit\n\tif both_deprecated_args:\n\t\tself.exit()\n\n\t# Exit if there are deprecated arguments\n\tself.exit_if_deprecated_args(cli_args, answer_file_args)\n\n\t# Get the deprecated arguments that are not deprecated\n\tnot_deprecated_args = self.deprecated_args_not_deprecated(cli_args, answer_file_args)\n\n\t# If there are not deprecated arguments, exit\n\tif not_deprecated_args:\n\t\tself.exit_if_not_deprecated_args(not_deprecated_args)\n\n\t# Exit if there are deprecated arguments that are", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tif answer_file_args is not None:\n\t\t\tfor key, value in answer_file_args.items():\n\t\t\t\tif key in cli_args:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in both cli and answer file. Command line argument will be used\".format(key))\n\t\t\t\telse:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in answer file. Use this argument in the cli\".format(key))\n\t\telse:\n\t\t\tfor key, value in cli_args.items():\n\t\t\t\tif key in answer_file_args:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in both cli and answer file. Answer file argument will be used\".format(key))\n\t\t\t\telse:\n\t\t\t\t\tprint(\"WARNING: the argument '{}' is found in cli. Use this argument in the answer file\".format(key))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tvalid_args = {\n\t    'config_file': ['--config-file'],\n\t    'config_file_name': ['--config-file-name'],\n\t    'config_file_path': ['--config-file-path'],\n\t    'default_config_file': ['--default-config-file'],\n\t    'default_config_file_name': ['--default-config-file-name'],\n\t    'default_config_file_path': ['--default-config-file-path'],\n\t    'output_file_name': ['--output-file-name'],\n\t    'output_file_path': ['--output-file-path'],\n\t    'answer_file_name': ['--answer-file-name'],\n\t    'answer_file_path': ['--answer-file-path'],\n\t    'output_file': ['--output-file'],\n\t    'answer_file': ['--answer-file'],\n\t    'default_output_file_name': ['--default-output-file-name'],\n\t    'default_output_file_path': ['--default-output-file-path'],\n\t    'default_answer_file_name': ['--default-answer-file-name'],\n\t    'default_answer_file_path': ['--default", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tfor arg in self.deprecated_args:\n\t\t\tif arg in cli_args:\n\t\t\t\tprint(\"WARNING: -%s option is deprecated.\" % cli_args[arg])\n\t\t\t\tif arg in answer_file_args:\n\t\t\t\t\tprint(\"WARNING: %s option is deprecated.\" % arg)\n\t\t\t\t\tprint(\"WARNING: The new option is -%s.\" % answer_file_args[arg])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\t# Deprecated arguments\n\tprint(\"This tool is deprecated, please use 'qtest --help' for the latest command line options\")\n\tdeprecated_args = ['testcase', 'testrun', 'testcase_file', 'testrun_file', 'qtest_type', 'testcase_type', 'testrun_type', 'qtest_type', 'testcase_type', 'testrun_type', 'testrun', 'testrun_file', 'tc_id', 'tc_name', 'tc_type', 'tc_file', 'tc_file_type', 'tc_file_path', 'tc_file_name', 'testrun_name', 'testrun_type', 'plan_file', 'plan_file_type', 'plan_file_path', 'plan_file_name', 'plan_file_type', 'test_plan', 'test_plan_type', 'test_plan_path', 'test_plan_name', 'test_plan_file_type', 'test_plan_file_path', 'test_plan_file_name', 'test_plan_file_type', 'test_plan_file_path', 'test_plan_file_name', 'test_plan_file_type', 'test_plan_file_path', 'test_plan_file_name', 'test_plan_file_type']\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tdeprecated_args = []\n\n\t\tdeprecated_args += self.check_deprecated_args(cli_args, answer_file_args)\n\t\tdeprecated_args += self.check_deprecated_args(answer_file_args, cli_args)\n\n\t\tif len(deprecated_args) > 0:\n\t\t\tself.logger.warn(\"The following arguments are deprecated and will be removed in the next release: \" + \",\".join(deprecated_args))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\toptions = []\n\t\ttry:\n\t\t\toptions = self.commands[command_name]['options']\n\t\texcept KeyError:\n\t\t\tpass\n\t\treturn options\n/src/libvirt_cli/commands/main.py\nfrom libvirt_cli.commands.base import BaseCommand\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.get_parser_option_specs_by_command_name(command_name)\n\n\tdef get_parser_option_specs_by_command_name(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.parser_option_specs[command_name]\n\n\tdef get_parser_option_specs_by_command_name_and_action(self, command_name, action):\n\t\"\"\"\n\tGets all the options for the specified command and action\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param action: the action to look for\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.get_parser_option_specs_by_command_name(command_name)[action]\n\n\tdef get_parser_option_spec(self, command_name, action, option_name):\n\t\"\"\"\n\tGets the option spec for the specified command, action and option\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param action: the action to look for\n:param option_name", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.options[command_name]\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tcommand_options = []\n\t\tfor action in self.parser._actions:\n\t\t\tif isinstance(action, Option):\n\t\t\t\tif action.dest.startswith(command_name):\n\t\t\t\t\tcommand_options.append(action)\n\t\treturn command_options\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 2011-01-15\n\n@author: \n\"\"\"\n\nimport sys\nimport os\nfrom optparse import OptionParser\n\nfrom utils.options import Options\nfrom utils.cli_parser import CLIParser\nfrom utils.command_factory import CommandFactory\nfrom utils.log import Log\nfrom utils.constants import Constants\nfrom utils.exceptions import *\nfrom utils.version import Version\nfrom utils.version import get_version_from_file\nfrom utils.globals import Globals\n\nfrom commands.main import MainCommand\nfrom commands.virsh import VirshCommand\nfrom commands.ospd import OspdCommand\nfrom commands.spice import SpiceCommand\nfrom commands.ovf import OvfCommand\nfrom commands.ovf_import import OvfImportCommand\nfrom commands.ovf_export import OvfExportCommand\nfrom commands.ovf_export_image import OvfExportImageCommand\nfrom commands.ovf_export_network import OvfExportNetworkCommand\nfrom commands.ovf_export_disk import O", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tassert command_name in self.COMMANDS\n\t\treturn self.COMMANDS[command_name]\n\n\tdef _get_option_from_name(self, command_name, option_name):\n\t\t\"\"\"\n\tGets a single option from a specific command\n\n:param command_name: the name of the command\n:param option_name: the name of the option\n:return: the option object\n\t\"\"\"\n\t\tassert command_name in self.COMMANDS\n\t\tfor option in self.COMMANDS[command_name]:\n\t\t\tif option.name == option_name:\n\t\t\t\treturn option\n\n\tdef get_option_from_name(self, command_name, option_name):\n\t\t\"\"\"\n\tGets a single option from a specific command\n\n:param command_name: the name of the command\n:param option_name: the name of the option\n:return: the option object\n\t\"\"\"\n\t\treturn self._get_option_from_name(command_name, option_name)\n\n\tdef get_option_from_name_list(self, command_name, option_name_list):\n\t\t\"\"\"\n\tGets a list of options from a specific command\n\n:param command_name: the name of the command\n:param option_name_list: the list of option names\n:return: the option object list\n\t\"\"\"\n\t", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tif command_name not in self.commands:\n\t\t\traise ValueError(\"unknown command: %s\" % command_name)\n\n\t\treturn self.commands[command_name][\"options\"]\n\n\tdef get_parser_command_specs(self):\n\t\"\"\"\n\tGets all the commands\n\n:return: the list of all commands\n\t\"\"\"\n\t\treturn self.commands.keys()\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.parser.get_command(command_name).option_list\n\n\tdef get_parser_option_specs_by_option_name(self, command_name, option_name):\n\t\"\"\"\n\tGets all the options for the specified command and option\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param option_name: the option name\n:return: the list of all command options\n\t\"\"\"\n\t\toption_specs = self.get_parser_option_specs(command_name)\n\t\toption_spec_list = []\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.dest == option_name:\n\t\t\t\toption_spec_list.append(option_spec)\n\t\treturn option_spec_list\n\n\tdef get_parser_option_spec_by_option_name(self, command_name, option_name):\n\t\"\"\"\n\tGets the option spec for the specified command and option\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param option_name: the option name\n:return: the command option spec\n\t\"\"\"\n\t\toption_spec_list = self.get_parser_option_specs_by_option_name(command_name, option_name)\n\t\tif len", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.options_list.get(command_name)\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tcommand = self.get_command(command_name)\n\t\treturn command.get_option_specs()\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.command_options.get(command_name)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\toptions = []\n\t\ttry:\n\t\t\toptions = self.commands[command_name]['options']\n\t\texcept KeyError:\n\t\t\tpass\n\t\treturn options\n/src/libvirt_cli/commands/main.py\nfrom libvirt_cli.commands.base import BaseCommand\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.get_parser_option_specs_by_command_name(command_name)\n\n\tdef get_parser_option_specs_by_command_name(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.parser_option_specs[command_name]\n\n\tdef get_parser_option_specs_by_command_name_and_action(self, command_name, action):\n\t\"\"\"\n\tGets all the options for the specified command and action\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param action: the action to look for\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.get_parser_option_specs_by_command_name(command_name)[action]\n\n\tdef get_parser_option_spec(self, command_name, action, option_name):\n\t\"\"\"\n\tGets the option spec for the specified command, action and option\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param action: the action to look for\n:param option_name", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.options[command_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tcommand_options = []\n\t\tfor action in self.parser._actions:\n\t\t\tif isinstance(action, Option):\n\t\t\t\tif action.dest.startswith(command_name):\n\t\t\t\t\tcommand_options.append(action)\n\t\treturn command_options\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 2011-01-15\n\n@author: \n\"\"\"\n\nimport sys\nimport os\nfrom optparse import OptionParser\n\nfrom utils.options import Options\nfrom utils.cli_parser import CLIParser\nfrom utils.command_factory import CommandFactory\nfrom utils.log import Log\nfrom utils.constants import Constants\nfrom utils.exceptions import *\nfrom utils.version import Version\nfrom utils.version import get_version_from_file\nfrom utils.globals import Globals\n\nfrom commands.main import MainCommand\nfrom commands.virsh import VirshCommand\nfrom commands.ospd import OspdCommand\nfrom commands.spice import SpiceCommand\nfrom commands.ovf import OvfCommand\nfrom commands.ovf_import import OvfImportCommand\nfrom commands.ovf_export import OvfExportCommand\nfrom commands.ovf_export_image import OvfExportImageCommand\nfrom commands.ovf_export_network import OvfExportNetworkCommand\nfrom commands.ovf_export_disk import O", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tassert command_name in self.COMMANDS\n\t\treturn self.COMMANDS[command_name]\n\n\tdef _get_option_from_name(self, command_name, option_name):\n\t\t\"\"\"\n\tGets a single option from a specific command\n\n:param command_name: the name of the command\n:param option_name: the name of the option\n:return: the option object\n\t\"\"\"\n\t\tassert command_name in self.COMMANDS\n\t\tfor option in self.COMMANDS[command_name]:\n\t\t\tif option.name == option_name:\n\t\t\t\treturn option\n\n\tdef get_option_from_name(self, command_name, option_name):\n\t\t\"\"\"\n\tGets a single option from a specific command\n\n:param command_name: the name of the command\n:param option_name: the name of the option\n:return: the option object\n\t\"\"\"\n\t\treturn self._get_option_from_name(command_name, option_name)\n\n\tdef get_option_from_name_list(self, command_name, option_name_list):\n\t\t\"\"\"\n\tGets a list of options from a specific command\n\n:param command_name: the name of the command\n:param option_name_list: the list of option names\n:return: the option object list\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tif command_name not in self.commands:\n\t\t\traise ValueError(\"unknown command: %s\" % command_name)\n\n\t\treturn self.commands[command_name][\"options\"]\n\n\tdef get_parser_command_specs(self):\n\t\"\"\"\n\tGets all the commands\n\n:return: the list of all commands\n\t\"\"\"\n\t\treturn self.commands.keys()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.parser.get_command(command_name).option_list\n\n\tdef get_parser_option_specs_by_option_name(self, command_name, option_name):\n\t\"\"\"\n\tGets all the options for the specified command and option\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param option_name: the option name\n:return: the list of all command options\n\t\"\"\"\n\t\toption_specs = self.get_parser_option_specs(command_name)\n\t\toption_spec_list = []\n\t\tfor option_spec in option_specs:\n\t\t\tif option_spec.dest == option_name:\n\t\t\t\toption_spec_list.append(option_spec)\n\t\treturn option_spec_list\n\n\tdef get_parser_option_spec_by_option_name(self, command_name, option_name):\n\t\"\"\"\n\tGets the option spec for the specified command and option\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:param option_name: the option name\n:return: the command option spec\n\t\"\"\"\n\t\toption_spec_list = self.get_parser_option_specs_by_option_name(command_name, option_name)\n\t\tif len", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.options_list.get(command_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\tcommand = self.get_command(command_name)\n\t\treturn command.get_option_specs()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\t\treturn self.command_options.get(command_name)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n\n\n\tdef get_parser_option_specs(self):\n\t\"\"\"\n\tGets all the option specifications for the parser as a list of tuples.\n\t\"\"\"\n\t\n\n\n\tdef get_option_values(self):\n\t\"\"\"\n\tGets all the option values for the parser as a dictionary of option name and option value.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_default(self, option_name, default_value=None):\n\t\"\"\"\n\tGets the option value for the specified option name. If the option is not specified, return the default value.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_raise(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name. If the option is not specified, raises a ValueError.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_error(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name. If the option is not specified, raises a ValueError.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_raise_error(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name. If the", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\tparser_option_specs = self.get_parser_option_specs()\n\t\tfor spec in parser_option_specs:\n\t\t\tif spec[CONFIG_COMMAND_NAME] == command_name and spec[CONFIG_OPTION_NAME] == argument_name:\n\t\t\t\treturn spec\n\t\treturn None\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        #print \"get_option_spec\", command_name, argument_name, self._option_spec\n\treturn self._option_spec[command_name][argument_name]\n/src/python/p2/lib/p2/p2_log.py\n\"\"\"\n\tThis file contains the P2Logger class.\n\"\"\"\n\nfrom p2.lib.p2_logging import P2Logging\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\tspec = self.get_parser_option_specs(command_name).get(argument_name)\n\t\treturn spec\n\tdef get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the specification for the specified options.\n\t\"\"\"\n\t\treturn self.parser_option_specs.get(command_name)\n\tdef get_command_option_specs(self, command_name):\n\t\"\"\"\n\tGets the specification for the specified options of the given command.\n\t\"\"\"\n\t\treturn self.command_option_specs.get(command_name)\n\tdef get_options_specs(self):\n\t\"\"\"\n\tGets the specification for all options in all commands.\n\t\"\"\"\n\t\treturn self.options_specs\n\tdef get_parser_specs(self):\n\t\"\"\"\n\tGets the specification for all commands.\n\t\"\"\"\n\t\treturn self.parser_specs\n\tdef get_option_spec_of_command_option(self, command_name, option_name):\n\t\"\"\"\n\tGets the specification for the specified option_name of the command_name.\n\t\"\"\"\n\t\tspec = self.get_parser_option_specs(command_name).get(option_name)\n\t\treturn spec\n\tdef get_option_spec_of_option(self, option_name):\n\t\"\"\"", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        for spec in self.get_parser_option_specs(command_name):\n            if spec.get_name() == argument_name:\n                return spec\n        return None\t\n\n    def get_command_spec(self, command_name):\n\t\"\"\"\n\tGets the specification for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name]\n\n    def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the list of option specifications for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name].get_option_specs()\n\n    def get_parser_option_values(self, command_name):\n\t\"\"\"\n\tGets the list of option values for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name].get_option_values()\n\n    def get_parser_option_names(self, command_name):\n\t\"\"\"\n\tGets the list of option names for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name].get_option_names()\n\n    def get_parser_option_values_by_name(self, command_name, option_name):\n\t\"\"\"\n\tGets the list of option values for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        argument_name = argument_name.lower()\n        command_name = command_name.lower()\n\n        # Check that the command name and the argument name are valid\n        if (not self.options_tree.has_key(command_name)):\n            raise exceptions.CommandLineException(\"Unknown command: %s\" % command_name)\n        if (not self.options_tree[command_name].has_key(argument_name)):\n            raise exceptions.CommandLineException(\"Unknown argument: %s\" % argument_name)\n\n        return self.options_tree[command_name][argument_name]\n\n    # ------------------------------------------------------\n    # Get the specification of all the options\n    def get_parser_option_specs(self):\n        \"\"\"\n        Return the specifications of all the options for this command line.\n        \"\"\"\n        return self.options_tree\n\n    # ------------------------------------------------------\n    # Get the option names for the given command\n    def get_parser_option_names(self, command_name):\n        \"\"\"\n        Return the names of all the options for this command line.\n        \"\"\"\n        if (not self.options_tree.has_key(command_name)):\n            raise exceptions.CommandLineException(\"Unknown command: %s\" % command_name)\n\n        return self.options_tree[command_name].keys()\n\n    # ------------------------------------------------------\n    # Get the option names for the given command\n    def get_parser_option_names_case_ins", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        for option_spec in self.parser.option_list:\n            if option_spec.dest == self.parser._option_string_actions[command_name].dest:\n                for sub_option_spec in option_spec.option_list:\n                    if sub_option_spec.dest == self.parser._option_string_actions[argument_name].dest:\n                        return sub_option_spec\n        return None\n\n    def get_parser_option_specs(self):\n\t\"\"\"\n\tGets the list of option_spec for the current parser.\n\t\"\"\"\n\treturn self.parser.option_list\n\n    def get_parser_argument_specs(self):\n\t\"\"\"\n\tGets the list of argument_spec for the current parser.\n\t\"\"\"\n\treturn self.parser.argument_list\n\n\n    ###########################################################################\n    #\n    # Command Execution\n    #\n    ###########################################################################\n\n\n    def execute_command(self, command_name, arguments=None):\n\t\"\"\"\n\tExecutes the given command, using the arguments provided.\n\t\"\"\"\n\tcommand_spec = self.get_command_spec(command_name)\n\tif command_spec is None:\n\t    raise Exception(\"Invalid command: %s\" % command_name)\n\n        for option_spec in self.parser.option_list:\n            if option_spec.dest == self.parser._option_string_actions[command", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\t# get the option list\n\tparser_option_specs = self.get_parser_option_specs()\n\t\n\t# get the option by name\n\toption_spec = parser_option_specs.get_option_spec(command_name, argument_name)\n\treturn option_spec\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        parser_option_spec = self.get_parser_option_specs(command_name)\n\tif parser_option_spec:\n\t    option_spec = parser_option_spec.get(argument_name)\n\t    if option_spec:\n\t\treturn option_spec\n\t    else:\n\t\treturn None\n\telse:\n\t    return None\n\n    def set_option_spec(self, command_name, argument_name, option_spec):\n\t\"\"\"\n\tSets the option specification for the specified command and argument name.\n\t\"\"\"\n\t\n        parser_option_spec = self.get_parser_option_specs(command_name)\n\tif not parser_option_spec:\n\t    parser_option_spec = {}\n\t    self.parser_option_specs[command_name] = parser_option_spec\n\t\n\tparser_option_spec[argument_name] = option_spec\n\n    def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the option specification for the specified command name.\n\t\"\"\"\n\t\n        return self.parser_option_specs.get(command_name)\n\n    def get_option_value(self, command_name, argument_name):\n\t\"\"\"\n\tGets the value of the specified option. Get options of the given command_name with self.get_parser_option_values()\n\t\"\"\"\n\t\n        parser_option_", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\toption_spec = []\n\t\tfor spec in self.get_parser_option_specs():\n\t\t\tif spec[\"command_name\"]!= command_name:\n\t\t\t\tcontinue\n\t\t\tif spec[\"argument_name\"]!= argument_name:\n\t\t\t\tcontinue\n\t\t\toption_spec.append(spec)\n\t\tif len(option_spec)!= 1:\n\t\t\traise ValueError(\"No option_spec for command_name: {}, argument_name: {} found\".format(command_name, argument_name))\n\t\treturn option_spec[0]\n\n\tdef get_option_value(self, command_name, argument_name):\n\t\"\"\"\n\tGet the value of the specified option.\n\t\"\"\"\n\t\toption_spec = self.get_option_spec(command_name, argument_name)\n\t\treturn option_spec[\"value\"]\n\n\tdef get_option_type(self, command_name, argument_name):\n\t\"\"\"\n\tGet the type of the specified option.\n\t\"\"\"\n\t\toption_spec = self.get_option_spec(command_name, argument_name)\n\t\treturn option_spec[\"type\"]\n\n\tdef get_option_default(self, command_name, argument_name):\n\t\"\"\"\n\tGet the default value of the specified option.\n\t\"\"\"\n\t\toption_spec = self.get_option_spec"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n\n\n\tdef get_parser_option_specs(self):\n\t\"\"\"\n\tGets all the option specifications for the parser as a list of tuples.\n\t\"\"\"\n\t\n\n\n\tdef get_option_values(self):\n\t\"\"\"\n\tGets all the option values for the parser as a dictionary of option name and option value.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_default(self, option_name, default_value=None):\n\t\"\"\"\n\tGets the option value for the specified option name. If the option is not specified, return the default value.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_raise(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name. If the option is not specified, raises a ValueError.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_error(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name. If the option is not specified, raises a ValueError.\n\t\"\"\"\n\t\n\n\n\tdef get_option_value_or_raise_error(self, option_name):\n\t\"\"\"\n\tGets the option value for the specified option name. If the", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\tparser_option_specs = self.get_parser_option_specs()\n\t\tfor spec in parser_option_specs:\n\t\t\tif spec[CONFIG_COMMAND_NAME] == command_name and spec[CONFIG_OPTION_NAME] == argument_name:\n\t\t\t\treturn spec\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        #print \"get_option_spec\", command_name, argument_name, self._option_spec\n\treturn self._option_spec[command_name][argument_name]\n/src/python/p2/lib/p2/p2_log.py\n\"\"\"\n\tThis file contains the P2Logger class.\n\"\"\"\n\nfrom p2.lib.p2_logging import P2Logging\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\tspec = self.get_parser_option_specs(command_name).get(argument_name)\n\t\treturn spec\n\tdef get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the specification for the specified options.\n\t\"\"\"\n\t\treturn self.parser_option_specs.get(command_name)\n\tdef get_command_option_specs(self, command_name):\n\t\"\"\"\n\tGets the specification for the specified options of the given command.\n\t\"\"\"\n\t\treturn self.command_option_specs.get(command_name)\n\tdef get_options_specs(self):\n\t\"\"\"\n\tGets the specification for all options in all commands.\n\t\"\"\"\n\t\treturn self.options_specs\n\tdef get_parser_specs(self):\n\t\"\"\"\n\tGets the specification for all commands.\n\t\"\"\"\n\t\treturn self.parser_specs\n\tdef get_option_spec_of_command_option(self, command_name, option_name):\n\t\"\"\"\n\tGets the specification for the specified option_name of the command_name.\n\t\"\"\"\n\t\tspec = self.get_parser_option_specs(command_name).get(option_name)\n\t\treturn spec\n\tdef get_option_spec_of_option(self, option_name):\n\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        for spec in self.get_parser_option_specs(command_name):\n            if spec.get_name() == argument_name:\n                return spec\n        return None\t\n\n    def get_command_spec(self, command_name):\n\t\"\"\"\n\tGets the specification for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name]\n\n    def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the list of option specifications for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name].get_option_specs()\n\n    def get_parser_option_values(self, command_name):\n\t\"\"\"\n\tGets the list of option values for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name].get_option_values()\n\n    def get_parser_option_names(self, command_name):\n\t\"\"\"\n\tGets the list of option names for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[command_name].get_option_names()\n\n    def get_parser_option_values_by_name(self, command_name, option_name):\n\t\"\"\"\n\tGets the list of option values for the specified command name.\n\t\"\"\"\n\treturn self._command_specs[", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        argument_name = argument_name.lower()\n        command_name = command_name.lower()\n\n        # Check that the command name and the argument name are valid\n        if (not self.options_tree.has_key(command_name)):\n            raise exceptions.CommandLineException(\"Unknown command: %s\" % command_name)\n        if (not self.options_tree[command_name].has_key(argument_name)):\n            raise exceptions.CommandLineException(\"Unknown argument: %s\" % argument_name)\n\n        return self.options_tree[command_name][argument_name]\n\n    # ------------------------------------------------------\n    # Get the specification of all the options\n    def get_parser_option_specs(self):\n        \"\"\"\n        Return the specifications of all the options for this command line.\n        \"\"\"\n        return self.options_tree\n\n    # ------------------------------------------------------\n    # Get the option names for the given command\n    def get_parser_option_names(self, command_name):\n        \"\"\"\n        Return the names of all the options for this command line.\n        \"\"\"\n        if (not self.options_tree.has_key(command_name)):\n            raise exceptions.CommandLineException(\"Unknown command: %s\" % command_name)\n\n        return self.options_tree[command_name].keys()\n\n    # ------------------------------------------------------\n    # Get the option names for the given command\n    def get_parser_option_names_case_ins", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        for option_spec in self.parser.option_list:\n            if option_spec.dest == self.parser._option_string_actions[command_name].dest:\n                for sub_option_spec in option_spec.option_list:\n                    if sub_option_spec.dest == self.parser._option_string_actions[argument_name].dest:\n                        return sub_option_spec\n        return None\n\n    def get_parser_option_specs(self):\n\t\"\"\"\n\tGets the list of option_spec for the current parser.\n\t\"\"\"\n\treturn self.parser.option_list\n\n    def get_parser_argument_specs(self):\n\t\"\"\"\n\tGets the list of argument_spec for the current parser.\n\t\"\"\"\n\treturn self.parser.argument_list\n\n\n    ###########################################################################\n    #\n    # Command Execution\n    #\n    ###########################################################################\n\n\n    def execute_command(self, command_name, arguments=None):\n\t\"\"\"\n\tExecutes the given command, using the arguments provided.\n\t\"\"\"\n\tcommand_spec = self.get_command_spec(command_name)\n\tif command_spec is None:\n\t    raise Exception(\"Invalid command: %s\" % command_name)\n\n        for option_spec in self.parser.option_list:\n            if option_spec.dest == self.parser._option_string_actions[command", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\t# get the option list\n\tparser_option_specs = self.get_parser_option_specs()\n\t\n\t# get the option by name\n\toption_spec = parser_option_specs.get_option_spec(command_name, argument_name)\n\treturn option_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\n        parser_option_spec = self.get_parser_option_specs(command_name)\n\tif parser_option_spec:\n\t    option_spec = parser_option_spec.get(argument_name)\n\t    if option_spec:\n\t\treturn option_spec\n\t    else:\n\t\treturn None\n\telse:\n\t    return None\n\n    def set_option_spec(self, command_name, argument_name, option_spec):\n\t\"\"\"\n\tSets the option specification for the specified command and argument name.\n\t\"\"\"\n\t\n        parser_option_spec = self.get_parser_option_specs(command_name)\n\tif not parser_option_spec:\n\t    parser_option_spec = {}\n\t    self.parser_option_specs[command_name] = parser_option_spec\n\t\n\tparser_option_spec[argument_name] = option_spec\n\n    def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets the option specification for the specified command name.\n\t\"\"\"\n\t\n        return self.parser_option_specs.get(command_name)\n\n    def get_option_value(self, command_name, argument_name):\n\t\"\"\"\n\tGets the value of the specified option. Get options of the given command_name with self.get_parser_option_values()\n\t\"\"\"\n\t\n        parser_option_", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\t\toption_spec = []\n\t\tfor spec in self.get_parser_option_specs():\n\t\t\tif spec[\"command_name\"]!= command_name:\n\t\t\t\tcontinue\n\t\t\tif spec[\"argument_name\"]!= argument_name:\n\t\t\t\tcontinue\n\t\t\toption_spec.append(spec)\n\t\tif len(option_spec)!= 1:\n\t\t\traise ValueError(\"No option_spec for command_name: {}, argument_name: {} found\".format(command_name, argument_name))\n\t\treturn option_spec[0]\n\n\tdef get_option_value(self, command_name, argument_name):\n\t\"\"\"\n\tGet the value of the specified option.\n\t\"\"\"\n\t\toption_spec = self.get_option_spec(command_name, argument_name)\n\t\treturn option_spec[\"value\"]\n\n\tdef get_option_type(self, command_name, argument_name):\n\t\"\"\"\n\tGet the type of the specified option.\n\t\"\"\"\n\t\toption_spec = self.get_option_spec(command_name, argument_name)\n\t\treturn option_spec[\"type\"]\n\n\tdef get_option_default(self, command_name, argument_name):\n\t\"\"\"\n\tGet the default value of the specified option.\n\t\"\"\"\n\t\toption_spec = self.get_option_spec", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn self.silent_args\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if arg.startswith('--')]\n\t@property\n\tdef silent_args(self):\n\t\"\"\"\n\tlist of silenced argument\n\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn self.get_silent_args(self.args)\n\tdef get_subcommand(self):\n\t\"\"\"\n\tget subcommand name\n\n:return: string, subcommand name\n\t\"\"\"\n\t\treturn self.args[0]\n\t@property\n\tdef subcommand(self):\n\t\"\"\"\n\tget subcommand name\n\n:return: string, subcommand name\n\t\"\"\"\n\t\treturn self.get_subcommand()\n\tdef get_arguments(self):\n\t\"\"\"\n\tget arguments\n\n:return: list, arguments\n\t\"\"\"\n\t\treturn self.args[1:]\n\t@property\n\tdef arguments(self):\n\t\"\"\"\n\tget arguments\n\n:return: list, arguments\n\t\"\"\"\n\t\treturn self.get_arguments()\n\tdef get_argument(self, index):\n\t\"\"\"\n\tGet argument by index\n\n:param index: int, argument index\n:return: string, argument\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self.args[index]\n\t\texcept IndexError:\n\t\t\treturn None\n\tdef get_argument_by_name(self", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if arg[0] == '-']\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [i.strip() for i in args.strip().split(',')]\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn args.split(' ')\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if self.is_silent(arg)]\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn self.silent_args\n\tdef parse(self, args):\n\t\t\"\"\"\n\t\tParse the arguments\n\n:param args: The received arguments.\n:return: The parsed arguments.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='This is a test')\n\t\tparser.add_argument('--silent', action='store_true', help='Silence warnings.')\n\t\tparser.add_argument('--silent_args', nargs='+', help='Silence warnings.')\n\t\tparser.add_argument('filename', type=argparse.FileType('r'), help='The input file.')\n\t\tparser.add_argument('number', type=int, help='The number to square.')\n\t\tparser.add_argument('number', type=int, help='The number to square.')\n\t\tparser.add_argument('number', type=int, help='The number to square.')\n\t\tself.silent_args = parser.parse_args(args).silent_args\n\t\treturn parser.parse_args(args)\n\tdef __call__(self, args):\n\t\t\"\"\"\n\n:param args: The received arguments.\n:return: The parsed arguments.\n\t\"\"\"\n\t\treturn self.parse(args)\n/pypi_install_script/pyknot-0.1.tar/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n   ", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [\n\t\t\targ for arg in args\n\t\t\tif arg.startswith('--') and arg[2:] in self.silent_args\n\t\t]\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if arg.startswith(\"--silent\")]\n/modules/system_modules/sudo.py\nimport subprocess\nimport os\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\tsilent_args = []\n\t\treturn silent_args\n/src/python/lib/utils/utils.py\nimport os\nimport sysconfig as _sysconfig\n\n#\n# This is a list of all the python modules we know about.\n# It is mainly used to build the path to the modules,\n# so python can import them.\n#\nPYTHON_MODULES = [\n\t'pykka',\n\t'pykka.actor',\n\t'pykka.actor_ref',\n\t'pykka.asyncio',\n\t'pykka.mailbox',\n\t'pykka.registry',\n\t'pykka.setup',\n\t'pykka.utils',\n\t'pykka.weakrefset',\n\t'pykka.compat',\n\t'pykka.version',\n]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn self.silent_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if arg.startswith('--')]\n\t@property\n\tdef silent_args(self):\n\t\"\"\"\n\tlist of silenced argument\n\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn self.get_silent_args(self.args)\n\tdef get_subcommand(self):\n\t\"\"\"\n\tget subcommand name\n\n:return: string, subcommand name\n\t\"\"\"\n\t\treturn self.args[0]\n\t@property\n\tdef subcommand(self):\n\t\"\"\"\n\tget subcommand name\n\n:return: string, subcommand name\n\t\"\"\"\n\t\treturn self.get_subcommand()\n\tdef get_arguments(self):\n\t\"\"\"\n\tget arguments\n\n:return: list, arguments\n\t\"\"\"\n\t\treturn self.args[1:]\n\t@property\n\tdef arguments(self):\n\t\"\"\"\n\tget arguments\n\n:return: list, arguments\n\t\"\"\"\n\t\treturn self.get_arguments()\n\tdef get_argument(self, index):\n\t\"\"\"\n\tGet argument by index\n\n:param index: int, argument index\n:return: string, argument\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self.args[index]\n\t\texcept IndexError:\n\t\t\treturn None\n\tdef get_argument_by_name(self", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if arg[0] == '-']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [i.strip() for i in args.strip().split(',')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn args.split(' ')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if self.is_silent(arg)]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn self.silent_args\n\tdef parse(self, args):\n\t\t\"\"\"\n\t\tParse the arguments\n\n:param args: The received arguments.\n:return: The parsed arguments.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='This is a test')\n\t\tparser.add_argument('--silent', action='store_true', help='Silence warnings.')\n\t\tparser.add_argument('--silent_args', nargs='+', help='Silence warnings.')\n\t\tparser.add_argument('filename', type=argparse.FileType('r'), help='The input file.')\n\t\tparser.add_argument('number', type=int, help='The number to square.')\n\t\tparser.add_argument('number', type=int, help='The number to square.')\n\t\tparser.add_argument('number', type=int, help='The number to square.')\n\t\tself.silent_args = parser.parse_args(args).silent_args\n\t\treturn parser.parse_args(args)\n\tdef __call__(self, args):\n\t\t\"\"\"\n\n:param args: The received arguments.\n:return: The parsed arguments.\n\t\"\"\"\n\t\treturn self.parse(args)\n/pypi_install_script/pyknot-0.1.tar/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n   ", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [\n\t\t\targ for arg in args\n\t\t\tif arg.startswith('--') and arg[2:] in self.silent_args\n\t\t]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\treturn [arg for arg in args if arg.startswith(\"--silent\")]\n/modules/system_modules/sudo.py\nimport subprocess\nimport os\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\t\tsilent_args = []\n\t\treturn silent_args\n/src/python/lib/utils/utils.py\nimport os\nimport sysconfig as _sysconfig\n\n#\n# This is a list of all the python modules we know about.\n# It is mainly used to build the path to the modules,\n# so python can import them.\n#\nPYTHON_MODULES = [\n\t'pykka',\n\t'pykka.actor',\n\t'pykka.actor_ref',\n\t'pykka.asyncio',\n\t'pykka.mailbox',\n\t'pykka.registry',\n\t'pykka.setup',\n\t'pykka.utils',\n\t'pykka.weakrefset',\n\t'pykka.compat',\n\t'pykka.version',\n]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t \n\tfor i in self.required_args:\n\t\tif i not in args:\n\t\t\tprint \"\\nERROR: the required argument '%s' has not been provided.\" % (i)\n\t\t\tprint \"Try running the following command:\"\n\t\t\tprint \"python %s -h\" % (self.name)\n\t\t\tsys.exit()\n\t\n\treturn\n\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.requires_args:\n\t\t\tif arg not in args:\n\t\t\t\treturn False\n\t\t\n\t\treturn True\n\t\t\n\tdef validate_requires_not_args(self, args):\n\t\"\"\"\n\tCheck if all the not required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.requires_not_args:\n\t\t\tif arg in args:\n\t\t\t\treturn False\n\t\t\n\t\treturn True\n\t\t", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tself.validate_requires_args_requires_args(args)\n\n\t\tself.validate_requires_args_requires_args_1(args)\n\n\texcept Exception as e:\n\t\traise ValidationError(e)\n\n\treturn True\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor req in self.requires_args:\n\t\t\tif not args.has_key(req):\n\t\t\t\traise Exception('argument %s is required.' % req)\n\n\tdef parse_args(self, args):\n\t\t\"\"\"\n\t\tParse the arguments given to the command.\n\t\t\"\"\"\n\t\tself.validate_requires_args(args)\n\t\tself.args = args\n\n\tdef usage(self):\n\t\t\"\"\"\n\t\tPrint the usage for this command.\n\t\t\"\"\"\n\t\tprint self.help_msg\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.required_args:\n\t\t\tif arg not in args:\n\t\t\t\treturn False\n\t\treturn True\n\tdef parse_args(self, args):\n\t\"\"\"\n\tParse the arguments and return a dict of the arguments.\n\t\"\"\"\n\t\tret = {}\n\t\tfor key, value in args.items():\n\t\t\tif key in self.args:\n\t\t\t\tret[key] = value\n\t\treturn ret\n\tdef run(self, args):\n\t\"\"\"\n\tRun the command.\n\t\"\"\"\n\t\tif self.validate_requires_args(args):\n\t\t\tparsed_args = self.parse_args(args)\n\t\t\treturn self.func(**parsed_args)\n\t\telse:\n\t\t\treturn False\nThis is the example of the command class:\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\n\t# Check if all the required arguments have been provided.\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tmsg = \"Missing required argument: %s\" % arg\n\t\t\traise ValueError(msg)\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n        if not args:\n\t\treturn False\n\tfor arg in self.requires_args:\n\t\tif not args.get(arg):\n\t\t\treturn False\n\treturn True\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tself.args = args\n\t\tif not all(k in self.args for k in self.required_args):\n\t\t\traise MissingRequiredArguments(\"Missing required arguments: {}\".format(self.required_args))\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.required_args:\n\t\t\tif args.get(arg) is None:\n\t\t\t\treturn False\n\treturn True\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t \n\tfor i in self.required_args:\n\t\tif i not in args:\n\t\t\tprint \"\\nERROR: the required argument '%s' has not been provided.\" % (i)\n\t\t\tprint \"Try running the following command:\"\n\t\t\tprint \"python %s -h\" % (self.name)\n\t\t\tsys.exit()\n\t\n\treturn\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.requires_args:\n\t\t\tif arg not in args:\n\t\t\t\treturn False\n\t\t\n\t\treturn True\n\t\t\n\tdef validate_requires_not_args(self, args):\n\t\"\"\"\n\tCheck if all the not required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.requires_not_args:\n\t\t\tif arg in args:\n\t\t\t\treturn False\n\t\t\n\t\treturn True\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tself.validate_requires_args_requires_args(args)\n\n\t\tself.validate_requires_args_requires_args_1(args)\n\n\texcept Exception as e:\n\t\traise ValidationError(e)\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor req in self.requires_args:\n\t\t\tif not args.has_key(req):\n\t\t\t\traise Exception('argument %s is required.' % req)\n\n\tdef parse_args(self, args):\n\t\t\"\"\"\n\t\tParse the arguments given to the command.\n\t\t\"\"\"\n\t\tself.validate_requires_args(args)\n\t\tself.args = args\n\n\tdef usage(self):\n\t\t\"\"\"\n\t\tPrint the usage for this command.\n\t\t\"\"\"\n\t\tprint self.help_msg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.required_args:\n\t\t\tif arg not in args:\n\t\t\t\treturn False\n\t\treturn True\n\tdef parse_args(self, args):\n\t\"\"\"\n\tParse the arguments and return a dict of the arguments.\n\t\"\"\"\n\t\tret = {}\n\t\tfor key, value in args.items():\n\t\t\tif key in self.args:\n\t\t\t\tret[key] = value\n\t\treturn ret\n\tdef run(self, args):\n\t\"\"\"\n\tRun the command.\n\t\"\"\"\n\t\tif self.validate_requires_args(args):\n\t\t\tparsed_args = self.parse_args(args)\n\t\t\treturn self.func(**parsed_args)\n\t\telse:\n\t\t\treturn False\nThis is the example of the command class:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\n\t# Check if all the required arguments have been provided.\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tmsg = \"Missing required argument: %s\" % arg\n\t\t\traise ValueError(msg)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n        if not args:\n\t\treturn False\n\tfor arg in self.requires_args:\n\t\tif not args.get(arg):\n\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tself.args = args\n\t\tif not all(k in self.args for k in self.required_args):\n\t\t\traise MissingRequiredArguments(\"Missing required arguments: {}\".format(self.required_args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self.required_args:\n\t\t\tif args.get(arg) is None:\n\t\t\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\tfor arg in args:\n\t\ttry:\n\t\t\toptions_spec[arg]\n\t\t\t# If the argument is not in the options_spec it is\n\t\t\t# probably an option.\n\t\t\tcontinue\n\t\texcept KeyError:\n\t\t\tpass\n\t\tfor option in options_spec:\n\t\t\tfor condition in options_spec[option].get('required_when', []):\n\t\t\t\tif condition[0] == command_name and condition[1] == arg:\n\t\t\t\t\trequired_args.append(arg)\n\t\t\t\t\tbreak\n\treturn required_args\n```\n\n```python", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_when = []\n\t\tfor opt_spec in options_spec:\n\t\t\tif (opt_spec.get('required_when', None) and\n\t\t\t    opt_spec.get('required_when') == command_name):\n\t\t\t\trequired_when.append(opt_spec['name'])\n\t\treturn required_when\n\n    def _get_required_args(self, args):\n        \"\"\"\n        List arguments with ``required`` condition matched.\n\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required``\n            condition\n        \"\"\"\n        required = []\n        for arg in args:\n            if arg.required:\n                required.append(arg.name)\n        return required\n\n    def _get_optional_args(self, args):\n        \"\"\"\n        List optional arguments.\n\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``optional``\n            condition\n        \"\"\"\n        optional = []\n        for arg in args:\n            if arg.optional:\n                optional.append(arg.name)\n        return optional\n\n    def _get_no_args(self, args):\n        \"\"\"\n        List arguments with ``no_args`` condition matched.\n\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``no_args``\n            condition\n        \"\"\"\n        no_args = []\n        for arg in args:\n            if arg.", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_when_args = []\n\t\tfor arg in args:\n\t\t\tspec = _get_option_spec(command_name, options_spec, arg)\n\t\t\tif spec is None:\n\t\t\t\tcontinue\n\t\t\tif spec.get('required_when'):\n\t\t\t\tfor required_when in spec.get('required_when'):\n\t\t\t\t\tif required_when_arg_is_set(required_when, args):\n\t\t\t\t\t\trequired_when_args.append(arg)\n\t\t\t\t\t\tbreak\n\t\treturn required_when_args\n\n:param required_when: the required_when parameter.\n:param args: the received input arguments.\n:return: boolean, ``True`` if required_when is set, ``False`` otherwise.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg.startswith(required_when):\n\t\t\t\treturn True\n\t\treturn False\n\n:param required_when_arg_is_set: the required_when_arg_is_set parameter.\n:param args: the received input arguments.\n:return: boolean, ``True`` if required_when is set, ``False`` otherwise.\n\t\"\"\"\n\t\treturn required_when_arg_is_set\n\n:param required_when: the required_when parameter.\n:param args: the received input arguments.\n:return: boolean, ``True`` if required_when is set,", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor option_spec in options_spec:\n\t\t\tfor condition in option_spec.get('required_when', {}):\n\t\t\t\tif condition[0] == command_name:\n\t\t\t\t\tif self._matches_condition(condition[1], args):\n\t\t\t\t\t\trequired_args.append(option_spec['name'])\n\t\treturn required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_when_args = []\n\t\tfor option_spec in options_spec:\n\t\t\tif option_spec.required_when:\n\t\t\t\tfor condition in option_spec.required_when:\n\t\t\t\t\tif condition.check(command_name, args):\n\t\t\t\t\t\trequired_when_args.append(\n\t\t\t\t\t\t\toption_spec.name)\n\t\treturn required_when_args\n\n\tdef _get_required_args(self, command_name, options_spec, args):\n\t\t\"\"\"\n\t\tList arguments with ``required`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required`` condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor option_spec in options_spec:\n\t\t\tif option_spec.required:\n\t\t\t\trequired_args.append(option_spec.name)\n\t\treturn required_args\n\n\tdef _get_required_args_with_values(self, command_name, options_spec,\n                                       args):\n\t\t\"\"\"\n\t\tList arguments with ``required`` condition matched and have its value.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\tif not self.command_spec.get(command_name):\n\t\t\treturn []\n\t\trequired_args = []\n\t\tfor spec in options_spec:\n\t\t\tfor required_when in spec.get('required_when', []):\n\t\t\t\tif required_when[0] in args and required_when[1]:\n\t\t\t\t\trequired_args.append(spec['arg'])\n\t\treturn required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor arg in args:\n\t\t\tif arg.required_when:\n\t\t\t\tif self._match_condition(arg.required_when, options_spec, args):\n\t\t\t\t\trequired_args.append(arg.name)\n\n\t\treturn required_args\n\n\tdef _match_condition(self, condition, options_spec, args):\n\t\"\"\"\n\tMatch input args with ``required_when`` condition.\n\n:param condition:  the ``required_when`` condition.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: boolean, True if the condition matched.\n\t\"\"\"\n\t\tif not condition:\n\t\t\treturn False\n\n\t\tmatched = False\n\t\tif condition.startswith('not '):\n\t\t\tmatched = not self._match_condition(condition[4:], options_spec, args)\n\t\telse:\n\t\t\t# Match the condition with the options spec\n\t\t\tfor option in options_spec:\n\t\t\t\tif condition == option.name:\n\t\t\t\t\tmatched = True\n\t\t\t\t\tbreak\n\t\t\t\telif condition == option.option:\n\t\t\t\t\tmatched = True\n\t\t\t\t\tbreak\n\n\t\treturn matched\n\n\tdef _get_command_args(self, command_name, options_spec, args):\n\t\"\"\"\n\tList arguments with command name matched.\n\n:param command_name: the command", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\treturn [opt['name'] for opt in options_spec if opt['required_when'] == command_name]\n\n\tdef _get_required_args(self, command_name, options_spec):\n\t\"\"\"\n\tList arguments with ``required`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:return: list, list of argument names with matched ``required``\n    condition\n\t\"\"\"\n\t\treturn [opt['name'] for opt in options_spec if opt['required']]\n\n\tdef _get_optional_args(self, command_name, options_spec):\n\t\"\"\"\n\tList arguments with ``optional`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:return: list, list of argument names with matched ``optional``\n    condition\n\t\"\"\"\n\t\treturn [opt['name'] for opt in options_spec if opt['optional']]\n\n\tdef _get_args(self, command_name, options_spec, args):\n\t\"\"\"\n\tList arguments with ``args`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor name, spec in options_spec:\n\t\t\tif'required_when' in spec and spec['required_when'](args):\n\t\t\t\trequired_args.append(name)\n\t\treturn required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor name, spec in options_spec.items():\n\t\t\tif spec.get('required_when') and \\\n\t\t\t\t\tspec['required_when'](command_name, args):\n\t\t\t\trequired_args.append(name)\n\t\treturn required_args\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\tfor arg in args:\n\t\ttry:\n\t\t\toptions_spec[arg]\n\t\t\t# If the argument is not in the options_spec it is\n\t\t\t# probably an option.\n\t\t\tcontinue\n\t\texcept KeyError:\n\t\t\tpass\n\t\tfor option in options_spec:\n\t\t\tfor condition in options_spec[option].get('required_when', []):\n\t\t\t\tif condition[0] == command_name and condition[1] == arg:\n\t\t\t\t\trequired_args.append(arg)\n\t\t\t\t\tbreak\n\treturn required_args\n```\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_when = []\n\t\tfor opt_spec in options_spec:\n\t\t\tif (opt_spec.get('required_when', None) and\n\t\t\t    opt_spec.get('required_when') == command_name):\n\t\t\t\trequired_when.append(opt_spec['name'])\n\t\treturn required_when\n\n    def _get_required_args(self, args):\n        \"\"\"\n        List arguments with ``required`` condition matched.\n\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required``\n            condition\n        \"\"\"\n        required = []\n        for arg in args:\n            if arg.required:\n                required.append(arg.name)\n        return required\n\n    def _get_optional_args(self, args):\n        \"\"\"\n        List optional arguments.\n\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``optional``\n            condition\n        \"\"\"\n        optional = []\n        for arg in args:\n            if arg.optional:\n                optional.append(arg.name)\n        return optional\n\n    def _get_no_args(self, args):\n        \"\"\"\n        List arguments with ``no_args`` condition matched.\n\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``no_args``\n            condition\n        \"\"\"\n        no_args = []\n        for arg in args:\n            if arg.", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_when_args = []\n\t\tfor arg in args:\n\t\t\tspec = _get_option_spec(command_name, options_spec, arg)\n\t\t\tif spec is None:\n\t\t\t\tcontinue\n\t\t\tif spec.get('required_when'):\n\t\t\t\tfor required_when in spec.get('required_when'):\n\t\t\t\t\tif required_when_arg_is_set(required_when, args):\n\t\t\t\t\t\trequired_when_args.append(arg)\n\t\t\t\t\t\tbreak\n\t\treturn required_when_args\n\n:param required_when: the required_when parameter.\n:param args: the received input arguments.\n:return: boolean, ``True`` if required_when is set, ``False`` otherwise.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg.startswith(required_when):\n\t\t\t\treturn True\n\t\treturn False\n\n:param required_when_arg_is_set: the required_when_arg_is_set parameter.\n:param args: the received input arguments.\n:return: boolean, ``True`` if required_when is set, ``False`` otherwise.\n\t\"\"\"\n\t\treturn required_when_arg_is_set\n\n:param required_when: the required_when parameter.\n:param args: the received input arguments.\n:return: boolean, ``True`` if required_when is set,", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor option_spec in options_spec:\n\t\t\tfor condition in option_spec.get('required_when', {}):\n\t\t\t\tif condition[0] == command_name:\n\t\t\t\t\tif self._matches_condition(condition[1], args):\n\t\t\t\t\t\trequired_args.append(option_spec['name'])\n\t\treturn required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_when_args = []\n\t\tfor option_spec in options_spec:\n\t\t\tif option_spec.required_when:\n\t\t\t\tfor condition in option_spec.required_when:\n\t\t\t\t\tif condition.check(command_name, args):\n\t\t\t\t\t\trequired_when_args.append(\n\t\t\t\t\t\t\toption_spec.name)\n\t\treturn required_when_args\n\n\tdef _get_required_args(self, command_name, options_spec, args):\n\t\t\"\"\"\n\t\tList arguments with ``required`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required`` condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor option_spec in options_spec:\n\t\t\tif option_spec.required:\n\t\t\t\trequired_args.append(option_spec.name)\n\t\treturn required_args\n\n\tdef _get_required_args_with_values(self, command_name, options_spec,\n                                       args):\n\t\t\"\"\"\n\t\tList arguments with ``required`` condition matched and have its value.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\tif not self.command_spec.get(command_name):\n\t\t\treturn []\n\t\trequired_args = []\n\t\tfor spec in options_spec:\n\t\t\tfor required_when in spec.get('required_when', []):\n\t\t\t\tif required_when[0] in args and required_when[1]:\n\t\t\t\t\trequired_args.append(spec['arg'])\n\t\treturn required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor arg in args:\n\t\t\tif arg.required_when:\n\t\t\t\tif self._match_condition(arg.required_when, options_spec, args):\n\t\t\t\t\trequired_args.append(arg.name)\n\n\t\treturn required_args\n\n\tdef _match_condition(self, condition, options_spec, args):\n\t\"\"\"\n\tMatch input args with ``required_when`` condition.\n\n:param condition:  the ``required_when`` condition.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: boolean, True if the condition matched.\n\t\"\"\"\n\t\tif not condition:\n\t\t\treturn False\n\n\t\tmatched = False\n\t\tif condition.startswith('not '):\n\t\t\tmatched = not self._match_condition(condition[4:], options_spec, args)\n\t\telse:\n\t\t\t# Match the condition with the options spec\n\t\t\tfor option in options_spec:\n\t\t\t\tif condition == option.name:\n\t\t\t\t\tmatched = True\n\t\t\t\t\tbreak\n\t\t\t\telif condition == option.option:\n\t\t\t\t\tmatched = True\n\t\t\t\t\tbreak\n\n\t\treturn matched\n\n\tdef _get_command_args(self, command_name, options_spec, args):\n\t\"\"\"\n\tList arguments with command name matched.\n\n:param command_name: the command", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\treturn [opt['name'] for opt in options_spec if opt['required_when'] == command_name]\n\n\tdef _get_required_args(self, command_name, options_spec):\n\t\"\"\"\n\tList arguments with ``required`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:return: list, list of argument names with matched ``required``\n    condition\n\t\"\"\"\n\t\treturn [opt['name'] for opt in options_spec if opt['required']]\n\n\tdef _get_optional_args(self, command_name, options_spec):\n\t\"\"\"\n\tList arguments with ``optional`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:return: list, list of argument names with matched ``optional``\n    condition\n\t\"\"\"\n\t\treturn [opt['name'] for opt in options_spec if opt['optional']]\n\n\tdef _get_args(self, command_name, options_spec, args):\n\t\"\"\"\n\tList arguments with ``args`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor name, spec in options_spec:\n\t\t\tif'required_when' in spec and spec['required_when'](args):\n\t\t\t\trequired_args.append(name)\n\t\treturn required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\t\trequired_args = []\n\t\tfor name, spec in options_spec.items():\n\t\t\tif spec.get('required_when') and \\\n\t\t\t\t\tspec['required_when'](command_name, args):\n\t\t\t\trequired_args.append(name)\n\t\treturn required_args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\t\treturn False\n\t\treturn True\n\n\tdef validate_length_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs is not longer than length specified. \\n:param kwargs: The received kwargs.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif len(value) > self.length:\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_args_in_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if args values are in kwargs. \\n:param kwargs: The received kwargs.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif key in self.args:\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_kwargs_in_args(self, args):\n\t\"\"\"\n\tCheck if kwargs values are in args. \\n:param args: The received args.\n\t\"\"\"\n\t\tfor key, value in args.items():\n\t\t\tif key in self.kwargs:\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_args_in_kwargs_values(self, kwargs):\n\t\"\"\"\n\tCheck if args values are in kwargs values. \\n:param kwargs: The received kwargs.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif key in self.args and key not in", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\tself.error_message = \"Error: Too long argument!\"\n\t\treturn False\n\treturn True\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args)!= self.len:\n\t\t\traise ValueError(\"Length of the given value is not correct!\")\n\n\tdef validate_args(self, args):\n\t\"\"\"\n\tValidate the given arguments.\n\t\"\"\"\n\t\tself.validate_length_args(args)\n\t\tif not self.validate(args):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\tdef validate_value(self, value):\n\t\"\"\"\n\tValidate the given value.\n\t\"\"\"\n\t\tif not self.validate(value):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\tdef validate_values(self, args):\n\t\"\"\"\n\tValidate the given values.\n\t\"\"\"\n\t\tif not self.validate_type(args):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\tdef validate_type(self, args):\n\t\"\"\"\n\tValidate the given value.\n\t\"\"\"\n\t\tif not isinstance(args, tuple) and not isinstance(args, list):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\t\tfor arg in args:\n\t\t\tif not self.validate(arg):\n\t\t\t\traise ValueError(\"Given value is not correct!\")\n\n\t\treturn True\n\n\tdef validate_tuple(self, args):\n\t\"\"\"\n\tValidate the given tuple.\n\t\"\"\"\n\t\tif not isinstance(args, tuple):\n\t\t\traise ValueError(\"Given value", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\t#if len(args) > self.length:\n\t\t#\tself.error_message = \"Argument length must be less than \" + str(self.length)\n\t\t#\treturn False\n\t\t#return True\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args)!= self.length:\n\t\t\traise Exception('Invalid number of arguments')\n\t\t\treturn False\n\t\treturn True\n/README.md\n# HCI_Museum\n\n## 1. About the project\n\nThe project was conceived during the course of Human Computer Interaction at the University of Trento (Italy) in the academic year 2019/2020. It is a virtual tour of the Museum of Natural History of Trento, a museum that offers a large collection of natural history objects.\n\n## 2. What is the project about?\n\nIn the first part of the project, we had to develop a digital tour of the museum. The tour was developed by using the Unity Game Engine.\n\nThe tour is divided into three main sections. The first section, named \"Introduction\", provides a brief description of the museum and its main attractions. The second section, named \"Natural History\", describes the first part of the natural history collection. The third section, named \"History\", describes the second part of the natural history collection.\n\n## 3. How to install and use the project\n\nThe project consists of 5 different parts:\n\n- Unity project (https://unity3d.com/it)\n- Unity package (https://docs.unity3d.com/Manual/cus", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\t\treturn False\n\t\treturn True\n\t\t", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\tif len(arg) > self.length:\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_length_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs is not longer than length specified. \\n:param kwargs: The received arguments.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\tif len(value) > self.length:\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_regexp_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args matches the regular expression specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\tif not re.match(self.regexp, arg):\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_regexp_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs matches the regular expression specified. \\n:param kwargs: The received arguments.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\tif not re.match(self.regexp, value):\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_type_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args matches the type specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif len(arg) > self.length:\n\t\t\t\treturn False\n\t\treturn True\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.max_length:\n\t\t\traise ValueError('Number of args cannot exceed {}.'.format(self.max_length))\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\t\traise InvalidLengthException(self.name, self.length)\n\n\tdef validate_number_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not shorter than number specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) < self.number:\n\t\t\traise InvalidNumberException(self.name, self.length)\n\n\tdef validate_values_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not one of the given values. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.values:\n\t\t\traise InvalidValuesException(self.name, self.values)\n\n\tdef validate_values_or_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not one of the given values or is longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.values and len(args) > self.length:\n\t\t\traise InvalidValueOrLengthException(self.name, self.values, self.length)\n\n\tdef validate_values_or_number_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not one of the given"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\t\treturn False\n\t\treturn True\n\n\tdef validate_length_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs is not longer than length specified. \\n:param kwargs: The received kwargs.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif len(value) > self.length:\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_args_in_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if args values are in kwargs. \\n:param kwargs: The received kwargs.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif key in self.args:\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_kwargs_in_args(self, args):\n\t\"\"\"\n\tCheck if kwargs values are in args. \\n:param args: The received args.\n\t\"\"\"\n\t\tfor key, value in args.items():\n\t\t\tif key in self.kwargs:\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_args_in_kwargs_values(self, kwargs):\n\t\"\"\"\n\tCheck if args values are in kwargs values. \\n:param kwargs: The received kwargs.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif key in self.args and key not in", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\tself.error_message = \"Error: Too long argument!\"\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args)!= self.len:\n\t\t\traise ValueError(\"Length of the given value is not correct!\")\n\n\tdef validate_args(self, args):\n\t\"\"\"\n\tValidate the given arguments.\n\t\"\"\"\n\t\tself.validate_length_args(args)\n\t\tif not self.validate(args):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\tdef validate_value(self, value):\n\t\"\"\"\n\tValidate the given value.\n\t\"\"\"\n\t\tif not self.validate(value):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\tdef validate_values(self, args):\n\t\"\"\"\n\tValidate the given values.\n\t\"\"\"\n\t\tif not self.validate_type(args):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\tdef validate_type(self, args):\n\t\"\"\"\n\tValidate the given value.\n\t\"\"\"\n\t\tif not isinstance(args, tuple) and not isinstance(args, list):\n\t\t\traise ValueError(\"Given value is not correct!\")\n\n\t\tfor arg in args:\n\t\t\tif not self.validate(arg):\n\t\t\t\traise ValueError(\"Given value is not correct!\")\n\n\t\treturn True\n\n\tdef validate_tuple(self, args):\n\t\"\"\"\n\tValidate the given tuple.\n\t\"\"\"\n\t\tif not isinstance(args, tuple):\n\t\t\traise ValueError(\"Given value", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\t#if len(args) > self.length:\n\t\t#\tself.error_message = \"Argument length must be less than \" + str(self.length)\n\t\t#\treturn False\n\t\t#return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args)!= self.length:\n\t\t\traise Exception('Invalid number of arguments')\n\t\t\treturn False\n\t\treturn True\n/README.md\n# HCI_Museum\n\n## 1. About the project\n\nThe project was conceived during the course of Human Computer Interaction at the University of Trento (Italy) in the academic year 2019/2020. It is a virtual tour of the Museum of Natural History of Trento, a museum that offers a large collection of natural history objects.\n\n## 2. What is the project about?\n\nIn the first part of the project, we had to develop a digital tour of the museum. The tour was developed by using the Unity Game Engine.\n\nThe tour is divided into three main sections. The first section, named \"Introduction\", provides a brief description of the museum and its main attractions. The second section, named \"Natural History\", describes the first part of the natural history collection. The third section, named \"History\", describes the second part of the natural history collection.\n\n## 3. How to install and use the project\n\nThe project consists of 5 different parts:\n\n- Unity project (https://unity3d.com/it)\n- Unity package (https://docs.unity3d.com/Manual/cus", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\t\treturn False\n\t\treturn True\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\tif len(arg) > self.length:\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_length_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs is not longer than length specified. \\n:param kwargs: The received arguments.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\tif len(value) > self.length:\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_regexp_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args matches the regular expression specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\tif not re.match(self.regexp, arg):\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_regexp_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs matches the regular expression specified. \\n:param kwargs: The received arguments.\n\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\tif not re.match(self.regexp, value):\n\t\treturn False\n\t\treturn True\n\t\n\tdef validate_type_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args matches the type specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif len(arg) > self.length:\n\t\t\t\treturn False\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.max_length:\n\t\t\traise ValueError('Number of args cannot exceed {}.'.format(self.max_length))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.length:\n\t\t\traise InvalidLengthException(self.name, self.length)\n\n\tdef validate_number_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not shorter than number specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) < self.number:\n\t\t\traise InvalidNumberException(self.name, self.length)\n\n\tdef validate_values_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not one of the given values. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.values:\n\t\t\traise InvalidValuesException(self.name, self.values)\n\n\tdef validate_values_or_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not one of the given values or is longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.values and len(args) > self.length:\n\t\t\traise InvalidValueOrLengthException(self.name, self.values, self.length)\n\n\tdef validate_values_or_number_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not one of the given", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.choices:\n\t\t\traise ValueError(self.choices)\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tself.check_arg_type(args, [str], \"args\")\n\t\tif args not in self.args.choices:\n\t\t\traise ArgValidationError(\"The value of the given args is not one of the available choices.\")\n\tdef validate_choices_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs is one of the available choices. \\n:param kwargs: The received keyword arguments.\n\t\"\"\"\n\t\tself.check_arg_type(kwargs, [str], \"kwargs\")\n\t\tif kwargs not in self.args.choices:\n\t\t\traise ArgValidationError(\"The value of the given kwargs is not one of the available choices.\")\n\tdef validate_choices_extra(self, extra):\n\t\"\"\"\n\tCheck if value of the given extra is one of the available choices. \\n:param extra: The received extra positional arguments.\n\t\"\"\"\n\t\tself.check_arg_type(extra, [str], \"extra\")\n\t\tif extra not in self.args.choices:\n\t\t\traise ArgValidationError(\"The value of the given extra is not one of the available choices.\")\n\tdef validate_choices_all(self, choices):\n\t\"\"\"\n\tCheck if value of the given choices is one of the available choices. \\n:param choices: The received choices.\n\t\"\"\"\n\t\tself.check_arg_type(choices, [", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif not args in self.choices:\n\t\t\tself.fail('Invalid choice: %s (choose from %s)' % (args, self.choices))\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) == 0:\n\t\t\treturn True\n\t\telif len(args) == 1:\n\t\t\tif args[0] in self.choices:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.choices:\n\t\traise argparse.ArgumentTypeError('Invalid choice: %s' % args)\n\t\treturn args\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\t\tfor arg in args:\n\t\t\tif arg not in self.choices:\n\t\t\t\traise ValueError(\"Value not available: %s\" % arg)\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\t#Check if there is only one value\n\tif len(args)!= 1:\n\t\treturn False\n\t\t#Check if the value is one of the available choices\n\telif not args[0] in self.choices:\n\t\treturn False\n\telse:\n\t\treturn True\n\n#Validate the arguments of the validate_choices_args method\nvalidator = arg_validator.ArgValidator()\nvalidator.validate_args(validate_choices_args, ['a'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['b'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['c'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['a'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['c'])\n\n\"\"\"\nWe can see that the validate_choices_args method works as expected.\n\"\"\"\n\n# +\n\"\"\"\nLet's add some more validations.\n\"\"\"\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args in self.choices:\n\t\t\treturn True\n\t\treturn False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\treturn True if args in self.choices else False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor x in self.choices_args:\n\t\t\tif args[x] in self.choices_args[x]:\n\t\t\t\tcontinue\n\t\t\traise Exception('Value of {0} is not valid. Valid values are: {1}.'.format(x, self.choices_args[x]))\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.choices:\n\t\t\traise ValueError(self.choices)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tself.check_arg_type(args, [str], \"args\")\n\t\tif args not in self.args.choices:\n\t\t\traise ArgValidationError(\"The value of the given args is not one of the available choices.\")\n\tdef validate_choices_kwargs(self, kwargs):\n\t\"\"\"\n\tCheck if value of the given kwargs is one of the available choices. \\n:param kwargs: The received keyword arguments.\n\t\"\"\"\n\t\tself.check_arg_type(kwargs, [str], \"kwargs\")\n\t\tif kwargs not in self.args.choices:\n\t\t\traise ArgValidationError(\"The value of the given kwargs is not one of the available choices.\")\n\tdef validate_choices_extra(self, extra):\n\t\"\"\"\n\tCheck if value of the given extra is one of the available choices. \\n:param extra: The received extra positional arguments.\n\t\"\"\"\n\t\tself.check_arg_type(extra, [str], \"extra\")\n\t\tif extra not in self.args.choices:\n\t\t\traise ArgValidationError(\"The value of the given extra is not one of the available choices.\")\n\tdef validate_choices_all(self, choices):\n\t\"\"\"\n\tCheck if value of the given choices is one of the available choices. \\n:param choices: The received choices.\n\t\"\"\"\n\t\tself.check_arg_type(choices, [", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif not args in self.choices:\n\t\t\tself.fail('Invalid choice: %s (choose from %s)' % (args, self.choices))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) == 0:\n\t\t\treturn True\n\t\telif len(args) == 1:\n\t\t\tif args[0] in self.choices:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args not in self.choices:\n\t\traise argparse.ArgumentTypeError('Invalid choice: %s' % args)\n\t\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\t\tfor arg in args:\n\t\t\tif arg not in self.choices:\n\t\t\t\traise ValueError(\"Value not available: %s\" % arg)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\t#Check if there is only one value\n\tif len(args)!= 1:\n\t\treturn False\n\t\t#Check if the value is one of the available choices\n\telif not args[0] in self.choices:\n\t\treturn False\n\telse:\n\t\treturn True\n\n#Validate the arguments of the validate_choices_args method\nvalidator = arg_validator.ArgValidator()\nvalidator.validate_args(validate_choices_args, ['a'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['b'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['c'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['a'])\n\n#Validate the arguments of the validate_choices_args method\nvalidator.validate_args(validate_choices_args, ['c'])\n\n\"\"\"\nWe can see that the validate_choices_args method works as expected.\n\"\"\"\n\n# +\n\"\"\"\nLet's add some more validations.\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tif args in self.choices:\n\t\t\treturn True\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\treturn True if args in self.choices else False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\t\tfor x in self.choices_args:\n\t\t\tif args[x] in self.choices_args[x]:\n\t\t\t\tcontinue\n\t\t\traise Exception('Value of {0} is not valid. Valid values are: {1}.'.format(x, self.choices_args[x]))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg in self.configuration:\n\t\t\t\tif self.configuration[arg] < self.min_value[arg]:\n\t\t\t\t\tself.error_code = 4\n\t\t\t\t\treturn False\n\t\t\t\tif self.configuration[arg] > self.max_value[arg]:\n\t\t\t\t\tself.error_code = 5\n\t\t\t\t\treturn False\n                return True\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n        if'min' in args and'max' in args:\n            if args['min'] > args['max']:\n                raise InvalidArgument('min value should be less than max value.')\n        else:\n            raise InvalidArgument('min and max values are required.')\n\n    def validate_min_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is greater than minimum value\n\t\"\"\"\n\t\n        if'min' in args:\n            if args['min'] > args['value']:\n                raise InvalidArgument('min value should be less than given value.')\n\n    def validate_max_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is less than maximum value\n\t\"\"\"\n\t\n        if'max' in args:\n            if args['max'] < args['value']:\n                raise InvalidArgument('max value should be greater than given value.')\n\n    def validate_not_equal_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not equal to the given value\n\t\"\"\"\n\t\n        if 'value' in args:\n            if args['value'] == args['not_equal']:\n                raise InvalidArgument('value should not be equal to not equal value.')\n\n    def validate_equal_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is equal to the given value\n\t\"\"\"\n\t\n        if 'value' in args", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\t# Get minimum and maximum values for the given args\n\targs_min_max = self.args_min_max.get(args)\n\tif not args_min_max:\n\t\treturn\n\n\t# Get value of the given args\n\targs_value = self.get_value(args)\n\tif not args_value:\n\t\treturn\n\n\t# Validate the given value\n\tif args_value < args_min_max[0]:\n\t\tself.logger.warning(\"Minimum value for %s is %d\", args, args_min_max[0])\n\n\tif args_value > args_min_max[1]:\n\t\tself.logger.warning(\"Maximum value for %s is %d\", args, args_min_max[1])\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\t# get the name of the function\n\tfunction_name = sys._getframe().f_code.co_name\n\t# check if the given args are numbers\n\tif not all(isinstance(arg, numbers.Number) for arg in args):\n\t\traise TypeError(\"The argument of the function {0} must be a number\".format(function_name))\n\n\t# check if the values are within the given bounds\n\telif not all((arg >= self.min and arg <= self.max) for arg in args):\n\t\traise ValueError(\"The given values are outside of the bounds of the function {0}\".format(function_name))\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\n\tif args['min_value'] and args['max_value']:\n\t\tif (args['min_value'] >= args['max_value']):\n\t\t\traise ValueError(\"Minimum is greater than maximum\")\n\n\t\tif args['value'] < args['min_value'] or args['value'] > args['max_value']:\n\t\t\traise ValueError(\"Value is out of range\")\n\n\telif args['min_value']:\n\t\tif args['value'] < args['min_value']:\n\t\t\traise ValueError(\"Value is out of range\")\n\n\telif args['max_value']:\n\t\tif args['value'] > args['max_value']:\n\t\t\traise ValueError(\"Value is out of range\")\n\n\treturn True\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\n\tif (args.min_range > args.max_range):\n\t\traise ValueError(\"min_range should be smaller than max_range\")\n\n\tif (args.min_range < 0 or args.max_range < 0):\n\t\traise ValueError(\"min_range and max_range should be positive\")\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\ttry:\n\t\t\tif (float(args) < self.min_value) or (float(args) > self.max_value):\n\t\t\t\traise ValueError\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Value of the argument is not in the range of valid values\")\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\tif args.min_value <= args.value <= args.max_value:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\tif not (self.min_value <= args <= self.max_value):\n\t\t\traise argparse.ArgumentTypeError(\"Value should be between %d and %d\" % (self.min_value, self.max_value))\n\t\treturn args\n\n#define all the arguments\nparser = argparse.ArgumentParser(description='A Tool To Extract The Number Of Bytes From The Given Input File')\nparser.add_argument('-f', '--file',type=str,help='Input File')\nparser.add_argument('-s', '--start', type=int, help='Start Byte', default=0)\nparser.add_argument('-e', '--end', type=int, help='End Byte', default=0)\nparser.add_argument('--byte', action=ByteAction, type=int, help='Number Of Byte', default=1)\nargs = parser.parse_args()\n\n#parse the arguments\nfile = args.file\nstart = args.start\nend = args.end\nbyte = args.byte\n\n#check if the file is given\nif file is None:\n\tparser.print_help()\n\tsys.exit(1)\n\n#check if the start or end values is given\nif start == 0 and end == 0:\n\tparser.print_help()\n\tsys.exit(1)\n\n#if start and end", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n        for (arg, min_val, max_val) in args:\n            if arg < min_val or arg > max_val:\n                return False\n        return True\n\n\n    def validate_min_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is greater than minimum value\n\t\"\"\"\n\t\n        for arg, min_val in args:\n            if arg < min_val:\n                return False\n        return True\n\n\n    def validate_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is less than maximum value\n\t\"\"\"\n\t\n        for arg, max_val in args:\n            if arg > max_val:\n                return False\n        return True\n\n\n    def validate_one_of_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the given values\n\t\"\"\"\n\t\n        for arg, values in args:\n            if arg not in values:\n                return False\n        return True\n\n\n    def validate_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the given values\n\t\"\"\"\n\t\n        for arg, values in args:\n            if arg not in values:\n                return False\n        return True\n\n\n    def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n        for (arg"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg in self.configuration:\n\t\t\t\tif self.configuration[arg] < self.min_value[arg]:\n\t\t\t\t\tself.error_code = 4\n\t\t\t\t\treturn False\n\t\t\t\tif self.configuration[arg] > self.max_value[arg]:\n\t\t\t\t\tself.error_code = 5\n\t\t\t\t\treturn False\n                return True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n        if'min' in args and'max' in args:\n            if args['min'] > args['max']:\n                raise InvalidArgument('min value should be less than max value.')\n        else:\n            raise InvalidArgument('min and max values are required.')\n\n    def validate_min_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is greater than minimum value\n\t\"\"\"\n\t\n        if'min' in args:\n            if args['min'] > args['value']:\n                raise InvalidArgument('min value should be less than given value.')\n\n    def validate_max_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is less than maximum value\n\t\"\"\"\n\t\n        if'max' in args:\n            if args['max'] < args['value']:\n                raise InvalidArgument('max value should be greater than given value.')\n\n    def validate_not_equal_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not equal to the given value\n\t\"\"\"\n\t\n        if 'value' in args:\n            if args['value'] == args['not_equal']:\n                raise InvalidArgument('value should not be equal to not equal value.')\n\n    def validate_equal_arg(self, args):\n\t\"\"\"\n\tCheck if value of the given args is equal to the given value\n\t\"\"\"\n\t\n        if 'value' in args", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\t# Get minimum and maximum values for the given args\n\targs_min_max = self.args_min_max.get(args)\n\tif not args_min_max:\n\t\treturn\n\n\t# Get value of the given args\n\targs_value = self.get_value(args)\n\tif not args_value:\n\t\treturn\n\n\t# Validate the given value\n\tif args_value < args_min_max[0]:\n\t\tself.logger.warning(\"Minimum value for %s is %d\", args, args_min_max[0])\n\n\tif args_value > args_min_max[1]:\n\t\tself.logger.warning(\"Maximum value for %s is %d\", args, args_min_max[1])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\t# get the name of the function\n\tfunction_name = sys._getframe().f_code.co_name\n\t# check if the given args are numbers\n\tif not all(isinstance(arg, numbers.Number) for arg in args):\n\t\traise TypeError(\"The argument of the function {0} must be a number\".format(function_name))\n\n\t# check if the values are within the given bounds\n\telif not all((arg >= self.min and arg <= self.max) for arg in args):\n\t\traise ValueError(\"The given values are outside of the bounds of the function {0}\".format(function_name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\n\tif args['min_value'] and args['max_value']:\n\t\tif (args['min_value'] >= args['max_value']):\n\t\t\traise ValueError(\"Minimum is greater than maximum\")\n\n\t\tif args['value'] < args['min_value'] or args['value'] > args['max_value']:\n\t\t\traise ValueError(\"Value is out of range\")\n\n\telif args['min_value']:\n\t\tif args['value'] < args['min_value']:\n\t\t\traise ValueError(\"Value is out of range\")\n\n\telif args['max_value']:\n\t\tif args['value'] > args['max_value']:\n\t\t\traise ValueError(\"Value is out of range\")\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\n\tif (args.min_range > args.max_range):\n\t\traise ValueError(\"min_range should be smaller than max_range\")\n\n\tif (args.min_range < 0 or args.max_range < 0):\n\t\traise ValueError(\"min_range and max_range should be positive\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\ttry:\n\t\t\tif (float(args) < self.min_value) or (float(args) > self.max_value):\n\t\t\t\traise ValueError\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Value of the argument is not in the range of valid values\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\tif args.min_value <= args.value <= args.max_value:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\tif not (self.min_value <= args <= self.max_value):\n\t\t\traise argparse.ArgumentTypeError(\"Value should be between %d and %d\" % (self.min_value, self.max_value))\n\t\treturn args\n\n#define all the arguments\nparser = argparse.ArgumentParser(description='A Tool To Extract The Number Of Bytes From The Given Input File')\nparser.add_argument('-f', '--file',type=str,help='Input File')\nparser.add_argument('-s', '--start', type=int, help='Start Byte', default=0)\nparser.add_argument('-e', '--end', type=int, help='End Byte', default=0)\nparser.add_argument('--byte', action=ByteAction, type=int, help='Number Of Byte', default=1)\nargs = parser.parse_args()\n\n#parse the arguments\nfile = args.file\nstart = args.start\nend = args.end\nbyte = args.byte\n\n#check if the file is given\nif file is None:\n\tparser.print_help()\n\tsys.exit(1)\n\n#check if the start or end values is given\nif start == 0 and end == 0:\n\tparser.print_help()\n\tsys.exit(1)\n\n#if start and end", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n        for (arg, min_val, max_val) in args:\n            if arg < min_val or arg > max_val:\n                return False\n        return True\n\n\n    def validate_min_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is greater than minimum value\n\t\"\"\"\n\t\n        for arg, min_val in args:\n            if arg < min_val:\n                return False\n        return True\n\n\n    def validate_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is less than maximum value\n\t\"\"\"\n\t\n        for arg, max_val in args:\n            if arg > max_val:\n                return False\n        return True\n\n\n    def validate_one_of_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the given values\n\t\"\"\"\n\t\n        for arg, values in args:\n            if arg not in values:\n                return False\n        return True\n\n\n    def validate_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the given values\n\t\"\"\"\n\t\n        for arg, values in args:\n            if arg not in values:\n                return False\n        return True\n\n\n    def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n        for (arg", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        if (type_name.endswith(\"[]\")):\n            type_name = type_name[:-2]\n            array = True\n        else:\n            array = False\n\n        if (type_name in COMPLEX_TYPES):\n            complex_type = COMPLEX_TYPES[type_name]\n            if (array):\n                if (len(complex_type) > 1):\n                    raise Exception(\"Complex type %s cannot be an array\" % type_name)\n                return complex_type[0]\n            else:\n                if (len(complex_type) == 1):\n                    return complex_type[0]\n                else:\n                    if (len(complex_type) == 2):\n                        return complex_type[1]\n                    else:\n                        raise Exception(\"Complex type %s cannot be a scalar\" % type_name)\n        else:\n            raise Exception(\"Unknown complex type %s\" % type_name)\n\n    def create_argumet_type(self, subcommand, type_name, option_name, spec_option):\n\t\"\"\"\n\tReturns the corresponding function in TYPES based on type_name and returns the result obtained by action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\n        if (type_name.endswith(\"[]\")):\n            type_name = type_name[:-2]\n            array =", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\ttype_name = type_name.lower()\n\t\tif type_name in COMPLEX_TYPES:\n\t\t\treturn COMPLEX_TYPES[type_name](self, subcommand, type_name, option_name, spec_option)\n\t\telse:\n\t\t\treturn None\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\tif type_name in COMPLEX_TYPES:\n\t\t\treturn COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n\t\telse:\n\t\t\traise ValueError(\"Invalid type name: %s\" % (type_name))\n\tdef complex_action(self, subcommand, option_name, spec_option):\n\t\t\"\"\"\n\t\tCalled when a complex option is encountered.\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttype_name = spec_option.get_type_name()\n\t\t\treturn self.create_complex_argumet_type(subcommand, type_name, option_name, spec_option)\n\t\texcept ValueError as e:\n\t\t\traise ValueError(\"Invalid type name: %s\" % (type_name))\n\tdef complex_list_action(self, subcommand, option_name, spec_option):\n\t\t\"\"\"\n\t\tCalled when a complex list option is encountered.\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttype_name = spec_option.get_type_name()\n\t\t\treturn self.create_complex_argumet_type(subcommand, type_name, option_name, spec_option)\n\t\texcept ValueError as e:\n\t\t\traise ValueError(\"Invalid type name: %s\" % (type_name))\n\tdef complex_action_with_multiple_types(self, subcommand", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\ncreate_complex_argumet_type = classmethod(create_complex_argumet_type)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        return self.COMPLEX_TYPES[type_name](subcommand, option_name, self.vars, self.defaults, self.plugin_path, spec_option)\n/lib/commander/tests/test_create_complex_argumet_type.py\nfrom commander import complex_types\nfrom commander.complex_types import *\nfrom commander.utils import *\n\nfrom mock import *\nimport unittest\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\tif type_name not in self.COMPLEX_TYPES:\n\t\t\treturn None\n\n\t\tif self.COMPLEX_TYPES[type_name][0] is None:\n\t\t\treturn self.COMPLEX_TYPES[type_name][1]\n\t\treturn self.COMPLEX_TYPES[type_name][0](self, subcommand, type_name, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        if type_name not in self.COMPLEX_TYPES:\n            raise TypeError(f\"Unrecognized type {type_name} for {option_name} in {subcommand}\")\n        complex_action = self.COMPLEX_TYPES[type_name]\n        return complex_action(self.vars, self.defaults, self.plugin_path, subcommand, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        if type_name in COMPLEX_TYPES:\n            if type_name == \"path\":\n                return COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option, self.plugin_path)\n            return COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n        else:\n            raise Exception(\"Invalid argument type: \" + type_name)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\tif type_name not in COMPLEX_TYPES:\n\t\t\traise ValueError(\"Unknown complex type %s\" % type_name)\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\t\treturn complex_action(self, subcommand, option_name, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        # This is a workaround for a bug in Python 3.5.3.\n        # See https://bugs.python.org/issue29249\n        #\n        # The issue is that if you specify the same option twice in the commandline, and if the second one has a default value, the first one won't get the default value.\n        # This is a bug in the Python 3.5.3 parser.\n        #\n        # The workaround is to add a special check in the command line parser to detect this case and assign the default value to the first option.\n        #\n        # This bug is fixed in Python 3.6.0.\n        #\n        # The workaround is to add a special check in the command line parser to detect this case and assign the default value to the first option.\n        #\n        # This is a workaround for a bug in Python 3.5.3.\n        # See https://bugs.python.org/issue29249\n        #\n        # The issue is that if you specify the same option twice in the commandline, and if the second one has a default value, the first one won't get the default value.\n        # This is a bug in the Python 3.5.3 parser.\n        #\n        # The workaround is to add a special check in the command line parser to detect this case and assign the default value to the first option.\n        #\n        #"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        if (type_name.endswith(\"[]\")):\n            type_name = type_name[:-2]\n            array = True\n        else:\n            array = False\n\n        if (type_name in COMPLEX_TYPES):\n            complex_type = COMPLEX_TYPES[type_name]\n            if (array):\n                if (len(complex_type) > 1):\n                    raise Exception(\"Complex type %s cannot be an array\" % type_name)\n                return complex_type[0]\n            else:\n                if (len(complex_type) == 1):\n                    return complex_type[0]\n                else:\n                    if (len(complex_type) == 2):\n                        return complex_type[1]\n                    else:\n                        raise Exception(\"Complex type %s cannot be a scalar\" % type_name)\n        else:\n            raise Exception(\"Unknown complex type %s\" % type_name)\n\n    def create_argumet_type(self, subcommand, type_name, option_name, spec_option):\n\t\"\"\"\n\tReturns the corresponding function in TYPES based on type_name and returns the result obtained by action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\n        if (type_name.endswith(\"[]\")):\n            type_name = type_name[:-2]\n            array =", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\ttype_name = type_name.lower()\n\t\tif type_name in COMPLEX_TYPES:\n\t\t\treturn COMPLEX_TYPES[type_name](self, subcommand, type_name, option_name, spec_option)\n\t\telse:\n\t\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\tif type_name in COMPLEX_TYPES:\n\t\t\treturn COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n\t\telse:\n\t\t\traise ValueError(\"Invalid type name: %s\" % (type_name))\n\tdef complex_action(self, subcommand, option_name, spec_option):\n\t\t\"\"\"\n\t\tCalled when a complex option is encountered.\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttype_name = spec_option.get_type_name()\n\t\t\treturn self.create_complex_argumet_type(subcommand, type_name, option_name, spec_option)\n\t\texcept ValueError as e:\n\t\t\traise ValueError(\"Invalid type name: %s\" % (type_name))\n\tdef complex_list_action(self, subcommand, option_name, spec_option):\n\t\t\"\"\"\n\t\tCalled when a complex list option is encountered.\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttype_name = spec_option.get_type_name()\n\t\t\treturn self.create_complex_argumet_type(subcommand, type_name, option_name, spec_option)\n\t\texcept ValueError as e:\n\t\t\traise ValueError(\"Invalid type name: %s\" % (type_name))\n\tdef complex_action_with_multiple_types(self, subcommand", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\ncreate_complex_argumet_type = classmethod(create_complex_argumet_type)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        return self.COMPLEX_TYPES[type_name](subcommand, option_name, self.vars, self.defaults, self.plugin_path, spec_option)\n/lib/commander/tests/test_create_complex_argumet_type.py\nfrom commander import complex_types\nfrom commander.complex_types import *\nfrom commander.utils import *\n\nfrom mock import *\nimport unittest\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\tif type_name not in self.COMPLEX_TYPES:\n\t\t\treturn None\n\n\t\tif self.COMPLEX_TYPES[type_name][0] is None:\n\t\t\treturn self.COMPLEX_TYPES[type_name][1]\n\t\treturn self.COMPLEX_TYPES[type_name][0](self, subcommand, type_name, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        if type_name not in self.COMPLEX_TYPES:\n            raise TypeError(f\"Unrecognized type {type_name} for {option_name} in {subcommand}\")\n        complex_action = self.COMPLEX_TYPES[type_name]\n        return complex_action(self.vars, self.defaults, self.plugin_path, subcommand, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        if type_name in COMPLEX_TYPES:\n            if type_name == \"path\":\n                return COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option, self.plugin_path)\n            return COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n        else:\n            raise Exception(\"Invalid argument type: \" + type_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\tif type_name not in COMPLEX_TYPES:\n\t\t\traise ValueError(\"Unknown complex type %s\" % type_name)\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\t\treturn complex_action(self, subcommand, option_name, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n        # This is a workaround for a bug in Python 3.5.3.\n        # See https://bugs.python.org/issue29249\n        #\n        # The issue is that if you specify the same option twice in the commandline, and if the second one has a default value, the first one won't get the default value.\n        # This is a bug in the Python 3.5.3 parser.\n        #\n        # The workaround is to add a special check in the command line parser to detect this case and assign the default value to the first option.\n        #\n        # This bug is fixed in Python 3.6.0.\n        #\n        # The workaround is to add a special check in the command line parser to detect this case and assign the default value to the first option.\n        #\n        # This is a workaround for a bug in Python 3.5.3.\n        # See https://bugs.python.org/issue29249\n        #\n        # The issue is that if you specify the same option twice in the commandline, and if the second one has a default value, the first one won't get the default value.\n        # This is a bug in the Python 3.5.3 parser.\n        #\n        # The workaround is to add a special check in the command line parser to detect this case and assign the default value to the first option.\n        #\n        #", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {\n\t\t\t'ir_timeout': 300,\n\t\t\t'ir_retries': 3,\n\t\t\t'ir_retry_interval': 10,\n\t\t\t'ir_skip_failed_stage': False,\n\t\t\t'ir_skip_failed_tasks': False,\n\t\t\t'ir_skip_non_existent_tasks': True,\n\t\t\t'ir_skip_tags': [],\n\t\t\t'ir_skip_tags_regex': [],\n\t\t\t'ir_skip_tags_exclude': [],\n\t\t\t'ir_skip_tags_exclude_regex': [],\n\t\t\t'ir_skip_tags_include': [],\n\t\t\t'ir_skip_tags_include_regex': [],\n\t\t\t'ir_skip_tags_include_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_regex': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_regex': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude_regex': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude_exclude_regex': [],", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args, nested_args = {}, {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith(self.CONTROL_ARG_PREFIX):\n\t\t\t\tcontrol_args[arg] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg] = args[arg]\n\t\treturn control_args, nested_args\n\n\n    def get_nested_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args, nested_args = {}, {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith(self.CONTROL_ARG_PREFIX):\n\t\t\t\tcontrol_args[arg] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg] = args[arg]\n\t\treturn nested_args\n# -*- coding: utf-8 -*-\n\"\"\"\nThe MIT License (MIT)\n\nCopyright (c) 2017-", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {}\n\t\tnested_args = {}\n\t\tcustom_args = {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith('--control-'):\n\t\t\t\tcontrol_args[arg] = True\n\t\t\telif arg.startswith('--nested-'):\n\t\t\t\tnested_args[arg] = True\n\t\t\telif arg.startswith('--custom-'):\n\t\t\t\tcustom_args[arg] = True\n\t\t\telse:\n\t\t\t\tnested_args[arg] = True\n\t\treturn control_args, nested_args, custom_args\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\t# split args to control and nested arguments\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg in self.nested_args:\n\t\t\tnested_args[arg] = args[arg]\n\t\telse:\n\t\t\tself.log.error(\"argument %s is not a known argument\", arg)\n\n\treturn (control_args, nested_args)\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {}\n\t\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(_CONTROL_FLAG):\n\t\t\tcontrol_args[arg[len(_CONTROL_FLAG):]] = arg\n\t\telif '=' in arg:\n\t\t\tnested_args[arg.split('=')[0]] = arg.split('=')[1]\n\t\telse:\n\t\t\tnested_args[arg] = True\n\t\treturn control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\t# Controls arguments: control the IR behavior. These arguments\n\t\t# will not be put into the spec yml file\n\t# Nested arguments: are used by the Ansible playbooks and will be put\n\t# into the spec yml file.\n\t# Custom arguments: Custom ansible variables to be used instead of the\n\t# normal nested usage.\n\tcontrol_args = {\n\t\t'--skip-reboot': IR_SKIP_REBOOT,\n\t\t'--skip-commit': IR_SKIP_COMMIT,\n\t\t'--keep-ssh-alive': IR_KEEP_SSH_ALIVE,\n\t\t'--no-check-version': IR_NO_CHECK_VERSION,\n\t\t'--skip-download-files': IR_SKIP_DOWNLOAD_FILES,\n\t\t'--skip-install': IR_SKIP_INSTALL,\n\t\t'--skip-download-binaries': IR_SKIP_DOWNLOAD_BINARIES,\n\t\t'--skip-download-images': IR_SKIP_DOWNLOAD_IMAGES,\n\t\t'--skip-download-kernel': IR_SKIP_DOWNLOAD_KERNEL,\n\t\t'--skip-download-rootfs': IR_SKIP_DOWNLOAD_ROOTFS,\n\t\t'--skip-download-config': IR_SKIP_DOWNLOAD_CONFIG,\n\t\t'--skip-download-rootfs-files': IR_SKIP_DOWNLOAD_ROOTFS_FILES,\n\t\t'--skip", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args, nested_args = {}, {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith('--'):\n\t\t\t\tnested_args[arg[2:]] = self.get_arg_value(arg)\n\t\t\telse:\n\t\t\t\tcontrol_args[arg] = self.get_arg_value(arg)\n\t\treturn control_args, nested_args\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\t# Create a flat dictionary of all the arguments.\n\tflat_args = {}\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tflat_args[arg] = args[arg]\n\t\telse:\n\t\t\tflat_args[arg.split('=')[0]] = arg.split('=')[1]\n\t# Create a flat dictionary of all the custom arguments.\n\tcustom_args = {}\n\tfor arg in flat_args:\n\t\tif arg.startswith('--custom_'):\n\t\t\tcustom_args[arg] = flat_args[arg]\n\t# Remove the custom arguments from the flat args.\n\tfor arg in custom_args:\n\t\tflat_args.pop(arg)\n\treturn custom_args, flat_args\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tif hasattr(self, 'control_args'):\n\t\t\tcontrol_args = self.control_args\n\t\telse:\n\t\t\tself.control_args = {}\n\t\t\tcontrol_args = self.control_args\n\t\tif hasattr(self, 'nested_args'):\n\t\t\tnested_args = self.nested_args\n\t\telse:\n\t\t\tself.nested_args = {}\n\t\t\tnested_args = self.nested_args\n\t\tif hasattr(self, 'custom_args'):\n\t\t\tcustom_args = self.custom_args\n\t\telse:\n\t\t\tself.custom_args = {}\n\t\t\tcustom_args = self.custom_args\n\t\tfor key in args:\n\t\t\tif hasattr(self, key):\n\t\t\t\tif key in self.control_args:\n\t\t\t\t\tcontrol_args[key] = getattr(self, key)\n\t\t\t\telif key in self.nested_args:\n\t\t\t\t\tnested_args[key] = getattr(self, key)\n\t\t\t\telif key in self.custom_args:\n\t\t\t\t\tcustom_args[key] = getattr(self, key)\n\t\t\t\telse:\n\t\t\t\t\tprint(\"\\n\\t\\t\\tWARNING: No definition of the argument \\\"%s\\\" for this class!\" % key)\n\t\treturn control_args, nested_args, custom_args\n\tdef set_nested_custom_and_control_args(self, args):\n\t", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {}\n\t\tnested_args = {}\n\t\tcustom_args = {}\n\t\tfor a in args:\n\t\t\tif a.startswith(self.control_arg_prefix):\n\t\t\t\tcontrol_args[a] = args[a]\n\t\t\telif a.startswith(self.nested_arg_prefix):\n\t\t\t\tnested_args[a] = args[a]\n\t\t\telse:\n\t\t\t\tcustom_args[a] = args[a]\n\t\treturn control_args, nested_args, custom_args\n/ansible/roles/os-ironic-conductor/templates/ironic-conductor.service.j2\n#!/bin/bash\n#\n# ironic-conductor       Ironic Conductor service\n#\n# chkconfig: 2345 99 01\n# description: Ironic Conductor service\n#\n# processname: ironic-conductor\n# pidfile: /var/run/ironic-conductor.pid\n\n. /etc/init.d/functions\n. /etc/sysconfig/network\n\nLOCKFILE=/var/lock/subsys/ironic-conductor\n\nstart() {\n\tif [! -f $LOCKFILE ]; then\n\t\techo -n $\"Starting ironic-conductor: \"\n\t\tdaemon --check ironic-conductor --user ir"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {\n\t\t\t'ir_timeout': 300,\n\t\t\t'ir_retries': 3,\n\t\t\t'ir_retry_interval': 10,\n\t\t\t'ir_skip_failed_stage': False,\n\t\t\t'ir_skip_failed_tasks': False,\n\t\t\t'ir_skip_non_existent_tasks': True,\n\t\t\t'ir_skip_tags': [],\n\t\t\t'ir_skip_tags_regex': [],\n\t\t\t'ir_skip_tags_exclude': [],\n\t\t\t'ir_skip_tags_exclude_regex': [],\n\t\t\t'ir_skip_tags_include': [],\n\t\t\t'ir_skip_tags_include_regex': [],\n\t\t\t'ir_skip_tags_include_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_regex': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_regex': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude_regex': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude_exclude': [],\n\t\t\t'ir_skip_tags_include_exclude_exclude_exclude_exclude_regex': [],", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args, nested_args = {}, {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith(self.CONTROL_ARG_PREFIX):\n\t\t\t\tcontrol_args[arg] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg] = args[arg]\n\t\treturn control_args, nested_args\n\n\n    def get_nested_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args, nested_args = {}, {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith(self.CONTROL_ARG_PREFIX):\n\t\t\t\tcontrol_args[arg] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg] = args[arg]\n\t\treturn nested_args\n# -*- coding: utf-8 -*-\n\"\"\"\nThe MIT License (MIT)\n\nCopyright (c) 2017-", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {}\n\t\tnested_args = {}\n\t\tcustom_args = {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith('--control-'):\n\t\t\t\tcontrol_args[arg] = True\n\t\t\telif arg.startswith('--nested-'):\n\t\t\t\tnested_args[arg] = True\n\t\t\telif arg.startswith('--custom-'):\n\t\t\t\tcustom_args[arg] = True\n\t\t\telse:\n\t\t\t\tnested_args[arg] = True\n\t\treturn control_args, nested_args, custom_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\t# split args to control and nested arguments\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg in self.nested_args:\n\t\t\tnested_args[arg] = args[arg]\n\t\telse:\n\t\t\tself.log.error(\"argument %s is not a known argument\", arg)\n\n\treturn (control_args, nested_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {}\n\t\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(_CONTROL_FLAG):\n\t\t\tcontrol_args[arg[len(_CONTROL_FLAG):]] = arg\n\t\telif '=' in arg:\n\t\t\tnested_args[arg.split('=')[0]] = arg.split('=')[1]\n\t\telse:\n\t\t\tnested_args[arg] = True\n\t\treturn control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\t# Controls arguments: control the IR behavior. These arguments\n\t\t# will not be put into the spec yml file\n\t# Nested arguments: are used by the Ansible playbooks and will be put\n\t# into the spec yml file.\n\t# Custom arguments: Custom ansible variables to be used instead of the\n\t# normal nested usage.\n\tcontrol_args = {\n\t\t'--skip-reboot': IR_SKIP_REBOOT,\n\t\t'--skip-commit': IR_SKIP_COMMIT,\n\t\t'--keep-ssh-alive': IR_KEEP_SSH_ALIVE,\n\t\t'--no-check-version': IR_NO_CHECK_VERSION,\n\t\t'--skip-download-files': IR_SKIP_DOWNLOAD_FILES,\n\t\t'--skip-install': IR_SKIP_INSTALL,\n\t\t'--skip-download-binaries': IR_SKIP_DOWNLOAD_BINARIES,\n\t\t'--skip-download-images': IR_SKIP_DOWNLOAD_IMAGES,\n\t\t'--skip-download-kernel': IR_SKIP_DOWNLOAD_KERNEL,\n\t\t'--skip-download-rootfs': IR_SKIP_DOWNLOAD_ROOTFS,\n\t\t'--skip-download-config': IR_SKIP_DOWNLOAD_CONFIG,\n\t\t'--skip-download-rootfs-files': IR_SKIP_DOWNLOAD_ROOTFS_FILES,\n\t\t'--skip", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args, nested_args = {}, {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith('--'):\n\t\t\t\tnested_args[arg[2:]] = self.get_arg_value(arg)\n\t\t\telse:\n\t\t\t\tcontrol_args[arg] = self.get_arg_value(arg)\n\t\treturn control_args, nested_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\t# Create a flat dictionary of all the arguments.\n\tflat_args = {}\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tflat_args[arg] = args[arg]\n\t\telse:\n\t\t\tflat_args[arg.split('=')[0]] = arg.split('=')[1]\n\t# Create a flat dictionary of all the custom arguments.\n\tcustom_args = {}\n\tfor arg in flat_args:\n\t\tif arg.startswith('--custom_'):\n\t\t\tcustom_args[arg] = flat_args[arg]\n\t# Remove the custom arguments from the flat args.\n\tfor arg in custom_args:\n\t\tflat_args.pop(arg)\n\treturn custom_args, flat_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tif hasattr(self, 'control_args'):\n\t\t\tcontrol_args = self.control_args\n\t\telse:\n\t\t\tself.control_args = {}\n\t\t\tcontrol_args = self.control_args\n\t\tif hasattr(self, 'nested_args'):\n\t\t\tnested_args = self.nested_args\n\t\telse:\n\t\t\tself.nested_args = {}\n\t\t\tnested_args = self.nested_args\n\t\tif hasattr(self, 'custom_args'):\n\t\t\tcustom_args = self.custom_args\n\t\telse:\n\t\t\tself.custom_args = {}\n\t\t\tcustom_args = self.custom_args\n\t\tfor key in args:\n\t\t\tif hasattr(self, key):\n\t\t\t\tif key in self.control_args:\n\t\t\t\t\tcontrol_args[key] = getattr(self, key)\n\t\t\t\telif key in self.nested_args:\n\t\t\t\t\tnested_args[key] = getattr(self, key)\n\t\t\t\telif key in self.custom_args:\n\t\t\t\t\tcustom_args[key] = getattr(self, key)\n\t\t\t\telse:\n\t\t\t\t\tprint(\"\\n\\t\\t\\tWARNING: No definition of the argument \\\"%s\\\" for this class!\" % key)\n\t\treturn control_args, nested_args, custom_args\n\tdef set_nested_custom_and_control_args(self, args):\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcontrol_args = {}\n\t\tnested_args = {}\n\t\tcustom_args = {}\n\t\tfor a in args:\n\t\t\tif a.startswith(self.control_arg_prefix):\n\t\t\t\tcontrol_args[a] = args[a]\n\t\t\telif a.startswith(self.nested_arg_prefix):\n\t\t\t\tnested_args[a] = args[a]\n\t\t\telse:\n\t\t\t\tcustom_args[a] = args[a]\n\t\treturn control_args, nested_args, custom_args\n/ansible/roles/os-ironic-conductor/templates/ironic-conductor.service.j2\n#!/bin/bash\n#\n# ironic-conductor       Ironic Conductor service\n#\n# chkconfig: 2345 99 01\n# description: Ironic Conductor service\n#\n# processname: ironic-conductor\n# pidfile: /var/run/ironic-conductor.pid\n\n. /etc/init.d/functions\n. /etc/sysconfig/network\n\nLOCKFILE=/var/lock/subsys/ironic-conductor\n\nstart() {\n\tif [! -f $LOCKFILE ]; then\n\t\techo -n $\"Starting ironic-conductor: \"\n\t\tdaemon --check ironic-conductor --user ir", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif extra_vars is not None:\n\t\t\tvars_dict['vars'] = vars_dict.get('vars', {})\n\t\t\tfor key, value in extra_vars.items():\n\t\t\t\tif key in vars_dict['vars']:\n\t\t\t\t\twarn(f\"Overwriting '{key}' with '{value}'\")\n\t\t\t\tvars_dict['vars'][key] = value\n\"\"\"\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t/pypi_install_script/python-grip-0.1.2.tar/setup.py\n#!/usr/bin/env python\n\nfrom setuptools import setup\n\nsetup(\n    name='python-grip',\n    version='0.1.2',\n    description='A Python wrapper for the grip command line tool.',\n    long_description='A Python wrapper for the grip command line tool.',\n    author='',\n    author_email='',\n    url='https://github.com/m31273/python-grip',\n    packages=['grip'],\n    install_requires=['six'],\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Topic :: Software Development :: Documentation',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'Topic :: Text Processing :: Markup',\n    ],\n    entry_points={\n        'console_scripts': [\n            'grip = grip.grip:main',\n        ]\n    }\n)\n/pycfiles/django_ultracache-0.4.3-py2.7/test_ultracache_tags.py\n# uncompyle6", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif not extra_vars:\n\t\treturn vars_dict\n\n\tfor var in extra_vars:\n\t\tif var['name'] not in vars_dict:\n\t\t\tvars_dict[var['name']] = var['value']\n\n\treturn vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars:\n\t\tvars_dict.update(extra_vars)\n\n\treturn vars_dict\n/README.rst\nSaltStack Deployment Tool\n=========================\n\nThis project is a set of python scripts that can be used to automate the deployment process of a salt-master.\n\nIt is designed to be used in a multi-environment setup, where the same configuration parameters are used to deploy salt-masters in different environments.\n\nIt uses the salt-api to deploy the salt-master and also to deploy the minions.\n\nUsage\n=====\n\nThe deployment tool is divided in two parts:\n\n* The ``salt-deploy`` command\n* The ``salt-deploy-minion`` command\n\nThe ``salt-deploy`` command is used to create a salt-master and is called from the command line.\n\nThe ``salt-deploy-minion`` command is used to create minions in an existing salt-master and it is called from the command line.\n\nThe ``salt-deploy`` command is used to deploy the salt-master and it requires a configuration file.\n\nThe ``salt-deploy-minion`` command is used to deploy the minions and it requires a configuration file.\n\nThe configuration file is divided in two parts:\n\n* The ``master`` configuration\n* The ``minions`` configuration\n\nThe ``master`` configuration is", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tif not isinstance(extra_vars, list):\n\t\textra_vars = [extra_vars]\n\n\tfor extra_var in extra_vars:\n\t\tif not isinstance(extra_var, dict):\n\t\t\textra_var = {extra_var: None}\n\n\t\tfor key in extra_var:\n\t\t\tvars_dict[key] = extra_var[key]\n\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif not extra_vars:\n\t\t\treturn vars_dict\n\t\tnew_dict = {}\n\t\tfor key, value in vars_dict.iteritems():\n\t\t\tnew_dict[key] = value\n\t\tfor key, value in extra_vars.iteritems():\n\t\t\tnew_dict[key] = value\n\t\treturn new_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars:\n\t\tfor var in extra_vars:\n\t\t\tvars_dict[var[0]] = var[1]\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif extra_vars is not None:\n\t\t\tfor extra_var in extra_vars:\n\t\t\t\tif '=' in extra_var:\n\t\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\t\t\tvars_dict[key] = value\n\treturn vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif not extra_vars:\n\t\t\treturn vars_dict\n\t\tif not isinstance(extra_vars, list):\n\t\t\traise Exception(\"extra-vars must be a list\")\n\t\tfor extra_var in extra_vars:\n\t\t\tif not isinstance(extra_var, dict):\n\t\t\t\traise Exception(\"extra-vars must be a list of dictionaries\")\n\t\t\tvars_dict.update(extra_var)\n\t\treturn vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\textra_vars = extra_vars or []\n\t\tfor var in extra_vars:\n\t\t\tif var not in vars_dict:\n\t\t\t\tvars_dict[var] = None\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif extra_vars is not None:\n\t\t\tvars_dict['vars'] = vars_dict.get('vars', {})\n\t\t\tfor key, value in extra_vars.items():\n\t\t\t\tif key in vars_dict['vars']:\n\t\t\t\t\twarn(f\"Overwriting '{key}' with '{value}'\")\n\t\t\t\tvars_dict['vars'][key] = value\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t/pypi_install_script/python-grip-0.1.2.tar/setup.py\n#!/usr/bin/env python\n\nfrom setuptools import setup\n\nsetup(\n    name='python-grip',\n    version='0.1.2',\n    description='A Python wrapper for the grip command line tool.',\n    long_description='A Python wrapper for the grip command line tool.',\n    author='',\n    author_email='',\n    url='https://github.com/m31273/python-grip',\n    packages=['grip'],\n    install_requires=['six'],\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Topic :: Software Development :: Documentation',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'Topic :: Text Processing :: Markup',\n    ],\n    entry_points={\n        'console_scripts': [\n            'grip = grip.grip:main',\n        ]\n    }\n)\n/pycfiles/django_ultracache-0.4.3-py2.7/test_ultracache_tags.py\n# uncompyle6", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif not extra_vars:\n\t\treturn vars_dict\n\n\tfor var in extra_vars:\n\t\tif var['name'] not in vars_dict:\n\t\t\tvars_dict[var['name']] = var['value']\n\n\treturn vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars:\n\t\tvars_dict.update(extra_vars)\n\n\treturn vars_dict\n/README.rst\nSaltStack Deployment Tool\n=========================\n\nThis project is a set of python scripts that can be used to automate the deployment process of a salt-master.\n\nIt is designed to be used in a multi-environment setup, where the same configuration parameters are used to deploy salt-masters in different environments.\n\nIt uses the salt-api to deploy the salt-master and also to deploy the minions.\n\nUsage\n=====\n\nThe deployment tool is divided in two parts:\n\n* The ``salt-deploy`` command\n* The ``salt-deploy-minion`` command\n\nThe ``salt-deploy`` command is used to create a salt-master and is called from the command line.\n\nThe ``salt-deploy-minion`` command is used to create minions in an existing salt-master and it is called from the command line.\n\nThe ``salt-deploy`` command is used to deploy the salt-master and it requires a configuration file.\n\nThe ``salt-deploy-minion`` command is used to deploy the minions and it requires a configuration file.\n\nThe configuration file is divided in two parts:\n\n* The ``master`` configuration\n* The ``minions`` configuration\n\nThe ``master`` configuration is", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tif not isinstance(extra_vars, list):\n\t\textra_vars = [extra_vars]\n\n\tfor extra_var in extra_vars:\n\t\tif not isinstance(extra_var, dict):\n\t\t\textra_var = {extra_var: None}\n\n\t\tfor key in extra_var:\n\t\t\tvars_dict[key] = extra_var[key]\n\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif not extra_vars:\n\t\t\treturn vars_dict\n\t\tnew_dict = {}\n\t\tfor key, value in vars_dict.iteritems():\n\t\t\tnew_dict[key] = value\n\t\tfor key, value in extra_vars.iteritems():\n\t\t\tnew_dict[key] = value\n\t\treturn new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars:\n\t\tfor var in extra_vars:\n\t\t\tvars_dict[var[0]] = var[1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif extra_vars is not None:\n\t\t\tfor extra_var in extra_vars:\n\t\t\t\tif '=' in extra_var:\n\t\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\t\t\tvars_dict[key] = value\n\treturn vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif not extra_vars:\n\t\t\treturn vars_dict\n\t\tif not isinstance(extra_vars, list):\n\t\t\traise Exception(\"extra-vars must be a list\")\n\t\tfor extra_var in extra_vars:\n\t\t\tif not isinstance(extra_var, dict):\n\t\t\t\traise Exception(\"extra-vars must be a list of dictionaries\")\n\t\t\tvars_dict.update(extra_var)\n\t\treturn vars_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\textra_vars = extra_vars or []\n\t\tfor var in extra_vars:\n\t\t\tif var not in vars_dict:\n\t\t\t\tvars_dict[var] = None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tansible_args = ansible_args or {}\n\t\tansible_args.update(dict(\n\t\t\tplaybook=playbook_path,\n\t\t\tverbose=verbose,\n\t\t\textra_vars=extra_vars,\n\t\t\tinventory=ir_plugin.inventory_file,\n\t\t))\n\t\treturn ir_workspace.execute_cli('ansible-playbook', **ansible_args)\n/ir_ansible/models/ansible_task.py\nfrom typing import List\n\nfrom ir_ansible.models.ansible_task_result import AnsibleTaskResult\nfrom ir_ansible.models.ansible_task_state import AnsibleTaskState\nfrom ir_ansible.models.ansible_task_type import AnsibleTaskType\nfrom ir_ansible.models.ansible_task_type_enum import AnsibleTaskTypeEnum\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\n\n\t# 1. Load the plugin object\n\t# 2. Load the plugin configuration\n\t# 3. Initialize the Ansible runner\n\t# 4. Run the playbook\n\t# 5. Return the Ansible output\n\t# 6. Cleanup\n\n\t# Load the plugin object\n\tplugin = ir_plugin\n\n\t# Load the plugin configuration\n\tconfig = plugin.config()\n\n\t# Initialize the Ansible runner\n\trunner = AnsibleRunner(ir_workspace, config)\n\n\t# Run the playbook\n\trunner.run_playbook(playbook_path, verbose=verbose, extra_vars=extra_vars, ansible_args=ansible_args)\n\n\t# Return the Ansible output\n\treturn runner.output\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\t# TODO: Add support for ansible-playbook extra_vars\n\t\t# TODO: Add support for ansible-playbook -v\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\t# TODO: Add support for ansible-playbook -i\n\t# TODO: Add support for ansible-playbook -l\n\t# TODO: Add support for ansible-playbook -t\n\t# TODO: Add support for ansible-playbook -e\n\t# TODO: Add support for ansible-playbook --syntax-check\n\t# TODO: Add support for ansible-playbook --list-hosts\n\t# TODO: Add support for ansible-playbook --list-tasks\n\t# TODO: Add support for ansible-playbook --list-tags\n\t# TODO: Add support for ansible-playbook --skip-tags\n\t# TODO: Add support for ansible-playbook --start-at-task\n\t# TODO: Add support for ansible-playbook --ask-vault-pass\n\t# TODO: Add support for ansible-playbook --vault-password-file\n\t# TODO: Add support for ansible-playbook --ask-pass\n\t# TODO: Add support for ansible-playbook --ask-sudo-pass\n\t# TODO: Add support for ansible-playbook --become\n\t# TODO: Add support for ansible-playbook --become-method\n\t# TODO", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\treturn ir_plugin.ansible_playbook(ir_workspace, playbook_path, verbose, extra_vars, ansible_args)\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t/pypi_install_script/django-waffle-django-1.4.5/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='django-waffle-django',\n    version='1.4.5',\n    author='',\n    author_email='',\n    packages=find_packages(),\n    url='https://github.com/jsocol/django-waffle',\n    license='BSD',\n    description='''\n        A feature flagging app for Django.\n    ''',\n    long_description=open('README.rst').read(),\n    install_requires=[\n        'django>=1.4',\n    ],\n    include_package_data=True,\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Web Environment',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\t# TODO: make this use ansible_args\n        if verbose:\n            ansible_args = dict(ansible_args or {})\n            ansible_args['verbosity'] = verbose\n        if extra_vars:\n            ansible_args = dict(ansible_args or {})\n            ansible_args['extra_vars'] = extra_vars\n        return ir_plugin.execute(\n            ['ansible-playbook', '-i', ir_workspace.inventory_path, playbook_path] +\n            list(ansible_args.get('extra_args', [])),\n            cwd=ir_workspace.git_root_path,\n            env={\n                'ANSIBLE_HOST_KEY_CHECKING': 'False',\n                'ANSIBLE_RETRY_FILES_ENABLED': 'False',\n            },\n        )\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tresult = ir_plugin.execute_command(\n\t\t\t['ansible-playbook', playbook_path,\n\t\t\t\t'--inventory-file={}'.format(ir_workspace.inventory_path)],\n\t\t\textra_vars=extra_vars, ansible_args=ansible_args,\n\t\t\tverbose=verbose)\n\t\treturn result\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tif verbose:\n\t\t\tansible_args['verbose'] = verbose\n\t\tif extra_vars:\n\t\t\tansible_args['extra_vars'] = extra_vars\n\n\t\t# Find the ansible-playbook location\n\t\tansible_playbook_path = ir_plugin.ansible_module_path('ansible-playbook')\n\t\tir_workspace.logger.info('Ansible-play-book: {}'.format(ansible_playbook_path))\n\n\t\t# Invoke Ansible-playbook\n\t\tif ansible_args:\n\t\t\tir_workspace.logger.debug('Ansible-playbook args: {}'.format(ansible_args))\n\t\t\tir_workspace.logger.debug('Ansible-playbook cmd: {}'.format(' '.join([ansible_playbook_path, playbook_path] + ['-s'] + ['--{}'.format(k) for k, v in ansible_args.items() if v])))\n\t\t\tsubprocess.check_call([ansible_playbook_path, playbook_path] + ['-s'] + ['--{}'.format(k) for k, v in ansible_args.items() if v])\n\t\telse:\n\t\t\tir_workspace.logger.debug('Ansible-playbook cmd: {}'.format(' '.join([ansible_playbook_path, playbook_path])))\n\t\t\tsubprocess.check_call([ansible_playbook_path, playbook_path])\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tverbose = verbose or ir_plugin.verbose\n\t\textra_vars = extra_vars or ir_plugin.extra_vars\n\t\tansible_args = ansible_args or ir_plugin.ansible_args\n\t\targs = ['ansible-playbook', '-i', ir_workspace.hosts_file,\n\t\t\tplaybook_path]\n\t\tif verbose:\n\t\t\targs.append('-' + verbose)\n\t\tif extra_vars:\n\t\t\targs.append('--extra-vars')\n\t\t\targs.append(' '.join(\n\t\t\t\t['%s=%s' % (k, v) for k, v in extra_vars.items()]))\n\t\tif ansible_args:\n\t\t\targs.extend(ansible_args)\n\t\ttry:\n\t\t\treturn ir_plugin.run(args)\n\t\texcept SubprocessError as e:\n\t\t\traise AnsibleError(e)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\treturn _ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tansible_args = ansible_args or {}\n\t\tansible_args.update(dict(\n\t\t\tplaybook=playbook_path,\n\t\t\tverbose=verbose,\n\t\t\textra_vars=extra_vars,\n\t\t\tinventory=ir_plugin.inventory_file,\n\t\t))\n\t\treturn ir_workspace.execute_cli('ansible-playbook', **ansible_args)\n/ir_ansible/models/ansible_task.py\nfrom typing import List\n\nfrom ir_ansible.models.ansible_task_result import AnsibleTaskResult\nfrom ir_ansible.models.ansible_task_state import AnsibleTaskState\nfrom ir_ansible.models.ansible_task_type import AnsibleTaskType\nfrom ir_ansible.models.ansible_task_type_enum import AnsibleTaskTypeEnum\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\n\n\t# 1. Load the plugin object\n\t# 2. Load the plugin configuration\n\t# 3. Initialize the Ansible runner\n\t# 4. Run the playbook\n\t# 5. Return the Ansible output\n\t# 6. Cleanup\n\n\t# Load the plugin object\n\tplugin = ir_plugin\n\n\t# Load the plugin configuration\n\tconfig = plugin.config()\n\n\t# Initialize the Ansible runner\n\trunner = AnsibleRunner(ir_workspace, config)\n\n\t# Run the playbook\n\trunner.run_playbook(playbook_path, verbose=verbose, extra_vars=extra_vars, ansible_args=ansible_args)\n\n\t# Return the Ansible output\n\treturn runner.output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\t# TODO: Add support for ansible-playbook extra_vars\n\t\t# TODO: Add support for ansible-playbook -v\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\t# TODO: Add support for ansible-playbook -i\n\t# TODO: Add support for ansible-playbook -l\n\t# TODO: Add support for ansible-playbook -t\n\t# TODO: Add support for ansible-playbook -e\n\t# TODO: Add support for ansible-playbook --syntax-check\n\t# TODO: Add support for ansible-playbook --list-hosts\n\t# TODO: Add support for ansible-playbook --list-tasks\n\t# TODO: Add support for ansible-playbook --list-tags\n\t# TODO: Add support for ansible-playbook --skip-tags\n\t# TODO: Add support for ansible-playbook --start-at-task\n\t# TODO: Add support for ansible-playbook --ask-vault-pass\n\t# TODO: Add support for ansible-playbook --vault-password-file\n\t# TODO: Add support for ansible-playbook --ask-pass\n\t# TODO: Add support for ansible-playbook --ask-sudo-pass\n\t# TODO: Add support for ansible-playbook --become\n\t# TODO: Add support for ansible-playbook --become-method\n\t# TODO", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\treturn ir_plugin.ansible_playbook(ir_workspace, playbook_path, verbose, extra_vars, ansible_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t/pypi_install_script/django-waffle-django-1.4.5/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='django-waffle-django',\n    version='1.4.5',\n    author='',\n    author_email='',\n    packages=find_packages(),\n    url='https://github.com/jsocol/django-waffle',\n    license='BSD',\n    description='''\n        A feature flagging app for Django.\n    ''',\n    long_description=open('README.rst').read(),\n    install_requires=[\n        'django>=1.4',\n    ],\n    include_package_data=True,\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Web Environment',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\t# TODO: make this use ansible_args\n        if verbose:\n            ansible_args = dict(ansible_args or {})\n            ansible_args['verbosity'] = verbose\n        if extra_vars:\n            ansible_args = dict(ansible_args or {})\n            ansible_args['extra_vars'] = extra_vars\n        return ir_plugin.execute(\n            ['ansible-playbook', '-i', ir_workspace.inventory_path, playbook_path] +\n            list(ansible_args.get('extra_args', [])),\n            cwd=ir_workspace.git_root_path,\n            env={\n                'ANSIBLE_HOST_KEY_CHECKING': 'False',\n                'ANSIBLE_RETRY_FILES_ENABLED': 'False',\n            },\n        )\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tresult = ir_plugin.execute_command(\n\t\t\t['ansible-playbook', playbook_path,\n\t\t\t\t'--inventory-file={}'.format(ir_workspace.inventory_path)],\n\t\t\textra_vars=extra_vars, ansible_args=ansible_args,\n\t\t\tverbose=verbose)\n\t\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tif verbose:\n\t\t\tansible_args['verbose'] = verbose\n\t\tif extra_vars:\n\t\t\tansible_args['extra_vars'] = extra_vars\n\n\t\t# Find the ansible-playbook location\n\t\tansible_playbook_path = ir_plugin.ansible_module_path('ansible-playbook')\n\t\tir_workspace.logger.info('Ansible-play-book: {}'.format(ansible_playbook_path))\n\n\t\t# Invoke Ansible-playbook\n\t\tif ansible_args:\n\t\t\tir_workspace.logger.debug('Ansible-playbook args: {}'.format(ansible_args))\n\t\t\tir_workspace.logger.debug('Ansible-playbook cmd: {}'.format(' '.join([ansible_playbook_path, playbook_path] + ['-s'] + ['--{}'.format(k) for k, v in ansible_args.items() if v])))\n\t\t\tsubprocess.check_call([ansible_playbook_path, playbook_path] + ['-s'] + ['--{}'.format(k) for k, v in ansible_args.items() if v])\n\t\telse:\n\t\t\tir_workspace.logger.debug('Ansible-playbook cmd: {}'.format(' '.join([ansible_playbook_path, playbook_path])))\n\t\t\tsubprocess.check_call([ansible_playbook_path, playbook_path])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\tverbose = verbose or ir_plugin.verbose\n\t\textra_vars = extra_vars or ir_plugin.extra_vars\n\t\tansible_args = ansible_args or ir_plugin.ansible_args\n\t\targs = ['ansible-playbook', '-i', ir_workspace.hosts_file,\n\t\t\tplaybook_path]\n\t\tif verbose:\n\t\t\targs.append('-' + verbose)\n\t\tif extra_vars:\n\t\t\targs.append('--extra-vars')\n\t\t\targs.append(' '.join(\n\t\t\t\t['%s=%s' % (k, v) for k, v in extra_vars.items()]))\n\t\tif ansible_args:\n\t\t\targs.extend(ansible_args)\n\t\ttry:\n\t\t\treturn ir_plugin.run(args)\n\t\texcept SubprocessError as e:\n\t\t\traise AnsibleError(e)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\t\treturn _ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t \n\tansible_cli = ir_plugin.cli_command(\"ansible\")\n\tansible_cli += cli_args\n\tansible_cli += [\"-i\", ir_workspace.ansible_inventory_file]\n\tansible_cli += [\"-e\", json.dumps(vars_dict)]\n\treturn run(ansible_cli)\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n```\n\n## Usage\n\nPlease refer to the Infrared documentation: [Usage](https://github.com/redhat-performance/infrared/blob/master/docs/usage.md)\n/docs/plugins/rally.md\n---\ntitle: Rally\ndescription: Rally plugin\n---\n\n## Description\n\nThe Rally plugin is an integration with the Rally framework and allows you to:\n\n* Run Rally benchmarks\n* Upload and query Rally results\n\n## Installation\n\n```\n$ pip install infrared\n$ infrared plugin add https://github.com/redhat-performance/infrared-rally-plugin\n```\n\n## Usage\n\n```\n$ infrared rally <command>\n```\n\n### Upload results\n\nUploads the rally result from the current workspace to the Rally server. The result can be viewed by navigating to the following URL: `http://<rally_server_ip>:8080/ui/index.html#results`\n\n```\n$ infrared rally upload\n```\n\n### Run a benchmark\n\nRuns the `rally` benchmark on the current workspace.\n\n```\n$ infrared rally run\n```\n\n### Download results\n\nDownloads the rally result from the Rally server to the current workspace.\n\n```\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\tif vars_dict:\n\t\t\tcli_args.append(\"extra-vars\")\n\t\t\tcli_args.append(json.dumps(vars_dict))\n\n\t\t# Call Ansible CLI with the passed arguments\n\t\tir_log.info(\"Running Ansible CLI with {0}\".format(cli_args))\n\t\tir_log.debug(\"Ansible CLI is: {0}\".format(cli_args))\n\t\t# Ansible CLI returns an ansible.runner.results.TaskResult object\n\t\t# This object has a results attribute which is a dict.\n\t\t# The dict has the following key value pairs:\n\t\t#    'contacted' - a dict of all the contacted hosts.\n\t\t#       'ip_address' - the IP address of the host\n\t\t#       'changed' - a bool whether the host changed\n\t\t#       'rc' - the return code of the host\n\t\t#      'stderr' - stderr from the host\n\t\t#      'stdout' - stdout from the host\n\t\t#       'end' - the time the host finished\n\t\t#      'start' - the time the host started\n\t\t#       'invocation' - the dict of the command that was run\n\t\t#       'warnings' - a list of ansible warnings\n\t\t#       'failed' - a bool whether the host failed\n\t\t#      'skipped' - a bool whether the host was skipped\n\t", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n\n\tif not vars_dict:\n\t\tvars_dict = {}\n\n\tif not cli_args:\n\t\tcli_args = []\n\n\tif ir_plugin.name == \"test\":\n\t\t# TODO: This should be removed when \"test\" plugin will be removed in favor of \"infra\"\n\t\tir_plugin.name = \"infra\"\n\n\tlogger.info(\"Running {} plugin with vars: {}\".format(ir_plugin.name, vars_dict))\n\n\tcli_args = [\n\t\tir_plugin.name,\n\t\t\"-i\",\n\t\tir_workspace.inventory_file,\n\t\t\"--extra-vars\",\n\t\tjson.dumps(vars_dict)\n\t] + cli_args\n\n\treturn ir_workspace._run_ansible_cli(cli_args)\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n\n\t# Create Ansible CLI command\n\tcli_command = \"ansible-playbook\"\n\tcli_command += \" {} --extra-vars '{}'\".format(\n\t\tcli_args, json.dumps(vars_dict)\n\t)\n\n\t# Run Ansible CLI\n\t# TODO: Should we run it inside the container?\n\t# \t- In case of an error the error will be\n\t# \tvisible in the container.\n\t# \t- If we do it inside the container the error\n\t# \twill be hidden.\n\t# \t- Ansible in the container is the real Ansible,\n\t# \tit's not the one in the host.\n\t# \t- We can't just do `ansible-playbook` in the container,\n\t# \tbecause it will be like running ansible outside of the container\n\t# \tand the container has no ansible in it.\n\tansible_results = ir_workspace.run(cli_command)\n\n\t# Print ansible results\n\tir_plugin.logger.info(ansible_results)\n\n\treturn ansible_results\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\t# TODO: add support for multiple inv files.\n\t\tif not ir_workspace.inventory_file:\n\t\t\tir_workspace.inventory_file = ir_workspace.get_inventory_file()\n\n\t\t# TODO: create a default host_vars file if not exists\n\t\t# TODO: create a default group_vars file if not exists\n\t\t# TODO: create a default vars file if not exists\n\n\t\t# TODO: create a default ansible.cfg if not exists\n\n\t\t# TODO: support --tags and --skip-tags\n\t\t# TODO: support --skip-tags-v2\n\n\t\t# TODO: support --list-hosts\n\t\t# TODO: support --list-tasks\n\t\t# TODO: support --list-tags\n\t\t# TODO: support --syntax-check\n\n\t\tcli_args.append(\"-i\")\n\t\tcli_args.append(ir_workspace.inventory_file)\n\t\tcli_args.append(\"--inventory-file\")\n\t\tcli_args.append(ir_workspace.get_inventory_file())\n\n\t\t# TODO: Add support for all ansible cli flags\n\t\t# TODO: Add support for --force-handlers\n\t\t# TODO: Add support for --flush-cache\n\t\t# TODO: Add support for --forks\n\t\t# TODO: Add support for --limit\n\t\t# TODO: Add support for --module-path\n\t\t# TODO: Add support for --", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\ttry:\n\t\t\tansible_runner = AnsibleRunner(cli_args, vars_dict, ir_workspace, ir_plugin)\n\t\t\tansible_runner.run()\n\t\t\treturn ansible_runner.get_results()\n\t\texcept AnsibleRunnerException:\n\t\t\ttraceback.print_exc()\n\t\t\treturn {'failed': True,'msg': 'Failure during running ansible-runner'}\n```\n\n## Run Ansible Playbook\n\nThe `run` method of the `AnsibleRunner` class is the one that actually runs the playbook:\n\n```python", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\treturn ir_plugin.run_command(cli_args, vars_dict=vars_dict, ir_workspace=ir_workspace)\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n\n\tif not ir_workspace:\n\t\tir_workspace = Workspace(os.getcwd())\n\t# get the current plugin\n\tir_plugin = PluginLoader.get_plugin(ir_workspace, cli_args.plugin_name)\n\n\tinventory_path = ir_workspace.get_inventory_path(ir_plugin.name)\n\t# TODO: add support for multiple groups\n\thosts = ir_workspace.get_group(ir_plugin.name)\n\n\t# TODO: add support for multiple playbook\n\tplaybook_path = ir_workspace.get_playbook_path(ir_plugin.name)\n\t# TODO: add support for multiple vars_files\n\tvars_path = ir_workspace.get_vars_path(ir_plugin.name)\n\n\t# read the vars file\n\tvars_file_path = Path(vars_path)\n\tvars_file = vars_file_path.read_text()\n\tvars_dict = yaml.safe_load(vars_file)\n\n\t# add the vars to the ansible extra vars\n\tif vars_dict:\n\t\tcli_args.extra_vars.update(vars_dict)\n\n\t# TODO: add support for multiple extra_vars_files\n\textra_vars_file_path = Path(ir_workspace.get_extra_vars_path(ir_plugin.name))", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t \n\t# Check if the command line arguments contains a playbook\n\tif not cli_args:\n\t\traise CommandException(\n\t\t\t\"No playbook name specified. \"\n\t\t\t\"Please provide a playbook name in the command line, or \"\n\t\t\t\"use the --playbook parameter.\")\n\tif \"--playbook\" not in cli_args:\n\t\tcli_args.insert(0, \"--playbook\")\n\tif \"--extra-vars\" not in cli_args:\n\t\tcli_args.insert(1, \"--extra-vars\")\n\n\t# Run the playbook\n\treturn ir_plugin.run_playbook(\n\t\tir_workspace,\n\t\tcli_args[cli_args.index(\"--playbook\") + 1],\n\t\tcli_args[cli_args.index(\"--extra-vars\") + 1],\n\t\tvars_dict)\n/ansible_runner/test_runner.py\nimport os\nimport json\nimport re\nimport time\nimport sys\nimport logging\nimport shutil\nimport subprocess\nimport yaml\n\nfrom ansible.parsing.dataloader import DataLoader\nfrom ansible.vars.manager import VariableManager\nfrom ansible.inventory.manager import InventoryManager\nfrom ansible.utils.display import Display\nfrom ansible.executor.playbook_executor import PlaybookExecutor\nfrom ansible.plugins.callback import CallbackBase\nfrom ansible.errors import AnsibleError\nfrom ansible.utils.template"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t \n\tansible_cli = ir_plugin.cli_command(\"ansible\")\n\tansible_cli += cli_args\n\tansible_cli += [\"-i\", ir_workspace.ansible_inventory_file]\n\tansible_cli += [\"-e\", json.dumps(vars_dict)]\n\treturn run(ansible_cli)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n```\n\n## Usage\n\nPlease refer to the Infrared documentation: [Usage](https://github.com/redhat-performance/infrared/blob/master/docs/usage.md)\n/docs/plugins/rally.md\n---\ntitle: Rally\ndescription: Rally plugin\n---\n\n## Description\n\nThe Rally plugin is an integration with the Rally framework and allows you to:\n\n* Run Rally benchmarks\n* Upload and query Rally results\n\n## Installation\n\n```\n$ pip install infrared\n$ infrared plugin add https://github.com/redhat-performance/infrared-rally-plugin\n```\n\n## Usage\n\n```\n$ infrared rally <command>\n```\n\n### Upload results\n\nUploads the rally result from the current workspace to the Rally server. The result can be viewed by navigating to the following URL: `http://<rally_server_ip>:8080/ui/index.html#results`\n\n```\n$ infrared rally upload\n```\n\n### Run a benchmark\n\nRuns the `rally` benchmark on the current workspace.\n\n```\n$ infrared rally run\n```\n\n### Download results\n\nDownloads the rally result from the Rally server to the current workspace.\n\n```\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\tif vars_dict:\n\t\t\tcli_args.append(\"extra-vars\")\n\t\t\tcli_args.append(json.dumps(vars_dict))\n\n\t\t# Call Ansible CLI with the passed arguments\n\t\tir_log.info(\"Running Ansible CLI with {0}\".format(cli_args))\n\t\tir_log.debug(\"Ansible CLI is: {0}\".format(cli_args))\n\t\t# Ansible CLI returns an ansible.runner.results.TaskResult object\n\t\t# This object has a results attribute which is a dict.\n\t\t# The dict has the following key value pairs:\n\t\t#    'contacted' - a dict of all the contacted hosts.\n\t\t#       'ip_address' - the IP address of the host\n\t\t#       'changed' - a bool whether the host changed\n\t\t#       'rc' - the return code of the host\n\t\t#      'stderr' - stderr from the host\n\t\t#      'stdout' - stdout from the host\n\t\t#       'end' - the time the host finished\n\t\t#      'start' - the time the host started\n\t\t#       'invocation' - the dict of the command that was run\n\t\t#       'warnings' - a list of ansible warnings\n\t\t#       'failed' - a bool whether the host failed\n\t\t#      'skipped' - a bool whether the host was skipped\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n\n\tif not vars_dict:\n\t\tvars_dict = {}\n\n\tif not cli_args:\n\t\tcli_args = []\n\n\tif ir_plugin.name == \"test\":\n\t\t# TODO: This should be removed when \"test\" plugin will be removed in favor of \"infra\"\n\t\tir_plugin.name = \"infra\"\n\n\tlogger.info(\"Running {} plugin with vars: {}\".format(ir_plugin.name, vars_dict))\n\n\tcli_args = [\n\t\tir_plugin.name,\n\t\t\"-i\",\n\t\tir_workspace.inventory_file,\n\t\t\"--extra-vars\",\n\t\tjson.dumps(vars_dict)\n\t] + cli_args\n\n\treturn ir_workspace._run_ansible_cli(cli_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n\n\t# Create Ansible CLI command\n\tcli_command = \"ansible-playbook\"\n\tcli_command += \" {} --extra-vars '{}'\".format(\n\t\tcli_args, json.dumps(vars_dict)\n\t)\n\n\t# Run Ansible CLI\n\t# TODO: Should we run it inside the container?\n\t# \t- In case of an error the error will be\n\t# \tvisible in the container.\n\t# \t- If we do it inside the container the error\n\t# \twill be hidden.\n\t# \t- Ansible in the container is the real Ansible,\n\t# \tit's not the one in the host.\n\t# \t- We can't just do `ansible-playbook` in the container,\n\t# \tbecause it will be like running ansible outside of the container\n\t# \tand the container has no ansible in it.\n\tansible_results = ir_workspace.run(cli_command)\n\n\t# Print ansible results\n\tir_plugin.logger.info(ansible_results)\n\n\treturn ansible_results\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\t# TODO: add support for multiple inv files.\n\t\tif not ir_workspace.inventory_file:\n\t\t\tir_workspace.inventory_file = ir_workspace.get_inventory_file()\n\n\t\t# TODO: create a default host_vars file if not exists\n\t\t# TODO: create a default group_vars file if not exists\n\t\t# TODO: create a default vars file if not exists\n\n\t\t# TODO: create a default ansible.cfg if not exists\n\n\t\t# TODO: support --tags and --skip-tags\n\t\t# TODO: support --skip-tags-v2\n\n\t\t# TODO: support --list-hosts\n\t\t# TODO: support --list-tasks\n\t\t# TODO: support --list-tags\n\t\t# TODO: support --syntax-check\n\n\t\tcli_args.append(\"-i\")\n\t\tcli_args.append(ir_workspace.inventory_file)\n\t\tcli_args.append(\"--inventory-file\")\n\t\tcli_args.append(ir_workspace.get_inventory_file())\n\n\t\t# TODO: Add support for all ansible cli flags\n\t\t# TODO: Add support for --force-handlers\n\t\t# TODO: Add support for --flush-cache\n\t\t# TODO: Add support for --forks\n\t\t# TODO: Add support for --limit\n\t\t# TODO: Add support for --module-path\n\t\t# TODO: Add support for --", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\ttry:\n\t\t\tansible_runner = AnsibleRunner(cli_args, vars_dict, ir_workspace, ir_plugin)\n\t\t\tansible_runner.run()\n\t\t\treturn ansible_runner.get_results()\n\t\texcept AnsibleRunnerException:\n\t\t\ttraceback.print_exc()\n\t\t\treturn {'failed': True,'msg': 'Failure during running ansible-runner'}\n```\n\n## Run Ansible Playbook\n\nThe `run` method of the `AnsibleRunner` class is the one that actually runs the playbook:\n\n```python", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\treturn ir_plugin.run_command(cli_args, vars_dict=vars_dict, ir_workspace=ir_workspace)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t\n\n\tif not ir_workspace:\n\t\tir_workspace = Workspace(os.getcwd())\n\t# get the current plugin\n\tir_plugin = PluginLoader.get_plugin(ir_workspace, cli_args.plugin_name)\n\n\tinventory_path = ir_workspace.get_inventory_path(ir_plugin.name)\n\t# TODO: add support for multiple groups\n\thosts = ir_workspace.get_group(ir_plugin.name)\n\n\t# TODO: add support for multiple playbook\n\tplaybook_path = ir_workspace.get_playbook_path(ir_plugin.name)\n\t# TODO: add support for multiple vars_files\n\tvars_path = ir_workspace.get_vars_path(ir_plugin.name)\n\n\t# read the vars file\n\tvars_file_path = Path(vars_path)\n\tvars_file = vars_file_path.read_text()\n\tvars_dict = yaml.safe_load(vars_file)\n\n\t# add the vars to the ansible extra vars\n\tif vars_dict:\n\t\tcli_args.extra_vars.update(vars_dict)\n\n\t# TODO: add support for multiple extra_vars_files\n\textra_vars_file_path = Path(ir_workspace.get_extra_vars_path(ir_plugin.name))", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\t \n\t# Check if the command line arguments contains a playbook\n\tif not cli_args:\n\t\traise CommandException(\n\t\t\t\"No playbook name specified. \"\n\t\t\t\"Please provide a playbook name in the command line, or \"\n\t\t\t\"use the --playbook parameter.\")\n\tif \"--playbook\" not in cli_args:\n\t\tcli_args.insert(0, \"--playbook\")\n\tif \"--extra-vars\" not in cli_args:\n\t\tcli_args.insert(1, \"--extra-vars\")\n\n\t# Run the playbook\n\treturn ir_plugin.run_playbook(\n\t\tir_workspace,\n\t\tcli_args[cli_args.index(\"--playbook\") + 1],\n\t\tcli_args[cli_args.index(\"--extra-vars\") + 1],\n\t\tvars_dict)\n/ansible_runner/test_runner.py\nimport os\nimport json\nimport re\nimport time\nimport sys\nimport logging\nimport shutil\nimport subprocess\nimport yaml\n\nfrom ansible.parsing.dataloader import DataLoader\nfrom ansible.vars.manager import VariableManager\nfrom ansible.inventory.manager import InventoryManager\nfrom ansible.utils.display import Display\nfrom ansible.executor.playbook_executor import PlaybookExecutor\nfrom ansible.plugins.callback import CallbackBase\nfrom ansible.errors import AnsibleError\nfrom ansible.utils.template", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif parser_name == 'ospd':\n\t\t\t# If a value is a string, do nothing.\n\t\t\tif not isinstance(values_dict['ospd_type'], str):\n\t\t\t\tvalues_dict['ospd_type'] = str(values_dict['ospd_type'])\n\t\telif parser_name == 'virsh':\n\t\t\t# If a value is a string, do nothing.\n\t\t\tif not isinstance(values_dict['virsh_type'], str):\n\t\t\t\tvalues_dict['virsh_type'] = str(values_dict['virsh_type'])\n\t\telif parser_name =='main':\n\t\t\t# If a value is a string, do nothing.\n\t\t\tif not isinstance(values_dict['main_type'], str):\n\t\t\t\tvalues_dict['main_type'] = str(values_dict['main_type'])\n\t\telse:\n\t\t\tprint('Unknown parser', parser_name)\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tself.log.debug('_convert_non_cli_args: parser_name=%s values_dict=%s' % (parser_name, values_dict))\n\n\t# Do not modify the input dict\n\tvalues_dict = values_dict.copy()\n\n\t# Check that we have the expected values\n\tif parser_name in self.arg_types:\n\t\tfor arg in self.arg_types[parser_name]:\n\t\t\tif arg not in values_dict:\n\t\t\t\tself.log.error('Missing argument: %s' % arg)\n\t\t\t\tself.usage()\n\t\t\t\texit(1)\n\n\t# Convert values\n\tfor arg in self.arg_types[parser_name]:\n\t\tif arg in values_dict:\n\t\t\tif self.arg_types[parser_name][arg] == 'int':\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[arg] = int(values_dict[arg])\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.log.error('Cannot cast %s to int' % arg)\n\t\t\t\t\tself.usage()\n\t\t\t\t\texit(1)\n\t\t\telif self.arg_types[parser_name][arg] == 'float':\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[arg] = float(values_dict[arg])\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.log.error('Cannot cast %s to float' %", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif not values_dict:\n\t\t\treturn\n\t\tif parser_name =='main':\n\t\t\tself._convert_main_args(values_dict)\n\t\telif parser_name == 'virsh':\n\t\t\tself._convert_virsh_args(values_dict)\n\t\telif parser_name == 'ospd':\n\t\t\tself._convert_ospd_args(values_dict)\n\n\tdef _convert_main_args(self, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif not values_dict:\n\t\t\treturn\n\t\tif values_dict.get('memory'):\n\t\t\tvalues_dict['memory'] = int(values_dict['memory'])\n\t\tif values_dict.get('cpumode'):\n\t\t\tvalues_dict['cpumode'] = str(values_dict['cpumode'])\n\t\tif values_dict.get('numa'):\n\t\t\tvalues_dict['numa'] = str2bool(values_dict['numa'])\n\t\tif values_dict.get('vcpus'):\n\t\t\tvalues_dict['vcpus'] = int(values_dict['vcpus'])\n\t\tif values_dict.get('smp'):\n\t\t\tvalues_dict['", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif parser_name == \"main\":\n\t\t\tself._convert_non_cli_args_main(values_dict)\n\t\telif parser_name == \"virsh\":\n\t\t\tself._convert_non_cli_args_virsh(values_dict)\n\t\telif parser_name == \"ospd\":\n\t\t\tself._convert_non_cli_args_ospd(values_dict)\n\t\telif parser_name == \"os\":\n\t\t\tself._convert_non_cli_args_os(values_dict)\n\t\telif parser_name == \"os-admin\":\n\t\t\tself._convert_non_cli_args_os_admin(values_dict)\n\t\telse:\n\t\t\tself._convert_non_cli_args_default(values_dict)\n\n\tdef _convert_non_cli_args_main(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_virsh(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_ospd(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_os(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_os", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif parser_name!='main':\n\t\t\treturn\n\t\tif values_dict.get('debug_level', False):\n\t\t\tvalues_dict['debug_level'] = int(values_dict['debug_level'])\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tpass\n\n@classmethod", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\t# TODO: Add support for other types\n\t\t# https://docs.python.org/3/library/argparse.html#type\n\n\tdef _process_non_cli_args(self):\n\t\t\"\"\"\n\t\tProcess command line arguments.\n\nThis function is used to process the arguments passed to the command.\n\n:return: The values_dict\n:rtype: dict\n:raises: SystemExit\n\t\t\"\"\"\n\t\tparser = self._get_cli_parser()\n\t\tvalues_dict = vars(parser.parse_args())\n\t\tself._convert_non_cli_args(parser.prog, values_dict)\n\t\treturn values_dict\n\n\tdef _get_cli_parser(self):\n\t\t\"\"\"\n\t\tGet the command line parser.\n\n:return: The parser instance\n:rtype: argparse.ArgumentParser\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=self.description)\n\t\tself._add_cli_arguments(parser)\n\t\treturn parser\n\n\tdef _add_cli_arguments(self, parser):\n\t\t\"\"\"\n\t\tAdd command line arguments.\n\nThis function is used to add command line arguments to the parser.\n\n:param parser: The parser instance\n:type parser: argparse.ArgumentParser\n\t\t\"\"\"\n\t\t# TODO: Add support for other types\n\t\t# https://docs.python.org/3/library/arg", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\t# NOTE: This is a hack to convert the non-cli args to the correct type\n\t\t# (e.g. int for --port, bool for --wait, etc).\n\t\t# This is necessary because argparse only supports string values.\n\t\t# We have to do it manually because we need to perform validation.\n\t\tnon_cli_args_map = {\n\t\t\t\"main\": {\n\t\t\t\t\"port\": int,\n\t\t\t\t\"wait\": bool,\n\t\t\t},\n\t\t\t\"virsh\": {\n\t\t\t\t\"port\": int,\n\t\t\t\t\"wait\": bool,\n\t\t\t\t\"stateless\": bool,\n\t\t\t},\n\t\t\t\"ospd\": {\n\t\t\t\t\"port\": int,\n\t\t\t\t\"wait\": bool,\n\t\t\t\t\"stateless\": bool,\n\t\t\t},\n\t\t}\n\t\tfor key, value in six.iteritems(values_dict):\n\t\t\tif key in non_cli_args_map[parser_name]:\n\t\t\t\tvalues_dict[key] = non_cli_args_map[parser_name][key](value)\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tif parser_name =='main':\n\t\tif values_dict['--debug'] == 'True':\n\t\t\tvalues_dict['--debug'] = True\n\t\telse:\n\t\t\tvalues_dict['--debug'] = False\n\t\tif values_dict['--verbose'] == 'True':\n\t\t\tvalues_dict['--verbose'] = True\n\t\telse:\n\t\t\tvalues_dict['--verbose'] = False\n\n\treturn True\n   $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\t# TODO: This is not generic, but it will do for now.\n\tfor k, v in values_dict.items():\n\t\tif k in (\"--no-cache\", \"--debug\", \"--dry-run\", \"--verbose\", \"--no-reboot\"):\n\t\t\tvalues_dict[k] = v == \"yes\"\n\t\tif k in (\"--force\", \"--force-yes\"):\n\t\t\tvalues_dict[k] = True\n\t\tif k in (\"--no-force\", \"--no-force-yes\"):\n\t\t\tvalues_dict[k] = False\n\t\tif k in (\"--no-reboot\", ):\n\t\t\tvalues_dict[k] = v == \"no\"\n\t\tif k in (\"--force-yes\", \"--no-force-yes\"):\n\t\t\tvalues_dict[k] = v == \"yes\"\n\t\tif k in (\"--force\", \"--no-force\"):\n\t\t\tvalues_dict[k] = v == \"no\"\n\t\tif k in (\"--debug\", \"--dry-run\", \"--verbose\"):\n\t\t\tvalues_dict[k] = v == \"yes\"\n\t\tif k in (\"--no-cache\", ):\n\t\t\tvalues_dict[k] = v == \"no\"\n\t\tif k in (\"--boot-order\", \"--boot-device\", \"--boot-menu\", \"--boot-menu-timeout\", \"--boot-path\", \"--boot"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif parser_name == 'ospd':\n\t\t\t# If a value is a string, do nothing.\n\t\t\tif not isinstance(values_dict['ospd_type'], str):\n\t\t\t\tvalues_dict['ospd_type'] = str(values_dict['ospd_type'])\n\t\telif parser_name == 'virsh':\n\t\t\t# If a value is a string, do nothing.\n\t\t\tif not isinstance(values_dict['virsh_type'], str):\n\t\t\t\tvalues_dict['virsh_type'] = str(values_dict['virsh_type'])\n\t\telif parser_name =='main':\n\t\t\t# If a value is a string, do nothing.\n\t\t\tif not isinstance(values_dict['main_type'], str):\n\t\t\t\tvalues_dict['main_type'] = str(values_dict['main_type'])\n\t\telse:\n\t\t\tprint('Unknown parser', parser_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tself.log.debug('_convert_non_cli_args: parser_name=%s values_dict=%s' % (parser_name, values_dict))\n\n\t# Do not modify the input dict\n\tvalues_dict = values_dict.copy()\n\n\t# Check that we have the expected values\n\tif parser_name in self.arg_types:\n\t\tfor arg in self.arg_types[parser_name]:\n\t\t\tif arg not in values_dict:\n\t\t\t\tself.log.error('Missing argument: %s' % arg)\n\t\t\t\tself.usage()\n\t\t\t\texit(1)\n\n\t# Convert values\n\tfor arg in self.arg_types[parser_name]:\n\t\tif arg in values_dict:\n\t\t\tif self.arg_types[parser_name][arg] == 'int':\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[arg] = int(values_dict[arg])\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.log.error('Cannot cast %s to int' % arg)\n\t\t\t\t\tself.usage()\n\t\t\t\t\texit(1)\n\t\t\telif self.arg_types[parser_name][arg] == 'float':\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[arg] = float(values_dict[arg])\n\t\t\t\texcept ValueError:\n\t\t\t\t\tself.log.error('Cannot cast %s to float' %", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif not values_dict:\n\t\t\treturn\n\t\tif parser_name =='main':\n\t\t\tself._convert_main_args(values_dict)\n\t\telif parser_name == 'virsh':\n\t\t\tself._convert_virsh_args(values_dict)\n\t\telif parser_name == 'ospd':\n\t\t\tself._convert_ospd_args(values_dict)\n\n\tdef _convert_main_args(self, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif not values_dict:\n\t\t\treturn\n\t\tif values_dict.get('memory'):\n\t\t\tvalues_dict['memory'] = int(values_dict['memory'])\n\t\tif values_dict.get('cpumode'):\n\t\t\tvalues_dict['cpumode'] = str(values_dict['cpumode'])\n\t\tif values_dict.get('numa'):\n\t\t\tvalues_dict['numa'] = str2bool(values_dict['numa'])\n\t\tif values_dict.get('vcpus'):\n\t\t\tvalues_dict['vcpus'] = int(values_dict['vcpus'])\n\t\tif values_dict.get('smp'):\n\t\t\tvalues_dict['", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif parser_name == \"main\":\n\t\t\tself._convert_non_cli_args_main(values_dict)\n\t\telif parser_name == \"virsh\":\n\t\t\tself._convert_non_cli_args_virsh(values_dict)\n\t\telif parser_name == \"ospd\":\n\t\t\tself._convert_non_cli_args_ospd(values_dict)\n\t\telif parser_name == \"os\":\n\t\t\tself._convert_non_cli_args_os(values_dict)\n\t\telif parser_name == \"os-admin\":\n\t\t\tself._convert_non_cli_args_os_admin(values_dict)\n\t\telse:\n\t\t\tself._convert_non_cli_args_default(values_dict)\n\n\tdef _convert_non_cli_args_main(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_virsh(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_ospd(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_os(self, values_dict):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\n\tdef _convert_non_cli_args_os", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tif parser_name!='main':\n\t\t\treturn\n\t\tif values_dict.get('debug_level', False):\n\t\t\tvalues_dict['debug_level'] = int(values_dict['debug_level'])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tpass\n\n@classmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\t# TODO: Add support for other types\n\t\t# https://docs.python.org/3/library/argparse.html#type\n\n\tdef _process_non_cli_args(self):\n\t\t\"\"\"\n\t\tProcess command line arguments.\n\nThis function is used to process the arguments passed to the command.\n\n:return: The values_dict\n:rtype: dict\n:raises: SystemExit\n\t\t\"\"\"\n\t\tparser = self._get_cli_parser()\n\t\tvalues_dict = vars(parser.parse_args())\n\t\tself._convert_non_cli_args(parser.prog, values_dict)\n\t\treturn values_dict\n\n\tdef _get_cli_parser(self):\n\t\t\"\"\"\n\t\tGet the command line parser.\n\n:return: The parser instance\n:rtype: argparse.ArgumentParser\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=self.description)\n\t\tself._add_cli_arguments(parser)\n\t\treturn parser\n\n\tdef _add_cli_arguments(self, parser):\n\t\t\"\"\"\n\t\tAdd command line arguments.\n\nThis function is used to add command line arguments to the parser.\n\n:param parser: The parser instance\n:type parser: argparse.ArgumentParser\n\t\t\"\"\"\n\t\t# TODO: Add support for other types\n\t\t# https://docs.python.org/3/library/arg", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\t# NOTE: This is a hack to convert the non-cli args to the correct type\n\t\t# (e.g. int for --port, bool for --wait, etc).\n\t\t# This is necessary because argparse only supports string values.\n\t\t# We have to do it manually because we need to perform validation.\n\t\tnon_cli_args_map = {\n\t\t\t\"main\": {\n\t\t\t\t\"port\": int,\n\t\t\t\t\"wait\": bool,\n\t\t\t},\n\t\t\t\"virsh\": {\n\t\t\t\t\"port\": int,\n\t\t\t\t\"wait\": bool,\n\t\t\t\t\"stateless\": bool,\n\t\t\t},\n\t\t\t\"ospd\": {\n\t\t\t\t\"port\": int,\n\t\t\t\t\"wait\": bool,\n\t\t\t\t\"stateless\": bool,\n\t\t\t},\n\t\t}\n\t\tfor key, value in six.iteritems(values_dict):\n\t\t\tif key in non_cli_args_map[parser_name]:\n\t\t\t\tvalues_dict[key] = non_cli_args_map[parser_name][key](value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tif parser_name =='main':\n\t\tif values_dict['--debug'] == 'True':\n\t\t\tvalues_dict['--debug'] = True\n\t\telse:\n\t\t\tvalues_dict['--debug'] = False\n\t\tif values_dict['--verbose'] == 'True':\n\t\t\tvalues_dict['--verbose'] = True\n\t\telse:\n\t\t\tvalues_dict['--verbose'] = False\n\n\treturn True\n   $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_cli_args.py\n    $OSPD_ROOT/ospd_tests/test_ospd_", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\t# TODO: This is not generic, but it will do for now.\n\tfor k, v in values_dict.items():\n\t\tif k in (\"--no-cache\", \"--debug\", \"--dry-run\", \"--verbose\", \"--no-reboot\"):\n\t\t\tvalues_dict[k] = v == \"yes\"\n\t\tif k in (\"--force\", \"--force-yes\"):\n\t\t\tvalues_dict[k] = True\n\t\tif k in (\"--no-force\", \"--no-force-yes\"):\n\t\t\tvalues_dict[k] = False\n\t\tif k in (\"--no-reboot\", ):\n\t\t\tvalues_dict[k] = v == \"no\"\n\t\tif k in (\"--force-yes\", \"--no-force-yes\"):\n\t\t\tvalues_dict[k] = v == \"yes\"\n\t\tif k in (\"--force\", \"--no-force\"):\n\t\t\tvalues_dict[k] = v == \"no\"\n\t\tif k in (\"--debug\", \"--dry-run\", \"--verbose\"):\n\t\t\tvalues_dict[k] = v == \"yes\"\n\t\tif k in (\"--no-cache\", ):\n\t\t\tvalues_dict[k] = v == \"no\"\n\t\tif k in (\"--boot-order\", \"--boot-device\", \"--boot-menu\", \"--boot-menu-timeout\", \"--boot-path\", \"--boot", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t \n\tif not os.path.isdir(plugin_dir):\n\t\traise Exception(\"Plugin directory does not exist.\")\n\t\t\n\t# Get the name of the plugin.\n\tname_path = os.path.join(plugin_dir, \"name\")\n\tif not os.path.isfile(name_path):\n\t\traise Exception(\"Plugin name is missing.\")\n\n\tplugin_name = open(name_path).read().strip()\n\n\t# Get the version of the plugin.\n\tversion_path = os.path.join(plugin_dir, \"version\")\n\tif not os.path.isfile(version_path):\n\t\traise Exception(\"Plugin version is missing.\")\n\n\tplugin_version = open(version_path).read().strip()\n\n\t# Get the description of the plugin.\n\tdescription_path = os.path.join(plugin_dir, \"description\")\n\tif not os.path.isfile(description_path):\n\t\traise Exception(\"Plugin description is missing.\")\n\n\tplugin_description = open(description_path).read().strip()\n\n\t# Get the homepage of the plugin.\n\thomepage_path = os.path.join(plugin_dir, \"homepage\")\n\tif not os.path.isfile(homepage_path):\n\t\traise Exception(\"Plugin homepage is missing.\")\n\n\tplugin_homepage = open(homepage_path).read().strip", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\tinfo_dict = {}\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin.spec')\n\tif os.path.isfile(plugin_spec_path):\n\t\twith open(plugin_spec_path, 'r') as f:\n\t\t\tinfo_dict = yaml.load(f)\n\treturn info_dict\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t \n\tplugin_spec_file_path = os.path.join(plugin_dir, 'plugin.yaml')\n\tplugin_spec = {}\n\t\t\n\tif os.path.exists(plugin_spec_file_path):\n\t\twith open(plugin_spec_file_path, 'r') as f:\n\t\t\tplugin_spec = yaml.load(f)\t\n\n\tplugin_spec_flatten_dict = {}\n\tfor k, v in plugin_spec.items():\n\t\tif isinstance(v, dict):\n\t\t\tfor k1, v1 in v.items():\n\t\t\t\tplugin_spec_flatten_dict[k + '.' + k1] = v1\n\t\telse:\n\t\t\tplugin_spec_flatten_dict[k] = v\n\n\treturn plugin_spec_flatten_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\t# read the plugin.yaml file\n\tplugin_spec = yaml.load(open(plugin_dir + 'plugin.yaml'))\n\n\t# get the plugin name\n\tplugin_name = plugin_spec['name']\n\n\t# get the plugin description\n\tplugin_description = plugin_spec['description']\n\n\t# get the plugin version\n\tplugin_version = plugin_spec['version']\n\n\t# get the plugin type\n\tplugin_type = plugin_spec['type']\n\n\t# get the plugin author\n\tplugin_author = plugin_spec['author']\n\n\t# get the plugin dependencies\n\tplugin_dependencies = plugin_spec['dependencies']\n\n\t# get the plugin keywords\n\tplugin_keywords = plugin_spec['keywords']\n\n\t# get the plugin documentation\n\tplugin_documentation = plugin_spec['documentation']\n\n\t# get the plugin dependencies\n\tplugin_metadata = plugin_spec['metadata']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t#", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t \n\t# Read plugin spec\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin_spec.yaml')\n\tplugin_spec = yaml.load(open(plugin_spec_path))\n\n\t# Read plugin spec flatten dictionary\n\tplugin_spec_flatten_dict = {}\n\tfor item in plugin_spec:\n\t\tif type(plugin_spec[item]) == dict:\n\t\t\tfor subitem in plugin_spec[item]:\n\t\t\t\tplugin_spec_flatten_dict[item+'_'+subitem] = plugin_spec[item][subitem]\n\t\telse:\n\t\t\tplugin_spec_flatten_dict[item] = plugin_spec[item]\n\n\treturn plugin_spec_flatten_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\tplugin_spec_dict = {}\n\tplugin_spec_dict[\"plugin_name\"] = get_plugin_name(plugin_dir)\n\t\n\tplugin_spec_dict[\"plugin_version\"] = get_plugin_version(plugin_dir)\n\tplugin_spec_dict[\"plugin_build_date\"] = get_plugin_build_date(plugin_dir)\n\tplugin_spec_dict[\"plugin_author\"] = get_plugin_author(plugin_dir)\n\tplugin_spec_dict[\"plugin_email\"] = get_plugin_email(plugin_dir)\n\tplugin_spec_dict[\"plugin_website\"] = get_plugin_website(plugin_dir)\n\tplugin_spec_dict[\"plugin_license\"] = get_plugin_license(plugin_dir)\n\tplugin_spec_dict[\"plugin_description\"] = get_plugin_description(plugin_dir)\n\tplugin_spec_dict[\"plugin_type\"] = get_plugin_type(plugin_dir)\n\n\tplugin_spec_dict[\"plugin_script\"] = get_plugin_script(plugin_dir)\n\tplugin_spec_dict[\"plugin_script_version\"] = get_plugin_script_version(plugin_dir)\n\tplugin_spec_dict[\"plugin_script_date\"] = get_plugin_script_date(plugin_dir)\n\tplugin", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\t# Read plugin.yaml\n\tplugin_spec = {}\n\tplugin_yaml_file = os.path.join(plugin_dir, \"plugin.yaml\")\n\ttry:\n\t\twith open(plugin_yaml_file, 'r') as plugin_yaml:\n\t\t\tplugin_spec = yaml.load(plugin_yaml)\n\texcept Exception as ex:\n\t\tprint(\"Could not read plugin.yaml: %s\" % ex)\n\t\treturn None\n\t\n\t# Read plugin.spec.yaml\n\tplugin_spec_yaml_file = os.path.join(plugin_dir, \"plugin.spec.yaml\")\n\ttry:\n\t\twith open(plugin_spec_yaml_file, 'r') as plugin_spec_yaml:\n\t\t\tplugin_spec = dict(plugin_spec, **yaml.load(plugin_spec_yaml))\n\texcept Exception as ex:\n\t\tprint(\"Could not read plugin.spec.yaml: %s\" % ex)\n\t\treturn None\n\n\treturn plugin_spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\twith open(os.path.join(plugin_dir, \"plugin.yaml\"), 'r') as f:\n\t\t\tplugin_dict = yaml.load(f)\n\t\treturn plugin_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin.spec\")\n\tif not os.path.isfile(plugin_spec_file):\n\t\traise Exception(\"Plugin.spec file not found!\")\n\twith open(plugin_spec_file, \"r\") as f:\n\t\tplugin_spec = yaml.load(f)\n\n\tplugin_spec_dict = dict()\n\n\t# Plugin name\n\tif \"name\" not in plugin_spec:\n\t\traise Exception(\"Plugin name not found!\")\n\tplugin_spec_dict[\"name\"] = plugin_spec[\"name\"]\n\n\t# Plugin version\n\tif \"version\" not in plugin_spec:\n\t\traise Exception(\"Plugin version not found!\")\n\tplugin_spec_dict[\"version\"] = str(plugin_spec[\"version\"])\n\n\t# Plugin description\n\tif \"description\" not in plugin_spec:\n\t\traise Exception(\"Plugin description not found!\")\n\tplugin_spec_dict[\"description\"] = plugin_spec[\"description\"]\n\n\t# Plugin homepage\n\tif \"homepage\" not in plugin_spec:\n\t\traise Exception(\"Plugin homepage not found!\")\n\tplugin_spec_dict[\"homepage\"] = plugin_spec[\"homepage\"]\n\n\t# Plugin author\n\tif \"author\" not in plugin_spec:\n\t\traise Exception(\"Plugin author not found!\")\n\tplugin_spec"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t \n\tif not os.path.isdir(plugin_dir):\n\t\traise Exception(\"Plugin directory does not exist.\")\n\t\t\n\t# Get the name of the plugin.\n\tname_path = os.path.join(plugin_dir, \"name\")\n\tif not os.path.isfile(name_path):\n\t\traise Exception(\"Plugin name is missing.\")\n\n\tplugin_name = open(name_path).read().strip()\n\n\t# Get the version of the plugin.\n\tversion_path = os.path.join(plugin_dir, \"version\")\n\tif not os.path.isfile(version_path):\n\t\traise Exception(\"Plugin version is missing.\")\n\n\tplugin_version = open(version_path).read().strip()\n\n\t# Get the description of the plugin.\n\tdescription_path = os.path.join(plugin_dir, \"description\")\n\tif not os.path.isfile(description_path):\n\t\traise Exception(\"Plugin description is missing.\")\n\n\tplugin_description = open(description_path).read().strip()\n\n\t# Get the homepage of the plugin.\n\thomepage_path = os.path.join(plugin_dir, \"homepage\")\n\tif not os.path.isfile(homepage_path):\n\t\traise Exception(\"Plugin homepage is missing.\")\n\n\tplugin_homepage = open(homepage_path).read().strip", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\tinfo_dict = {}\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin.spec')\n\tif os.path.isfile(plugin_spec_path):\n\t\twith open(plugin_spec_path, 'r') as f:\n\t\t\tinfo_dict = yaml.load(f)\n\treturn info_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t \n\tplugin_spec_file_path = os.path.join(plugin_dir, 'plugin.yaml')\n\tplugin_spec = {}\n\t\t\n\tif os.path.exists(plugin_spec_file_path):\n\t\twith open(plugin_spec_file_path, 'r') as f:\n\t\t\tplugin_spec = yaml.load(f)\t\n\n\tplugin_spec_flatten_dict = {}\n\tfor k, v in plugin_spec.items():\n\t\tif isinstance(v, dict):\n\t\t\tfor k1, v1 in v.items():\n\t\t\t\tplugin_spec_flatten_dict[k + '.' + k1] = v1\n\t\telse:\n\t\t\tplugin_spec_flatten_dict[k] = v\n\n\treturn plugin_spec_flatten_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\t# read the plugin.yaml file\n\tplugin_spec = yaml.load(open(plugin_dir + 'plugin.yaml'))\n\n\t# get the plugin name\n\tplugin_name = plugin_spec['name']\n\n\t# get the plugin description\n\tplugin_description = plugin_spec['description']\n\n\t# get the plugin version\n\tplugin_version = plugin_spec['version']\n\n\t# get the plugin type\n\tplugin_type = plugin_spec['type']\n\n\t# get the plugin author\n\tplugin_author = plugin_spec['author']\n\n\t# get the plugin dependencies\n\tplugin_dependencies = plugin_spec['dependencies']\n\n\t# get the plugin keywords\n\tplugin_keywords = plugin_spec['keywords']\n\n\t# get the plugin documentation\n\tplugin_documentation = plugin_spec['documentation']\n\n\t# get the plugin dependencies\n\tplugin_metadata = plugin_spec['metadata']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t# get the required python version\n\tplugin_python_version = plugin_spec['python_version']\n\n\t#", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t \n\t# Read plugin spec\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin_spec.yaml')\n\tplugin_spec = yaml.load(open(plugin_spec_path))\n\n\t# Read plugin spec flatten dictionary\n\tplugin_spec_flatten_dict = {}\n\tfor item in plugin_spec:\n\t\tif type(plugin_spec[item]) == dict:\n\t\t\tfor subitem in plugin_spec[item]:\n\t\t\t\tplugin_spec_flatten_dict[item+'_'+subitem] = plugin_spec[item][subitem]\n\t\telse:\n\t\t\tplugin_spec_flatten_dict[item] = plugin_spec[item]\n\n\treturn plugin_spec_flatten_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\tplugin_spec_dict = {}\n\tplugin_spec_dict[\"plugin_name\"] = get_plugin_name(plugin_dir)\n\t\n\tplugin_spec_dict[\"plugin_version\"] = get_plugin_version(plugin_dir)\n\tplugin_spec_dict[\"plugin_build_date\"] = get_plugin_build_date(plugin_dir)\n\tplugin_spec_dict[\"plugin_author\"] = get_plugin_author(plugin_dir)\n\tplugin_spec_dict[\"plugin_email\"] = get_plugin_email(plugin_dir)\n\tplugin_spec_dict[\"plugin_website\"] = get_plugin_website(plugin_dir)\n\tplugin_spec_dict[\"plugin_license\"] = get_plugin_license(plugin_dir)\n\tplugin_spec_dict[\"plugin_description\"] = get_plugin_description(plugin_dir)\n\tplugin_spec_dict[\"plugin_type\"] = get_plugin_type(plugin_dir)\n\n\tplugin_spec_dict[\"plugin_script\"] = get_plugin_script(plugin_dir)\n\tplugin_spec_dict[\"plugin_script_version\"] = get_plugin_script_version(plugin_dir)\n\tplugin_spec_dict[\"plugin_script_date\"] = get_plugin_script_date(plugin_dir)\n\tplugin", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\t# Read plugin.yaml\n\tplugin_spec = {}\n\tplugin_yaml_file = os.path.join(plugin_dir, \"plugin.yaml\")\n\ttry:\n\t\twith open(plugin_yaml_file, 'r') as plugin_yaml:\n\t\t\tplugin_spec = yaml.load(plugin_yaml)\n\texcept Exception as ex:\n\t\tprint(\"Could not read plugin.yaml: %s\" % ex)\n\t\treturn None\n\t\n\t# Read plugin.spec.yaml\n\tplugin_spec_yaml_file = os.path.join(plugin_dir, \"plugin.spec.yaml\")\n\ttry:\n\t\twith open(plugin_spec_yaml_file, 'r') as plugin_spec_yaml:\n\t\t\tplugin_spec = dict(plugin_spec, **yaml.load(plugin_spec_yaml))\n\texcept Exception as ex:\n\t\tprint(\"Could not read plugin.spec.yaml: %s\" % ex)\n\t\treturn None\n\n\treturn plugin_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\twith open(os.path.join(plugin_dir, \"plugin.yaml\"), 'r') as f:\n\t\t\tplugin_dict = yaml.load(f)\n\t\treturn plugin_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\t\n\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin.spec\")\n\tif not os.path.isfile(plugin_spec_file):\n\t\traise Exception(\"Plugin.spec file not found!\")\n\twith open(plugin_spec_file, \"r\") as f:\n\t\tplugin_spec = yaml.load(f)\n\n\tplugin_spec_dict = dict()\n\n\t# Plugin name\n\tif \"name\" not in plugin_spec:\n\t\traise Exception(\"Plugin name not found!\")\n\tplugin_spec_dict[\"name\"] = plugin_spec[\"name\"]\n\n\t# Plugin version\n\tif \"version\" not in plugin_spec:\n\t\traise Exception(\"Plugin version not found!\")\n\tplugin_spec_dict[\"version\"] = str(plugin_spec[\"version\"])\n\n\t# Plugin description\n\tif \"description\" not in plugin_spec:\n\t\traise Exception(\"Plugin description not found!\")\n\tplugin_spec_dict[\"description\"] = plugin_spec[\"description\"]\n\n\t# Plugin homepage\n\tif \"homepage\" not in plugin_spec:\n\t\traise Exception(\"Plugin homepage not found!\")\n\tplugin_spec_dict[\"homepage\"] = plugin_spec[\"homepage\"]\n\n\t# Plugin author\n\tif \"author\" not in plugin_spec:\n\t\traise Exception(\"Plugin author not found!\")\n\tplugin_spec", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\tdef inject_inventory(self):\n\t\t\"\"\"\n\t\tIf the ANSIBLE_INVENTORY property does not exist in os.environ, set it to self.ansible_inventory_path.\n\t\t\"\"\"\n\t\tif \"ANSIBLE_INVENTORY\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_INVENTORY\"] = self.ansible_inventory_path\n\n\tdef inject_playbook(self):\n\t\t\"\"\"\n\t\tIf the ANSIBLE_PLAYBOOK property does not exist in os.environ, set it to self.ansible_playbook_path.\n\t\t\"\"\"\n\t\tif \"ANSIBLE_PLAYBOOK\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_PLAYBOOK\"] = self.ansible_playbook_path\n\n\tdef get_ansible_command(self):\n\t\t\"\"\"\n\t\tReturns the ansible command to run with the current configuration.\n\t\t\"\"\"\n\t\treturn \"ansible-playbook -vvv {} -e {}\".format(self.ansible_playbook_path, self.extra_vars)\n\n\tdef run(self):\n\t\t\"\"\"\n\t\tRuns ansible-playbook with the current configuration\n\t\t\"\"\"\n\t\tself.inject_config()\n\t\tself.inject_inventory()\n\t\tself.inject", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif ANSIBLE_CONFIG not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\t\tif ANSIBLE_CONFIG in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n    def _create_inventory(self):\n        \"\"\"\n        Create the inventory file in the temp directory.\n        \"\"\"\n        self.inventory_path = os.path.join(self.temp_path, \"inventory\")\n        with open(self.inventory_path, \"w\") as f:\n            f.write('[{}]\\n'.format(self.cluster_name))\n            for node in self.nodes:\n                f.write(str(node))\n                f.write('\\n')\n\n    def _create_playbook(self):\n        \"\"\"\n        Create the playbook file in the temp directory.\n        \"\"\"\n        self.playbook_path = os.path.join(self.temp_path, \"playbook.yml\")\n        with open(self.playbook_path, \"w\") as f:\n            f.write(\"- hosts: {}\\n\".format(self.cluster_name))\n            f.write(\"  tasks:\\n\")\n            f.write(\"  - name: Create cluster\\n\")\n            f.write(\"    command: /bin/true\\n\")\n\n    def _create_ansible_config(self):\n        \"\"\"\n        Create the ansible config file in the temp directory.\n        \"\"\"\n        self.ansible_config_path = os.path.join(self.temp", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\t\tprint \"ANSIBLE_CONFIG not in os.environ, setting to {0}\".format(self.ansible_config_path)\n\t\tprint \"os.environ['ANSIBLE_CONFIG'] = \", os.environ['ANSIBLE_CONFIG']\n\t\tprint \"\"\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\tdef _get_ansible_config_path(self):\n\t\t\"\"\"\n\t\tSearch for a config file in the current directory and the project's root directory.\n\t\t\"\"\"\n\t\troot_dir = self._get_root_dir()\n\t\tif root_dir:\n\t\t\treturn os.path.join(root_dir, 'ansible.cfg')\n\t\telse:\n\t\t\treturn os.path.join(os.getcwd(), 'ansible.cfg')\n\tdef _get_root_dir(self):\n\t\t\"\"\"\n\t\tFind the root directory of the project by walking up the directory tree.\n\t\t\"\"\"\n\t\tfile = os.path.join(os.getcwd(), 'ansible.cfg')\n\t\twhile os.path.isfile(file) is False:\n\t\t\tos.chdir('..')\n\t\t\tfile = os.path.join(os.getcwd(), 'ansible.cfg')\n\t\treturn os.getcwd()\n\tdef run_ansible(self, action, playbook):\n\t\t\"\"\"\n\t\tRun the specified Ansible playbook.\n\t\t\"\"\"\n\t\tself.inject_config()\n\t\tself.inject_inventory()\n\t\targs = ['ansible-playbook', '-i', self.inventory_path, '-e', self"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\tdef inject_inventory(self):\n\t\t\"\"\"\n\t\tIf the ANSIBLE_INVENTORY property does not exist in os.environ, set it to self.ansible_inventory_path.\n\t\t\"\"\"\n\t\tif \"ANSIBLE_INVENTORY\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_INVENTORY\"] = self.ansible_inventory_path\n\n\tdef inject_playbook(self):\n\t\t\"\"\"\n\t\tIf the ANSIBLE_PLAYBOOK property does not exist in os.environ, set it to self.ansible_playbook_path.\n\t\t\"\"\"\n\t\tif \"ANSIBLE_PLAYBOOK\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_PLAYBOOK\"] = self.ansible_playbook_path\n\n\tdef get_ansible_command(self):\n\t\t\"\"\"\n\t\tReturns the ansible command to run with the current configuration.\n\t\t\"\"\"\n\t\treturn \"ansible-playbook -vvv {} -e {}\".format(self.ansible_playbook_path, self.extra_vars)\n\n\tdef run(self):\n\t\t\"\"\"\n\t\tRuns ansible-playbook with the current configuration\n\t\t\"\"\"\n\t\tself.inject_config()\n\t\tself.inject_inventory()\n\t\tself.inject", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif ANSIBLE_CONFIG not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\t\tif ANSIBLE_CONFIG in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n    def _create_inventory(self):\n        \"\"\"\n        Create the inventory file in the temp directory.\n        \"\"\"\n        self.inventory_path = os.path.join(self.temp_path, \"inventory\")\n        with open(self.inventory_path, \"w\") as f:\n            f.write('[{}]\\n'.format(self.cluster_name))\n            for node in self.nodes:\n                f.write(str(node))\n                f.write('\\n')\n\n    def _create_playbook(self):\n        \"\"\"\n        Create the playbook file in the temp directory.\n        \"\"\"\n        self.playbook_path = os.path.join(self.temp_path, \"playbook.yml\")\n        with open(self.playbook_path, \"w\") as f:\n            f.write(\"- hosts: {}\\n\".format(self.cluster_name))\n            f.write(\"  tasks:\\n\")\n            f.write(\"  - name: Create cluster\\n\")\n            f.write(\"    command: /bin/true\\n\")\n\n    def _create_ansible_config(self):\n        \"\"\"\n        Create the ansible config file in the temp directory.\n        \"\"\"\n        self.ansible_config_path = os.path.join(self.temp", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\t\tprint \"ANSIBLE_CONFIG not in os.environ, setting to {0}\".format(self.ansible_config_path)\n\t\tprint \"os.environ['ANSIBLE_CONFIG'] = \", os.environ['ANSIBLE_CONFIG']\n\t\tprint \"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\tdef _get_ansible_config_path(self):\n\t\t\"\"\"\n\t\tSearch for a config file in the current directory and the project's root directory.\n\t\t\"\"\"\n\t\troot_dir = self._get_root_dir()\n\t\tif root_dir:\n\t\t\treturn os.path.join(root_dir, 'ansible.cfg')\n\t\telse:\n\t\t\treturn os.path.join(os.getcwd(), 'ansible.cfg')\n\tdef _get_root_dir(self):\n\t\t\"\"\"\n\t\tFind the root directory of the project by walking up the directory tree.\n\t\t\"\"\"\n\t\tfile = os.path.join(os.getcwd(), 'ansible.cfg')\n\t\twhile os.path.isfile(file) is False:\n\t\t\tos.chdir('..')\n\t\t\tfile = os.path.join(os.getcwd(), 'ansible.cfg')\n\t\treturn os.getcwd()\n\tdef run_ansible(self, action, playbook):\n\t\t\"\"\"\n\t\tRun the specified Ansible playbook.\n\t\t\"\"\"\n\t\tself.inject_config()\n\t\tself.inject_inventory()\n\t\targs = ['ansible-playbook', '-i', self.inventory_path, '-e', self", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser(\n\t\t\t\"spec\",\n\t\t\thelp=\"Command line interface for the specification of the model.\"\n\t\t)\n\n\t\tparser.add_argument(\n\t\t\t\"-c\",\n\t\t\t\"--config\",\n\t\t\ttype=str,\n\t\t\tdefault=None,\n\t\t\thelp=\"Path to the configuration file to be used.\"\n\t\t)\n\n\t\tsubparsers = parser.add_subparsers(\n\t\t\tdest=\"cmd\",\n\t\t\thelp=\"Available commands.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"init\",\n\t\t\thelp=\"Initializes the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"generate\",\n\t\t\thelp=\"Generates the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"build\",\n\t\t\thelp=\"Builds the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"run\",\n\t\t\thelp=\"Runs the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"run-all\",\n\t\t\thelp=\"Runs all the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"test\",\n\t\t\thelp=\"Tests the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"test-all\",\n\t\t\thelp=\"Tests all the model specification.\"\n\t\t)", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser(\n\t\t\t\"spec\",\n\t\t\thelp=\"Generate the spec from the model\",\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-s\",\n\t\t\t\"--schema\",\n\t\t\trequired=True,\n\t\t\thelp=\"Model to generate spec from\",\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-o\",\n\t\t\t\"--output\",\n\t\t\thelp=\"Output file (default: stdout)\",\n\t\t)\n\t\tparser.set_defaults(func=self.spec)\n/core/rest/spec.py\nfrom __future__ import absolute_import\n\nfrom core.spec import Spec\nfrom core.fields import fields_for\nfrom core.rest import rest_for\nfrom core.rest.serializers import ModelSerializer\n\nfrom django.db import models\nfrom django.core.urlresolvers import reverse\nfrom django.conf import settings\n\nimport six\nimport json\nimport re\n\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser('spec', help=\"Generate a specification file\")\n\t\tparser.set_defaults(func=self.spec_cli)\n\t\tparser.add_argument('input', help=\"Input JSON file\")\n\t\tparser.add_argument('output', help=\"Output YAML file\")\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tspec_parser = root_subparsers.add_parser('spec')\n\t\tspec_parser.add_argument(\n\t\t\t'--spec',\n\t\t\ttype=str,\n\t\t\tmetavar='SPEC',\n\t\t\tdefault=None,\n\t\t\thelp='Path to a SPEC file to generate the schema from.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--output',\n\t\t\ttype=str,\n\t\t\tmetavar='OUTPUT',\n\t\t\tdefault=None,\n\t\t\thelp='Path to save the generated schema to.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--no-comments',\n\t\t\taction='store_true',\n\t\t\thelp='Do not include comments in the generated schema.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--no-comments-spec',\n\t\t\taction='store_true',\n\t\t\thelp='Do not include comments in the spec file.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--no-default',\n\t\t\taction='store_true',\n\t\t\thelp='Do not include default values in the generated schema.'\n\t\t)\n\t\tspec_parser.set_defaults(func=self.spec)\n\tdef spec(self, args):\n\t\t\"\"\"\n\t\tGenerates a schema from a spec file.\n\t\n:param args: Arguments from the command line.\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tsubparser = root_subparsers.add_parser('spec', help='Generate a specification file for your project.')\n\t\tsubparser.add_argument('file', metavar='FILENAME', nargs='?', help='the name of the specification file to generate. Defaults to spec.json in the current directory.')\n\t\tsubparser.add_argument('--help', action='help', help='show help for the specification command')\n\t\tsubparser.set_defaults(func=self.spec)\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tsubparser = root_subparsers.add_parser('spec', help='Prints the spec for the package.')\n\t\tsubparser.add_argument('-v', '--version', action='version', version=self.version)\n\t\tsubparser.add_argument('-s', '--spec', help='The spec file to print.', required=True)\n\t\tsubparser.add_argument('-m', '--module', help='The module to print.', required=True)\n\t\tsubparser.set_defaults(func=self.spec)\n/pycfiles/tendenci-12.0.4-py3-none-any/0003_auto_20191016_1327.cpython-37.py\n# uncompyle6 version 3.7.4\n# Python bytecode 3.7 (3394)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: /Users/jennyq/.pyenv/versions/venv_t12/lib/python3.7/site-packages/tendenci/apps/site_settings/migrations/0003_auto_20191", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tspec_parser = root_subparsers.add_parser('spec', help='Generate a specification file')\n\t\tspec_parser.add_argument('-o', '--output', default='./', help='Output directory for the generated file')\n\t\tspec_parser.add_argument('-f', '--format', default='json', help='Output format (json or yaml)')\n\t\tspec_parser.add_argument('-n', '--name', default='service', help='Name of the service')\n\t\tspec_parser.add_argument('spec_file', help='Path to the specification file')\n\t\tspec_parser.set_defaults(func=self.spec_cli)\n/repodata/zachwill-django-generic-m2m-field/description.txt\nA generic M2M field for Django models.\r\n\r\nInstallation\r\n============\r\n\r\nInstall the latest version of the `generic-m2m-field`_ from PyPi::\r\n\r\n    pip install django-generic-m2m-field\r\n\r\nAdd ``generic_m2m_field`` to your ``INSTALLED_APPS``::\r\n\r\n    INSTALLED_APPS = (\r\n       ...\r\n        'generic_m2m_field',\r\n    )\r\n\r\nUsage\r\n=====\r\n\r\nCreate a model with a ``GenericM2MField``::\r\n\r\n    from django.db import models\r\n    from generic_m2m_field.fields import", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser(\n\t\t\tself.name,\n\t\t\thelp='Get spec information')\n\n\t\tparser.add_argument('--list-specs', action='store_true',\n\t\t\thelp='list all specs in the system')\n\n\t\tparser.add_argument('--spec-file', type=str,\n\t\t\thelp='spec file to get info from')\n\n\t\tparser.add_argument('--spec-name', type=str,\n\t\t\thelp='name of the spec to get info from')\n\n\t\tparser.add_argument('--spec-version', type=str,\n\t\t\thelp='version of the spec to get info from')\n\n\t\tparser.add_argument('--spec-url', type=str,\n\t\t\thelp='url of the spec to get info from')\n\n\t\tparser.add_argument('--spec-description', type=str,\n\t\t\thelp='description of the spec')\n\n\t\tparser.add_argument('--spec-maintainer', type=str,\n\t\t\thelp='maintainer of the spec')\n\n\t\tparser.add_argument('--spec-owner', type=str,\n\t\t\thelp='owner of the spec')\n\n\t\tparser.add_argument('--spec-license', type=str,\n\t\t\thelp='license of the spec')\n\n\t\tparser.add_argument('--spec-copyright', type=str,\n\t\t\thelp='copyright of the spec')\n\n\t\tparser.add_", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser('spec', help='Spec related commands.')\n\t\tparser.set_defaults(action=self.spec_command)\n\t\tparser.add_argument('--spec', choices=['list', 'create', 'update', 'delete','show'], help='Spec related commands.')\n\t\tparser.add_argument('--name', help='Name of the spec to perform action on.')\n\t\tparser.add_argument('--url', help='URL of the spec to perform action on.')\n\t\tparser.add_argument('--description', help='Description of the spec.')\n\t\tparser.add_argument('--group', help='Group of the spec.')\n\t\tparser.add_argument('--source', help='Source of the spec.')\n\t\tparser.add_argument('--source_url', help='Source URL of the spec.')\n\t\tparser.add_argument('--version', help='Version of the spec.')\n\t\tparser.add_argument('--type', choices=['standard', 'custom'], help='Type of the spec.')\n\t\tparser.add_argument('--kind', choices=['system', 'application'], help='Kind of the spec.')\n\t\tparser.add_argument('--state', choices=['draft', 'experimental', 'published'], help='State of the spec.')\n\t\tparser.add_argument('--contact_email', help='Contact email of the spec.')\n\t\tparser.add_argument", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\t# Create the parser for the \"spec\" command.\n\tspec_parser = root_subparsers.add_parser('spec',\n\t\thelp='Build the spec.')\n\n\tspec_parser.add_argument('-f', '--force',\n\t\taction='store_true',\n\t\tdefault=False,\n\t\thelp='Force rebuilding the spec')\n\n\tspec_parser.add_argument('-t', '--type',\n\t\tchoices=['json', 'xml', 'yaml'],\n\t\tdefault='json',\n\t\thelp='The type of spec to build')\n\n\tspec_parser.add_argument('-o', '--output',\n\t\tdefault='spec.json',\n\t\thelp='The location to write the spec to')\n\n\tspec_parser.add_argument('-r', '--render',\n\t\taction='store_true',\n\t\tdefault=False,\n\t\thelp='Render the spec to the output type')\n\n\tspec_parser.set_defaults(func=self.spec)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser(\n\t\t\t\"spec\",\n\t\t\thelp=\"Command line interface for the specification of the model.\"\n\t\t)\n\n\t\tparser.add_argument(\n\t\t\t\"-c\",\n\t\t\t\"--config\",\n\t\t\ttype=str,\n\t\t\tdefault=None,\n\t\t\thelp=\"Path to the configuration file to be used.\"\n\t\t)\n\n\t\tsubparsers = parser.add_subparsers(\n\t\t\tdest=\"cmd\",\n\t\t\thelp=\"Available commands.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"init\",\n\t\t\thelp=\"Initializes the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"generate\",\n\t\t\thelp=\"Generates the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"build\",\n\t\t\thelp=\"Builds the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"run\",\n\t\t\thelp=\"Runs the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"run-all\",\n\t\t\thelp=\"Runs all the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"test\",\n\t\t\thelp=\"Tests the model specification.\"\n\t\t)\n\n\t\tsubparsers.add_parser(\n\t\t\t\"test-all\",\n\t\t\thelp=\"Tests all the model specification.\"\n\t\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser(\n\t\t\t\"spec\",\n\t\t\thelp=\"Generate the spec from the model\",\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-s\",\n\t\t\t\"--schema\",\n\t\t\trequired=True,\n\t\t\thelp=\"Model to generate spec from\",\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-o\",\n\t\t\t\"--output\",\n\t\t\thelp=\"Output file (default: stdout)\",\n\t\t)\n\t\tparser.set_defaults(func=self.spec)\n/core/rest/spec.py\nfrom __future__ import absolute_import\n\nfrom core.spec import Spec\nfrom core.fields import fields_for\nfrom core.rest import rest_for\nfrom core.rest.serializers import ModelSerializer\n\nfrom django.db import models\nfrom django.core.urlresolvers import reverse\nfrom django.conf import settings\n\nimport six\nimport json\nimport re\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser('spec', help=\"Generate a specification file\")\n\t\tparser.set_defaults(func=self.spec_cli)\n\t\tparser.add_argument('input', help=\"Input JSON file\")\n\t\tparser.add_argument('output', help=\"Output YAML file\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tspec_parser = root_subparsers.add_parser('spec')\n\t\tspec_parser.add_argument(\n\t\t\t'--spec',\n\t\t\ttype=str,\n\t\t\tmetavar='SPEC',\n\t\t\tdefault=None,\n\t\t\thelp='Path to a SPEC file to generate the schema from.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--output',\n\t\t\ttype=str,\n\t\t\tmetavar='OUTPUT',\n\t\t\tdefault=None,\n\t\t\thelp='Path to save the generated schema to.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--no-comments',\n\t\t\taction='store_true',\n\t\t\thelp='Do not include comments in the generated schema.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--no-comments-spec',\n\t\t\taction='store_true',\n\t\t\thelp='Do not include comments in the spec file.'\n\t\t)\n\t\tspec_parser.add_argument(\n\t\t\t'--no-default',\n\t\t\taction='store_true',\n\t\t\thelp='Do not include default values in the generated schema.'\n\t\t)\n\t\tspec_parser.set_defaults(func=self.spec)\n\tdef spec(self, args):\n\t\t\"\"\"\n\t\tGenerates a schema from a spec file.\n\t\n:param args: Arguments from the command line.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tsubparser = root_subparsers.add_parser('spec', help='Generate a specification file for your project.')\n\t\tsubparser.add_argument('file', metavar='FILENAME', nargs='?', help='the name of the specification file to generate. Defaults to spec.json in the current directory.')\n\t\tsubparser.add_argument('--help', action='help', help='show help for the specification command')\n\t\tsubparser.set_defaults(func=self.spec)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tsubparser = root_subparsers.add_parser('spec', help='Prints the spec for the package.')\n\t\tsubparser.add_argument('-v', '--version', action='version', version=self.version)\n\t\tsubparser.add_argument('-s', '--spec', help='The spec file to print.', required=True)\n\t\tsubparser.add_argument('-m', '--module', help='The module to print.', required=True)\n\t\tsubparser.set_defaults(func=self.spec)\n/pycfiles/tendenci-12.0.4-py3-none-any/0003_auto_20191016_1327.cpython-37.py\n# uncompyle6 version 3.7.4\n# Python bytecode 3.7 (3394)\n# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n# [GCC 8.4.0]\n# Embedded file name: /Users/jennyq/.pyenv/versions/venv_t12/lib/python3.7/site-packages/tendenci/apps/site_settings/migrations/0003_auto_20191", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tspec_parser = root_subparsers.add_parser('spec', help='Generate a specification file')\n\t\tspec_parser.add_argument('-o', '--output', default='./', help='Output directory for the generated file')\n\t\tspec_parser.add_argument('-f', '--format', default='json', help='Output format (json or yaml)')\n\t\tspec_parser.add_argument('-n', '--name', default='service', help='Name of the service')\n\t\tspec_parser.add_argument('spec_file', help='Path to the specification file')\n\t\tspec_parser.set_defaults(func=self.spec_cli)\n/repodata/zachwill-django-generic-m2m-field/description.txt\nA generic M2M field for Django models.\r\n\r\nInstallation\r\n============\r\n\r\nInstall the latest version of the `generic-m2m-field`_ from PyPi::\r\n\r\n    pip install django-generic-m2m-field\r\n\r\nAdd ``generic_m2m_field`` to your ``INSTALLED_APPS``::\r\n\r\n    INSTALLED_APPS = (\r\n       ...\r\n        'generic_m2m_field',\r\n    )\r\n\r\nUsage\r\n=====\r\n\r\nCreate a model with a ``GenericM2MField``::\r\n\r\n    from django.db import models\r\n    from generic_m2m_field.fields import", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser(\n\t\t\tself.name,\n\t\t\thelp='Get spec information')\n\n\t\tparser.add_argument('--list-specs', action='store_true',\n\t\t\thelp='list all specs in the system')\n\n\t\tparser.add_argument('--spec-file', type=str,\n\t\t\thelp='spec file to get info from')\n\n\t\tparser.add_argument('--spec-name', type=str,\n\t\t\thelp='name of the spec to get info from')\n\n\t\tparser.add_argument('--spec-version', type=str,\n\t\t\thelp='version of the spec to get info from')\n\n\t\tparser.add_argument('--spec-url', type=str,\n\t\t\thelp='url of the spec to get info from')\n\n\t\tparser.add_argument('--spec-description', type=str,\n\t\t\thelp='description of the spec')\n\n\t\tparser.add_argument('--spec-maintainer', type=str,\n\t\t\thelp='maintainer of the spec')\n\n\t\tparser.add_argument('--spec-owner', type=str,\n\t\t\thelp='owner of the spec')\n\n\t\tparser.add_argument('--spec-license', type=str,\n\t\t\thelp='license of the spec')\n\n\t\tparser.add_argument('--spec-copyright', type=str,\n\t\t\thelp='copyright of the spec')\n\n\t\tparser.add_", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser = root_subparsers.add_parser('spec', help='Spec related commands.')\n\t\tparser.set_defaults(action=self.spec_command)\n\t\tparser.add_argument('--spec', choices=['list', 'create', 'update', 'delete','show'], help='Spec related commands.')\n\t\tparser.add_argument('--name', help='Name of the spec to perform action on.')\n\t\tparser.add_argument('--url', help='URL of the spec to perform action on.')\n\t\tparser.add_argument('--description', help='Description of the spec.')\n\t\tparser.add_argument('--group', help='Group of the spec.')\n\t\tparser.add_argument('--source', help='Source of the spec.')\n\t\tparser.add_argument('--source_url', help='Source URL of the spec.')\n\t\tparser.add_argument('--version', help='Version of the spec.')\n\t\tparser.add_argument('--type', choices=['standard', 'custom'], help='Type of the spec.')\n\t\tparser.add_argument('--kind', choices=['system', 'application'], help='Kind of the spec.')\n\t\tparser.add_argument('--state', choices=['draft', 'experimental', 'published'], help='State of the spec.')\n\t\tparser.add_argument('--contact_email', help='Contact email of the spec.')\n\t\tparser.add_argument", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\t# Create the parser for the \"spec\" command.\n\tspec_parser = root_subparsers.add_parser('spec',\n\t\thelp='Build the spec.')\n\n\tspec_parser.add_argument('-f', '--force',\n\t\taction='store_true',\n\t\tdefault=False,\n\t\thelp='Force rebuilding the spec')\n\n\tspec_parser.add_argument('-t', '--type',\n\t\tchoices=['json', 'xml', 'yaml'],\n\t\tdefault='json',\n\t\thelp='The type of spec to build')\n\n\tspec_parser.add_argument('-o', '--output',\n\t\tdefault='spec.json',\n\t\thelp='The location to write the spec to')\n\n\tspec_parser.add_argument('-r', '--render',\n\t\taction='store_true',\n\t\tdefault=False,\n\t\thelp='Render the spec to the output type')\n\n\tspec_parser.set_defaults(func=self.spec)\n", "is_pass": false, "return_code": 1}]}
